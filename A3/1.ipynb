{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbf12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "020aa5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "#         self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.start = 26\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = torch.tensor([self.start]+[ord(c) - ord('a') for c in self.data.iloc[idx, 0]])\n",
    "#         input_sentence = self.embedding(input_sentence) \n",
    "        target_sentence = torch.tensor([self.start]+[ord(c) - ord('a') for c in self.data.iloc[idx, 1]])\n",
    "       \n",
    "        return (input_sentence,target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec4e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DataLoader(CustomDataset(\"Data/train_data.csv\"))\n",
    "training_source = []\n",
    "training_target = []\n",
    "for input_,target in training_data:\n",
    "    training_source.append(input_)\n",
    "    training_target.append(target)\n",
    "#     print(x.shape)\n",
    "training_source = torch.cat(training_source,dim=0)[:,1:]\n",
    "training_target = torch.cat(training_target,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06b6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataLoader(CustomDataset(\"Data/eval_data.csv\"))\n",
    "test_source = []\n",
    "test_target = []\n",
    "for input_,target in test_data:\n",
    "    test_source.append(input_)\n",
    "    test_target.append(target)\n",
    "#     print(x.shape)\n",
    "test_source = torch.cat(test_source,dim=0)[:,1:]\n",
    "test_target = torch.cat(test_target,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6176f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 8]) torch.Size([2000, 9])\n",
      "torch.Size([7000, 8]) torch.Size([7000, 9])\n"
     ]
    }
   ],
   "source": [
    "print(test_source.shape,test_target.shape)\n",
    "print(training_source.shape,training_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4228300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dataset and DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, source, target):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    \n",
    "train_dataset = MyDataset(training_source, training_target)\n",
    "eval_dataset = MyDataset(test_source, test_target)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b36e7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout_p, max_len = 10):\n",
    "        super().__init__()\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, d_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)) / d_model) # 1000^(2i/d_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(1), :])\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, dropout_p, nhead, num_encoder_layers, num_decoder_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding_src = nn.Embedding(vocab_size-1, d_model)\n",
    "        self.embedding_tgt = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            d_model=d_model, dropout_p=dropout_p, max_len=10\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, vocab_size-1)\n",
    "        self.output_layer = nn.Softmax(dim = 2)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[1])\n",
    "        src = self.embedding_src(src)\n",
    "        tgt = self.embedding_tgt(tgt)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        prob_outputs = self.fc(output)\n",
    "        return prob_outputs\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        start = 26\n",
    "        input_sentence = torch.tensor([ord(c) - ord('a') for c in inp])\n",
    "        src = input_sentence.view(1,-1)\n",
    "        tgt = torch.tensor([start]).reshape(1,-1)\n",
    "        for i in range(8):         \n",
    "            logits = self.forward(src,tgt)              # logits = [batch_size,seq_length,27]\n",
    "            \n",
    "            logits = self.output_layer(logits).argmax(dim=2)            # logits = [batch_size,seq_length]\n",
    "            \n",
    "            ## Append this to tgt\n",
    "            tgt = torch.cat((tgt,logits[:,-1].reshape(1,-1)),dim=1)\n",
    "            \n",
    "        \n",
    "        # print(tgt.shape)      # [1,9]\n",
    "        # Convert tgt to string\n",
    "        final = \"\"\n",
    "        \n",
    "        for i in range(1,9):\n",
    "            # print(tgt[0][i])\n",
    "            final += chr(ord('a')+int(tgt[0][i]))\n",
    "\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e9cfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 27  # Lowercase English alphabet letters with start token\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d618e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 3.2819\n",
      "Epoch [1/50], Eval Loss: 3.2616\n",
      "Epoch [2/50], Train Loss: 3.2654\n",
      "Epoch [2/50], Eval Loss: 3.2532\n",
      "Epoch [3/50], Train Loss: 3.2427\n",
      "Epoch [3/50], Eval Loss: 3.1981\n",
      "Epoch [4/50], Train Loss: 3.0348\n",
      "Epoch [4/50], Eval Loss: 2.9409\n",
      "Epoch [5/50], Train Loss: 2.8568\n",
      "Epoch [5/50], Eval Loss: 2.6507\n",
      "Epoch [6/50], Train Loss: 2.6477\n",
      "Epoch [6/50], Eval Loss: 2.6325\n",
      "Epoch [7/50], Train Loss: 2.6328\n",
      "Epoch [7/50], Eval Loss: 2.6288\n",
      "Epoch [8/50], Train Loss: 2.6254\n",
      "Epoch [8/50], Eval Loss: 2.6291\n",
      "Epoch [9/50], Train Loss: 2.6179\n",
      "Epoch [9/50], Eval Loss: 2.6250\n",
      "Epoch [10/50], Train Loss: 2.6090\n",
      "Epoch [10/50], Eval Loss: 2.5939\n",
      "Epoch [11/50], Train Loss: 2.5901\n",
      "Epoch [11/50], Eval Loss: 2.5806\n",
      "Epoch [12/50], Train Loss: 2.5693\n",
      "Epoch [12/50], Eval Loss: 2.5632\n",
      "Epoch [13/50], Train Loss: 2.5221\n",
      "Epoch [13/50], Eval Loss: 2.4581\n",
      "Epoch [14/50], Train Loss: 2.4234\n",
      "Epoch [14/50], Eval Loss: 2.3968\n",
      "Epoch [15/50], Train Loss: 2.3820\n",
      "Epoch [15/50], Eval Loss: 2.3707\n",
      "Epoch [16/50], Train Loss: 2.3415\n",
      "Epoch [16/50], Eval Loss: 2.2782\n",
      "Epoch [17/50], Train Loss: 2.2447\n",
      "Epoch [17/50], Eval Loss: 2.1909\n",
      "Epoch [18/50], Train Loss: 2.1710\n",
      "Epoch [18/50], Eval Loss: 2.0773\n",
      "Epoch [19/50], Train Loss: 2.0678\n",
      "Epoch [19/50], Eval Loss: 1.9984\n",
      "Epoch [20/50], Train Loss: 2.0173\n",
      "Epoch [20/50], Eval Loss: 1.9257\n",
      "Epoch [21/50], Train Loss: 1.9536\n",
      "Epoch [21/50], Eval Loss: 1.8692\n",
      "Epoch [22/50], Train Loss: 1.9121\n",
      "Epoch [22/50], Eval Loss: 1.8542\n",
      "Epoch [23/50], Train Loss: 1.8838\n",
      "Epoch [23/50], Eval Loss: 1.8482\n",
      "Epoch [24/50], Train Loss: 1.8595\n",
      "Epoch [24/50], Eval Loss: 1.8346\n",
      "Epoch [25/50], Train Loss: 1.8351\n",
      "Epoch [25/50], Eval Loss: 1.8092\n",
      "Epoch [26/50], Train Loss: 1.7977\n",
      "Epoch [26/50], Eval Loss: 1.7666\n",
      "Epoch [27/50], Train Loss: 1.7415\n",
      "Epoch [27/50], Eval Loss: 1.6917\n",
      "Epoch [28/50], Train Loss: 1.6504\n",
      "Epoch [28/50], Eval Loss: 1.4325\n",
      "Epoch [29/50], Train Loss: 1.0938\n",
      "Epoch [29/50], Eval Loss: 0.1143\n",
      "Epoch [30/50], Train Loss: 0.2821\n",
      "Epoch [30/50], Eval Loss: 0.0166\n",
      "Epoch [31/50], Train Loss: 0.1616\n",
      "Epoch [31/50], Eval Loss: 0.0078\n",
      "Epoch [32/50], Train Loss: 0.1250\n",
      "Epoch [32/50], Eval Loss: 0.0045\n",
      "Epoch [33/50], Train Loss: 0.1094\n",
      "Epoch [33/50], Eval Loss: 0.0031\n",
      "Epoch [34/50], Train Loss: 0.0959\n",
      "Epoch [34/50], Eval Loss: 0.0028\n",
      "Epoch [35/50], Train Loss: 0.0752\n",
      "Epoch [35/50], Eval Loss: 0.0024\n",
      "Epoch [36/50], Train Loss: 0.0657\n",
      "Epoch [36/50], Eval Loss: 0.0011\n",
      "Epoch [37/50], Train Loss: 0.0583\n",
      "Epoch [37/50], Eval Loss: 0.0010\n",
      "Epoch [38/50], Train Loss: 0.0674\n",
      "Epoch [38/50], Eval Loss: 0.0011\n",
      "Epoch [39/50], Train Loss: 0.0623\n",
      "Epoch [39/50], Eval Loss: 0.0009\n",
      "Epoch [40/50], Train Loss: 0.0493\n",
      "Epoch [40/50], Eval Loss: 0.0007\n",
      "Epoch [41/50], Train Loss: 0.0556\n",
      "Epoch [41/50], Eval Loss: 0.0010\n",
      "Epoch [42/50], Train Loss: 0.0481\n",
      "Epoch [42/50], Eval Loss: 0.0006\n",
      "Epoch [43/50], Train Loss: 0.0507\n",
      "Epoch [43/50], Eval Loss: 0.0007\n",
      "Epoch [44/50], Train Loss: 0.0497\n",
      "Epoch [44/50], Eval Loss: 0.0006\n",
      "Epoch [45/50], Train Loss: 0.0429\n",
      "Epoch [45/50], Eval Loss: 0.0004\n",
      "Epoch [46/50], Train Loss: 0.0369\n",
      "Epoch [46/50], Eval Loss: 0.0004\n",
      "Epoch [47/50], Train Loss: 0.0342\n",
      "Epoch [47/50], Eval Loss: 0.0004\n",
      "Epoch [48/50], Train Loss: 0.0390\n",
      "Epoch [48/50], Eval Loss: 0.0004\n",
      "Epoch [49/50], Train Loss: 0.0386\n",
      "Epoch [49/50], Eval Loss: 0.0005\n",
      "Epoch [50/50], Train Loss: 0.0331\n",
      "Epoch [50/50], Eval Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerModel(vocab_size, d_model,0.0, nhead, num_encoder_layers, num_decoder_layers)\n",
    "# transformer = TransformerModel(d_model,nhead,num_encoder_layers,9,vocab_size,vocab_size,dropout = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0005)\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "loss_val = 0.0\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Loop\n",
    "    transformer.train()\n",
    "    total_loss = 0.0\n",
    "    for source, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(source,target)\n",
    "        output = output.permute(0,2,1)\n",
    "        loss = criterion(output[:,:,:-1],target[:,1:])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss / len(train_loader):.4f}')\n",
    "    \n",
    "    # Validation Loop\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ = transformer(test_source,test_target)\n",
    "        output_ = output_.permute(0,2,1)\n",
    "        loss_val = criterion(output_[:,:,:-1], test_target[:,1:])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Eval Loss: {loss_val.item():.4f}')\n",
    "    \n",
    "        if loss_val < best_val_loss:\n",
    "            best_val_loss = loss_val\n",
    "            epochs_since_improvement = 0\n",
    "            torch.save(transformer.state_dict(), 'Model/transformer.pth')\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "    # Check if we should stop training early\n",
    "    if epochs_since_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch+1} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "if loss_val < best_val_loss:\n",
    "    torch.save(transformer.state_dict(), 'Model/transformer.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f4d57e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(vocab_size, d_model,0.0, nhead, num_encoder_layers, num_decoder_layers)\n",
    "model.load_state_dict(torch.load('Model/transformer.pth')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fd87421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, inp):\n",
    "        start = 26\n",
    "        input_sentence = torch.tensor([ord(c) - ord('a') for c in inp])\n",
    "        src = input_sentence.view(1,-1)\n",
    "        tgt = torch.tensor([start]).reshape(1,-1)\n",
    "        for i in range(8):         \n",
    "            logits = model.forward(src,tgt)              # logits = [batch_size,seq_length,27]\n",
    "            \n",
    "            logits = model.output_layer(logits).argmax(dim=2)            # logits = [batch_size,seq_length]\n",
    "            \n",
    "            ## Append this to tgt\n",
    "            tgt = torch.cat((tgt,logits[:,-1].reshape(1,-1)),dim=1)\n",
    "            \n",
    "        \n",
    "        # print(tgt.shape)      # [1,9]\n",
    "        # Convert tgt to string\n",
    "        final = \"\"\n",
    "        \n",
    "        for i in range(1,9):\n",
    "            # print(tgt[0][i])\n",
    "            final += chr(ord('a')+int(tgt[0][i]))\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93388b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how many characters match in the two strings\n",
    "def check(pred: str, true: str):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct\n",
    "\n",
    "# Function to score the model's performance\n",
    "def evaluate(model):\n",
    "\n",
    "    # Train data\n",
    "    print(\"Obtaining results for training data:\")\n",
    "    train_data = pd.read_csv(\"Data/train_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in train_data:\n",
    "        pred = predict(model,x)\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Train dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_train.csv\", index=False)\n",
    "\n",
    "    #----------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"Obtaining metrics for eval data:\")\n",
    "    eval_data = pd.read_csv(\"Data/eval_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in eval_data:\n",
    "        pred = predict(model, x)\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Eval dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d527dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining results for training data:\n",
      "Train dataset results:\n",
      "Number of predictions with 0 correct predictions: 0\n",
      "Number of predictions with 1 correct predictions: 0\n",
      "Number of predictions with 2 correct predictions: 0\n",
      "Number of predictions with 3 correct predictions: 0\n",
      "Number of predictions with 4 correct predictions: 0\n",
      "Number of predictions with 5 correct predictions: 0\n",
      "Number of predictions with 6 correct predictions: 8\n",
      "Number of predictions with 7 correct predictions: 417\n",
      "Number of predictions with 8 correct predictions: 6575\n",
      "Obtaining metrics for eval data:\n",
      "Eval dataset results:\n",
      "Number of predictions with 0 correct predictions: 0\n",
      "Number of predictions with 1 correct predictions: 0\n",
      "Number of predictions with 2 correct predictions: 0\n",
      "Number of predictions with 3 correct predictions: 0\n",
      "Number of predictions with 4 correct predictions: 0\n",
      "Number of predictions with 5 correct predictions: 0\n",
      "Number of predictions with 6 correct predictions: 6\n",
      "Number of predictions with 7 correct predictions: 126\n",
      "Number of predictions with 8 correct predictions: 1868\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d354b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
