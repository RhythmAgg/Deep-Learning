{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838f7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eabc2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.means = torch.tensor([[0, 0], [2, 2], [-2, 2]], dtype=torch.float32)\n",
    "        self.sigma = math.sqrt(2)\n",
    "        self.len = size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z1 = torch.normal(self.means[0], self.sigma)\n",
    "        z2 = torch.normal(self.means[1], self.sigma)\n",
    "        z3 = torch.normal(self.means[2], self.sigma)\n",
    "        return z1\n",
    "#         return 1/math.sqrt(3)*(z1+z2+z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21df72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dataset(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02dc5109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0281, -0.0079])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_loader = DataLoader(samples, batch_size = 50, shuffle = True)\n",
    "total_mean = torch.tensor([0, 0], dtype = torch.float32)\n",
    "for batch in samples_loader:\n",
    "    total_mean[0] += batch[:,0].mean()\n",
    "    total_mean[1] += batch[:,1].mean()\n",
    "total_mean = total_mean / len(samples_loader)\n",
    "total_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6fdd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec130df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 1 D Loss: 1.4222370386123657, G Loss: 0.5943178534507751\n",
      "Epoch 0, batch 2 D Loss: 1.4063389301300049, G Loss: 0.5947249531745911\n",
      "Epoch 0, batch 3 D Loss: 1.3891561031341553, G Loss: 0.5956199765205383\n",
      "Epoch 0, batch 4 D Loss: 1.3737552165985107, G Loss: 0.5966014266014099\n",
      "Epoch 0, batch 5 D Loss: 1.364165186882019, G Loss: 0.5998711585998535\n",
      "Epoch 0, batch 6 D Loss: 1.3825154304504395, G Loss: 0.5996529459953308\n",
      "Epoch 0, batch 7 D Loss: 1.3460848331451416, G Loss: 0.6038751602172852\n",
      "Epoch 0, batch 8 D Loss: 1.3459309339523315, G Loss: 0.6027817726135254\n",
      "Epoch 0, batch 9 D Loss: 1.3410310745239258, G Loss: 0.6067225933074951\n",
      "Epoch 0, batch 10 D Loss: 1.3315165042877197, G Loss: 0.6071737408638\n",
      "Epoch 0, batch 11 D Loss: 1.320600986480713, G Loss: 0.607138991355896\n",
      "Epoch 0, batch 12 D Loss: 1.3185486793518066, G Loss: 0.612561047077179\n",
      "Epoch 0, batch 13 D Loss: 1.3012454509735107, G Loss: 0.6146442890167236\n",
      "Epoch 0, batch 14 D Loss: 1.31135094165802, G Loss: 0.6120714545249939\n",
      "Epoch 0, batch 15 D Loss: 1.2920660972595215, G Loss: 0.617042064666748\n",
      "Epoch 0, batch 16 D Loss: 1.3002018928527832, G Loss: 0.615231990814209\n",
      "Epoch 0, batch 17 D Loss: 1.268488883972168, G Loss: 0.6227695941925049\n",
      "Epoch 0, batch 18 D Loss: 1.2930245399475098, G Loss: 0.6093054413795471\n",
      "Epoch 0, batch 19 D Loss: 1.3020215034484863, G Loss: 0.6111276149749756\n",
      "Epoch 0, batch 20 D Loss: 1.2750005722045898, G Loss: 0.6241680383682251\n",
      "Epoch 0, batch 21 D Loss: 1.294705867767334, G Loss: 0.6173714399337769\n",
      "Epoch 0, batch 22 D Loss: 1.2730300426483154, G Loss: 0.6194043159484863\n",
      "Epoch 0, batch 23 D Loss: 1.2552108764648438, G Loss: 0.6271761059761047\n",
      "Epoch 0, batch 24 D Loss: 1.2882252931594849, G Loss: 0.6222007274627686\n",
      "Epoch 0, batch 25 D Loss: 1.2726233005523682, G Loss: 0.6260889172554016\n",
      "Epoch 0, batch 26 D Loss: 1.2443376779556274, G Loss: 0.6259050965309143\n",
      "Epoch 0, batch 27 D Loss: 1.2673404216766357, G Loss: 0.6349440217018127\n",
      "Epoch 0, batch 28 D Loss: 1.288581371307373, G Loss: 0.6441671848297119\n",
      "Epoch 0, batch 29 D Loss: 1.232595682144165, G Loss: 0.6487644910812378\n",
      "Epoch 0, batch 30 D Loss: 1.2649848461151123, G Loss: 0.6508435010910034\n",
      "Epoch 0, batch 31 D Loss: 1.207442283630371, G Loss: 0.6624764800071716\n",
      "Epoch 0, batch 32 D Loss: 1.260455846786499, G Loss: 0.6814468502998352\n",
      "Epoch 0, batch 33 D Loss: 1.2269375324249268, G Loss: 0.6805321574211121\n",
      "Epoch 0, batch 34 D Loss: 1.1881721019744873, G Loss: 0.7032047510147095\n",
      "Epoch 0, batch 35 D Loss: 1.195458173751831, G Loss: 0.7188788652420044\n",
      "Epoch 0, batch 36 D Loss: 1.1726465225219727, G Loss: 0.7276396751403809\n",
      "Epoch 0, batch 37 D Loss: 1.1411491632461548, G Loss: 0.7463757991790771\n",
      "Epoch 0, batch 38 D Loss: 1.1043291091918945, G Loss: 0.7726026773452759\n",
      "Epoch 0, batch 39 D Loss: 1.121807336807251, G Loss: 0.7948068380355835\n",
      "Epoch 0, batch 40 D Loss: 1.0240528583526611, G Loss: 0.8159043788909912\n",
      "Epoch 0, batch 41 D Loss: 1.0876843929290771, G Loss: 0.8447225689888\n",
      "Epoch 0, batch 42 D Loss: 1.103632926940918, G Loss: 0.8529503345489502\n",
      "Epoch 0, batch 43 D Loss: 1.0724071264266968, G Loss: 0.8518995642662048\n",
      "Epoch 0, batch 44 D Loss: 1.088392972946167, G Loss: 0.8803352117538452\n",
      "Epoch 0, batch 45 D Loss: 1.0306751728057861, G Loss: 0.9124952554702759\n",
      "Epoch 0, batch 46 D Loss: 0.975504994392395, G Loss: 0.9247139096260071\n",
      "Epoch 0, batch 47 D Loss: 1.0672428607940674, G Loss: 0.9383410811424255\n",
      "Epoch 0, batch 48 D Loss: 1.0740342140197754, G Loss: 0.9547460675239563\n",
      "Epoch 0, batch 49 D Loss: 1.0086092948913574, G Loss: 0.9815934896469116\n",
      "Epoch 0, batch 50 D Loss: 0.9533188939094543, G Loss: 0.9845454692840576\n",
      "Epoch 0, batch 51 D Loss: 0.9964514374732971, G Loss: 0.9493396878242493\n",
      "Epoch 0, batch 52 D Loss: 1.0356744527816772, G Loss: 1.0028702020645142\n",
      "Epoch 0, batch 53 D Loss: 1.0486054420471191, G Loss: 0.9867876172065735\n",
      "Epoch 0, batch 54 D Loss: 1.0399057865142822, G Loss: 1.025740623474121\n",
      "Epoch 0, batch 55 D Loss: 1.0172407627105713, G Loss: 0.9822757840156555\n",
      "Epoch 0, batch 56 D Loss: 0.9574722647666931, G Loss: 1.0011687278747559\n",
      "Epoch 0, batch 57 D Loss: 1.0608197450637817, G Loss: 1.0091989040374756\n",
      "Epoch 0, batch 58 D Loss: 1.1338601112365723, G Loss: 0.9217744469642639\n",
      "Epoch 0, batch 59 D Loss: 1.041186809539795, G Loss: 1.007766842842102\n",
      "Epoch 0, batch 60 D Loss: 1.078662633895874, G Loss: 0.9646861553192139\n",
      "Epoch 0, batch 61 D Loss: 1.1925537586212158, G Loss: 0.9103963971138\n",
      "Epoch 0, batch 62 D Loss: 1.053764820098877, G Loss: 0.9413583278656006\n",
      "Epoch 0, batch 63 D Loss: 1.0795468091964722, G Loss: 0.9180676341056824\n",
      "Epoch 0, batch 64 D Loss: 1.0288424491882324, G Loss: 0.9420533180236816\n",
      "Epoch 0, batch 65 D Loss: 1.2009716033935547, G Loss: 0.8340445756912231\n",
      "Epoch 0, batch 66 D Loss: 1.1115570068359375, G Loss: 0.8392566442489624\n",
      "Epoch 0, batch 67 D Loss: 1.1670011281967163, G Loss: 0.8390554189682007\n",
      "Epoch 0, batch 68 D Loss: 1.3006987571716309, G Loss: 0.8130391836166382\n",
      "Epoch 0, batch 69 D Loss: 1.0885426998138428, G Loss: 0.841942310333252\n",
      "Epoch 0, batch 70 D Loss: 1.2114169597625732, G Loss: 0.7617712616920471\n",
      "Epoch 0, batch 71 D Loss: 1.2025588750839233, G Loss: 0.7619389891624451\n",
      "Epoch 0, batch 72 D Loss: 1.221092939376831, G Loss: 0.7425774931907654\n",
      "Epoch 0, batch 73 D Loss: 1.3712575435638428, G Loss: 0.6756212711334229\n",
      "Epoch 0, batch 74 D Loss: 1.2236077785491943, G Loss: 0.6462129354476929\n",
      "Epoch 0, batch 75 D Loss: 1.3361904621124268, G Loss: 0.6956772804260254\n",
      "Epoch 0, batch 76 D Loss: 1.366185188293457, G Loss: 0.5657950043678284\n",
      "Epoch 0, batch 77 D Loss: 1.327458143234253, G Loss: 0.6297122836112976\n",
      "Epoch 0, batch 78 D Loss: 1.6000158786773682, G Loss: 0.572384238243103\n",
      "Epoch 0, batch 79 D Loss: 1.5631015300750732, G Loss: 0.5819008350372314\n",
      "Epoch 0, batch 80 D Loss: 1.4765350818634033, G Loss: 0.5659787058830261\n",
      "Epoch 0, batch 81 D Loss: 1.714887022972107, G Loss: 0.509006917476654\n",
      "Epoch 0, batch 82 D Loss: 1.5601911544799805, G Loss: 0.5095697045326233\n",
      "Epoch 0, batch 83 D Loss: 1.6905620098114014, G Loss: 0.4800463616847992\n",
      "Epoch 0, batch 84 D Loss: 1.6404860019683838, G Loss: 0.5254128575325012\n",
      "Epoch 0, batch 85 D Loss: 1.4968416690826416, G Loss: 0.5262176990509033\n",
      "Epoch 0, batch 86 D Loss: 1.7159783840179443, G Loss: 0.4986742436885834\n",
      "Epoch 0, batch 87 D Loss: 1.7018146514892578, G Loss: 0.4860411584377289\n",
      "Epoch 0, batch 88 D Loss: 1.7453269958496094, G Loss: 0.47938141226768494\n",
      "Epoch 0, batch 89 D Loss: 1.6735742092132568, G Loss: 0.4939650297164917\n",
      "Epoch 0, batch 90 D Loss: 1.688765048980713, G Loss: 0.5331112146377563\n",
      "Epoch 0, batch 91 D Loss: 1.8701844215393066, G Loss: 0.45564261078834534\n",
      "Epoch 0, batch 92 D Loss: 1.6987473964691162, G Loss: 0.5101635456085205\n",
      "Epoch 0, batch 93 D Loss: 1.8515293598175049, G Loss: 0.44269245862960815\n",
      "Epoch 0, batch 94 D Loss: 1.8065569400787354, G Loss: 0.4945925176143646\n",
      "Epoch 0, batch 95 D Loss: 1.8019018173217773, G Loss: 0.4987626373767853\n",
      "Epoch 0, batch 96 D Loss: 1.8726398944854736, G Loss: 0.4948830306529999\n",
      "Epoch 0, batch 97 D Loss: 1.743708848953247, G Loss: 0.5436651706695557\n",
      "Epoch 0, batch 98 D Loss: 1.6587306261062622, G Loss: 0.5262469053268433\n",
      "Epoch 0, batch 99 D Loss: 1.708416223526001, G Loss: 0.5857930779457092\n",
      "Epoch 0, batch 100 D Loss: 1.7264386415481567, G Loss: 0.5879557728767395\n",
      "Epoch 0, batch 101 D Loss: 1.7093288898468018, G Loss: 0.5722935199737549\n",
      "Epoch 0, batch 102 D Loss: 1.5764811038970947, G Loss: 0.653420090675354\n",
      "Epoch 0, batch 103 D Loss: 1.5061383247375488, G Loss: 0.6859634518623352\n",
      "Epoch 0, batch 104 D Loss: 1.505388855934143, G Loss: 0.7438634634017944\n",
      "Epoch 0, batch 105 D Loss: 1.573091983795166, G Loss: 0.7377705574035645\n",
      "Epoch 0, batch 106 D Loss: 1.6214772462844849, G Loss: 0.7883458137512207\n",
      "Epoch 0, batch 107 D Loss: 1.5008841753005981, G Loss: 0.8217871189117432\n",
      "Epoch 0, batch 108 D Loss: 1.611072063446045, G Loss: 0.8144155740737915\n",
      "Epoch 0, batch 109 D Loss: 1.520843505859375, G Loss: 0.8374990820884705\n",
      "Epoch 0, batch 110 D Loss: 1.3949885368347168, G Loss: 0.9622568488121033\n",
      "Epoch 0, batch 111 D Loss: 1.4729275703430176, G Loss: 0.9405010342597961\n",
      "Epoch 0, batch 112 D Loss: 1.5048233270645142, G Loss: 0.9180940985679626\n",
      "Epoch 0, batch 113 D Loss: 1.253584861755371, G Loss: 1.1695098876953125\n",
      "Epoch 0, batch 114 D Loss: 1.5370848178863525, G Loss: 0.9305223822593689\n",
      "Epoch 0, batch 115 D Loss: 1.2611745595932007, G Loss: 1.1909253597259521\n",
      "Epoch 0, batch 116 D Loss: 1.4305094480514526, G Loss: 1.1103214025497437\n",
      "Epoch 0, batch 117 D Loss: 1.376744031906128, G Loss: 1.1567981243133545\n",
      "Epoch 0, batch 118 D Loss: 1.0766239166259766, G Loss: 1.3365360498428345\n",
      "Epoch 0, batch 119 D Loss: 1.417922019958496, G Loss: 1.2820782661437988\n",
      "Epoch 0, batch 120 D Loss: 1.1685965061187744, G Loss: 1.3417118787765503\n",
      "Epoch 0, batch 121 D Loss: 1.4218668937683105, G Loss: 1.4375516176223755\n",
      "Epoch 0, batch 122 D Loss: 1.3464237451553345, G Loss: 1.2730807065963745\n",
      "Epoch 0, batch 123 D Loss: 1.2894282341003418, G Loss: 1.2551641464233398\n",
      "Epoch 0, batch 124 D Loss: 1.241884708404541, G Loss: 1.1771283149719238\n",
      "Epoch 0, batch 125 D Loss: 1.018371820449829, G Loss: 1.4955815076828003\n",
      "Epoch 0, batch 126 D Loss: 1.3437319993972778, G Loss: 1.4623605012893677\n",
      "Epoch 0, batch 127 D Loss: 1.2911585569381714, G Loss: 1.2873501777648926\n",
      "Epoch 0, batch 128 D Loss: 1.1710699796676636, G Loss: 1.5141878128051758\n",
      "Epoch 0, batch 129 D Loss: 1.2007088661193848, G Loss: 1.4672682285308838\n",
      "Epoch 0, batch 130 D Loss: 1.286569356918335, G Loss: 1.2675280570983887\n",
      "Epoch 0, batch 131 D Loss: 1.2222163677215576, G Loss: 1.4752490520477295\n",
      "Epoch 0, batch 132 D Loss: 1.245543360710144, G Loss: 1.4699907302856445\n",
      "Epoch 0, batch 133 D Loss: 1.2181010246276855, G Loss: 1.4422093629837036\n",
      "Epoch 0, batch 134 D Loss: 1.151793122291565, G Loss: 1.579745888710022\n",
      "Epoch 0, batch 135 D Loss: 1.214599847793579, G Loss: 1.3925708532333374\n",
      "Epoch 0, batch 136 D Loss: 1.145153522491455, G Loss: 1.5135807991027832\n",
      "Epoch 0, batch 137 D Loss: 1.2589777708053589, G Loss: 1.4057841300964355\n",
      "Epoch 0, batch 138 D Loss: 1.1859095096588135, G Loss: 1.4248321056365967\n",
      "Epoch 0, batch 139 D Loss: 1.2281821966171265, G Loss: 1.2601295709609985\n",
      "Epoch 0, batch 140 D Loss: 1.3075681924819946, G Loss: 1.380056619644165\n",
      "Epoch 0, batch 141 D Loss: 1.3282434940338135, G Loss: 1.3217390775680542\n",
      "Epoch 0, batch 142 D Loss: 1.246534824371338, G Loss: 1.4762616157531738\n",
      "Epoch 0, batch 143 D Loss: 1.1695456504821777, G Loss: 1.4564987421035767\n",
      "Epoch 0, batch 144 D Loss: 1.2698628902435303, G Loss: 1.3501408100128174\n",
      "Epoch 0, batch 145 D Loss: 1.290280818939209, G Loss: 1.4852404594421387\n",
      "Epoch 0, batch 146 D Loss: 1.1762288808822632, G Loss: 1.7034982442855835\n",
      "Epoch 0, batch 147 D Loss: 1.1161402463912964, G Loss: 1.555515170097351\n",
      "Epoch 0, batch 148 D Loss: 1.0850228071212769, G Loss: 1.3153492212295532\n",
      "Epoch 0, batch 149 D Loss: 1.2637032270431519, G Loss: 1.3841602802276611\n",
      "Epoch 0, batch 150 D Loss: 1.1657145023345947, G Loss: 1.5477097034454346\n",
      "Epoch 0, batch 151 D Loss: 1.0788582563400269, G Loss: 1.5161926746368408\n",
      "Epoch 0, batch 152 D Loss: 1.1470410823822021, G Loss: 1.5618771314620972\n",
      "Epoch 0, batch 153 D Loss: 1.2219347953796387, G Loss: 1.4918409585952759\n",
      "Epoch 0, batch 154 D Loss: 0.9851774573326111, G Loss: 1.5231742858886719\n",
      "Epoch 0, batch 155 D Loss: 1.2891100645065308, G Loss: 1.565981149673462\n",
      "Epoch 0, batch 156 D Loss: 1.1723411083221436, G Loss: 1.4298347234725952\n",
      "Epoch 0, batch 157 D Loss: 1.1623964309692383, G Loss: 1.5129177570343018\n",
      "Epoch 0, batch 158 D Loss: 1.084953784942627, G Loss: 1.5425360202789307\n",
      "Epoch 0, batch 159 D Loss: 1.217236042022705, G Loss: 1.475894808769226\n",
      "Epoch 0, batch 160 D Loss: 1.362343192100525, G Loss: 1.3511195182800293\n",
      "Epoch 0, batch 161 D Loss: 0.9663459658622742, G Loss: 1.5078718662261963\n",
      "Epoch 0, batch 162 D Loss: 1.2059332132339478, G Loss: 1.3605947494506836\n",
      "Epoch 0, batch 163 D Loss: 1.1100248098373413, G Loss: 1.4215235710144043\n",
      "Epoch 0, batch 164 D Loss: 1.1564221382141113, G Loss: 1.4079413414001465\n",
      "Epoch 0, batch 165 D Loss: 1.1349878311157227, G Loss: 1.3283839225769043\n",
      "Epoch 0, batch 166 D Loss: 1.1073371171951294, G Loss: 1.382575511932373\n",
      "Epoch 0, batch 167 D Loss: 1.1276053190231323, G Loss: 1.2990444898605347\n",
      "Epoch 0, batch 168 D Loss: 1.1048367023468018, G Loss: 1.3169687986373901\n",
      "Epoch 0, batch 169 D Loss: 1.0680421590805054, G Loss: 1.3655979633331299\n",
      "Epoch 0, batch 170 D Loss: 1.251251220703125, G Loss: 1.2389516830444336\n",
      "Epoch 0, batch 171 D Loss: 1.414806842803955, G Loss: 1.2744636535644531\n",
      "Epoch 0, batch 172 D Loss: 1.528326153755188, G Loss: 1.2490483522415161\n",
      "Epoch 0, batch 173 D Loss: 1.52199387550354, G Loss: 1.2113151550292969\n",
      "Epoch 0, batch 174 D Loss: 1.311538577079773, G Loss: 1.2680329084396362\n",
      "Epoch 0, batch 175 D Loss: 1.2675020694732666, G Loss: 1.1724241971969604\n",
      "Epoch 0, batch 176 D Loss: 1.4554823637008667, G Loss: 1.2845104932785034\n",
      "Epoch 0, batch 177 D Loss: 1.5549633502960205, G Loss: 1.217627763748169\n",
      "Epoch 0, batch 178 D Loss: 1.3709875345230103, G Loss: 1.2274916172027588\n",
      "Epoch 0, batch 179 D Loss: 1.4802132844924927, G Loss: 1.1285814046859741\n",
      "Epoch 0, batch 180 D Loss: 1.2399438619613647, G Loss: 1.1593137979507446\n",
      "Epoch 0, batch 181 D Loss: 1.284157156944275, G Loss: 1.181828498840332\n",
      "Epoch 0, batch 182 D Loss: 1.558394432067871, G Loss: 1.1607658863067627\n",
      "Epoch 0, batch 183 D Loss: 1.3544412851333618, G Loss: 1.1608209609985352\n",
      "Epoch 0, batch 184 D Loss: 1.4898886680603027, G Loss: 1.060243844985962\n",
      "Epoch 0, batch 185 D Loss: 1.6918340921401978, G Loss: 1.2116496562957764\n",
      "Epoch 0, batch 186 D Loss: 1.4122662544250488, G Loss: 1.0863059759140015\n",
      "Epoch 0, batch 187 D Loss: 1.6132712364196777, G Loss: 1.1712895631790161\n",
      "Epoch 0, batch 188 D Loss: 1.6173714399337769, G Loss: 1.126309871673584\n",
      "Epoch 0, batch 189 D Loss: 1.2661616802215576, G Loss: 1.2017724514007568\n",
      "Epoch 0, batch 190 D Loss: 1.5409575700759888, G Loss: 1.098669171333313\n",
      "Epoch 0, batch 191 D Loss: 1.4511898756027222, G Loss: 1.136985421180725\n",
      "Epoch 0, batch 192 D Loss: 1.413370132446289, G Loss: 1.2328994274139404\n",
      "Epoch 0, batch 193 D Loss: 1.6287271976470947, G Loss: 1.0917953252792358\n",
      "Epoch 0, batch 194 D Loss: 1.6257842779159546, G Loss: 1.1079432964324951\n",
      "Epoch 0, batch 195 D Loss: 1.3966585397720337, G Loss: 1.100222110748291\n",
      "Epoch 0, batch 196 D Loss: 1.5797842741012573, G Loss: 1.2306758165359497\n",
      "Epoch 0, batch 197 D Loss: 1.4252694845199585, G Loss: 1.3661372661590576\n",
      "Epoch 0, batch 198 D Loss: 1.5728925466537476, G Loss: 1.1518733501434326\n",
      "Epoch 0, batch 199 D Loss: 1.4569331407546997, G Loss: 1.336119532585144\n",
      "Epoch 0, batch 200 D Loss: 1.2462577819824219, G Loss: 1.271977186203003\n",
      "Epoch 1, batch 1 D Loss: 1.456272840499878, G Loss: 1.3419297933578491\n",
      "Epoch 1, batch 2 D Loss: 1.2900367975234985, G Loss: 1.2938783168792725\n",
      "Epoch 1, batch 3 D Loss: 1.409060001373291, G Loss: 1.140501856803894\n",
      "Epoch 1, batch 4 D Loss: 1.5921165943145752, G Loss: 1.4940292835235596\n",
      "Epoch 1, batch 5 D Loss: 1.3872661590576172, G Loss: 1.5219523906707764\n",
      "Epoch 1, batch 6 D Loss: 1.479212760925293, G Loss: 1.4123667478561401\n",
      "Epoch 1, batch 7 D Loss: 1.5200233459472656, G Loss: 1.4001039266586304\n",
      "Epoch 1, batch 8 D Loss: 1.5775582790374756, G Loss: 1.2134149074554443\n",
      "Epoch 1, batch 9 D Loss: 1.3518717288970947, G Loss: 1.4939477443695068\n",
      "Epoch 1, batch 10 D Loss: 1.566204309463501, G Loss: 1.3643351793289185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 11 D Loss: 1.5922552347183228, G Loss: 1.3772639036178589\n",
      "Epoch 1, batch 12 D Loss: 1.842559576034546, G Loss: 1.5051072835922241\n",
      "Epoch 1, batch 13 D Loss: 1.69477117061615, G Loss: 1.2659727334976196\n",
      "Epoch 1, batch 14 D Loss: 1.4434890747070312, G Loss: 1.4823522567749023\n",
      "Epoch 1, batch 15 D Loss: 1.6111857891082764, G Loss: 1.2415906190872192\n",
      "Epoch 1, batch 16 D Loss: 1.2731268405914307, G Loss: 1.5482062101364136\n",
      "Epoch 1, batch 17 D Loss: 1.5146435499191284, G Loss: 1.3890471458435059\n",
      "Epoch 1, batch 18 D Loss: 1.325931191444397, G Loss: 1.4241340160369873\n",
      "Epoch 1, batch 19 D Loss: 1.3779809474945068, G Loss: 1.3611210584640503\n",
      "Epoch 1, batch 20 D Loss: 1.3989458084106445, G Loss: 1.4332183599472046\n",
      "Epoch 1, batch 21 D Loss: 1.7433891296386719, G Loss: 1.4942272901535034\n",
      "Epoch 1, batch 22 D Loss: 1.4886126518249512, G Loss: 1.4827293157577515\n",
      "Epoch 1, batch 23 D Loss: 1.4972665309906006, G Loss: 1.576045274734497\n",
      "Epoch 1, batch 24 D Loss: 1.4192047119140625, G Loss: 1.4288614988327026\n",
      "Epoch 1, batch 25 D Loss: 1.417393684387207, G Loss: 1.5007249116897583\n",
      "Epoch 1, batch 26 D Loss: 1.4269275665283203, G Loss: 1.3653281927108765\n",
      "Epoch 1, batch 27 D Loss: 1.622222900390625, G Loss: 1.4690264463424683\n",
      "Epoch 1, batch 28 D Loss: 1.6203457117080688, G Loss: 1.5476775169372559\n",
      "Epoch 1, batch 29 D Loss: 1.5181829929351807, G Loss: 1.332411766052246\n",
      "Epoch 1, batch 30 D Loss: 1.5725687742233276, G Loss: 1.3080021142959595\n",
      "Epoch 1, batch 31 D Loss: 1.4763219356536865, G Loss: 1.5026015043258667\n",
      "Epoch 1, batch 32 D Loss: 1.3738973140716553, G Loss: 1.4918025732040405\n",
      "Epoch 1, batch 33 D Loss: 1.7333543300628662, G Loss: 1.4014674425125122\n",
      "Epoch 1, batch 34 D Loss: 1.5160695314407349, G Loss: 1.2340588569641113\n",
      "Epoch 1, batch 35 D Loss: 1.4725178480148315, G Loss: 1.2033586502075195\n",
      "Epoch 1, batch 36 D Loss: 1.58670973777771, G Loss: 1.1998414993286133\n",
      "Epoch 1, batch 37 D Loss: 1.4969199895858765, G Loss: 1.2842462062835693\n",
      "Epoch 1, batch 38 D Loss: 1.1245630979537964, G Loss: 1.2887279987335205\n",
      "Epoch 1, batch 39 D Loss: 1.368433952331543, G Loss: 1.2359760999679565\n",
      "Epoch 1, batch 40 D Loss: 1.4365053176879883, G Loss: 1.3843766450881958\n",
      "Epoch 1, batch 41 D Loss: 1.7579152584075928, G Loss: 1.2675458192825317\n",
      "Epoch 1, batch 42 D Loss: 1.4253369569778442, G Loss: 1.3095344305038452\n",
      "Epoch 1, batch 43 D Loss: 1.42156183719635, G Loss: 1.3197271823883057\n",
      "Epoch 1, batch 44 D Loss: 1.5518285036087036, G Loss: 1.1606734991073608\n",
      "Epoch 1, batch 45 D Loss: 1.2760465145111084, G Loss: 1.3370811939239502\n",
      "Epoch 1, batch 46 D Loss: 1.3044017553329468, G Loss: 1.1788334846496582\n",
      "Epoch 1, batch 47 D Loss: 1.385181188583374, G Loss: 1.3086800575256348\n",
      "Epoch 1, batch 48 D Loss: 1.4649012088775635, G Loss: 1.1242175102233887\n",
      "Epoch 1, batch 49 D Loss: 1.2318439483642578, G Loss: 1.433711051940918\n",
      "Epoch 1, batch 50 D Loss: 1.3692835569381714, G Loss: 1.1826977729797363\n",
      "Epoch 1, batch 51 D Loss: 1.4351856708526611, G Loss: 1.2151058912277222\n",
      "Epoch 1, batch 52 D Loss: 1.2416009902954102, G Loss: 1.3098164796829224\n",
      "Epoch 1, batch 53 D Loss: 1.5246975421905518, G Loss: 1.2469911575317383\n",
      "Epoch 1, batch 54 D Loss: 1.3174004554748535, G Loss: 1.165634036064148\n",
      "Epoch 1, batch 55 D Loss: 1.3414908647537231, G Loss: 1.1304903030395508\n",
      "Epoch 1, batch 56 D Loss: 1.4032541513442993, G Loss: 1.1394580602645874\n",
      "Epoch 1, batch 57 D Loss: 1.277315378189087, G Loss: 1.3212093114852905\n",
      "Epoch 1, batch 58 D Loss: 1.4054399728775024, G Loss: 1.1745895147323608\n",
      "Epoch 1, batch 59 D Loss: 1.440424919128418, G Loss: 1.2726843357086182\n",
      "Epoch 1, batch 60 D Loss: 1.3426445722579956, G Loss: 1.1645574569702148\n",
      "Epoch 1, batch 61 D Loss: 1.3517181873321533, G Loss: 1.1508965492248535\n",
      "Epoch 1, batch 62 D Loss: 1.2768583297729492, G Loss: 1.157304048538208\n",
      "Epoch 1, batch 63 D Loss: 1.3106862306594849, G Loss: 1.2096636295318604\n",
      "Epoch 1, batch 64 D Loss: 1.316411018371582, G Loss: 1.1981920003890991\n",
      "Epoch 1, batch 65 D Loss: 1.163439154624939, G Loss: 1.1842597723007202\n",
      "Epoch 1, batch 66 D Loss: 1.1572645902633667, G Loss: 1.1769461631774902\n",
      "Epoch 1, batch 67 D Loss: 1.4981778860092163, G Loss: 1.1369826793670654\n",
      "Epoch 1, batch 68 D Loss: 1.1981191635131836, G Loss: 1.1870845556259155\n",
      "Epoch 1, batch 69 D Loss: 1.3235766887664795, G Loss: 1.1625289916992188\n",
      "Epoch 1, batch 70 D Loss: 1.3558712005615234, G Loss: 1.1596421003341675\n",
      "Epoch 1, batch 71 D Loss: 1.1277439594268799, G Loss: 1.2088043689727783\n",
      "Epoch 1, batch 72 D Loss: 1.1621778011322021, G Loss: 1.2272796630859375\n",
      "Epoch 1, batch 73 D Loss: 1.4456888437271118, G Loss: 1.2066630125045776\n",
      "Epoch 1, batch 74 D Loss: 1.1999021768569946, G Loss: 1.2127946615219116\n",
      "Epoch 1, batch 75 D Loss: 1.3478525876998901, G Loss: 1.185852289199829\n",
      "Epoch 1, batch 76 D Loss: 1.101935625076294, G Loss: 1.1479034423828125\n",
      "Epoch 1, batch 77 D Loss: 1.3893015384674072, G Loss: 1.0945549011230469\n",
      "Epoch 1, batch 78 D Loss: 0.9587786197662354, G Loss: 1.1256167888641357\n",
      "Epoch 1, batch 79 D Loss: 1.2145719528198242, G Loss: 1.081801414489746\n",
      "Epoch 1, batch 80 D Loss: 0.9728718996047974, G Loss: 1.1428158283233643\n",
      "Epoch 1, batch 81 D Loss: 1.2593308687210083, G Loss: 1.119278907775879\n",
      "Epoch 1, batch 82 D Loss: 1.1050355434417725, G Loss: 1.0676543712615967\n",
      "Epoch 1, batch 83 D Loss: 1.1165540218353271, G Loss: 1.0299992561340332\n",
      "Epoch 1, batch 84 D Loss: 1.2192773818969727, G Loss: 1.0594438314437866\n",
      "Epoch 1, batch 85 D Loss: 1.0681660175323486, G Loss: 1.1019724607467651\n",
      "Epoch 1, batch 86 D Loss: 1.2761805057525635, G Loss: 1.0872135162353516\n",
      "Epoch 1, batch 87 D Loss: 1.0783779621124268, G Loss: 1.060847520828247\n",
      "Epoch 1, batch 88 D Loss: 1.383312463760376, G Loss: 1.0453522205352783\n",
      "Epoch 1, batch 89 D Loss: 1.1712250709533691, G Loss: 1.005162000656128\n",
      "Epoch 1, batch 90 D Loss: 1.0957156419754028, G Loss: 1.0081456899642944\n",
      "Epoch 1, batch 91 D Loss: 1.102025032043457, G Loss: 1.0530498027801514\n",
      "Epoch 1, batch 92 D Loss: 1.2897462844848633, G Loss: 0.9820960760116577\n",
      "Epoch 1, batch 93 D Loss: 1.141081690788269, G Loss: 1.0161209106445312\n",
      "Epoch 1, batch 94 D Loss: 1.2073538303375244, G Loss: 1.000657558441162\n",
      "Epoch 1, batch 95 D Loss: 1.402238368988037, G Loss: 1.035805106163025\n",
      "Epoch 1, batch 96 D Loss: 1.0598384141921997, G Loss: 0.9666601419448853\n",
      "Epoch 1, batch 97 D Loss: 1.2021962404251099, G Loss: 0.9454663991928101\n",
      "Epoch 1, batch 98 D Loss: 1.436086893081665, G Loss: 0.9345924854278564\n",
      "Epoch 1, batch 99 D Loss: 1.3659238815307617, G Loss: 0.9103186726570129\n",
      "Epoch 1, batch 100 D Loss: 1.4301512241363525, G Loss: 0.9334185123443604\n",
      "Epoch 1, batch 101 D Loss: 1.1944453716278076, G Loss: 0.9245438575744629\n",
      "Epoch 1, batch 102 D Loss: 1.1378265619277954, G Loss: 0.9148544073104858\n",
      "Epoch 1, batch 103 D Loss: 1.4410977363586426, G Loss: 0.9222354888916016\n",
      "Epoch 1, batch 104 D Loss: 1.3010213375091553, G Loss: 0.8481875658035278\n",
      "Epoch 1, batch 105 D Loss: 1.15352201461792, G Loss: 0.9424389600753784\n",
      "Epoch 1, batch 106 D Loss: 1.414642572402954, G Loss: 0.91275954246521\n",
      "Epoch 1, batch 107 D Loss: 1.2580344676971436, G Loss: 0.8903631567955017\n",
      "Epoch 1, batch 108 D Loss: 1.4525517225265503, G Loss: 0.8847023844718933\n",
      "Epoch 1, batch 109 D Loss: 1.2520105838775635, G Loss: 0.8480756878852844\n",
      "Epoch 1, batch 110 D Loss: 1.3040474653244019, G Loss: 0.856199324131012\n",
      "Epoch 1, batch 111 D Loss: 1.4642409086227417, G Loss: 0.8689242005348206\n",
      "Epoch 1, batch 112 D Loss: 1.2786009311676025, G Loss: 0.8342702984809875\n",
      "Epoch 1, batch 113 D Loss: 1.2680244445800781, G Loss: 0.8412714600563049\n",
      "Epoch 1, batch 114 D Loss: 1.162595272064209, G Loss: 0.8349260091781616\n",
      "Epoch 1, batch 115 D Loss: 1.1935958862304688, G Loss: 0.8355510830879211\n",
      "Epoch 1, batch 116 D Loss: 1.1871774196624756, G Loss: 0.8371517658233643\n",
      "Epoch 1, batch 117 D Loss: 1.4315593242645264, G Loss: 0.8439551591873169\n",
      "Epoch 1, batch 118 D Loss: 1.3370985984802246, G Loss: 0.828798234462738\n",
      "Epoch 1, batch 119 D Loss: 1.320648193359375, G Loss: 0.7937286496162415\n",
      "Epoch 1, batch 120 D Loss: 1.3836767673492432, G Loss: 0.8098759651184082\n",
      "Epoch 1, batch 121 D Loss: 1.2910890579223633, G Loss: 0.806753933429718\n",
      "Epoch 1, batch 122 D Loss: 1.1654436588287354, G Loss: 0.7905415892601013\n",
      "Epoch 1, batch 123 D Loss: 1.111179232597351, G Loss: 0.7855488061904907\n",
      "Epoch 1, batch 124 D Loss: 1.3021241426467896, G Loss: 0.8203462958335876\n",
      "Epoch 1, batch 125 D Loss: 1.2465784549713135, G Loss: 0.8278947472572327\n",
      "Epoch 1, batch 126 D Loss: 1.3455595970153809, G Loss: 0.8133010864257812\n",
      "Epoch 1, batch 127 D Loss: 1.4692696332931519, G Loss: 0.82060706615448\n",
      "Epoch 1, batch 128 D Loss: 1.3174355030059814, G Loss: 0.794718325138092\n",
      "Epoch 1, batch 129 D Loss: 1.384008526802063, G Loss: 0.8298192620277405\n",
      "Epoch 1, batch 130 D Loss: 1.3365031480789185, G Loss: 0.7890922427177429\n",
      "Epoch 1, batch 131 D Loss: 1.159883737564087, G Loss: 0.8322328925132751\n",
      "Epoch 1, batch 132 D Loss: 1.1372573375701904, G Loss: 0.812812864780426\n",
      "Epoch 1, batch 133 D Loss: 1.2778401374816895, G Loss: 0.841488242149353\n",
      "Epoch 1, batch 134 D Loss: 1.425473690032959, G Loss: 0.7878864407539368\n",
      "Epoch 1, batch 135 D Loss: 1.2143198251724243, G Loss: 0.7948352098464966\n",
      "Epoch 1, batch 136 D Loss: 1.498254656791687, G Loss: 0.7765334844589233\n",
      "Epoch 1, batch 137 D Loss: 1.2939354181289673, G Loss: 0.8178272843360901\n",
      "Epoch 1, batch 138 D Loss: 1.3077095746994019, G Loss: 0.7706271409988403\n",
      "Epoch 1, batch 139 D Loss: 1.3267922401428223, G Loss: 0.7900199890136719\n",
      "Epoch 1, batch 140 D Loss: 1.2783663272857666, G Loss: 0.7776690721511841\n",
      "Epoch 1, batch 141 D Loss: 1.3844631910324097, G Loss: 0.7906646728515625\n",
      "Epoch 1, batch 142 D Loss: 1.3884367942810059, G Loss: 0.7813568115234375\n",
      "Epoch 1, batch 143 D Loss: 1.372175693511963, G Loss: 0.7499589323997498\n",
      "Epoch 1, batch 144 D Loss: 1.2254741191864014, G Loss: 0.7689428925514221\n",
      "Epoch 1, batch 145 D Loss: 1.2386484146118164, G Loss: 0.7779644131660461\n",
      "Epoch 1, batch 146 D Loss: 1.3040177822113037, G Loss: 0.7903472781181335\n",
      "Epoch 1, batch 147 D Loss: 1.3090171813964844, G Loss: 0.7972354292869568\n",
      "Epoch 1, batch 148 D Loss: 1.3278961181640625, G Loss: 0.7756146192550659\n",
      "Epoch 1, batch 149 D Loss: 1.1605241298675537, G Loss: 0.7656152248382568\n",
      "Epoch 1, batch 150 D Loss: 1.2921676635742188, G Loss: 0.7563897967338562\n",
      "Epoch 1, batch 151 D Loss: 1.4515166282653809, G Loss: 0.7286061644554138\n",
      "Epoch 1, batch 152 D Loss: 1.4631240367889404, G Loss: 0.7322715520858765\n",
      "Epoch 1, batch 153 D Loss: 1.4176987409591675, G Loss: 0.7685456871986389\n",
      "Epoch 1, batch 154 D Loss: 1.2606236934661865, G Loss: 0.7594808340072632\n",
      "Epoch 1, batch 155 D Loss: 1.2849255800247192, G Loss: 0.7614703178405762\n",
      "Epoch 1, batch 156 D Loss: 1.2314292192459106, G Loss: 0.7262904644012451\n",
      "Epoch 1, batch 157 D Loss: 1.2327879667282104, G Loss: 0.7274648547172546\n",
      "Epoch 1, batch 158 D Loss: 1.3941686153411865, G Loss: 0.7343461513519287\n",
      "Epoch 1, batch 159 D Loss: 1.3257157802581787, G Loss: 0.7227052450180054\n",
      "Epoch 1, batch 160 D Loss: 1.2827953100204468, G Loss: 0.7383816242218018\n",
      "Epoch 1, batch 161 D Loss: 1.2436820268630981, G Loss: 0.7466810345649719\n",
      "Epoch 1, batch 162 D Loss: 1.3368451595306396, G Loss: 0.7152760028839111\n",
      "Epoch 1, batch 163 D Loss: 1.3196983337402344, G Loss: 0.7323429584503174\n",
      "Epoch 1, batch 164 D Loss: 1.266136646270752, G Loss: 0.7810336351394653\n",
      "Epoch 1, batch 165 D Loss: 1.3541306257247925, G Loss: 0.7290074825286865\n",
      "Epoch 1, batch 166 D Loss: 1.3238145112991333, G Loss: 0.7284444570541382\n",
      "Epoch 1, batch 167 D Loss: 1.3415358066558838, G Loss: 0.7435333132743835\n",
      "Epoch 1, batch 168 D Loss: 1.324570655822754, G Loss: 0.6982990503311157\n",
      "Epoch 1, batch 169 D Loss: 1.363823652267456, G Loss: 0.6703170537948608\n",
      "Epoch 1, batch 170 D Loss: 1.3657112121582031, G Loss: 0.7033189535140991\n",
      "Epoch 1, batch 171 D Loss: 1.4198201894760132, G Loss: 0.7579215168952942\n",
      "Epoch 1, batch 172 D Loss: 1.3647688627243042, G Loss: 0.6902392506599426\n",
      "Epoch 1, batch 173 D Loss: 1.3613722324371338, G Loss: 0.6787046194076538\n",
      "Epoch 1, batch 174 D Loss: 1.3713349103927612, G Loss: 0.7136146426200867\n",
      "Epoch 1, batch 175 D Loss: 1.458094835281372, G Loss: 0.6806586980819702\n",
      "Epoch 1, batch 176 D Loss: 1.329437017440796, G Loss: 0.7660898566246033\n",
      "Epoch 1, batch 177 D Loss: 1.2203516960144043, G Loss: 0.698473334312439\n",
      "Epoch 1, batch 178 D Loss: 1.2207642793655396, G Loss: 0.7331428527832031\n",
      "Epoch 1, batch 179 D Loss: 1.3743407726287842, G Loss: 0.6716499328613281\n",
      "Epoch 1, batch 180 D Loss: 1.257856845855713, G Loss: 0.7328476905822754\n",
      "Epoch 1, batch 181 D Loss: 1.2177784442901611, G Loss: 0.7031496167182922\n",
      "Epoch 1, batch 182 D Loss: 1.3213435411453247, G Loss: 0.6843719482421875\n",
      "Epoch 1, batch 183 D Loss: 1.3991618156433105, G Loss: 0.6603862047195435\n",
      "Epoch 1, batch 184 D Loss: 1.3243722915649414, G Loss: 0.6776675581932068\n",
      "Epoch 1, batch 185 D Loss: 1.4946701526641846, G Loss: 0.6693671941757202\n",
      "Epoch 1, batch 186 D Loss: 1.401794195175171, G Loss: 0.6660470366477966\n",
      "Epoch 1, batch 187 D Loss: 1.3958237171173096, G Loss: 0.6811417937278748\n",
      "Epoch 1, batch 188 D Loss: 1.2989330291748047, G Loss: 0.7272816300392151\n",
      "Epoch 1, batch 189 D Loss: 1.334815502166748, G Loss: 0.7184625864028931\n",
      "Epoch 1, batch 190 D Loss: 1.2835750579833984, G Loss: 0.7243229150772095\n",
      "Epoch 1, batch 191 D Loss: 1.2360522747039795, G Loss: 0.7003459930419922\n",
      "Epoch 1, batch 192 D Loss: 1.3721740245819092, G Loss: 0.7092337012290955\n",
      "Epoch 1, batch 193 D Loss: 1.2950325012207031, G Loss: 0.7307524681091309\n",
      "Epoch 1, batch 194 D Loss: 1.288744330406189, G Loss: 0.7053611874580383\n",
      "Epoch 1, batch 195 D Loss: 1.4302494525909424, G Loss: 0.6765773296356201\n",
      "Epoch 1, batch 196 D Loss: 1.3567702770233154, G Loss: 0.7369945049285889\n",
      "Epoch 1, batch 197 D Loss: 1.437119960784912, G Loss: 0.6834433078765869\n",
      "Epoch 1, batch 198 D Loss: 1.4372808933258057, G Loss: 0.6123785972595215\n",
      "Epoch 1, batch 199 D Loss: 1.2970538139343262, G Loss: 0.688823401927948\n",
      "Epoch 1, batch 200 D Loss: 1.3658983707427979, G Loss: 0.647250235080719\n",
      "Epoch 2, batch 1 D Loss: 1.3150289058685303, G Loss: 0.7115258574485779\n",
      "Epoch 2, batch 2 D Loss: 1.275170922279358, G Loss: 0.676643431186676\n",
      "Epoch 2, batch 3 D Loss: 1.2623746395111084, G Loss: 0.7254052758216858\n",
      "Epoch 2, batch 4 D Loss: 1.329709768295288, G Loss: 0.683552086353302\n",
      "Epoch 2, batch 5 D Loss: 1.355581521987915, G Loss: 0.6682391166687012\n",
      "Epoch 2, batch 6 D Loss: 1.328330397605896, G Loss: 0.6855778694152832\n",
      "Epoch 2, batch 7 D Loss: 1.3721718788146973, G Loss: 0.6646029949188232\n",
      "Epoch 2, batch 8 D Loss: 1.3604313135147095, G Loss: 0.6408405303955078\n",
      "Epoch 2, batch 9 D Loss: 1.385711431503296, G Loss: 0.6345505714416504\n",
      "Epoch 2, batch 10 D Loss: 1.4590611457824707, G Loss: 0.6865724325180054\n",
      "Epoch 2, batch 11 D Loss: 1.312850832939148, G Loss: 0.628329873085022\n",
      "Epoch 2, batch 12 D Loss: 1.3986197710037231, G Loss: 0.7194756269454956\n",
      "Epoch 2, batch 13 D Loss: 1.3482321500778198, G Loss: 0.6666316390037537\n",
      "Epoch 2, batch 14 D Loss: 1.227668285369873, G Loss: 0.7070590257644653\n",
      "Epoch 2, batch 15 D Loss: 1.3555184602737427, G Loss: 0.6774663329124451\n",
      "Epoch 2, batch 16 D Loss: 1.3311858177185059, G Loss: 0.6694046854972839\n",
      "Epoch 2, batch 17 D Loss: 1.4327185153961182, G Loss: 0.6427758932113647\n",
      "Epoch 2, batch 18 D Loss: 1.291117787361145, G Loss: 0.6776084899902344\n",
      "Epoch 2, batch 19 D Loss: 1.3278383016586304, G Loss: 0.696444034576416\n",
      "Epoch 2, batch 20 D Loss: 1.2878103256225586, G Loss: 0.699390709400177\n",
      "Epoch 2, batch 21 D Loss: 1.2283810377120972, G Loss: 0.684872031211853\n",
      "Epoch 2, batch 22 D Loss: 1.3638275861740112, G Loss: 0.6637107133865356\n",
      "Epoch 2, batch 23 D Loss: 1.2517189979553223, G Loss: 0.7311546206474304\n",
      "Epoch 2, batch 24 D Loss: 1.3840422630310059, G Loss: 0.638414740562439\n",
      "Epoch 2, batch 25 D Loss: 1.3286683559417725, G Loss: 0.6783425807952881\n",
      "Epoch 2, batch 26 D Loss: 1.3247897624969482, G Loss: 0.680765688419342\n",
      "Epoch 2, batch 27 D Loss: 1.2926976680755615, G Loss: 0.6623820662498474\n",
      "Epoch 2, batch 28 D Loss: 1.213991403579712, G Loss: 0.6963986158370972\n",
      "Epoch 2, batch 29 D Loss: 1.1820321083068848, G Loss: 0.7264885902404785\n",
      "Epoch 2, batch 30 D Loss: 1.2894768714904785, G Loss: 0.6827607154846191\n",
      "Epoch 2, batch 31 D Loss: 1.436110258102417, G Loss: 0.7042316198348999\n",
      "Epoch 2, batch 32 D Loss: 1.229182243347168, G Loss: 0.7269436120986938\n",
      "Epoch 2, batch 33 D Loss: 1.3867979049682617, G Loss: 0.6191022992134094\n",
      "Epoch 2, batch 34 D Loss: 1.3452012538909912, G Loss: 0.6410748362541199\n",
      "Epoch 2, batch 35 D Loss: 1.2380013465881348, G Loss: 0.7001416087150574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 36 D Loss: 1.2151119709014893, G Loss: 0.686694324016571\n",
      "Epoch 2, batch 37 D Loss: 1.3698101043701172, G Loss: 0.6624295711517334\n",
      "Epoch 2, batch 38 D Loss: 1.4191926717758179, G Loss: 0.7380397319793701\n",
      "Epoch 2, batch 39 D Loss: 1.2690246105194092, G Loss: 0.6811984181404114\n",
      "Epoch 2, batch 40 D Loss: 1.172500491142273, G Loss: 0.7505702376365662\n",
      "Epoch 2, batch 41 D Loss: 1.2341904640197754, G Loss: 0.7218788266181946\n",
      "Epoch 2, batch 42 D Loss: 1.36381995677948, G Loss: 0.7000271677970886\n",
      "Epoch 2, batch 43 D Loss: 1.2128262519836426, G Loss: 0.7273072004318237\n",
      "Epoch 2, batch 44 D Loss: 1.338676929473877, G Loss: 0.6995827555656433\n",
      "Epoch 2, batch 45 D Loss: 1.267277717590332, G Loss: 0.711185097694397\n",
      "Epoch 2, batch 46 D Loss: 1.4794107675552368, G Loss: 0.6700713634490967\n",
      "Epoch 2, batch 47 D Loss: 1.3296923637390137, G Loss: 0.6644529104232788\n",
      "Epoch 2, batch 48 D Loss: 1.29657781124115, G Loss: 0.7411485910415649\n",
      "Epoch 2, batch 49 D Loss: 1.3345410823822021, G Loss: 0.6695876121520996\n",
      "Epoch 2, batch 50 D Loss: 1.3982675075531006, G Loss: 0.6964811682701111\n",
      "Epoch 2, batch 51 D Loss: 1.2364466190338135, G Loss: 0.7245055437088013\n",
      "Epoch 2, batch 52 D Loss: 1.2927601337432861, G Loss: 0.6729349493980408\n",
      "Epoch 2, batch 53 D Loss: 1.2336645126342773, G Loss: 0.7384606003761292\n",
      "Epoch 2, batch 54 D Loss: 1.2952706813812256, G Loss: 0.7103825211524963\n",
      "Epoch 2, batch 55 D Loss: 1.3136587142944336, G Loss: 0.6756930351257324\n",
      "Epoch 2, batch 56 D Loss: 1.304366111755371, G Loss: 0.7402809858322144\n",
      "Epoch 2, batch 57 D Loss: 1.2338659763336182, G Loss: 0.7874130010604858\n",
      "Epoch 2, batch 58 D Loss: 1.3152084350585938, G Loss: 0.6907223463058472\n",
      "Epoch 2, batch 59 D Loss: 1.3299362659454346, G Loss: 0.7115630507469177\n",
      "Epoch 2, batch 60 D Loss: 1.3416669368743896, G Loss: 0.703133225440979\n",
      "Epoch 2, batch 61 D Loss: 1.2812721729278564, G Loss: 0.7390815019607544\n",
      "Epoch 2, batch 62 D Loss: 1.2975327968597412, G Loss: 0.714451014995575\n",
      "Epoch 2, batch 63 D Loss: 1.2943646907806396, G Loss: 0.7770482897758484\n",
      "Epoch 2, batch 64 D Loss: 1.2807056903839111, G Loss: 0.7496504187583923\n",
      "Epoch 2, batch 65 D Loss: 1.4025273323059082, G Loss: 0.7303728461265564\n",
      "Epoch 2, batch 66 D Loss: 1.0602176189422607, G Loss: 0.7972107529640198\n",
      "Epoch 2, batch 67 D Loss: 1.2853715419769287, G Loss: 0.7455486059188843\n",
      "Epoch 2, batch 68 D Loss: 1.277766466140747, G Loss: 0.7213023900985718\n",
      "Epoch 2, batch 69 D Loss: 1.2907629013061523, G Loss: 0.7367486357688904\n",
      "Epoch 2, batch 70 D Loss: 1.2559527158737183, G Loss: 0.7911889553070068\n",
      "Epoch 2, batch 71 D Loss: 1.35447096824646, G Loss: 0.7140835523605347\n",
      "Epoch 2, batch 72 D Loss: 1.3118343353271484, G Loss: 0.7384375929832458\n",
      "Epoch 2, batch 73 D Loss: 1.2896182537078857, G Loss: 0.7466383576393127\n",
      "Epoch 2, batch 74 D Loss: 1.3170835971832275, G Loss: 0.7185750603675842\n",
      "Epoch 2, batch 75 D Loss: 1.3741061687469482, G Loss: 0.7349144220352173\n",
      "Epoch 2, batch 76 D Loss: 1.198742151260376, G Loss: 0.7178505063056946\n",
      "Epoch 2, batch 77 D Loss: 1.2813338041305542, G Loss: 0.7507253885269165\n",
      "Epoch 2, batch 78 D Loss: 1.3016917705535889, G Loss: 0.7220523357391357\n",
      "Epoch 2, batch 79 D Loss: 1.345886468887329, G Loss: 0.7192562818527222\n",
      "Epoch 2, batch 80 D Loss: 1.273112177848816, G Loss: 0.7500045895576477\n",
      "Epoch 2, batch 81 D Loss: 1.3552520275115967, G Loss: 0.7422307729721069\n",
      "Epoch 2, batch 82 D Loss: 1.189352035522461, G Loss: 0.7511776685714722\n",
      "Epoch 2, batch 83 D Loss: 1.3306055068969727, G Loss: 0.7353919744491577\n",
      "Epoch 2, batch 84 D Loss: 1.2576158046722412, G Loss: 0.7388498783111572\n",
      "Epoch 2, batch 85 D Loss: 1.1381787061691284, G Loss: 0.8023012280464172\n",
      "Epoch 2, batch 86 D Loss: 1.29127836227417, G Loss: 0.7531150579452515\n",
      "Epoch 2, batch 87 D Loss: 1.29547119140625, G Loss: 0.7474179863929749\n",
      "Epoch 2, batch 88 D Loss: 1.3497209548950195, G Loss: 0.7320734262466431\n",
      "Epoch 2, batch 89 D Loss: 1.2709786891937256, G Loss: 0.7395010590553284\n",
      "Epoch 2, batch 90 D Loss: 1.166602611541748, G Loss: 0.7657586932182312\n",
      "Epoch 2, batch 91 D Loss: 1.2952182292938232, G Loss: 0.7744235396385193\n",
      "Epoch 2, batch 92 D Loss: 1.1379084587097168, G Loss: 0.813997745513916\n",
      "Epoch 2, batch 93 D Loss: 1.3105430603027344, G Loss: 0.7688724994659424\n",
      "Epoch 2, batch 94 D Loss: 1.254983901977539, G Loss: 0.821128249168396\n",
      "Epoch 2, batch 95 D Loss: 1.2158148288726807, G Loss: 0.793195366859436\n",
      "Epoch 2, batch 96 D Loss: 1.2813637256622314, G Loss: 0.8654956817626953\n",
      "Epoch 2, batch 97 D Loss: 1.3443288803100586, G Loss: 0.7824344635009766\n",
      "Epoch 2, batch 98 D Loss: 1.2177472114562988, G Loss: 0.8397144079208374\n",
      "Epoch 2, batch 99 D Loss: 1.3169881105422974, G Loss: 0.8224190473556519\n",
      "Epoch 2, batch 100 D Loss: 1.2278521060943604, G Loss: 0.853808581829071\n",
      "Epoch 2, batch 101 D Loss: 1.3125145435333252, G Loss: 0.7996009588241577\n",
      "Epoch 2, batch 102 D Loss: 1.247819423675537, G Loss: 0.8480796217918396\n",
      "Epoch 2, batch 103 D Loss: 1.2198795080184937, G Loss: 0.8611047863960266\n",
      "Epoch 2, batch 104 D Loss: 1.3008289337158203, G Loss: 0.8142073750495911\n",
      "Epoch 2, batch 105 D Loss: 1.1810870170593262, G Loss: 0.8291721940040588\n",
      "Epoch 2, batch 106 D Loss: 1.3516303300857544, G Loss: 0.8295552134513855\n",
      "Epoch 2, batch 107 D Loss: 1.2770226001739502, G Loss: 0.8601793646812439\n",
      "Epoch 2, batch 108 D Loss: 1.3129372596740723, G Loss: 0.8444194197654724\n",
      "Epoch 2, batch 109 D Loss: 1.10398268699646, G Loss: 0.8341385722160339\n",
      "Epoch 2, batch 110 D Loss: 1.0888490676879883, G Loss: 0.8519040942192078\n",
      "Epoch 2, batch 111 D Loss: 1.1981053352355957, G Loss: 0.8682812452316284\n",
      "Epoch 2, batch 112 D Loss: 1.2684001922607422, G Loss: 0.8720220923423767\n",
      "Epoch 2, batch 113 D Loss: 1.2574708461761475, G Loss: 0.8642057180404663\n",
      "Epoch 2, batch 114 D Loss: 1.1512936353683472, G Loss: 0.8847081661224365\n",
      "Epoch 2, batch 115 D Loss: 1.16800057888031, G Loss: 0.8890078067779541\n",
      "Epoch 2, batch 116 D Loss: 1.333369493484497, G Loss: 0.8890383243560791\n",
      "Epoch 2, batch 117 D Loss: 1.1703095436096191, G Loss: 0.8892382979393005\n",
      "Epoch 2, batch 118 D Loss: 1.1820087432861328, G Loss: 0.8947238326072693\n",
      "Epoch 2, batch 119 D Loss: 1.1507185697555542, G Loss: 0.8988720774650574\n",
      "Epoch 2, batch 120 D Loss: 1.1812041997909546, G Loss: 0.9160944223403931\n",
      "Epoch 2, batch 121 D Loss: 1.1897149085998535, G Loss: 0.9287545084953308\n",
      "Epoch 2, batch 122 D Loss: 1.190061330795288, G Loss: 0.9065587520599365\n",
      "Epoch 2, batch 123 D Loss: 1.1924543380737305, G Loss: 0.9408408403396606\n",
      "Epoch 2, batch 124 D Loss: 1.1316907405853271, G Loss: 0.9568195939064026\n",
      "Epoch 2, batch 125 D Loss: 1.0408577919006348, G Loss: 0.9385591149330139\n",
      "Epoch 2, batch 126 D Loss: 1.193457007408142, G Loss: 0.9627023935317993\n",
      "Epoch 2, batch 127 D Loss: 1.0394078493118286, G Loss: 0.9682941436767578\n",
      "Epoch 2, batch 128 D Loss: 1.0921200513839722, G Loss: 0.9646560549736023\n",
      "Epoch 2, batch 129 D Loss: 1.2308692932128906, G Loss: 0.9914358258247375\n",
      "Epoch 2, batch 130 D Loss: 1.1586964130401611, G Loss: 0.9620184302330017\n",
      "Epoch 2, batch 131 D Loss: 1.1426405906677246, G Loss: 0.9755495190620422\n",
      "Epoch 2, batch 132 D Loss: 1.104630947113037, G Loss: 0.9942176342010498\n",
      "Epoch 2, batch 133 D Loss: 1.1189539432525635, G Loss: 0.9950354099273682\n",
      "Epoch 2, batch 134 D Loss: 1.1926016807556152, G Loss: 1.010949730873108\n",
      "Epoch 2, batch 135 D Loss: 1.1813925504684448, G Loss: 1.0089472532272339\n",
      "Epoch 2, batch 136 D Loss: 1.0357669591903687, G Loss: 0.9933520555496216\n",
      "Epoch 2, batch 137 D Loss: 1.1094307899475098, G Loss: 1.0088415145874023\n",
      "Epoch 2, batch 138 D Loss: 1.0825941562652588, G Loss: 0.997403621673584\n",
      "Epoch 2, batch 139 D Loss: 1.1226829290390015, G Loss: 1.0121208429336548\n",
      "Epoch 2, batch 140 D Loss: 1.0438823699951172, G Loss: 1.0087624788284302\n",
      "Epoch 2, batch 141 D Loss: 0.966338574886322, G Loss: 1.0148462057113647\n",
      "Epoch 2, batch 142 D Loss: 1.0860685110092163, G Loss: 1.0130730867385864\n",
      "Epoch 2, batch 143 D Loss: 1.116842269897461, G Loss: 1.022705078125\n",
      "Epoch 2, batch 144 D Loss: 1.0690999031066895, G Loss: 1.020963430404663\n",
      "Epoch 2, batch 145 D Loss: 1.0148687362670898, G Loss: 1.0555213689804077\n",
      "Epoch 2, batch 146 D Loss: 0.9856745004653931, G Loss: 1.037023901939392\n",
      "Epoch 2, batch 147 D Loss: 1.1436858177185059, G Loss: 1.0053789615631104\n",
      "Epoch 2, batch 148 D Loss: 1.2131507396697998, G Loss: 1.0021793842315674\n",
      "Epoch 2, batch 149 D Loss: 1.1127488613128662, G Loss: 1.0265461206436157\n",
      "Epoch 2, batch 150 D Loss: 1.0512691736221313, G Loss: 1.026943564414978\n",
      "Epoch 2, batch 151 D Loss: 0.9457621574401855, G Loss: 1.0404767990112305\n",
      "Epoch 2, batch 152 D Loss: 1.1522818803787231, G Loss: 0.9703283905982971\n",
      "Epoch 2, batch 153 D Loss: 1.1051384210586548, G Loss: 1.0369980335235596\n",
      "Epoch 2, batch 154 D Loss: 1.196559190750122, G Loss: 0.9739981293678284\n",
      "Epoch 2, batch 155 D Loss: 1.0986831188201904, G Loss: 0.9835168719291687\n",
      "Epoch 2, batch 156 D Loss: 1.1188545227050781, G Loss: 0.9894095659255981\n",
      "Epoch 2, batch 157 D Loss: 1.0645469427108765, G Loss: 0.9968868494033813\n",
      "Epoch 2, batch 158 D Loss: 1.0662851333618164, G Loss: 1.0345542430877686\n",
      "Epoch 2, batch 159 D Loss: 1.065488338470459, G Loss: 0.9760656952857971\n",
      "Epoch 2, batch 160 D Loss: 1.0750677585601807, G Loss: 0.961191713809967\n",
      "Epoch 2, batch 161 D Loss: 1.024397611618042, G Loss: 0.9915431141853333\n",
      "Epoch 2, batch 162 D Loss: 1.1935104131698608, G Loss: 0.9229178428649902\n",
      "Epoch 2, batch 163 D Loss: 1.11891770362854, G Loss: 0.9718393087387085\n",
      "Epoch 2, batch 164 D Loss: 0.9785598516464233, G Loss: 1.016107439994812\n",
      "Epoch 2, batch 165 D Loss: 1.0910742282867432, G Loss: 0.9356836080551147\n",
      "Epoch 2, batch 166 D Loss: 1.0537999868392944, G Loss: 0.9831682443618774\n",
      "Epoch 2, batch 167 D Loss: 1.1326234340667725, G Loss: 0.9963790774345398\n",
      "Epoch 2, batch 168 D Loss: 1.0577335357666016, G Loss: 0.9576631188392639\n",
      "Epoch 2, batch 169 D Loss: 0.99980628490448, G Loss: 1.0230563879013062\n",
      "Epoch 2, batch 170 D Loss: 1.1687041521072388, G Loss: 0.9721412658691406\n",
      "Epoch 2, batch 171 D Loss: 1.000489592552185, G Loss: 0.9566574692726135\n",
      "Epoch 2, batch 172 D Loss: 1.0510268211364746, G Loss: 0.9372962117195129\n",
      "Epoch 2, batch 173 D Loss: 1.16111421585083, G Loss: 0.9754112362861633\n",
      "Epoch 2, batch 174 D Loss: 1.1064517498016357, G Loss: 0.9838363528251648\n",
      "Epoch 2, batch 175 D Loss: 1.1978319883346558, G Loss: 0.920683741569519\n",
      "Epoch 2, batch 176 D Loss: 1.2014553546905518, G Loss: 0.9262188076972961\n",
      "Epoch 2, batch 177 D Loss: 1.2148100137710571, G Loss: 0.9348859190940857\n",
      "Epoch 2, batch 178 D Loss: 1.0615816116333008, G Loss: 0.9607165455818176\n",
      "Epoch 2, batch 179 D Loss: 1.1841294765472412, G Loss: 0.9078118801116943\n",
      "Epoch 2, batch 180 D Loss: 1.150048017501831, G Loss: 0.9424852728843689\n",
      "Epoch 2, batch 181 D Loss: 1.1478554010391235, G Loss: 0.9232662916183472\n",
      "Epoch 2, batch 182 D Loss: 1.147531270980835, G Loss: 0.9033759832382202\n",
      "Epoch 2, batch 183 D Loss: 0.9983587265014648, G Loss: 0.897647500038147\n",
      "Epoch 2, batch 184 D Loss: 1.1450400352478027, G Loss: 0.9093734622001648\n",
      "Epoch 2, batch 185 D Loss: 1.2236299514770508, G Loss: 0.8124364614486694\n",
      "Epoch 2, batch 186 D Loss: 1.101065754890442, G Loss: 0.9235299825668335\n",
      "Epoch 2, batch 187 D Loss: 0.9757304191589355, G Loss: 0.980511486530304\n",
      "Epoch 2, batch 188 D Loss: 1.0121217966079712, G Loss: 0.9169302582740784\n",
      "Epoch 2, batch 189 D Loss: 1.0630064010620117, G Loss: 0.9454419016838074\n",
      "Epoch 2, batch 190 D Loss: 1.0160608291625977, G Loss: 0.9180945754051208\n",
      "Epoch 2, batch 191 D Loss: 1.1565749645233154, G Loss: 0.9049263000488281\n",
      "Epoch 2, batch 192 D Loss: 1.1784535646438599, G Loss: 0.8499515056610107\n",
      "Epoch 2, batch 193 D Loss: 1.1166138648986816, G Loss: 0.9166491627693176\n",
      "Epoch 2, batch 194 D Loss: 1.0409640073776245, G Loss: 0.8904070258140564\n",
      "Epoch 2, batch 195 D Loss: 1.0483523607254028, G Loss: 0.893082857131958\n",
      "Epoch 2, batch 196 D Loss: 1.238867998123169, G Loss: 0.8957172632217407\n",
      "Epoch 2, batch 197 D Loss: 1.1503689289093018, G Loss: 0.8920447826385498\n",
      "Epoch 2, batch 198 D Loss: 1.1343975067138672, G Loss: 0.8579586148262024\n",
      "Epoch 2, batch 199 D Loss: 1.1765775680541992, G Loss: 0.9055562019348145\n",
      "Epoch 2, batch 200 D Loss: 1.1565814018249512, G Loss: 0.8831825256347656\n",
      "Epoch 3, batch 1 D Loss: 1.2175941467285156, G Loss: 0.8978533148765564\n",
      "Epoch 3, batch 2 D Loss: 1.2049100399017334, G Loss: 0.871846079826355\n",
      "Epoch 3, batch 3 D Loss: 1.1395771503448486, G Loss: 0.8739473819732666\n",
      "Epoch 3, batch 4 D Loss: 1.0981358289718628, G Loss: 0.8980960845947266\n",
      "Epoch 3, batch 5 D Loss: 1.1018118858337402, G Loss: 0.9167136549949646\n",
      "Epoch 3, batch 6 D Loss: 1.2544589042663574, G Loss: 0.8202269077301025\n",
      "Epoch 3, batch 7 D Loss: 1.205244779586792, G Loss: 0.9099240303039551\n",
      "Epoch 3, batch 8 D Loss: 1.109745740890503, G Loss: 0.9215660095214844\n",
      "Epoch 3, batch 9 D Loss: 1.1554572582244873, G Loss: 0.8339911103248596\n",
      "Epoch 3, batch 10 D Loss: 1.1413476467132568, G Loss: 0.9104825854301453\n",
      "Epoch 3, batch 11 D Loss: 1.2232732772827148, G Loss: 0.9281532764434814\n",
      "Epoch 3, batch 12 D Loss: 1.1768652200698853, G Loss: 0.8991484642028809\n",
      "Epoch 3, batch 13 D Loss: 1.171772837638855, G Loss: 0.9026868343353271\n",
      "Epoch 3, batch 14 D Loss: 1.062995195388794, G Loss: 0.9120761752128601\n",
      "Epoch 3, batch 15 D Loss: 1.2414178848266602, G Loss: 0.9407840967178345\n",
      "Epoch 3, batch 16 D Loss: 1.1361463069915771, G Loss: 0.9101536273956299\n",
      "Epoch 3, batch 17 D Loss: 1.1421010494232178, G Loss: 0.9066938161849976\n",
      "Epoch 3, batch 18 D Loss: 1.0748931169509888, G Loss: 0.9665943384170532\n",
      "Epoch 3, batch 19 D Loss: 1.11482834815979, G Loss: 0.9346896409988403\n",
      "Epoch 3, batch 20 D Loss: 1.1926419734954834, G Loss: 0.9336828589439392\n",
      "Epoch 3, batch 21 D Loss: 1.1100366115570068, G Loss: 0.9444370865821838\n",
      "Epoch 3, batch 22 D Loss: 1.135002851486206, G Loss: 0.9271130561828613\n",
      "Epoch 3, batch 23 D Loss: 1.1672500371932983, G Loss: 0.9879984259605408\n",
      "Epoch 3, batch 24 D Loss: 0.9757298231124878, G Loss: 0.9131042957305908\n",
      "Epoch 3, batch 25 D Loss: 1.2176002264022827, G Loss: 0.9242753386497498\n",
      "Epoch 3, batch 26 D Loss: 1.002835750579834, G Loss: 0.9093021154403687\n",
      "Epoch 3, batch 27 D Loss: 1.0125775337219238, G Loss: 1.0080097913742065\n",
      "Epoch 3, batch 28 D Loss: 1.1180872917175293, G Loss: 0.9776613712310791\n",
      "Epoch 3, batch 29 D Loss: 1.114391803741455, G Loss: 0.9634179472923279\n",
      "Epoch 3, batch 30 D Loss: 1.1028155088424683, G Loss: 0.9915071129798889\n",
      "Epoch 3, batch 31 D Loss: 1.1137951612472534, G Loss: 1.0514506101608276\n",
      "Epoch 3, batch 32 D Loss: 1.0119200944900513, G Loss: 1.018546223640442\n",
      "Epoch 3, batch 33 D Loss: 1.0890685319900513, G Loss: 1.0212453603744507\n",
      "Epoch 3, batch 34 D Loss: 1.1474825143814087, G Loss: 1.0360561609268188\n",
      "Epoch 3, batch 35 D Loss: 1.1440446376800537, G Loss: 1.0582729578018188\n",
      "Epoch 3, batch 36 D Loss: 1.118416428565979, G Loss: 1.0409824848175049\n",
      "Epoch 3, batch 37 D Loss: 1.237107753753662, G Loss: 0.984158456325531\n",
      "Epoch 3, batch 38 D Loss: 1.0578253269195557, G Loss: 1.098891258239746\n",
      "Epoch 3, batch 39 D Loss: 1.1375980377197266, G Loss: 1.0469967126846313\n",
      "Epoch 3, batch 40 D Loss: 1.1459400653839111, G Loss: 1.0197616815567017\n",
      "Epoch 3, batch 41 D Loss: 1.0783748626708984, G Loss: 1.0196219682693481\n",
      "Epoch 3, batch 42 D Loss: 1.1933338642120361, G Loss: 1.0313732624053955\n",
      "Epoch 3, batch 43 D Loss: 1.1347370147705078, G Loss: 1.0461968183517456\n",
      "Epoch 3, batch 44 D Loss: 1.1321611404418945, G Loss: 1.0894584655761719\n",
      "Epoch 3, batch 45 D Loss: 1.0705019235610962, G Loss: 1.0309540033340454\n",
      "Epoch 3, batch 46 D Loss: 1.2873992919921875, G Loss: 1.0179588794708252\n",
      "Epoch 3, batch 47 D Loss: 1.2924652099609375, G Loss: 0.9947953820228577\n",
      "Epoch 3, batch 48 D Loss: 1.0273170471191406, G Loss: 0.9960847496986389\n",
      "Epoch 3, batch 49 D Loss: 1.138061761856079, G Loss: 1.037670612335205\n",
      "Epoch 3, batch 50 D Loss: 1.0633797645568848, G Loss: 1.0688786506652832\n",
      "Epoch 3, batch 51 D Loss: 1.1863337755203247, G Loss: 1.0212286710739136\n",
      "Epoch 3, batch 52 D Loss: 1.0483700037002563, G Loss: 1.13569974899292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, batch 53 D Loss: 1.1767258644104004, G Loss: 1.0569778680801392\n",
      "Epoch 3, batch 54 D Loss: 1.05368173122406, G Loss: 1.0052599906921387\n",
      "Epoch 3, batch 55 D Loss: 1.194150447845459, G Loss: 1.0362308025360107\n",
      "Epoch 3, batch 56 D Loss: 1.1398677825927734, G Loss: 1.0914109945297241\n",
      "Epoch 3, batch 57 D Loss: 1.118438959121704, G Loss: 1.126818299293518\n",
      "Epoch 3, batch 58 D Loss: 1.1659252643585205, G Loss: 1.0276870727539062\n",
      "Epoch 3, batch 59 D Loss: 1.1120171546936035, G Loss: 0.9986006021499634\n",
      "Epoch 3, batch 60 D Loss: 1.0271179676055908, G Loss: 1.0858086347579956\n",
      "Epoch 3, batch 61 D Loss: 1.0715782642364502, G Loss: 1.0577845573425293\n",
      "Epoch 3, batch 62 D Loss: 1.089979887008667, G Loss: 1.0707486867904663\n",
      "Epoch 3, batch 63 D Loss: 1.0295199155807495, G Loss: 1.1035981178283691\n",
      "Epoch 3, batch 64 D Loss: 1.1359909772872925, G Loss: 1.0685997009277344\n",
      "Epoch 3, batch 65 D Loss: 1.1584117412567139, G Loss: 1.0707017183303833\n",
      "Epoch 3, batch 66 D Loss: 1.1377002000808716, G Loss: 1.0572563409805298\n",
      "Epoch 3, batch 67 D Loss: 1.1939061880111694, G Loss: 1.0343408584594727\n",
      "Epoch 3, batch 68 D Loss: 1.1068294048309326, G Loss: 1.0929893255233765\n",
      "Epoch 3, batch 69 D Loss: 1.055092692375183, G Loss: 1.1605185270309448\n",
      "Epoch 3, batch 70 D Loss: 0.9990396499633789, G Loss: 1.102457880973816\n",
      "Epoch 3, batch 71 D Loss: 1.154822587966919, G Loss: 1.078200101852417\n",
      "Epoch 3, batch 72 D Loss: 0.9242256879806519, G Loss: 1.2243572473526\n",
      "Epoch 3, batch 73 D Loss: 1.0122284889221191, G Loss: 1.0713555812835693\n",
      "Epoch 3, batch 74 D Loss: 1.0367827415466309, G Loss: 1.106041669845581\n",
      "Epoch 3, batch 75 D Loss: 1.0982545614242554, G Loss: 1.037838101387024\n",
      "Epoch 3, batch 76 D Loss: 0.9771651029586792, G Loss: 1.102665662765503\n",
      "Epoch 3, batch 77 D Loss: 1.0697481632232666, G Loss: 1.1404485702514648\n",
      "Epoch 3, batch 78 D Loss: 1.0874470472335815, G Loss: 1.124267339706421\n",
      "Epoch 3, batch 79 D Loss: 1.190575122833252, G Loss: 1.0140202045440674\n",
      "Epoch 3, batch 80 D Loss: 1.1021939516067505, G Loss: 1.091355562210083\n",
      "Epoch 3, batch 81 D Loss: 1.3596978187561035, G Loss: 1.0330052375793457\n",
      "Epoch 3, batch 82 D Loss: 1.2038543224334717, G Loss: 1.067561149597168\n",
      "Epoch 3, batch 83 D Loss: 1.2522532939910889, G Loss: 1.0497373342514038\n",
      "Epoch 3, batch 84 D Loss: 1.0906360149383545, G Loss: 1.0781123638153076\n",
      "Epoch 3, batch 85 D Loss: 1.1342347860336304, G Loss: 1.1181522607803345\n",
      "Epoch 3, batch 86 D Loss: 0.8535284996032715, G Loss: 1.19382643699646\n",
      "Epoch 3, batch 87 D Loss: 1.081363320350647, G Loss: 1.0777958631515503\n",
      "Epoch 3, batch 88 D Loss: 1.1518455743789673, G Loss: 1.1095666885375977\n",
      "Epoch 3, batch 89 D Loss: 1.1955963373184204, G Loss: 1.08893620967865\n",
      "Epoch 3, batch 90 D Loss: 1.08280611038208, G Loss: 1.1275336742401123\n",
      "Epoch 3, batch 91 D Loss: 1.1025724411010742, G Loss: 1.124284029006958\n",
      "Epoch 3, batch 92 D Loss: 1.176971435546875, G Loss: 1.1213867664337158\n",
      "Epoch 3, batch 93 D Loss: 1.1466615200042725, G Loss: 1.0557713508605957\n",
      "Epoch 3, batch 94 D Loss: 0.9271754026412964, G Loss: 1.0930895805358887\n",
      "Epoch 3, batch 95 D Loss: 1.0950077772140503, G Loss: 1.124584674835205\n",
      "Epoch 3, batch 96 D Loss: 1.04430091381073, G Loss: 1.1676685810089111\n",
      "Epoch 3, batch 97 D Loss: 1.283770203590393, G Loss: 1.1619881391525269\n",
      "Epoch 3, batch 98 D Loss: 0.9110475778579712, G Loss: 1.1305029392242432\n",
      "Epoch 3, batch 99 D Loss: 1.1052782535552979, G Loss: 1.1086392402648926\n",
      "Epoch 3, batch 100 D Loss: 1.2098348140716553, G Loss: 1.147973895072937\n",
      "Epoch 3, batch 101 D Loss: 1.13933265209198, G Loss: 1.1122581958770752\n",
      "Epoch 3, batch 102 D Loss: 1.0941530466079712, G Loss: 1.1032111644744873\n",
      "Epoch 3, batch 103 D Loss: 0.99150151014328, G Loss: 1.1470688581466675\n",
      "Epoch 3, batch 104 D Loss: 0.9014526605606079, G Loss: 1.1255292892456055\n",
      "Epoch 3, batch 105 D Loss: 1.1262211799621582, G Loss: 1.085410714149475\n",
      "Epoch 3, batch 106 D Loss: 1.2433035373687744, G Loss: 1.0461440086364746\n",
      "Epoch 3, batch 107 D Loss: 1.2202842235565186, G Loss: 1.0719050168991089\n",
      "Epoch 3, batch 108 D Loss: 1.1079753637313843, G Loss: 1.1320161819458008\n",
      "Epoch 3, batch 109 D Loss: 1.0429275035858154, G Loss: 1.099002718925476\n",
      "Epoch 3, batch 110 D Loss: 1.2054338455200195, G Loss: 1.1373401880264282\n",
      "Epoch 3, batch 111 D Loss: 1.1089050769805908, G Loss: 1.1373800039291382\n",
      "Epoch 3, batch 112 D Loss: 0.9749132394790649, G Loss: 1.083310604095459\n",
      "Epoch 3, batch 113 D Loss: 1.271935224533081, G Loss: 1.1344431638717651\n",
      "Epoch 3, batch 114 D Loss: 1.0691969394683838, G Loss: 1.1060045957565308\n",
      "Epoch 3, batch 115 D Loss: 0.968066930770874, G Loss: 1.0993621349334717\n",
      "Epoch 3, batch 116 D Loss: 1.07928466796875, G Loss: 1.130132794380188\n",
      "Epoch 3, batch 117 D Loss: 1.080305576324463, G Loss: 1.1137679815292358\n",
      "Epoch 3, batch 118 D Loss: 0.9549217224121094, G Loss: 1.095401644706726\n",
      "Epoch 3, batch 119 D Loss: 1.1722886562347412, G Loss: 1.112618327140808\n",
      "Epoch 3, batch 120 D Loss: 0.9742079973220825, G Loss: 1.0910872220993042\n",
      "Epoch 3, batch 121 D Loss: 0.9191168546676636, G Loss: 1.154748797416687\n",
      "Epoch 3, batch 122 D Loss: 1.2867350578308105, G Loss: 1.137176275253296\n",
      "Epoch 3, batch 123 D Loss: 1.1189250946044922, G Loss: 1.1381782293319702\n",
      "Epoch 3, batch 124 D Loss: 1.2705365419387817, G Loss: 1.1350399255752563\n",
      "Epoch 3, batch 125 D Loss: 1.0663444995880127, G Loss: 1.1074796915054321\n",
      "Epoch 3, batch 126 D Loss: 1.0125302076339722, G Loss: 1.147730827331543\n",
      "Epoch 3, batch 127 D Loss: 1.1480343341827393, G Loss: 1.1374309062957764\n",
      "Epoch 3, batch 128 D Loss: 0.9243152737617493, G Loss: 1.110603928565979\n",
      "Epoch 3, batch 129 D Loss: 1.0388115644454956, G Loss: 1.1237246990203857\n",
      "Epoch 3, batch 130 D Loss: 1.1074354648590088, G Loss: 1.0977956056594849\n",
      "Epoch 3, batch 131 D Loss: 0.9395212531089783, G Loss: 1.1011993885040283\n",
      "Epoch 3, batch 132 D Loss: 0.9927852153778076, G Loss: 1.1260061264038086\n",
      "Epoch 3, batch 133 D Loss: 1.1063871383666992, G Loss: 1.1139512062072754\n",
      "Epoch 3, batch 134 D Loss: 1.3385543823242188, G Loss: 1.0516186952590942\n",
      "Epoch 3, batch 135 D Loss: 1.0726351737976074, G Loss: 1.1058787107467651\n",
      "Epoch 3, batch 136 D Loss: 1.1183336973190308, G Loss: 1.1109707355499268\n",
      "Epoch 3, batch 137 D Loss: 1.2456330060958862, G Loss: 1.0398463010787964\n",
      "Epoch 3, batch 138 D Loss: 1.2404990196228027, G Loss: 1.084062099456787\n",
      "Epoch 3, batch 139 D Loss: 1.305993914604187, G Loss: 1.031579852104187\n",
      "Epoch 3, batch 140 D Loss: 1.1245192289352417, G Loss: 1.0406029224395752\n",
      "Epoch 3, batch 141 D Loss: 1.3306604623794556, G Loss: 1.0329387187957764\n",
      "Epoch 3, batch 142 D Loss: 1.3181698322296143, G Loss: 0.9974278211593628\n",
      "Epoch 3, batch 143 D Loss: 1.2692325115203857, G Loss: 0.993455171585083\n",
      "Epoch 3, batch 144 D Loss: 1.3057680130004883, G Loss: 1.0464215278625488\n",
      "Epoch 3, batch 145 D Loss: 1.3319642543792725, G Loss: 0.9687806963920593\n",
      "Epoch 3, batch 146 D Loss: 1.1483466625213623, G Loss: 0.9690273404121399\n",
      "Epoch 3, batch 147 D Loss: 1.2454586029052734, G Loss: 0.9295694828033447\n",
      "Epoch 3, batch 148 D Loss: 1.0567799806594849, G Loss: 0.9049105048179626\n",
      "Epoch 3, batch 149 D Loss: 1.309786319732666, G Loss: 0.9585949182510376\n",
      "Epoch 3, batch 150 D Loss: 1.3194475173950195, G Loss: 0.9352082014083862\n",
      "Epoch 3, batch 151 D Loss: 1.3096617460250854, G Loss: 0.9332743287086487\n",
      "Epoch 3, batch 152 D Loss: 1.3422906398773193, G Loss: 0.9022670984268188\n",
      "Epoch 3, batch 153 D Loss: 1.1493630409240723, G Loss: 0.9370914697647095\n",
      "Epoch 3, batch 154 D Loss: 1.4395415782928467, G Loss: 0.8574537038803101\n",
      "Epoch 3, batch 155 D Loss: 1.3150253295898438, G Loss: 0.8630552887916565\n",
      "Epoch 3, batch 156 D Loss: 1.3626058101654053, G Loss: 0.9006468057632446\n",
      "Epoch 3, batch 157 D Loss: 1.264615535736084, G Loss: 0.9036778807640076\n",
      "Epoch 3, batch 158 D Loss: 1.199858546257019, G Loss: 0.9027356505393982\n",
      "Epoch 3, batch 159 D Loss: 1.2611111402511597, G Loss: 0.9257592558860779\n",
      "Epoch 3, batch 160 D Loss: 1.3837416172027588, G Loss: 0.9311572313308716\n",
      "Epoch 3, batch 161 D Loss: 1.3307034969329834, G Loss: 0.9066958427429199\n",
      "Epoch 3, batch 162 D Loss: 1.2798359394073486, G Loss: 0.8523637652397156\n",
      "Epoch 3, batch 163 D Loss: 1.3956083059310913, G Loss: 0.8753942251205444\n",
      "Epoch 3, batch 164 D Loss: 1.7456071376800537, G Loss: 0.9604211449623108\n",
      "Epoch 3, batch 165 D Loss: 1.3636845350265503, G Loss: 0.9120434522628784\n",
      "Epoch 3, batch 166 D Loss: 1.5281537771224976, G Loss: 0.9061863422393799\n",
      "Epoch 3, batch 167 D Loss: 1.3235132694244385, G Loss: 0.9590622782707214\n",
      "Epoch 3, batch 168 D Loss: 1.1999564170837402, G Loss: 0.9553030133247375\n",
      "Epoch 3, batch 169 D Loss: 1.440953016281128, G Loss: 0.9439190030097961\n",
      "Epoch 3, batch 170 D Loss: 1.4437682628631592, G Loss: 0.9567217826843262\n",
      "Epoch 3, batch 171 D Loss: 1.3427046537399292, G Loss: 0.9761433601379395\n",
      "Epoch 3, batch 172 D Loss: 1.4236409664154053, G Loss: 0.9930717349052429\n",
      "Epoch 3, batch 173 D Loss: 1.2251477241516113, G Loss: 0.9624946713447571\n",
      "Epoch 3, batch 174 D Loss: 1.423635482788086, G Loss: 1.0224803686141968\n",
      "Epoch 3, batch 175 D Loss: 1.440803050994873, G Loss: 0.9558533430099487\n",
      "Epoch 3, batch 176 D Loss: 1.351086974143982, G Loss: 1.1368076801300049\n",
      "Epoch 3, batch 177 D Loss: 1.4576942920684814, G Loss: 1.1096336841583252\n",
      "Epoch 3, batch 178 D Loss: 1.2732123136520386, G Loss: 1.0615031719207764\n",
      "Epoch 3, batch 179 D Loss: 1.3505761623382568, G Loss: 1.1703743934631348\n",
      "Epoch 3, batch 180 D Loss: 1.7115154266357422, G Loss: 1.069214940071106\n",
      "Epoch 3, batch 181 D Loss: 1.4393362998962402, G Loss: 1.1454979181289673\n",
      "Epoch 3, batch 182 D Loss: 1.3841708898544312, G Loss: 1.0554286241531372\n",
      "Epoch 3, batch 183 D Loss: 1.2684576511383057, G Loss: 1.0961456298828125\n",
      "Epoch 3, batch 184 D Loss: 1.1679315567016602, G Loss: 1.151483416557312\n",
      "Epoch 3, batch 185 D Loss: 1.523261547088623, G Loss: 1.1916978359222412\n",
      "Epoch 3, batch 186 D Loss: 1.3489148616790771, G Loss: 1.194444179534912\n",
      "Epoch 3, batch 187 D Loss: 1.3964930772781372, G Loss: 1.0988438129425049\n",
      "Epoch 3, batch 188 D Loss: 1.0117270946502686, G Loss: 1.1593043804168701\n",
      "Epoch 3, batch 189 D Loss: 1.650396704673767, G Loss: 1.1516700983047485\n",
      "Epoch 3, batch 190 D Loss: 1.4263650178909302, G Loss: 1.2330312728881836\n",
      "Epoch 3, batch 191 D Loss: 1.5364692211151123, G Loss: 1.2543106079101562\n",
      "Epoch 3, batch 192 D Loss: 1.2764309644699097, G Loss: 1.350818157196045\n",
      "Epoch 3, batch 193 D Loss: 1.130138874053955, G Loss: 1.230445146560669\n",
      "Epoch 3, batch 194 D Loss: 1.1362982988357544, G Loss: 1.2390093803405762\n",
      "Epoch 3, batch 195 D Loss: 1.358804702758789, G Loss: 1.066219449043274\n",
      "Epoch 3, batch 196 D Loss: 1.318890929222107, G Loss: 1.327650785446167\n",
      "Epoch 3, batch 197 D Loss: 1.14241623878479, G Loss: 1.1181505918502808\n",
      "Epoch 3, batch 198 D Loss: 1.2776315212249756, G Loss: 1.2663213014602661\n",
      "Epoch 3, batch 199 D Loss: 1.300870418548584, G Loss: 1.1381558179855347\n",
      "Epoch 3, batch 200 D Loss: 1.303572416305542, G Loss: 1.237078070640564\n",
      "Epoch 4, batch 1 D Loss: 1.1355760097503662, G Loss: 1.1941649913787842\n",
      "Epoch 4, batch 2 D Loss: 1.6476659774780273, G Loss: 1.1002202033996582\n",
      "Epoch 4, batch 3 D Loss: 1.4074034690856934, G Loss: 1.2183843851089478\n",
      "Epoch 4, batch 4 D Loss: 1.2852678298950195, G Loss: 1.3518928289413452\n",
      "Epoch 4, batch 5 D Loss: 1.4107418060302734, G Loss: 1.2813355922698975\n",
      "Epoch 4, batch 6 D Loss: 1.2565988302230835, G Loss: 1.2325025796890259\n",
      "Epoch 4, batch 7 D Loss: 1.4483858346939087, G Loss: 1.0240778923034668\n",
      "Epoch 4, batch 8 D Loss: 1.4487500190734863, G Loss: 1.043903112411499\n",
      "Epoch 4, batch 9 D Loss: 1.4501551389694214, G Loss: 1.1471011638641357\n",
      "Epoch 4, batch 10 D Loss: 1.2819452285766602, G Loss: 1.204219102859497\n",
      "Epoch 4, batch 11 D Loss: 1.338322639465332, G Loss: 1.1831457614898682\n",
      "Epoch 4, batch 12 D Loss: 1.3649532794952393, G Loss: 0.9907229542732239\n",
      "Epoch 4, batch 13 D Loss: 1.3106839656829834, G Loss: 1.1710267066955566\n",
      "Epoch 4, batch 14 D Loss: 1.4904708862304688, G Loss: 1.0047298669815063\n",
      "Epoch 4, batch 15 D Loss: 1.4036166667938232, G Loss: 1.0426862239837646\n",
      "Epoch 4, batch 16 D Loss: 1.296417236328125, G Loss: 1.1735413074493408\n",
      "Epoch 4, batch 17 D Loss: 1.2481145858764648, G Loss: 1.2555670738220215\n",
      "Epoch 4, batch 18 D Loss: 1.7469114065170288, G Loss: 1.1771950721740723\n",
      "Epoch 4, batch 19 D Loss: 1.5080519914627075, G Loss: 1.206746220588684\n",
      "Epoch 4, batch 20 D Loss: 1.3032993078231812, G Loss: 1.1047972440719604\n",
      "Epoch 4, batch 21 D Loss: 1.8238850831985474, G Loss: 1.0442348718643188\n",
      "Epoch 4, batch 22 D Loss: 1.4060654640197754, G Loss: 1.0618840456008911\n",
      "Epoch 4, batch 23 D Loss: 1.6701557636260986, G Loss: 1.0260487794876099\n",
      "Epoch 4, batch 24 D Loss: 1.450293779373169, G Loss: 1.0544183254241943\n",
      "Epoch 4, batch 25 D Loss: 1.4931492805480957, G Loss: 0.9798346161842346\n",
      "Epoch 4, batch 26 D Loss: 1.4226374626159668, G Loss: 0.9910755753517151\n",
      "Epoch 4, batch 27 D Loss: 1.4202415943145752, G Loss: 0.9845219254493713\n",
      "Epoch 4, batch 28 D Loss: 1.5090959072113037, G Loss: 1.0515947341918945\n",
      "Epoch 4, batch 29 D Loss: 1.7171015739440918, G Loss: 1.071669578552246\n",
      "Epoch 4, batch 30 D Loss: 1.2788755893707275, G Loss: 1.0544471740722656\n",
      "Epoch 4, batch 31 D Loss: 1.3830623626708984, G Loss: 1.1182183027267456\n",
      "Epoch 4, batch 32 D Loss: 1.580960750579834, G Loss: 1.0879701375961304\n",
      "Epoch 4, batch 33 D Loss: 1.5141375064849854, G Loss: 1.145444393157959\n",
      "Epoch 4, batch 34 D Loss: 1.39732027053833, G Loss: 1.2139719724655151\n",
      "Epoch 4, batch 35 D Loss: 1.6122541427612305, G Loss: 1.272943139076233\n",
      "Epoch 4, batch 36 D Loss: 1.6006534099578857, G Loss: 1.1887621879577637\n",
      "Epoch 4, batch 37 D Loss: 1.4786818027496338, G Loss: 1.1455779075622559\n",
      "Epoch 4, batch 38 D Loss: 1.2510871887207031, G Loss: 1.247175931930542\n",
      "Epoch 4, batch 39 D Loss: 1.6563400030136108, G Loss: 1.1952674388885498\n",
      "Epoch 4, batch 40 D Loss: 1.4399261474609375, G Loss: 1.2307381629943848\n",
      "Epoch 4, batch 41 D Loss: 1.3367233276367188, G Loss: 1.2807297706604004\n",
      "Epoch 4, batch 42 D Loss: 1.1962218284606934, G Loss: 1.2866227626800537\n",
      "Epoch 4, batch 43 D Loss: 1.082044005393982, G Loss: 1.3758316040039062\n",
      "Epoch 4, batch 44 D Loss: 1.372453212738037, G Loss: 1.3495515584945679\n",
      "Epoch 4, batch 45 D Loss: 1.2540500164031982, G Loss: 1.1924185752868652\n",
      "Epoch 4, batch 46 D Loss: 1.3175036907196045, G Loss: 1.2157474756240845\n",
      "Epoch 4, batch 47 D Loss: 1.2951433658599854, G Loss: 1.400178074836731\n",
      "Epoch 4, batch 48 D Loss: 1.5930230617523193, G Loss: 1.4324833154678345\n",
      "Epoch 4, batch 49 D Loss: 1.4926433563232422, G Loss: 1.3768608570098877\n",
      "Epoch 4, batch 50 D Loss: 1.5955872535705566, G Loss: 1.2929404973983765\n",
      "Epoch 4, batch 51 D Loss: 1.229944109916687, G Loss: 1.261420726776123\n",
      "Epoch 4, batch 52 D Loss: 1.3577439785003662, G Loss: 1.2829890251159668\n",
      "Epoch 4, batch 53 D Loss: 1.349601149559021, G Loss: 1.3083748817443848\n",
      "Epoch 4, batch 54 D Loss: 1.3270065784454346, G Loss: 1.1852688789367676\n",
      "Epoch 4, batch 55 D Loss: 1.420395016670227, G Loss: 1.2482991218566895\n",
      "Epoch 4, batch 56 D Loss: 1.310360074043274, G Loss: 1.2152196168899536\n",
      "Epoch 4, batch 57 D Loss: 1.3321805000305176, G Loss: 1.3484095335006714\n",
      "Epoch 4, batch 58 D Loss: 1.4136642217636108, G Loss: 1.3518431186676025\n",
      "Epoch 4, batch 59 D Loss: 1.6520428657531738, G Loss: 1.2735590934753418\n",
      "Epoch 4, batch 60 D Loss: 1.3908528089523315, G Loss: 1.213352918624878\n",
      "Epoch 4, batch 61 D Loss: 1.1909599304199219, G Loss: 1.2603410482406616\n",
      "Epoch 4, batch 62 D Loss: 1.3562521934509277, G Loss: 1.1483089923858643\n",
      "Epoch 4, batch 63 D Loss: 1.5994822978973389, G Loss: 1.270490050315857\n",
      "Epoch 4, batch 64 D Loss: 1.638503074645996, G Loss: 1.2599012851715088\n",
      "Epoch 4, batch 65 D Loss: 1.477855920791626, G Loss: 1.2542753219604492\n",
      "Epoch 4, batch 66 D Loss: 1.4927529096603394, G Loss: 1.2264212369918823\n",
      "Epoch 4, batch 67 D Loss: 1.7137320041656494, G Loss: 1.2374622821807861\n",
      "Epoch 4, batch 68 D Loss: 1.7742376327514648, G Loss: 1.2284046411514282\n",
      "Epoch 4, batch 69 D Loss: 1.4076955318450928, G Loss: 1.206823468208313\n",
      "Epoch 4, batch 70 D Loss: 1.8808228969573975, G Loss: 1.336613655090332\n",
      "Epoch 4, batch 71 D Loss: 1.5155467987060547, G Loss: 1.2829214334487915\n",
      "Epoch 4, batch 72 D Loss: 1.2390015125274658, G Loss: 1.2434815168380737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, batch 73 D Loss: 1.6257396936416626, G Loss: 1.2578221559524536\n",
      "Epoch 4, batch 74 D Loss: 1.41579008102417, G Loss: 1.247704029083252\n",
      "Epoch 4, batch 75 D Loss: 1.5904974937438965, G Loss: 1.3103238344192505\n",
      "Epoch 4, batch 76 D Loss: 1.3748732805252075, G Loss: 1.302679419517517\n",
      "Epoch 4, batch 77 D Loss: 1.551226019859314, G Loss: 1.349021553993225\n",
      "Epoch 4, batch 78 D Loss: 1.4476397037506104, G Loss: 1.2729756832122803\n",
      "Epoch 4, batch 79 D Loss: 1.5798227787017822, G Loss: 1.308732032775879\n",
      "Epoch 4, batch 80 D Loss: 1.4398823976516724, G Loss: 1.2731295824050903\n",
      "Epoch 4, batch 81 D Loss: 1.4319634437561035, G Loss: 1.3005328178405762\n",
      "Epoch 4, batch 82 D Loss: 1.395586371421814, G Loss: 1.3017680644989014\n",
      "Epoch 4, batch 83 D Loss: 1.5686540603637695, G Loss: 1.2147547006607056\n",
      "Epoch 4, batch 84 D Loss: 1.486146092414856, G Loss: 1.2946336269378662\n",
      "Epoch 4, batch 85 D Loss: 1.5414409637451172, G Loss: 1.1938064098358154\n",
      "Epoch 4, batch 86 D Loss: 1.5817172527313232, G Loss: 1.1943259239196777\n",
      "Epoch 4, batch 87 D Loss: 1.4552855491638184, G Loss: 1.2884904146194458\n",
      "Epoch 4, batch 88 D Loss: 1.374819040298462, G Loss: 1.2471809387207031\n",
      "Epoch 4, batch 89 D Loss: 1.633781909942627, G Loss: 1.1749577522277832\n",
      "Epoch 4, batch 90 D Loss: 1.3650519847869873, G Loss: 1.1527889966964722\n",
      "Epoch 4, batch 91 D Loss: 1.3055055141448975, G Loss: 1.1714563369750977\n",
      "Epoch 4, batch 92 D Loss: 1.4878909587860107, G Loss: 1.1913933753967285\n",
      "Epoch 4, batch 93 D Loss: 1.3226068019866943, G Loss: 1.1814237833023071\n",
      "Epoch 4, batch 94 D Loss: 1.244869589805603, G Loss: 1.2976261377334595\n",
      "Epoch 4, batch 95 D Loss: 1.291029691696167, G Loss: 1.1358706951141357\n",
      "Epoch 4, batch 96 D Loss: 1.303431510925293, G Loss: 1.3300548791885376\n",
      "Epoch 4, batch 97 D Loss: 1.5101367235183716, G Loss: 1.219103217124939\n",
      "Epoch 4, batch 98 D Loss: 1.5325112342834473, G Loss: 1.2378753423690796\n",
      "Epoch 4, batch 99 D Loss: 1.6398160457611084, G Loss: 1.2548325061798096\n",
      "Epoch 4, batch 100 D Loss: 1.2997328042984009, G Loss: 1.192520022392273\n",
      "Epoch 4, batch 101 D Loss: 1.204420566558838, G Loss: 1.2235289812088013\n",
      "Epoch 4, batch 102 D Loss: 1.4718847274780273, G Loss: 1.223596215248108\n",
      "Epoch 4, batch 103 D Loss: 1.7066938877105713, G Loss: 1.2605866193771362\n",
      "Epoch 4, batch 104 D Loss: 1.3678507804870605, G Loss: 1.23871648311615\n",
      "Epoch 4, batch 105 D Loss: 1.5651124715805054, G Loss: 1.2818267345428467\n",
      "Epoch 4, batch 106 D Loss: 1.4554927349090576, G Loss: 1.1360228061676025\n",
      "Epoch 4, batch 107 D Loss: 1.9090960025787354, G Loss: 1.2308650016784668\n",
      "Epoch 4, batch 108 D Loss: 1.5846266746520996, G Loss: 1.1980633735656738\n",
      "Epoch 4, batch 109 D Loss: 1.5065425634384155, G Loss: 1.2229281663894653\n",
      "Epoch 4, batch 110 D Loss: 1.6310328245162964, G Loss: 1.1296288967132568\n",
      "Epoch 4, batch 111 D Loss: 1.2845717668533325, G Loss: 1.176937460899353\n",
      "Epoch 4, batch 112 D Loss: 1.5689527988433838, G Loss: 1.224707841873169\n",
      "Epoch 4, batch 113 D Loss: 1.2824761867523193, G Loss: 1.2692060470581055\n",
      "Epoch 4, batch 114 D Loss: 1.0380678176879883, G Loss: 1.168745756149292\n",
      "Epoch 4, batch 115 D Loss: 1.350341558456421, G Loss: 1.1964612007141113\n",
      "Epoch 4, batch 116 D Loss: 1.404059648513794, G Loss: 1.2346502542495728\n",
      "Epoch 4, batch 117 D Loss: 1.3285597562789917, G Loss: 1.1404913663864136\n",
      "Epoch 4, batch 118 D Loss: 1.4877679347991943, G Loss: 1.0710229873657227\n",
      "Epoch 4, batch 119 D Loss: 1.187713384628296, G Loss: 1.118200421333313\n",
      "Epoch 4, batch 120 D Loss: 1.1657005548477173, G Loss: 1.096604347229004\n",
      "Epoch 4, batch 121 D Loss: 1.4327725172042847, G Loss: 1.102992057800293\n",
      "Epoch 4, batch 122 D Loss: 1.3129831552505493, G Loss: 1.1614012718200684\n",
      "Epoch 4, batch 123 D Loss: 1.2575345039367676, G Loss: 1.160629153251648\n",
      "Epoch 4, batch 124 D Loss: 1.3212509155273438, G Loss: 1.1616220474243164\n",
      "Epoch 4, batch 125 D Loss: 1.448154091835022, G Loss: 1.114280104637146\n",
      "Epoch 4, batch 126 D Loss: 1.2309030294418335, G Loss: 1.1708896160125732\n",
      "Epoch 4, batch 127 D Loss: 1.1076068878173828, G Loss: 1.1137562990188599\n",
      "Epoch 4, batch 128 D Loss: 1.3566055297851562, G Loss: 1.0866150856018066\n",
      "Epoch 4, batch 129 D Loss: 1.3318358659744263, G Loss: 1.108007788658142\n",
      "Epoch 4, batch 130 D Loss: 1.6050993204116821, G Loss: 1.1977858543395996\n",
      "Epoch 4, batch 131 D Loss: 1.165432095527649, G Loss: 1.11281418800354\n",
      "Epoch 4, batch 132 D Loss: 1.3223856687545776, G Loss: 1.1483049392700195\n",
      "Epoch 4, batch 133 D Loss: 1.42080557346344, G Loss: 1.168981909751892\n",
      "Epoch 4, batch 134 D Loss: 1.2865041494369507, G Loss: 1.0811692476272583\n",
      "Epoch 4, batch 135 D Loss: 1.6075470447540283, G Loss: 1.043701410293579\n",
      "Epoch 4, batch 136 D Loss: 1.3227344751358032, G Loss: 1.0672938823699951\n",
      "Epoch 4, batch 137 D Loss: 1.529026746749878, G Loss: 1.058266043663025\n",
      "Epoch 4, batch 138 D Loss: 1.4219151735305786, G Loss: 1.032849907875061\n",
      "Epoch 4, batch 139 D Loss: 1.4133143424987793, G Loss: 1.078887701034546\n",
      "Epoch 4, batch 140 D Loss: 1.4584482908248901, G Loss: 1.0463813543319702\n",
      "Epoch 4, batch 141 D Loss: 1.3481132984161377, G Loss: 1.032028079032898\n",
      "Epoch 4, batch 142 D Loss: 1.2646859884262085, G Loss: 1.0316671133041382\n",
      "Epoch 4, batch 143 D Loss: 1.3709990978240967, G Loss: 0.980289101600647\n",
      "Epoch 4, batch 144 D Loss: 1.2816355228424072, G Loss: 0.9936948418617249\n",
      "Epoch 4, batch 145 D Loss: 1.4233362674713135, G Loss: 1.011358618736267\n",
      "Epoch 4, batch 146 D Loss: 1.5554282665252686, G Loss: 1.0121989250183105\n",
      "Epoch 4, batch 147 D Loss: 1.5351858139038086, G Loss: 0.9841103553771973\n",
      "Epoch 4, batch 148 D Loss: 1.668238639831543, G Loss: 0.9643234014511108\n",
      "Epoch 4, batch 149 D Loss: 1.2756237983703613, G Loss: 1.012084722518921\n",
      "Epoch 4, batch 150 D Loss: 1.261091709136963, G Loss: 0.983127236366272\n",
      "Epoch 4, batch 151 D Loss: 1.4541767835617065, G Loss: 1.008596658706665\n",
      "Epoch 4, batch 152 D Loss: 1.3567676544189453, G Loss: 0.9443042874336243\n",
      "Epoch 4, batch 153 D Loss: 1.4685304164886475, G Loss: 1.017109990119934\n",
      "Epoch 4, batch 154 D Loss: 1.2998868227005005, G Loss: 0.9305875897407532\n",
      "Epoch 4, batch 155 D Loss: 1.7132136821746826, G Loss: 0.9700646996498108\n",
      "Epoch 4, batch 156 D Loss: 1.2383074760437012, G Loss: 0.916795015335083\n",
      "Epoch 4, batch 157 D Loss: 1.3509328365325928, G Loss: 0.9062965512275696\n",
      "Epoch 4, batch 158 D Loss: 1.3187772035598755, G Loss: 0.9102323651313782\n",
      "Epoch 4, batch 159 D Loss: 1.1500585079193115, G Loss: 0.9367637038230896\n",
      "Epoch 4, batch 160 D Loss: 1.2814912796020508, G Loss: 0.903456449508667\n",
      "Epoch 4, batch 161 D Loss: 1.355908751487732, G Loss: 0.8999724388122559\n",
      "Epoch 4, batch 162 D Loss: 1.273350477218628, G Loss: 0.8997162580490112\n",
      "Epoch 4, batch 163 D Loss: 1.3041166067123413, G Loss: 0.8957558274269104\n",
      "Epoch 4, batch 164 D Loss: 1.3955427408218384, G Loss: 0.899501621723175\n",
      "Epoch 4, batch 165 D Loss: 1.3151578903198242, G Loss: 0.9142477512359619\n",
      "Epoch 4, batch 166 D Loss: 1.1955195665359497, G Loss: 0.9287867546081543\n",
      "Epoch 4, batch 167 D Loss: 1.5037883520126343, G Loss: 0.9364084005355835\n",
      "Epoch 4, batch 168 D Loss: 1.4261722564697266, G Loss: 0.9408113360404968\n",
      "Epoch 4, batch 169 D Loss: 1.2036621570587158, G Loss: 0.9362817406654358\n",
      "Epoch 4, batch 170 D Loss: 1.5040513277053833, G Loss: 0.9034320712089539\n",
      "Epoch 4, batch 171 D Loss: 1.3304299116134644, G Loss: 0.9507392644882202\n",
      "Epoch 4, batch 172 D Loss: 1.4889631271362305, G Loss: 0.9952413439750671\n",
      "Epoch 4, batch 173 D Loss: 1.5204871892929077, G Loss: 0.9815857410430908\n",
      "Epoch 4, batch 174 D Loss: 1.206928014755249, G Loss: 0.9639968872070312\n",
      "Epoch 4, batch 175 D Loss: 1.3401225805282593, G Loss: 0.9376699328422546\n",
      "Epoch 4, batch 176 D Loss: 1.4487544298171997, G Loss: 0.9391777515411377\n",
      "Epoch 4, batch 177 D Loss: 1.2922544479370117, G Loss: 0.9252844452857971\n",
      "Epoch 4, batch 178 D Loss: 1.2241477966308594, G Loss: 0.9474707245826721\n",
      "Epoch 4, batch 179 D Loss: 1.5747907161712646, G Loss: 0.9362378120422363\n",
      "Epoch 4, batch 180 D Loss: 1.3792481422424316, G Loss: 0.9307104349136353\n",
      "Epoch 4, batch 181 D Loss: 1.163074254989624, G Loss: 0.9474413990974426\n",
      "Epoch 4, batch 182 D Loss: 1.2068288326263428, G Loss: 0.9686549305915833\n",
      "Epoch 4, batch 183 D Loss: 1.2501949071884155, G Loss: 0.8915274143218994\n",
      "Epoch 4, batch 184 D Loss: 1.3846843242645264, G Loss: 0.9361215233802795\n",
      "Epoch 4, batch 185 D Loss: 1.360442876815796, G Loss: 0.9194137454032898\n",
      "Epoch 4, batch 186 D Loss: 1.1864619255065918, G Loss: 0.8649063110351562\n",
      "Epoch 4, batch 187 D Loss: 1.2032347917556763, G Loss: 0.8948619365692139\n",
      "Epoch 4, batch 188 D Loss: 1.2937688827514648, G Loss: 0.9387471675872803\n",
      "Epoch 4, batch 189 D Loss: 1.1931208372116089, G Loss: 0.9384861588478088\n",
      "Epoch 4, batch 190 D Loss: 1.15578031539917, G Loss: 0.954542875289917\n",
      "Epoch 4, batch 191 D Loss: 1.2936146259307861, G Loss: 0.9490216970443726\n",
      "Epoch 4, batch 192 D Loss: 1.2568422555923462, G Loss: 0.8844214677810669\n",
      "Epoch 4, batch 193 D Loss: 1.2999516725540161, G Loss: 0.9410176277160645\n",
      "Epoch 4, batch 194 D Loss: 1.271571397781372, G Loss: 0.9387480020523071\n",
      "Epoch 4, batch 195 D Loss: 1.1890277862548828, G Loss: 0.9296738505363464\n",
      "Epoch 4, batch 196 D Loss: 1.1923458576202393, G Loss: 0.8948268890380859\n",
      "Epoch 4, batch 197 D Loss: 1.2946083545684814, G Loss: 0.9364511966705322\n",
      "Epoch 4, batch 198 D Loss: 1.2100666761398315, G Loss: 0.9211857318878174\n",
      "Epoch 4, batch 199 D Loss: 1.2598168849945068, G Loss: 0.8940219283103943\n",
      "Epoch 4, batch 200 D Loss: 1.2216713428497314, G Loss: 0.9538730382919312\n",
      "Epoch 5, batch 1 D Loss: 1.2080320119857788, G Loss: 0.9146072268486023\n",
      "Epoch 5, batch 2 D Loss: 1.0785794258117676, G Loss: 0.9561315178871155\n",
      "Epoch 5, batch 3 D Loss: 1.278900146484375, G Loss: 0.8924762010574341\n",
      "Epoch 5, batch 4 D Loss: 1.114168405532837, G Loss: 0.9570345282554626\n",
      "Epoch 5, batch 5 D Loss: 1.1606550216674805, G Loss: 0.8658742308616638\n",
      "Epoch 5, batch 6 D Loss: 1.3345309495925903, G Loss: 0.9073688983917236\n",
      "Epoch 5, batch 7 D Loss: 1.227367877960205, G Loss: 0.8657607436180115\n",
      "Epoch 5, batch 8 D Loss: 1.19062077999115, G Loss: 0.8704514503479004\n",
      "Epoch 5, batch 9 D Loss: 1.1609628200531006, G Loss: 0.9165164828300476\n",
      "Epoch 5, batch 10 D Loss: 1.2191598415374756, G Loss: 0.899445652961731\n",
      "Epoch 5, batch 11 D Loss: 1.1402305364608765, G Loss: 0.898008406162262\n",
      "Epoch 5, batch 12 D Loss: 1.1306020021438599, G Loss: 0.924436628818512\n",
      "Epoch 5, batch 13 D Loss: 1.257185459136963, G Loss: 0.8637847900390625\n",
      "Epoch 5, batch 14 D Loss: 1.1862016916275024, G Loss: 0.9148684740066528\n",
      "Epoch 5, batch 15 D Loss: 1.2712346315383911, G Loss: 0.9008058309555054\n",
      "Epoch 5, batch 16 D Loss: 1.1457139253616333, G Loss: 0.8919658064842224\n",
      "Epoch 5, batch 17 D Loss: 1.1580147743225098, G Loss: 0.8963718414306641\n",
      "Epoch 5, batch 18 D Loss: 0.9594649076461792, G Loss: 0.8823312520980835\n",
      "Epoch 5, batch 19 D Loss: 1.054758906364441, G Loss: 0.8747232556343079\n",
      "Epoch 5, batch 20 D Loss: 1.3805255889892578, G Loss: 0.8515375256538391\n",
      "Epoch 5, batch 21 D Loss: 1.1415748596191406, G Loss: 0.878102719783783\n",
      "Epoch 5, batch 22 D Loss: 1.1776169538497925, G Loss: 0.8885490894317627\n",
      "Epoch 5, batch 23 D Loss: 1.1760094165802002, G Loss: 0.8490138053894043\n",
      "Epoch 5, batch 24 D Loss: 1.4262707233428955, G Loss: 0.8643300533294678\n",
      "Epoch 5, batch 25 D Loss: 1.2472877502441406, G Loss: 0.893769383430481\n",
      "Epoch 5, batch 26 D Loss: 1.2911357879638672, G Loss: 0.8439171314239502\n",
      "Epoch 5, batch 27 D Loss: 1.2974309921264648, G Loss: 0.8288942575454712\n",
      "Epoch 5, batch 28 D Loss: 1.246864676475525, G Loss: 0.8964249491691589\n",
      "Epoch 5, batch 29 D Loss: 1.2742891311645508, G Loss: 0.8274482488632202\n",
      "Epoch 5, batch 30 D Loss: 1.313068151473999, G Loss: 0.8859604001045227\n",
      "Epoch 5, batch 31 D Loss: 1.295064926147461, G Loss: 0.847908079624176\n",
      "Epoch 5, batch 32 D Loss: 1.2240018844604492, G Loss: 0.8743763566017151\n",
      "Epoch 5, batch 33 D Loss: 1.2157118320465088, G Loss: 0.8464218378067017\n",
      "Epoch 5, batch 34 D Loss: 1.057126522064209, G Loss: 0.8346462249755859\n",
      "Epoch 5, batch 35 D Loss: 1.4761667251586914, G Loss: 0.8148294687271118\n",
      "Epoch 5, batch 36 D Loss: 1.2782208919525146, G Loss: 0.8217335343360901\n",
      "Epoch 5, batch 37 D Loss: 1.1640150547027588, G Loss: 0.8285185098648071\n",
      "Epoch 5, batch 38 D Loss: 1.1524040699005127, G Loss: 0.7986710071563721\n",
      "Epoch 5, batch 39 D Loss: 1.344313621520996, G Loss: 0.7750784158706665\n",
      "Epoch 5, batch 40 D Loss: 1.2857530117034912, G Loss: 0.8130794763565063\n",
      "Epoch 5, batch 41 D Loss: 1.337450623512268, G Loss: 0.7808598279953003\n",
      "Epoch 5, batch 42 D Loss: 1.3548767566680908, G Loss: 0.7990775108337402\n",
      "Epoch 5, batch 43 D Loss: 1.307587742805481, G Loss: 0.8152836561203003\n",
      "Epoch 5, batch 44 D Loss: 1.239078402519226, G Loss: 0.8097971081733704\n",
      "Epoch 5, batch 45 D Loss: 1.1574479341506958, G Loss: 0.7660598158836365\n",
      "Epoch 5, batch 46 D Loss: 1.374410629272461, G Loss: 0.7664059996604919\n",
      "Epoch 5, batch 47 D Loss: 1.2893866300582886, G Loss: 0.7569882869720459\n",
      "Epoch 5, batch 48 D Loss: 1.1867886781692505, G Loss: 0.7723549604415894\n",
      "Epoch 5, batch 49 D Loss: 1.3154691457748413, G Loss: 0.7858482599258423\n",
      "Epoch 5, batch 50 D Loss: 1.2179028987884521, G Loss: 0.8065653443336487\n",
      "Epoch 5, batch 51 D Loss: 1.324718952178955, G Loss: 0.7619655132293701\n",
      "Epoch 5, batch 52 D Loss: 1.2483035326004028, G Loss: 0.7813582420349121\n",
      "Epoch 5, batch 53 D Loss: 1.2113587856292725, G Loss: 0.815205991268158\n",
      "Epoch 5, batch 54 D Loss: 1.1329238414764404, G Loss: 0.7966790795326233\n",
      "Epoch 5, batch 55 D Loss: 1.1149953603744507, G Loss: 0.7681955695152283\n",
      "Epoch 5, batch 56 D Loss: 1.2505273818969727, G Loss: 0.763508141040802\n",
      "Epoch 5, batch 57 D Loss: 1.254374384880066, G Loss: 0.7784208655357361\n",
      "Epoch 5, batch 58 D Loss: 1.275232195854187, G Loss: 0.7665337920188904\n",
      "Epoch 5, batch 59 D Loss: 1.2068369388580322, G Loss: 0.7888627052307129\n",
      "Epoch 5, batch 60 D Loss: 1.2003061771392822, G Loss: 0.8056014776229858\n",
      "Epoch 5, batch 61 D Loss: 1.3017809391021729, G Loss: 0.767427384853363\n",
      "Epoch 5, batch 62 D Loss: 1.1146743297576904, G Loss: 0.7865328788757324\n",
      "Epoch 5, batch 63 D Loss: 1.389737844467163, G Loss: 0.8106422424316406\n",
      "Epoch 5, batch 64 D Loss: 1.125529408454895, G Loss: 0.7928815484046936\n",
      "Epoch 5, batch 65 D Loss: 1.1664855480194092, G Loss: 0.7806469798088074\n",
      "Epoch 5, batch 66 D Loss: 1.2179641723632812, G Loss: 0.7759524583816528\n",
      "Epoch 5, batch 67 D Loss: 1.121882438659668, G Loss: 0.8188998699188232\n",
      "Epoch 5, batch 68 D Loss: 1.2253762483596802, G Loss: 0.786695122718811\n",
      "Epoch 5, batch 69 D Loss: 1.2814764976501465, G Loss: 0.7746555209159851\n",
      "Epoch 5, batch 70 D Loss: 1.2169959545135498, G Loss: 0.8095461130142212\n",
      "Epoch 5, batch 71 D Loss: 1.0302411317825317, G Loss: 0.7941224575042725\n",
      "Epoch 5, batch 72 D Loss: 1.3103432655334473, G Loss: 0.8294476270675659\n",
      "Epoch 5, batch 73 D Loss: 1.1067769527435303, G Loss: 0.7888948917388916\n",
      "Epoch 5, batch 74 D Loss: 1.2217674255371094, G Loss: 0.7800660729408264\n",
      "Epoch 5, batch 75 D Loss: 1.3239693641662598, G Loss: 0.7706111073493958\n",
      "Epoch 5, batch 76 D Loss: 1.2273211479187012, G Loss: 0.7395354509353638\n",
      "Epoch 5, batch 77 D Loss: 1.1635868549346924, G Loss: 0.7495405077934265\n",
      "Epoch 5, batch 78 D Loss: 1.1925997734069824, G Loss: 0.7998298406600952\n",
      "Epoch 5, batch 79 D Loss: 1.2274259328842163, G Loss: 0.8006225824356079\n",
      "Epoch 5, batch 80 D Loss: 1.2151082754135132, G Loss: 0.7736244201660156\n",
      "Epoch 5, batch 81 D Loss: 1.0415539741516113, G Loss: 0.7852034568786621\n",
      "Epoch 5, batch 82 D Loss: 1.2742054462432861, G Loss: 0.7789759635925293\n",
      "Epoch 5, batch 83 D Loss: 1.2498149871826172, G Loss: 0.8081551194190979\n",
      "Epoch 5, batch 84 D Loss: 1.2984354496002197, G Loss: 0.7822666168212891\n",
      "Epoch 5, batch 85 D Loss: 1.2399464845657349, G Loss: 0.7543002367019653\n",
      "Epoch 5, batch 86 D Loss: 1.1398634910583496, G Loss: 0.7602508664131165\n",
      "Epoch 5, batch 87 D Loss: 1.2433538436889648, G Loss: 0.7837574481964111\n",
      "Epoch 5, batch 88 D Loss: 1.2567042112350464, G Loss: 0.7890644073486328\n",
      "Epoch 5, batch 89 D Loss: 1.1550304889678955, G Loss: 0.7755052447319031\n",
      "Epoch 5, batch 90 D Loss: 1.1247596740722656, G Loss: 0.7874042987823486\n",
      "Epoch 5, batch 91 D Loss: 1.1669319868087769, G Loss: 0.7670391201972961\n",
      "Epoch 5, batch 92 D Loss: 1.2922130823135376, G Loss: 0.775438666343689\n",
      "Epoch 5, batch 93 D Loss: 1.222571611404419, G Loss: 0.7754424214363098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, batch 94 D Loss: 1.1756958961486816, G Loss: 0.7449739575386047\n",
      "Epoch 5, batch 95 D Loss: 1.2403318881988525, G Loss: 0.7437663078308105\n",
      "Epoch 5, batch 96 D Loss: 1.2443974018096924, G Loss: 0.7506536245346069\n",
      "Epoch 5, batch 97 D Loss: 1.1882264614105225, G Loss: 0.766661524772644\n",
      "Epoch 5, batch 98 D Loss: 1.197150468826294, G Loss: 0.7730402946472168\n",
      "Epoch 5, batch 99 D Loss: 1.2456252574920654, G Loss: 0.7756498456001282\n",
      "Epoch 5, batch 100 D Loss: 1.473649501800537, G Loss: 0.7742603421211243\n",
      "Epoch 5, batch 101 D Loss: 1.3866498470306396, G Loss: 0.7545998096466064\n",
      "Epoch 5, batch 102 D Loss: 1.205766201019287, G Loss: 0.7627972364425659\n",
      "Epoch 5, batch 103 D Loss: 1.240793228149414, G Loss: 0.750047504901886\n",
      "Epoch 5, batch 104 D Loss: 1.2081151008605957, G Loss: 0.7531477212905884\n",
      "Epoch 5, batch 105 D Loss: 1.261776089668274, G Loss: 0.7726609110832214\n",
      "Epoch 5, batch 106 D Loss: 1.2399563789367676, G Loss: 0.7599974870681763\n",
      "Epoch 5, batch 107 D Loss: 1.227724552154541, G Loss: 0.7418121099472046\n",
      "Epoch 5, batch 108 D Loss: 1.1930813789367676, G Loss: 0.7448539733886719\n",
      "Epoch 5, batch 109 D Loss: 1.2656431198120117, G Loss: 0.7671042084693909\n",
      "Epoch 5, batch 110 D Loss: 1.2200658321380615, G Loss: 0.749768853187561\n",
      "Epoch 5, batch 111 D Loss: 1.2539995908737183, G Loss: 0.7635947465896606\n",
      "Epoch 5, batch 112 D Loss: 1.3140525817871094, G Loss: 0.7512553334236145\n",
      "Epoch 5, batch 113 D Loss: 1.1320428848266602, G Loss: 0.7677772641181946\n",
      "Epoch 5, batch 114 D Loss: 1.3549299240112305, G Loss: 0.7811243534088135\n",
      "Epoch 5, batch 115 D Loss: 1.3125624656677246, G Loss: 0.7729272246360779\n",
      "Epoch 5, batch 116 D Loss: 1.2182526588439941, G Loss: 0.7679581642150879\n",
      "Epoch 5, batch 117 D Loss: 1.3265414237976074, G Loss: 0.7440903186798096\n",
      "Epoch 5, batch 118 D Loss: 1.3016722202301025, G Loss: 0.7693716287612915\n",
      "Epoch 5, batch 119 D Loss: 1.2068026065826416, G Loss: 0.7530485391616821\n",
      "Epoch 5, batch 120 D Loss: 1.2384912967681885, G Loss: 0.7753283977508545\n",
      "Epoch 5, batch 121 D Loss: 1.2473621368408203, G Loss: 0.7569160461425781\n",
      "Epoch 5, batch 122 D Loss: 1.2862622737884521, G Loss: 0.728025496006012\n",
      "Epoch 5, batch 123 D Loss: 1.1840229034423828, G Loss: 0.7706573009490967\n",
      "Epoch 5, batch 124 D Loss: 1.3975642919540405, G Loss: 0.7633380889892578\n",
      "Epoch 5, batch 125 D Loss: 1.3496766090393066, G Loss: 0.7845808267593384\n",
      "Epoch 5, batch 126 D Loss: 1.0948951244354248, G Loss: 0.7739627361297607\n",
      "Epoch 5, batch 127 D Loss: 1.2893071174621582, G Loss: 0.7597354650497437\n",
      "Epoch 5, batch 128 D Loss: 1.1844085454940796, G Loss: 0.7891490459442139\n",
      "Epoch 5, batch 129 D Loss: 1.3702223300933838, G Loss: 0.7730506658554077\n",
      "Epoch 5, batch 130 D Loss: 1.3853349685668945, G Loss: 0.7430210709571838\n",
      "Epoch 5, batch 131 D Loss: 1.1984552145004272, G Loss: 0.7700233459472656\n",
      "Epoch 5, batch 132 D Loss: 1.1777660846710205, G Loss: 0.7847535610198975\n",
      "Epoch 5, batch 133 D Loss: 1.0926111936569214, G Loss: 0.7861203551292419\n",
      "Epoch 5, batch 134 D Loss: 1.257036566734314, G Loss: 0.772830605506897\n",
      "Epoch 5, batch 135 D Loss: 1.165173888206482, G Loss: 0.7537691593170166\n",
      "Epoch 5, batch 136 D Loss: 1.2348235845565796, G Loss: 0.7604568600654602\n",
      "Epoch 5, batch 137 D Loss: 1.1678855419158936, G Loss: 0.7857874035835266\n",
      "Epoch 5, batch 138 D Loss: 1.1677050590515137, G Loss: 0.7976275086402893\n",
      "Epoch 5, batch 139 D Loss: 1.2745115756988525, G Loss: 0.7760055661201477\n",
      "Epoch 5, batch 140 D Loss: 1.2833287715911865, G Loss: 0.7668973803520203\n",
      "Epoch 5, batch 141 D Loss: 1.2514081001281738, G Loss: 0.7857109904289246\n",
      "Epoch 5, batch 142 D Loss: 1.2826919555664062, G Loss: 0.7649392485618591\n",
      "Epoch 5, batch 143 D Loss: 1.2727081775665283, G Loss: 0.7595009803771973\n",
      "Epoch 5, batch 144 D Loss: 1.2902538776397705, G Loss: 0.788034200668335\n",
      "Epoch 5, batch 145 D Loss: 1.2011959552764893, G Loss: 0.768893837928772\n",
      "Epoch 5, batch 146 D Loss: 1.1077337265014648, G Loss: 0.7604736089706421\n",
      "Epoch 5, batch 147 D Loss: 1.2664217948913574, G Loss: 0.752806544303894\n",
      "Epoch 5, batch 148 D Loss: 1.2980691194534302, G Loss: 0.7641265988349915\n",
      "Epoch 5, batch 149 D Loss: 1.1216866970062256, G Loss: 0.7990466356277466\n",
      "Epoch 5, batch 150 D Loss: 1.260949730873108, G Loss: 0.7752371430397034\n",
      "Epoch 5, batch 151 D Loss: 1.2709541320800781, G Loss: 0.7441272139549255\n",
      "Epoch 5, batch 152 D Loss: 1.0346190929412842, G Loss: 0.775858998298645\n",
      "Epoch 5, batch 153 D Loss: 1.2474749088287354, G Loss: 0.7549951076507568\n",
      "Epoch 5, batch 154 D Loss: 1.1468415260314941, G Loss: 0.7350322604179382\n",
      "Epoch 5, batch 155 D Loss: 1.2593015432357788, G Loss: 0.7549497485160828\n",
      "Epoch 5, batch 156 D Loss: 1.2191411256790161, G Loss: 0.7668092250823975\n",
      "Epoch 5, batch 157 D Loss: 1.2049267292022705, G Loss: 0.7573856115341187\n",
      "Epoch 5, batch 158 D Loss: 1.3045995235443115, G Loss: 0.7585039734840393\n",
      "Epoch 5, batch 159 D Loss: 1.2803173065185547, G Loss: 0.7535278797149658\n",
      "Epoch 5, batch 160 D Loss: 1.1985790729522705, G Loss: 0.7476884722709656\n",
      "Epoch 5, batch 161 D Loss: 1.1037142276763916, G Loss: 0.7624539136886597\n",
      "Epoch 5, batch 162 D Loss: 1.1575376987457275, G Loss: 0.7576848864555359\n",
      "Epoch 5, batch 163 D Loss: 1.1811819076538086, G Loss: 0.7609722018241882\n",
      "Epoch 5, batch 164 D Loss: 1.2220590114593506, G Loss: 0.7559804320335388\n",
      "Epoch 5, batch 165 D Loss: 1.0806572437286377, G Loss: 0.7523958683013916\n",
      "Epoch 5, batch 166 D Loss: 1.119757056236267, G Loss: 0.7666580080986023\n",
      "Epoch 5, batch 167 D Loss: 1.0752770900726318, G Loss: 0.7582196593284607\n",
      "Epoch 5, batch 168 D Loss: 1.1579502820968628, G Loss: 0.7643272280693054\n",
      "Epoch 5, batch 169 D Loss: 1.2095457315444946, G Loss: 0.7332403659820557\n",
      "Epoch 5, batch 170 D Loss: 1.2537777423858643, G Loss: 0.7547205090522766\n",
      "Epoch 5, batch 171 D Loss: 1.1523219347000122, G Loss: 0.7700535655021667\n",
      "Epoch 5, batch 172 D Loss: 1.1755592823028564, G Loss: 0.7743333578109741\n",
      "Epoch 5, batch 173 D Loss: 1.2133479118347168, G Loss: 0.7669195532798767\n",
      "Epoch 5, batch 174 D Loss: 1.0468522310256958, G Loss: 0.7594670057296753\n",
      "Epoch 5, batch 175 D Loss: 1.2145133018493652, G Loss: 0.7244918346405029\n",
      "Epoch 5, batch 176 D Loss: 1.1753480434417725, G Loss: 0.7732533812522888\n",
      "Epoch 5, batch 177 D Loss: 1.091991901397705, G Loss: 0.8140049576759338\n",
      "Epoch 5, batch 178 D Loss: 1.180340051651001, G Loss: 0.7909096479415894\n",
      "Epoch 5, batch 179 D Loss: 1.1481375694274902, G Loss: 0.7536709308624268\n",
      "Epoch 5, batch 180 D Loss: 1.2424335479736328, G Loss: 0.738588273525238\n",
      "Epoch 5, batch 181 D Loss: 1.1534628868103027, G Loss: 0.8050779700279236\n",
      "Epoch 5, batch 182 D Loss: 1.2648863792419434, G Loss: 0.779440701007843\n",
      "Epoch 5, batch 183 D Loss: 1.1992590427398682, G Loss: 0.8080439567565918\n",
      "Epoch 5, batch 184 D Loss: 1.1020958423614502, G Loss: 0.7910812497138977\n",
      "Epoch 5, batch 185 D Loss: 1.1583366394042969, G Loss: 0.7473940849304199\n",
      "Epoch 5, batch 186 D Loss: 1.3380789756774902, G Loss: 0.7446388006210327\n",
      "Epoch 5, batch 187 D Loss: 1.2207579612731934, G Loss: 0.7775671482086182\n",
      "Epoch 5, batch 188 D Loss: 1.2330524921417236, G Loss: 0.7819889783859253\n",
      "Epoch 5, batch 189 D Loss: 1.1981682777404785, G Loss: 0.778311014175415\n",
      "Epoch 5, batch 190 D Loss: 1.1660887002944946, G Loss: 0.791816234588623\n",
      "Epoch 5, batch 191 D Loss: 1.2246527671813965, G Loss: 0.7530902028083801\n",
      "Epoch 5, batch 192 D Loss: 1.3951852321624756, G Loss: 0.7439280152320862\n",
      "Epoch 5, batch 193 D Loss: 1.252679467201233, G Loss: 0.7473011016845703\n",
      "Epoch 5, batch 194 D Loss: 1.2431559562683105, G Loss: 0.7704277038574219\n",
      "Epoch 5, batch 195 D Loss: 1.2167894840240479, G Loss: 0.7431488037109375\n",
      "Epoch 5, batch 196 D Loss: 1.2526257038116455, G Loss: 0.7604260444641113\n",
      "Epoch 5, batch 197 D Loss: 1.2713077068328857, G Loss: 0.7435832023620605\n",
      "Epoch 5, batch 198 D Loss: 1.3049852848052979, G Loss: 0.7486376166343689\n",
      "Epoch 5, batch 199 D Loss: 1.1832547187805176, G Loss: 0.7870603203773499\n",
      "Epoch 5, batch 200 D Loss: 1.1984676122665405, G Loss: 0.7608242630958557\n",
      "Epoch 6, batch 1 D Loss: 1.19564950466156, G Loss: 0.7933682799339294\n",
      "Epoch 6, batch 2 D Loss: 1.2497856616973877, G Loss: 0.7201513051986694\n",
      "Epoch 6, batch 3 D Loss: 1.3144539594650269, G Loss: 0.726959764957428\n",
      "Epoch 6, batch 4 D Loss: 1.2326056957244873, G Loss: 0.7774530649185181\n",
      "Epoch 6, batch 5 D Loss: 1.213740348815918, G Loss: 0.7105885148048401\n",
      "Epoch 6, batch 6 D Loss: 1.3060986995697021, G Loss: 0.7365670204162598\n",
      "Epoch 6, batch 7 D Loss: 1.1521801948547363, G Loss: 0.7610142230987549\n",
      "Epoch 6, batch 8 D Loss: 1.1964163780212402, G Loss: 0.7331008911132812\n",
      "Epoch 6, batch 9 D Loss: 1.2769575119018555, G Loss: 0.7116878628730774\n",
      "Epoch 6, batch 10 D Loss: 1.2618335485458374, G Loss: 0.685983419418335\n",
      "Epoch 6, batch 11 D Loss: 1.314502239227295, G Loss: 0.7346768975257874\n",
      "Epoch 6, batch 12 D Loss: 1.2346081733703613, G Loss: 0.7277814745903015\n",
      "Epoch 6, batch 13 D Loss: 1.247509479522705, G Loss: 0.7507345676422119\n",
      "Epoch 6, batch 14 D Loss: 1.2344927787780762, G Loss: 0.7637059688568115\n",
      "Epoch 6, batch 15 D Loss: 1.3170937299728394, G Loss: 0.7433122396469116\n",
      "Epoch 6, batch 16 D Loss: 1.3664535284042358, G Loss: 0.7019037008285522\n",
      "Epoch 6, batch 17 D Loss: 1.3116787672042847, G Loss: 0.7101094126701355\n",
      "Epoch 6, batch 18 D Loss: 1.1202067136764526, G Loss: 0.7716606259346008\n",
      "Epoch 6, batch 19 D Loss: 1.1708474159240723, G Loss: 0.7661392688751221\n",
      "Epoch 6, batch 20 D Loss: 1.137925386428833, G Loss: 0.7909830212593079\n",
      "Epoch 6, batch 21 D Loss: 1.1893351078033447, G Loss: 0.7496243119239807\n",
      "Epoch 6, batch 22 D Loss: 1.2108078002929688, G Loss: 0.7393523454666138\n",
      "Epoch 6, batch 23 D Loss: 1.2732336521148682, G Loss: 0.7440913319587708\n",
      "Epoch 6, batch 24 D Loss: 1.379514455795288, G Loss: 0.7671568393707275\n",
      "Epoch 6, batch 25 D Loss: 1.3472328186035156, G Loss: 0.7678015828132629\n",
      "Epoch 6, batch 26 D Loss: 1.05959153175354, G Loss: 0.7564221024513245\n",
      "Epoch 6, batch 27 D Loss: 1.135948657989502, G Loss: 0.7545082569122314\n",
      "Epoch 6, batch 28 D Loss: 1.2527650594711304, G Loss: 0.7489373087882996\n",
      "Epoch 6, batch 29 D Loss: 1.2247744798660278, G Loss: 0.7417709231376648\n",
      "Epoch 6, batch 30 D Loss: 1.1943020820617676, G Loss: 0.8133466243743896\n",
      "Epoch 6, batch 31 D Loss: 1.1929519176483154, G Loss: 0.7839093208312988\n",
      "Epoch 6, batch 32 D Loss: 1.2707005739212036, G Loss: 0.7503679394721985\n",
      "Epoch 6, batch 33 D Loss: 1.1953622102737427, G Loss: 0.7942286729812622\n",
      "Epoch 6, batch 34 D Loss: 1.2243220806121826, G Loss: 0.7576476335525513\n",
      "Epoch 6, batch 35 D Loss: 1.1810250282287598, G Loss: 0.7817895412445068\n",
      "Epoch 6, batch 36 D Loss: 1.248368501663208, G Loss: 0.7746826410293579\n",
      "Epoch 6, batch 37 D Loss: 1.2177441120147705, G Loss: 0.7651168704032898\n",
      "Epoch 6, batch 38 D Loss: 1.1860005855560303, G Loss: 0.8165156841278076\n",
      "Epoch 6, batch 39 D Loss: 1.1944446563720703, G Loss: 0.7808870077133179\n",
      "Epoch 6, batch 40 D Loss: 1.1587553024291992, G Loss: 0.7969197630882263\n",
      "Epoch 6, batch 41 D Loss: 1.2324738502502441, G Loss: 0.7911471724510193\n",
      "Epoch 6, batch 42 D Loss: 1.252737283706665, G Loss: 0.7757728099822998\n",
      "Epoch 6, batch 43 D Loss: 1.1928162574768066, G Loss: 0.8273453712463379\n",
      "Epoch 6, batch 44 D Loss: 1.1282354593276978, G Loss: 0.8532034158706665\n",
      "Epoch 6, batch 45 D Loss: 1.1936876773834229, G Loss: 0.8192484974861145\n",
      "Epoch 6, batch 46 D Loss: 1.1188645362854004, G Loss: 0.8488264679908752\n",
      "Epoch 6, batch 47 D Loss: 1.1307775974273682, G Loss: 0.852664589881897\n",
      "Epoch 6, batch 48 D Loss: 1.1647273302078247, G Loss: 0.8096793293952942\n",
      "Epoch 6, batch 49 D Loss: 1.226679801940918, G Loss: 0.8597912788391113\n",
      "Epoch 6, batch 50 D Loss: 1.1214165687561035, G Loss: 0.8761424422264099\n",
      "Epoch 6, batch 51 D Loss: 1.202641248703003, G Loss: 0.8488180041313171\n",
      "Epoch 6, batch 52 D Loss: 1.1960980892181396, G Loss: 0.8372752666473389\n",
      "Epoch 6, batch 53 D Loss: 1.054165005683899, G Loss: 0.8774760365486145\n",
      "Epoch 6, batch 54 D Loss: 1.283501148223877, G Loss: 0.8734349608421326\n",
      "Epoch 6, batch 55 D Loss: 1.1041845083236694, G Loss: 0.8415133953094482\n",
      "Epoch 6, batch 56 D Loss: 1.240952968597412, G Loss: 0.8347113728523254\n",
      "Epoch 6, batch 57 D Loss: 1.0869619846343994, G Loss: 0.8922433257102966\n",
      "Epoch 6, batch 58 D Loss: 1.1388888359069824, G Loss: 0.868557870388031\n",
      "Epoch 6, batch 59 D Loss: 1.2368566989898682, G Loss: 0.8302780985832214\n",
      "Epoch 6, batch 60 D Loss: 1.2544560432434082, G Loss: 0.8865734934806824\n",
      "Epoch 6, batch 61 D Loss: 1.2276079654693604, G Loss: 0.8755833506584167\n",
      "Epoch 6, batch 62 D Loss: 1.140501618385315, G Loss: 0.9011991024017334\n",
      "Epoch 6, batch 63 D Loss: 1.1445167064666748, G Loss: 0.9101542830467224\n",
      "Epoch 6, batch 64 D Loss: 1.1140458583831787, G Loss: 0.8780471682548523\n",
      "Epoch 6, batch 65 D Loss: 1.1464624404907227, G Loss: 0.9153583645820618\n",
      "Epoch 6, batch 66 D Loss: 1.0972380638122559, G Loss: 0.8850614428520203\n",
      "Epoch 6, batch 67 D Loss: 1.1659185886383057, G Loss: 0.9167522192001343\n",
      "Epoch 6, batch 68 D Loss: 1.2329044342041016, G Loss: 0.8950051069259644\n",
      "Epoch 6, batch 69 D Loss: 1.1524296998977661, G Loss: 0.9489244222640991\n",
      "Epoch 6, batch 70 D Loss: 1.2126140594482422, G Loss: 0.9185054898262024\n",
      "Epoch 6, batch 71 D Loss: 1.1598588228225708, G Loss: 0.9108955264091492\n",
      "Epoch 6, batch 72 D Loss: 1.1817553043365479, G Loss: 0.8986358642578125\n",
      "Epoch 6, batch 73 D Loss: 1.033585548400879, G Loss: 0.9455810785293579\n",
      "Epoch 6, batch 74 D Loss: 1.139045000076294, G Loss: 0.9193840026855469\n",
      "Epoch 6, batch 75 D Loss: 1.1447319984436035, G Loss: 0.9159948229789734\n",
      "Epoch 6, batch 76 D Loss: 1.1465730667114258, G Loss: 0.9197843670845032\n",
      "Epoch 6, batch 77 D Loss: 1.1224076747894287, G Loss: 0.9598377346992493\n",
      "Epoch 6, batch 78 D Loss: 1.0969594717025757, G Loss: 0.9927624464035034\n",
      "Epoch 6, batch 79 D Loss: 1.1576769351959229, G Loss: 0.9152007102966309\n",
      "Epoch 6, batch 80 D Loss: 1.1692426204681396, G Loss: 0.955644965171814\n",
      "Epoch 6, batch 81 D Loss: 1.1351746320724487, G Loss: 0.9668627977371216\n",
      "Epoch 6, batch 82 D Loss: 1.1272070407867432, G Loss: 0.9673482775688171\n",
      "Epoch 6, batch 83 D Loss: 1.0979623794555664, G Loss: 0.9130406379699707\n",
      "Epoch 6, batch 84 D Loss: 1.1581774950027466, G Loss: 0.957449734210968\n",
      "Epoch 6, batch 85 D Loss: 1.090742826461792, G Loss: 0.957984983921051\n",
      "Epoch 6, batch 86 D Loss: 1.1594641208648682, G Loss: 0.9693729877471924\n",
      "Epoch 6, batch 87 D Loss: 1.0525362491607666, G Loss: 0.98893141746521\n",
      "Epoch 6, batch 88 D Loss: 1.0815657377243042, G Loss: 1.012256383895874\n",
      "Epoch 6, batch 89 D Loss: 1.1887248754501343, G Loss: 0.9335169792175293\n",
      "Epoch 6, batch 90 D Loss: 1.1392908096313477, G Loss: 0.9682619571685791\n",
      "Epoch 6, batch 91 D Loss: 1.011156678199768, G Loss: 0.9998610615730286\n",
      "Epoch 6, batch 92 D Loss: 1.1210989952087402, G Loss: 0.9696254730224609\n",
      "Epoch 6, batch 93 D Loss: 1.0414209365844727, G Loss: 1.0247657299041748\n",
      "Epoch 6, batch 94 D Loss: 1.0176033973693848, G Loss: 0.9702671766281128\n",
      "Epoch 6, batch 95 D Loss: 0.9968239665031433, G Loss: 1.0058971643447876\n",
      "Epoch 6, batch 96 D Loss: 1.0723893642425537, G Loss: 0.9958881139755249\n",
      "Epoch 6, batch 97 D Loss: 0.9598903059959412, G Loss: 1.013370394706726\n",
      "Epoch 6, batch 98 D Loss: 1.0385597944259644, G Loss: 1.0796984434127808\n",
      "Epoch 6, batch 99 D Loss: 1.0991848707199097, G Loss: 1.0113866329193115\n",
      "Epoch 6, batch 100 D Loss: 1.050461769104004, G Loss: 1.0140831470489502\n",
      "Epoch 6, batch 101 D Loss: 1.0029432773590088, G Loss: 1.0319626331329346\n",
      "Epoch 6, batch 102 D Loss: 1.0525853633880615, G Loss: 1.0213321447372437\n",
      "Epoch 6, batch 103 D Loss: 0.9384451508522034, G Loss: 1.0538288354873657\n",
      "Epoch 6, batch 104 D Loss: 1.069327473640442, G Loss: 1.0575615167617798\n",
      "Epoch 6, batch 105 D Loss: 1.023366928100586, G Loss: 1.0628548860549927\n",
      "Epoch 6, batch 106 D Loss: 1.0040206909179688, G Loss: 1.052088737487793\n",
      "Epoch 6, batch 107 D Loss: 1.150252103805542, G Loss: 1.047299861907959\n",
      "Epoch 6, batch 108 D Loss: 1.079158067703247, G Loss: 1.0457063913345337\n",
      "Epoch 6, batch 109 D Loss: 1.0021986961364746, G Loss: 1.1201519966125488\n",
      "Epoch 6, batch 110 D Loss: 1.0294573307037354, G Loss: 1.004361629486084\n",
      "Epoch 6, batch 111 D Loss: 1.005028486251831, G Loss: 1.0145788192749023\n",
      "Epoch 6, batch 112 D Loss: 1.0043803453445435, G Loss: 1.0625696182250977\n",
      "Epoch 6, batch 113 D Loss: 1.0387479066848755, G Loss: 1.0709835290908813\n",
      "Epoch 6, batch 114 D Loss: 1.0581036806106567, G Loss: 1.120558738708496\n",
      "Epoch 6, batch 115 D Loss: 1.0685151815414429, G Loss: 1.1113094091415405\n",
      "Epoch 6, batch 116 D Loss: 1.0427348613739014, G Loss: 1.0825752019882202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, batch 117 D Loss: 0.9580758810043335, G Loss: 1.084139347076416\n",
      "Epoch 6, batch 118 D Loss: 0.9246362447738647, G Loss: 1.1190080642700195\n",
      "Epoch 6, batch 119 D Loss: 1.1127452850341797, G Loss: 1.0829657316207886\n",
      "Epoch 6, batch 120 D Loss: 0.9890735745429993, G Loss: 1.0485963821411133\n",
      "Epoch 6, batch 121 D Loss: 1.1096465587615967, G Loss: 1.0439026355743408\n",
      "Epoch 6, batch 122 D Loss: 1.10213041305542, G Loss: 1.0640671253204346\n",
      "Epoch 6, batch 123 D Loss: 1.0343573093414307, G Loss: 1.0991705656051636\n",
      "Epoch 6, batch 124 D Loss: 1.0862126350402832, G Loss: 1.0754135847091675\n",
      "Epoch 6, batch 125 D Loss: 0.9105757474899292, G Loss: 1.0591185092926025\n",
      "Epoch 6, batch 126 D Loss: 0.9249283671379089, G Loss: 1.053015112876892\n",
      "Epoch 6, batch 127 D Loss: 1.0255610942840576, G Loss: 1.1143642663955688\n",
      "Epoch 6, batch 128 D Loss: 1.0599802732467651, G Loss: 1.07304048538208\n",
      "Epoch 6, batch 129 D Loss: 1.0260668992996216, G Loss: 1.0589628219604492\n",
      "Epoch 6, batch 130 D Loss: 1.056278109550476, G Loss: 1.0224486589431763\n",
      "Epoch 6, batch 131 D Loss: 0.8849643468856812, G Loss: 1.0968029499053955\n",
      "Epoch 6, batch 132 D Loss: 0.9829734563827515, G Loss: 1.0747687816619873\n",
      "Epoch 6, batch 133 D Loss: 0.9764072895050049, G Loss: 1.082430124282837\n",
      "Epoch 6, batch 134 D Loss: 1.0252901315689087, G Loss: 1.1188498735427856\n",
      "Epoch 6, batch 135 D Loss: 1.0304714441299438, G Loss: 1.0650203227996826\n",
      "Epoch 6, batch 136 D Loss: 0.945305585861206, G Loss: 1.114138126373291\n",
      "Epoch 6, batch 137 D Loss: 0.9211113452911377, G Loss: 1.0984090566635132\n",
      "Epoch 6, batch 138 D Loss: 0.8604575395584106, G Loss: 1.1378962993621826\n",
      "Epoch 6, batch 139 D Loss: 0.9236732721328735, G Loss: 1.070396900177002\n",
      "Epoch 6, batch 140 D Loss: 0.8659723401069641, G Loss: 1.1317819356918335\n",
      "Epoch 6, batch 141 D Loss: 0.9801548719406128, G Loss: 1.0955522060394287\n",
      "Epoch 6, batch 142 D Loss: 0.9418892860412598, G Loss: 1.1019552946090698\n",
      "Epoch 6, batch 143 D Loss: 0.9357699155807495, G Loss: 1.0735304355621338\n",
      "Epoch 6, batch 144 D Loss: 1.0207308530807495, G Loss: 1.1293467283248901\n",
      "Epoch 6, batch 145 D Loss: 0.9754303097724915, G Loss: 1.0227833986282349\n",
      "Epoch 6, batch 146 D Loss: 0.8510750532150269, G Loss: 1.0528528690338135\n",
      "Epoch 6, batch 147 D Loss: 0.8602449297904968, G Loss: 1.1021983623504639\n",
      "Epoch 6, batch 148 D Loss: 0.7260971069335938, G Loss: 1.173843264579773\n",
      "Epoch 6, batch 149 D Loss: 1.003197431564331, G Loss: 1.1014528274536133\n",
      "Epoch 6, batch 150 D Loss: 0.9678329229354858, G Loss: 1.144984483718872\n",
      "Epoch 6, batch 151 D Loss: 0.9120510816574097, G Loss: 1.0945541858673096\n",
      "Epoch 6, batch 152 D Loss: 0.9797251224517822, G Loss: 1.1008111238479614\n",
      "Epoch 6, batch 153 D Loss: 0.9381570219993591, G Loss: 1.0985188484191895\n",
      "Epoch 6, batch 154 D Loss: 0.8447906970977783, G Loss: 1.1023081541061401\n",
      "Epoch 6, batch 155 D Loss: 0.9494965076446533, G Loss: 1.0873243808746338\n",
      "Epoch 6, batch 156 D Loss: 0.9118038415908813, G Loss: 1.1462656259536743\n",
      "Epoch 6, batch 157 D Loss: 0.9311881065368652, G Loss: 1.0863186120986938\n",
      "Epoch 6, batch 158 D Loss: 0.8241780996322632, G Loss: 1.1750811338424683\n",
      "Epoch 6, batch 159 D Loss: 0.9956721663475037, G Loss: 1.139556884765625\n",
      "Epoch 6, batch 160 D Loss: 0.9203375577926636, G Loss: 1.1598165035247803\n",
      "Epoch 6, batch 161 D Loss: 1.0180959701538086, G Loss: 1.1084909439086914\n",
      "Epoch 6, batch 162 D Loss: 1.0451452732086182, G Loss: 1.1339765787124634\n",
      "Epoch 6, batch 163 D Loss: 1.0647594928741455, G Loss: 1.1256204843521118\n",
      "Epoch 6, batch 164 D Loss: 0.9820247888565063, G Loss: 1.1263113021850586\n",
      "Epoch 6, batch 165 D Loss: 0.9153692126274109, G Loss: 1.1396174430847168\n",
      "Epoch 6, batch 166 D Loss: 1.0457439422607422, G Loss: 1.1397452354431152\n",
      "Epoch 6, batch 167 D Loss: 1.1608606576919556, G Loss: 1.0807338953018188\n",
      "Epoch 6, batch 168 D Loss: 0.9552716016769409, G Loss: 1.1500548124313354\n",
      "Epoch 6, batch 169 D Loss: 1.021092414855957, G Loss: 1.0929443836212158\n",
      "Epoch 6, batch 170 D Loss: 0.8822512030601501, G Loss: 1.14601469039917\n",
      "Epoch 6, batch 171 D Loss: 1.0562071800231934, G Loss: 1.077416181564331\n",
      "Epoch 6, batch 172 D Loss: 0.993122935295105, G Loss: 1.050611138343811\n",
      "Epoch 6, batch 173 D Loss: 0.9425510168075562, G Loss: 1.1226660013198853\n",
      "Epoch 6, batch 174 D Loss: 1.0088521242141724, G Loss: 1.0290353298187256\n",
      "Epoch 6, batch 175 D Loss: 0.9082711935043335, G Loss: 1.138026475906372\n",
      "Epoch 6, batch 176 D Loss: 0.9404071569442749, G Loss: 1.063409686088562\n",
      "Epoch 6, batch 177 D Loss: 1.0579326152801514, G Loss: 1.081516981124878\n",
      "Epoch 6, batch 178 D Loss: 1.100881814956665, G Loss: 1.0435117483139038\n",
      "Epoch 6, batch 179 D Loss: 0.9992680549621582, G Loss: 1.0617595911026\n",
      "Epoch 6, batch 180 D Loss: 0.9707328677177429, G Loss: 1.1331943273544312\n",
      "Epoch 6, batch 181 D Loss: 0.9725484848022461, G Loss: 1.1073287725448608\n",
      "Epoch 6, batch 182 D Loss: 0.7604941129684448, G Loss: 1.1007320880889893\n",
      "Epoch 6, batch 183 D Loss: 1.0708900690078735, G Loss: 1.0683979988098145\n",
      "Epoch 6, batch 184 D Loss: 0.9900365471839905, G Loss: 1.1144036054611206\n",
      "Epoch 6, batch 185 D Loss: 0.8920260667800903, G Loss: 1.1328415870666504\n",
      "Epoch 6, batch 186 D Loss: 0.9599243402481079, G Loss: 1.1322965621948242\n",
      "Epoch 6, batch 187 D Loss: 1.010327935218811, G Loss: 1.1080490350723267\n",
      "Epoch 6, batch 188 D Loss: 0.8526335954666138, G Loss: 1.0202587842941284\n",
      "Epoch 6, batch 189 D Loss: 1.078992247581482, G Loss: 1.0838245153427124\n",
      "Epoch 6, batch 190 D Loss: 0.9682977199554443, G Loss: 1.047663688659668\n",
      "Epoch 6, batch 191 D Loss: 1.015538215637207, G Loss: 1.0582941770553589\n",
      "Epoch 6, batch 192 D Loss: 0.9455870389938354, G Loss: 1.101173996925354\n",
      "Epoch 6, batch 193 D Loss: 0.9658893942832947, G Loss: 1.0807907581329346\n",
      "Epoch 6, batch 194 D Loss: 1.1668081283569336, G Loss: 0.9907359480857849\n",
      "Epoch 6, batch 195 D Loss: 1.0809719562530518, G Loss: 1.0935877561569214\n",
      "Epoch 6, batch 196 D Loss: 1.0447771549224854, G Loss: 1.0542453527450562\n",
      "Epoch 6, batch 197 D Loss: 0.9701374769210815, G Loss: 1.0949245691299438\n",
      "Epoch 6, batch 198 D Loss: 0.9391809105873108, G Loss: 1.067373275756836\n",
      "Epoch 6, batch 199 D Loss: 0.9480628967285156, G Loss: 1.038422703742981\n",
      "Epoch 6, batch 200 D Loss: 0.8184863328933716, G Loss: 1.1057868003845215\n",
      "Epoch 7, batch 1 D Loss: 0.8892835974693298, G Loss: 1.1042989492416382\n",
      "Epoch 7, batch 2 D Loss: 1.124485969543457, G Loss: 1.1024372577667236\n",
      "Epoch 7, batch 3 D Loss: 0.952595591545105, G Loss: 1.108826756477356\n",
      "Epoch 7, batch 4 D Loss: 1.1759557723999023, G Loss: 1.1052405834197998\n",
      "Epoch 7, batch 5 D Loss: 1.0842393636703491, G Loss: 1.0288878679275513\n",
      "Epoch 7, batch 6 D Loss: 1.0579168796539307, G Loss: 1.0541326999664307\n",
      "Epoch 7, batch 7 D Loss: 1.168898105621338, G Loss: 0.9444368481636047\n",
      "Epoch 7, batch 8 D Loss: 0.9634084701538086, G Loss: 1.0319693088531494\n",
      "Epoch 7, batch 9 D Loss: 1.0775418281555176, G Loss: 1.1448272466659546\n",
      "Epoch 7, batch 10 D Loss: 1.0218660831451416, G Loss: 1.0787476301193237\n",
      "Epoch 7, batch 11 D Loss: 1.05422043800354, G Loss: 1.030348539352417\n",
      "Epoch 7, batch 12 D Loss: 0.992406964302063, G Loss: 1.0521310567855835\n",
      "Epoch 7, batch 13 D Loss: 0.9480071067810059, G Loss: 1.0426781177520752\n",
      "Epoch 7, batch 14 D Loss: 1.015078067779541, G Loss: 1.0649665594100952\n",
      "Epoch 7, batch 15 D Loss: 1.0737851858139038, G Loss: 1.0452749729156494\n",
      "Epoch 7, batch 16 D Loss: 1.2825274467468262, G Loss: 0.9232452511787415\n",
      "Epoch 7, batch 17 D Loss: 1.0068368911743164, G Loss: 1.0874253511428833\n",
      "Epoch 7, batch 18 D Loss: 1.1512339115142822, G Loss: 1.0367562770843506\n",
      "Epoch 7, batch 19 D Loss: 1.0165724754333496, G Loss: 1.1254897117614746\n",
      "Epoch 7, batch 20 D Loss: 0.8690330982208252, G Loss: 1.051465392112732\n",
      "Epoch 7, batch 21 D Loss: 1.0459965467453003, G Loss: 1.0635201930999756\n",
      "Epoch 7, batch 22 D Loss: 1.0245842933654785, G Loss: 1.0823334455490112\n",
      "Epoch 7, batch 23 D Loss: 0.979217529296875, G Loss: 1.1674432754516602\n",
      "Epoch 7, batch 24 D Loss: 1.1144278049468994, G Loss: 1.1113379001617432\n",
      "Epoch 7, batch 25 D Loss: 1.053961992263794, G Loss: 1.1045423746109009\n",
      "Epoch 7, batch 26 D Loss: 1.2178585529327393, G Loss: 1.0126709938049316\n",
      "Epoch 7, batch 27 D Loss: 1.0052423477172852, G Loss: 1.0945299863815308\n",
      "Epoch 7, batch 28 D Loss: 0.9906014204025269, G Loss: 1.1308767795562744\n",
      "Epoch 7, batch 29 D Loss: 1.2378406524658203, G Loss: 0.9998600482940674\n",
      "Epoch 7, batch 30 D Loss: 1.0775578022003174, G Loss: 1.0205659866333008\n",
      "Epoch 7, batch 31 D Loss: 0.9547601342201233, G Loss: 1.10812246799469\n",
      "Epoch 7, batch 32 D Loss: 1.2027876377105713, G Loss: 1.0718200206756592\n",
      "Epoch 7, batch 33 D Loss: 1.0691633224487305, G Loss: 1.0551512241363525\n",
      "Epoch 7, batch 34 D Loss: 1.0908896923065186, G Loss: 1.125747799873352\n",
      "Epoch 7, batch 35 D Loss: 1.0709084272384644, G Loss: 1.107763648033142\n",
      "Epoch 7, batch 36 D Loss: 1.042560338973999, G Loss: 1.1114592552185059\n",
      "Epoch 7, batch 37 D Loss: 1.0420787334442139, G Loss: 1.1445293426513672\n",
      "Epoch 7, batch 38 D Loss: 1.1130774021148682, G Loss: 1.102652907371521\n",
      "Epoch 7, batch 39 D Loss: 1.2584996223449707, G Loss: 1.090909481048584\n",
      "Epoch 7, batch 40 D Loss: 0.9993478059768677, G Loss: 1.096957802772522\n",
      "Epoch 7, batch 41 D Loss: 1.1188865900039673, G Loss: 1.0597645044326782\n",
      "Epoch 7, batch 42 D Loss: 1.1930772066116333, G Loss: 1.0893133878707886\n",
      "Epoch 7, batch 43 D Loss: 1.0322526693344116, G Loss: 1.1278942823410034\n",
      "Epoch 7, batch 44 D Loss: 0.9227440357208252, G Loss: 1.0623035430908203\n",
      "Epoch 7, batch 45 D Loss: 1.041975736618042, G Loss: 1.1326720714569092\n",
      "Epoch 7, batch 46 D Loss: 0.9923896789550781, G Loss: 1.0069602727890015\n",
      "Epoch 7, batch 47 D Loss: 1.2611287832260132, G Loss: 1.039809226989746\n",
      "Epoch 7, batch 48 D Loss: 1.0625145435333252, G Loss: 1.051434874534607\n",
      "Epoch 7, batch 49 D Loss: 1.0848076343536377, G Loss: 1.0393614768981934\n",
      "Epoch 7, batch 50 D Loss: 1.2332794666290283, G Loss: 1.0819287300109863\n",
      "Epoch 7, batch 51 D Loss: 0.9968645572662354, G Loss: 1.1394516229629517\n",
      "Epoch 7, batch 52 D Loss: 1.0448384284973145, G Loss: 1.1259959936141968\n",
      "Epoch 7, batch 53 D Loss: 1.2176960706710815, G Loss: 1.149526596069336\n",
      "Epoch 7, batch 54 D Loss: 1.091909646987915, G Loss: 1.1141749620437622\n",
      "Epoch 7, batch 55 D Loss: 1.1408789157867432, G Loss: 1.0793657302856445\n",
      "Epoch 7, batch 56 D Loss: 1.1651091575622559, G Loss: 1.0308722257614136\n",
      "Epoch 7, batch 57 D Loss: 1.0591355562210083, G Loss: 1.0218632221221924\n",
      "Epoch 7, batch 58 D Loss: 1.2884397506713867, G Loss: 0.9013470411300659\n",
      "Epoch 7, batch 59 D Loss: 1.0803649425506592, G Loss: 0.9455049633979797\n",
      "Epoch 7, batch 60 D Loss: 1.2536832094192505, G Loss: 0.9297594428062439\n",
      "Epoch 7, batch 61 D Loss: 1.1604716777801514, G Loss: 1.087969422340393\n",
      "Epoch 7, batch 62 D Loss: 1.2710785865783691, G Loss: 1.0092048645019531\n",
      "Epoch 7, batch 63 D Loss: 1.141340732574463, G Loss: 1.0564693212509155\n",
      "Epoch 7, batch 64 D Loss: 1.2469451427459717, G Loss: 1.022213339805603\n",
      "Epoch 7, batch 65 D Loss: 1.1702924966812134, G Loss: 1.0894615650177002\n",
      "Epoch 7, batch 66 D Loss: 1.2816762924194336, G Loss: 1.0847556591033936\n",
      "Epoch 7, batch 67 D Loss: 1.1350451707839966, G Loss: 1.0489689111709595\n",
      "Epoch 7, batch 68 D Loss: 1.1079927682876587, G Loss: 1.0545902252197266\n",
      "Epoch 7, batch 69 D Loss: 1.0638940334320068, G Loss: 1.035404086112976\n",
      "Epoch 7, batch 70 D Loss: 1.1792941093444824, G Loss: 1.0581011772155762\n",
      "Epoch 7, batch 71 D Loss: 1.19773530960083, G Loss: 0.9742708802223206\n",
      "Epoch 7, batch 72 D Loss: 1.3607685565948486, G Loss: 0.9771034717559814\n",
      "Epoch 7, batch 73 D Loss: 1.1491172313690186, G Loss: 1.0715824365615845\n",
      "Epoch 7, batch 74 D Loss: 1.1686842441558838, G Loss: 1.1014010906219482\n",
      "Epoch 7, batch 75 D Loss: 1.270517349243164, G Loss: 1.078574299812317\n",
      "Epoch 7, batch 76 D Loss: 1.2698228359222412, G Loss: 0.9984398484230042\n",
      "Epoch 7, batch 77 D Loss: 1.1251274347305298, G Loss: 1.012376308441162\n",
      "Epoch 7, batch 78 D Loss: 1.1460239887237549, G Loss: 1.057652473449707\n",
      "Epoch 7, batch 79 D Loss: 1.0963339805603027, G Loss: 1.1356785297393799\n",
      "Epoch 7, batch 80 D Loss: 1.2387115955352783, G Loss: 0.9732933044433594\n",
      "Epoch 7, batch 81 D Loss: 1.1558998823165894, G Loss: 1.1522462368011475\n",
      "Epoch 7, batch 82 D Loss: 1.322182059288025, G Loss: 1.083787202835083\n",
      "Epoch 7, batch 83 D Loss: 1.2330944538116455, G Loss: 1.0785821676254272\n",
      "Epoch 7, batch 84 D Loss: 1.2508950233459473, G Loss: 0.9733555316925049\n",
      "Epoch 7, batch 85 D Loss: 1.165875792503357, G Loss: 1.0423879623413086\n",
      "Epoch 7, batch 86 D Loss: 1.3353585004806519, G Loss: 1.0599347352981567\n",
      "Epoch 7, batch 87 D Loss: 1.3072917461395264, G Loss: 0.9823378920555115\n",
      "Epoch 7, batch 88 D Loss: 1.3142204284667969, G Loss: 0.9753148555755615\n",
      "Epoch 7, batch 89 D Loss: 1.2898541688919067, G Loss: 1.0238116979599\n",
      "Epoch 7, batch 90 D Loss: 1.201606035232544, G Loss: 1.0505565404891968\n",
      "Epoch 7, batch 91 D Loss: 1.1918070316314697, G Loss: 1.0383449792861938\n",
      "Epoch 7, batch 92 D Loss: 0.9927921891212463, G Loss: 1.1294658184051514\n",
      "Epoch 7, batch 93 D Loss: 1.2253787517547607, G Loss: 1.109722375869751\n",
      "Epoch 7, batch 94 D Loss: 1.2755463123321533, G Loss: 1.0327973365783691\n",
      "Epoch 7, batch 95 D Loss: 1.2566478252410889, G Loss: 1.1127755641937256\n",
      "Epoch 7, batch 96 D Loss: 1.243964672088623, G Loss: 1.0854790210723877\n",
      "Epoch 7, batch 97 D Loss: 1.2831227779388428, G Loss: 1.1115528345108032\n",
      "Epoch 7, batch 98 D Loss: 1.1930028200149536, G Loss: 1.082697868347168\n",
      "Epoch 7, batch 99 D Loss: 1.3587872982025146, G Loss: 1.0990582704544067\n",
      "Epoch 7, batch 100 D Loss: 1.2568233013153076, G Loss: 1.134179711341858\n",
      "Epoch 7, batch 101 D Loss: 1.344465732574463, G Loss: 0.9961375594139099\n",
      "Epoch 7, batch 102 D Loss: 1.232121467590332, G Loss: 1.1448456048965454\n",
      "Epoch 7, batch 103 D Loss: 1.3844184875488281, G Loss: 1.0733251571655273\n",
      "Epoch 7, batch 104 D Loss: 1.341905951499939, G Loss: 1.13756263256073\n",
      "Epoch 7, batch 105 D Loss: 1.136918544769287, G Loss: 1.113176941871643\n",
      "Epoch 7, batch 106 D Loss: 1.605904221534729, G Loss: 0.9913337826728821\n",
      "Epoch 7, batch 107 D Loss: 1.0493535995483398, G Loss: 1.2642265558242798\n",
      "Epoch 7, batch 108 D Loss: 1.405373454093933, G Loss: 0.9589529633522034\n",
      "Epoch 7, batch 109 D Loss: 1.4922772645950317, G Loss: 1.0241618156433105\n",
      "Epoch 7, batch 110 D Loss: 1.3066232204437256, G Loss: 1.039609432220459\n",
      "Epoch 7, batch 111 D Loss: 1.2350305318832397, G Loss: 1.105799913406372\n",
      "Epoch 7, batch 112 D Loss: 1.8281073570251465, G Loss: 0.7635640501976013\n",
      "Epoch 7, batch 113 D Loss: 1.4790774583816528, G Loss: 0.9645574688911438\n",
      "Epoch 7, batch 114 D Loss: 1.3797590732574463, G Loss: 1.0598076581954956\n",
      "Epoch 7, batch 115 D Loss: 1.079406976699829, G Loss: 1.15325129032135\n",
      "Epoch 7, batch 116 D Loss: 1.342362642288208, G Loss: 1.1170462369918823\n",
      "Epoch 7, batch 117 D Loss: 1.485887050628662, G Loss: 1.0008505582809448\n",
      "Epoch 7, batch 118 D Loss: 1.385527491569519, G Loss: 1.0106388330459595\n",
      "Epoch 7, batch 119 D Loss: 1.2734594345092773, G Loss: 1.0780483484268188\n",
      "Epoch 7, batch 120 D Loss: 1.1748924255371094, G Loss: 1.0936609506607056\n",
      "Epoch 7, batch 121 D Loss: 1.1687586307525635, G Loss: 1.1594899892807007\n",
      "Epoch 7, batch 122 D Loss: 1.4039099216461182, G Loss: 0.9497352838516235\n",
      "Epoch 7, batch 123 D Loss: 1.249213457107544, G Loss: 1.0641186237335205\n",
      "Epoch 7, batch 124 D Loss: 1.5145680904388428, G Loss: 1.0185136795043945\n",
      "Epoch 7, batch 125 D Loss: 1.3951611518859863, G Loss: 1.0680108070373535\n",
      "Epoch 7, batch 126 D Loss: 1.2801668643951416, G Loss: 1.0858365297317505\n",
      "Epoch 7, batch 127 D Loss: 1.1908955574035645, G Loss: 1.0810425281524658\n",
      "Epoch 7, batch 128 D Loss: 1.5914700031280518, G Loss: 0.9701635837554932\n",
      "Epoch 7, batch 129 D Loss: 1.3138138055801392, G Loss: 1.1455268859863281\n",
      "Epoch 7, batch 130 D Loss: 1.3994163274765015, G Loss: 1.015988826751709\n",
      "Epoch 7, batch 131 D Loss: 1.5593700408935547, G Loss: 1.0728576183319092\n",
      "Epoch 7, batch 132 D Loss: 1.458160161972046, G Loss: 0.9933391809463501\n",
      "Epoch 7, batch 133 D Loss: 1.09478759765625, G Loss: 1.129661202430725\n",
      "Epoch 7, batch 134 D Loss: 1.4034574031829834, G Loss: 1.1770168542861938\n",
      "Epoch 7, batch 135 D Loss: 1.3665728569030762, G Loss: 0.9069135189056396\n",
      "Epoch 7, batch 136 D Loss: 1.1748801469802856, G Loss: 1.0642508268356323\n",
      "Epoch 7, batch 137 D Loss: 1.3209826946258545, G Loss: 1.041827917098999\n",
      "Epoch 7, batch 138 D Loss: 1.5996484756469727, G Loss: 1.0319504737854004\n",
      "Epoch 7, batch 139 D Loss: 1.3946480751037598, G Loss: 1.0198193788528442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, batch 140 D Loss: 1.3578720092773438, G Loss: 1.0872981548309326\n",
      "Epoch 7, batch 141 D Loss: 1.4447600841522217, G Loss: 1.056505560874939\n",
      "Epoch 7, batch 142 D Loss: 1.3345651626586914, G Loss: 1.089594841003418\n",
      "Epoch 7, batch 143 D Loss: 1.478127360343933, G Loss: 0.9927768111228943\n",
      "Epoch 7, batch 144 D Loss: 1.4137628078460693, G Loss: 0.9224992990493774\n",
      "Epoch 7, batch 145 D Loss: 1.563934326171875, G Loss: 0.9282540678977966\n",
      "Epoch 7, batch 146 D Loss: 1.4158231019973755, G Loss: 1.0347700119018555\n",
      "Epoch 7, batch 147 D Loss: 1.3313167095184326, G Loss: 1.044073224067688\n",
      "Epoch 7, batch 148 D Loss: 1.3685126304626465, G Loss: 1.1438981294631958\n",
      "Epoch 7, batch 149 D Loss: 1.5235055685043335, G Loss: 0.9899482727050781\n",
      "Epoch 7, batch 150 D Loss: 1.4388517141342163, G Loss: 0.9650804996490479\n",
      "Epoch 7, batch 151 D Loss: 1.366971492767334, G Loss: 1.0110605955123901\n",
      "Epoch 7, batch 152 D Loss: 1.27174711227417, G Loss: 1.0791542530059814\n",
      "Epoch 7, batch 153 D Loss: 1.4206202030181885, G Loss: 0.9202477931976318\n",
      "Epoch 7, batch 154 D Loss: 1.2633860111236572, G Loss: 1.0373071432113647\n",
      "Epoch 7, batch 155 D Loss: 1.2882425785064697, G Loss: 1.0109262466430664\n",
      "Epoch 7, batch 156 D Loss: 1.4443705081939697, G Loss: 1.094399333000183\n",
      "Epoch 7, batch 157 D Loss: 1.3265676498413086, G Loss: 1.0326247215270996\n",
      "Epoch 7, batch 158 D Loss: 1.1933691501617432, G Loss: 1.1203404664993286\n",
      "Epoch 7, batch 159 D Loss: 1.3734941482543945, G Loss: 0.9880275726318359\n",
      "Epoch 7, batch 160 D Loss: 1.3451308012008667, G Loss: 1.084713339805603\n",
      "Epoch 7, batch 161 D Loss: 1.4706671237945557, G Loss: 0.9307212829589844\n",
      "Epoch 7, batch 162 D Loss: 1.4413111209869385, G Loss: 0.8925806283950806\n",
      "Epoch 7, batch 163 D Loss: 1.3015639781951904, G Loss: 1.0157427787780762\n",
      "Epoch 7, batch 164 D Loss: 1.3037132024765015, G Loss: 1.0438951253890991\n",
      "Epoch 7, batch 165 D Loss: 1.3730263710021973, G Loss: 1.1394834518432617\n",
      "Epoch 7, batch 166 D Loss: 1.1695647239685059, G Loss: 1.154991865158081\n",
      "Epoch 7, batch 167 D Loss: 1.280285120010376, G Loss: 1.0518558025360107\n",
      "Epoch 7, batch 168 D Loss: 1.3100926876068115, G Loss: 1.0104330778121948\n",
      "Epoch 7, batch 169 D Loss: 1.232625961303711, G Loss: 1.1339823007583618\n",
      "Epoch 7, batch 170 D Loss: 1.3687633275985718, G Loss: 1.035353660583496\n",
      "Epoch 7, batch 171 D Loss: 1.3392291069030762, G Loss: 1.0351777076721191\n",
      "Epoch 7, batch 172 D Loss: 1.1679956912994385, G Loss: 1.1551402807235718\n",
      "Epoch 7, batch 173 D Loss: 1.2260912656784058, G Loss: 0.9931164383888245\n",
      "Epoch 7, batch 174 D Loss: 1.6050224304199219, G Loss: 0.9944417476654053\n",
      "Epoch 7, batch 175 D Loss: 1.3151241540908813, G Loss: 1.092766523361206\n",
      "Epoch 7, batch 176 D Loss: 1.306881308555603, G Loss: 0.9958796501159668\n",
      "Epoch 7, batch 177 D Loss: 1.3533775806427002, G Loss: 1.0166523456573486\n",
      "Epoch 7, batch 178 D Loss: 1.2743639945983887, G Loss: 1.1206045150756836\n",
      "Epoch 7, batch 179 D Loss: 1.1397782564163208, G Loss: 1.182012915611267\n",
      "Epoch 7, batch 180 D Loss: 1.2849102020263672, G Loss: 1.1873286962509155\n",
      "Epoch 7, batch 181 D Loss: 1.3503265380859375, G Loss: 1.1048226356506348\n",
      "Epoch 7, batch 182 D Loss: 1.126323938369751, G Loss: 1.1095832586288452\n",
      "Epoch 7, batch 183 D Loss: 1.1583576202392578, G Loss: 1.1501376628875732\n",
      "Epoch 7, batch 184 D Loss: 1.211935043334961, G Loss: 1.1077384948730469\n",
      "Epoch 7, batch 185 D Loss: 1.1618597507476807, G Loss: 1.054490089416504\n",
      "Epoch 7, batch 186 D Loss: 1.2323448657989502, G Loss: 1.0353084802627563\n",
      "Epoch 7, batch 187 D Loss: 1.2707513570785522, G Loss: 1.0284042358398438\n",
      "Epoch 7, batch 188 D Loss: 1.3696084022521973, G Loss: 1.0824179649353027\n",
      "Epoch 7, batch 189 D Loss: 1.1958339214324951, G Loss: 1.119571566581726\n",
      "Epoch 7, batch 190 D Loss: 1.154358148574829, G Loss: 1.1842769384384155\n",
      "Epoch 7, batch 191 D Loss: 1.21815025806427, G Loss: 1.1684024333953857\n",
      "Epoch 7, batch 192 D Loss: 1.2060723304748535, G Loss: 1.1403183937072754\n",
      "Epoch 7, batch 193 D Loss: 1.197603702545166, G Loss: 1.104737401008606\n",
      "Epoch 7, batch 194 D Loss: 1.2957407236099243, G Loss: 1.0297131538391113\n",
      "Epoch 7, batch 195 D Loss: 1.1954326629638672, G Loss: 1.0465885400772095\n",
      "Epoch 7, batch 196 D Loss: 1.2345149517059326, G Loss: 1.0372624397277832\n",
      "Epoch 7, batch 197 D Loss: 1.2266608476638794, G Loss: 1.0803728103637695\n",
      "Epoch 7, batch 198 D Loss: 1.1870135068893433, G Loss: 1.0869382619857788\n",
      "Epoch 7, batch 199 D Loss: 1.2398580312728882, G Loss: 1.027235746383667\n",
      "Epoch 7, batch 200 D Loss: 1.280299425125122, G Loss: 0.987626850605011\n",
      "Epoch 8, batch 1 D Loss: 1.1989014148712158, G Loss: 1.0226317644119263\n",
      "Epoch 8, batch 2 D Loss: 1.2828712463378906, G Loss: 1.0470043420791626\n",
      "Epoch 8, batch 3 D Loss: 1.1935317516326904, G Loss: 1.025071620941162\n",
      "Epoch 8, batch 4 D Loss: 1.2885372638702393, G Loss: 1.0252529382705688\n",
      "Epoch 8, batch 5 D Loss: 1.201770544052124, G Loss: 1.0624706745147705\n",
      "Epoch 8, batch 6 D Loss: 1.2407808303833008, G Loss: 1.0507086515426636\n",
      "Epoch 8, batch 7 D Loss: 1.1042814254760742, G Loss: 1.118529200553894\n",
      "Epoch 8, batch 8 D Loss: 1.3497581481933594, G Loss: 1.0041635036468506\n",
      "Epoch 8, batch 9 D Loss: 1.164762020111084, G Loss: 0.9882591366767883\n",
      "Epoch 8, batch 10 D Loss: 1.2796311378479004, G Loss: 1.0470067262649536\n",
      "Epoch 8, batch 11 D Loss: 1.2236385345458984, G Loss: 0.9874122738838196\n",
      "Epoch 8, batch 12 D Loss: 1.1253914833068848, G Loss: 1.094874382019043\n",
      "Epoch 8, batch 13 D Loss: 1.0532360076904297, G Loss: 1.0612467527389526\n",
      "Epoch 8, batch 14 D Loss: 1.2888835668563843, G Loss: 1.0299631357192993\n",
      "Epoch 8, batch 15 D Loss: 1.2448481321334839, G Loss: 1.0296212434768677\n",
      "Epoch 8, batch 16 D Loss: 1.3393502235412598, G Loss: 1.0276724100112915\n",
      "Epoch 8, batch 17 D Loss: 1.155583143234253, G Loss: 1.02241849899292\n",
      "Epoch 8, batch 18 D Loss: 1.067842721939087, G Loss: 1.0496900081634521\n",
      "Epoch 8, batch 19 D Loss: 1.3165268898010254, G Loss: 0.9528817534446716\n",
      "Epoch 8, batch 20 D Loss: 1.342790961265564, G Loss: 0.9440614581108093\n",
      "Epoch 8, batch 21 D Loss: 1.313770055770874, G Loss: 0.993461012840271\n",
      "Epoch 8, batch 22 D Loss: 1.0625450611114502, G Loss: 1.1055614948272705\n",
      "Epoch 8, batch 23 D Loss: 1.262601375579834, G Loss: 1.0013943910598755\n",
      "Epoch 8, batch 24 D Loss: 1.0478501319885254, G Loss: 1.050473690032959\n",
      "Epoch 8, batch 25 D Loss: 1.2870433330535889, G Loss: 0.9651567339897156\n",
      "Epoch 8, batch 26 D Loss: 1.3566625118255615, G Loss: 0.8837095499038696\n",
      "Epoch 8, batch 27 D Loss: 1.3428103923797607, G Loss: 0.923420250415802\n",
      "Epoch 8, batch 28 D Loss: 1.2231742143630981, G Loss: 1.0271774530410767\n",
      "Epoch 8, batch 29 D Loss: 1.3614020347595215, G Loss: 0.9604426622390747\n",
      "Epoch 8, batch 30 D Loss: 1.203542947769165, G Loss: 1.001039981842041\n",
      "Epoch 8, batch 31 D Loss: 1.2006396055221558, G Loss: 0.9940246343612671\n",
      "Epoch 8, batch 32 D Loss: 1.0999783277511597, G Loss: 0.9741644859313965\n",
      "Epoch 8, batch 33 D Loss: 1.173814058303833, G Loss: 1.0226147174835205\n",
      "Epoch 8, batch 34 D Loss: 1.2883849143981934, G Loss: 0.9384641051292419\n",
      "Epoch 8, batch 35 D Loss: 1.3004610538482666, G Loss: 0.8922470808029175\n",
      "Epoch 8, batch 36 D Loss: 1.3635525703430176, G Loss: 0.9739844799041748\n",
      "Epoch 8, batch 37 D Loss: 1.2695116996765137, G Loss: 0.9439192414283752\n",
      "Epoch 8, batch 38 D Loss: 1.2591608762741089, G Loss: 0.99248868227005\n",
      "Epoch 8, batch 39 D Loss: 1.231480360031128, G Loss: 0.9608627557754517\n",
      "Epoch 8, batch 40 D Loss: 1.3732879161834717, G Loss: 0.9008123874664307\n",
      "Epoch 8, batch 41 D Loss: 1.3126862049102783, G Loss: 0.8945385217666626\n",
      "Epoch 8, batch 42 D Loss: 1.3796014785766602, G Loss: 0.8993789553642273\n",
      "Epoch 8, batch 43 D Loss: 1.1420350074768066, G Loss: 0.939400315284729\n",
      "Epoch 8, batch 44 D Loss: 1.3789010047912598, G Loss: 0.9228060841560364\n",
      "Epoch 8, batch 45 D Loss: 1.1467006206512451, G Loss: 0.9793882966041565\n",
      "Epoch 8, batch 46 D Loss: 1.1482455730438232, G Loss: 0.9216496348381042\n",
      "Epoch 8, batch 47 D Loss: 1.1952916383743286, G Loss: 0.9104366898536682\n",
      "Epoch 8, batch 48 D Loss: 1.5354349613189697, G Loss: 0.8258062601089478\n",
      "Epoch 8, batch 49 D Loss: 1.1883430480957031, G Loss: 0.9728997945785522\n",
      "Epoch 8, batch 50 D Loss: 1.445817470550537, G Loss: 0.8677911162376404\n",
      "Epoch 8, batch 51 D Loss: 1.267366647720337, G Loss: 0.9156923890113831\n",
      "Epoch 8, batch 52 D Loss: 1.1275553703308105, G Loss: 0.9837937355041504\n",
      "Epoch 8, batch 53 D Loss: 1.293358564376831, G Loss: 0.8975609540939331\n",
      "Epoch 8, batch 54 D Loss: 1.2087671756744385, G Loss: 0.9451274275779724\n",
      "Epoch 8, batch 55 D Loss: 1.314085602760315, G Loss: 0.9625663161277771\n",
      "Epoch 8, batch 56 D Loss: 1.3226418495178223, G Loss: 0.8903113007545471\n",
      "Epoch 8, batch 57 D Loss: 1.19916570186615, G Loss: 0.963877260684967\n",
      "Epoch 8, batch 58 D Loss: 1.2837629318237305, G Loss: 0.9355860352516174\n",
      "Epoch 8, batch 59 D Loss: 1.388697862625122, G Loss: 0.8503472805023193\n",
      "Epoch 8, batch 60 D Loss: 1.303464651107788, G Loss: 0.9017722606658936\n",
      "Epoch 8, batch 61 D Loss: 1.1507600545883179, G Loss: 0.9903531074523926\n",
      "Epoch 8, batch 62 D Loss: 1.3553109169006348, G Loss: 0.8489545583724976\n",
      "Epoch 8, batch 63 D Loss: 1.3442127704620361, G Loss: 0.8759362101554871\n",
      "Epoch 8, batch 64 D Loss: 1.597260594367981, G Loss: 0.866347074508667\n",
      "Epoch 8, batch 65 D Loss: 1.4066081047058105, G Loss: 0.830812394618988\n",
      "Epoch 8, batch 66 D Loss: 1.4482536315917969, G Loss: 0.7911142706871033\n",
      "Epoch 8, batch 67 D Loss: 1.3576951026916504, G Loss: 0.9553366303443909\n",
      "Epoch 8, batch 68 D Loss: 1.255932092666626, G Loss: 0.8882504105567932\n",
      "Epoch 8, batch 69 D Loss: 1.2105242013931274, G Loss: 0.9400970339775085\n",
      "Epoch 8, batch 70 D Loss: 1.2741053104400635, G Loss: 0.8870476484298706\n",
      "Epoch 8, batch 71 D Loss: 1.6419551372528076, G Loss: 0.7118756771087646\n",
      "Epoch 8, batch 72 D Loss: 1.5108416080474854, G Loss: 0.8102839589118958\n",
      "Epoch 8, batch 73 D Loss: 1.1985554695129395, G Loss: 0.9405872821807861\n",
      "Epoch 8, batch 74 D Loss: 1.2841284275054932, G Loss: 0.8965628743171692\n",
      "Epoch 8, batch 75 D Loss: 1.4605529308319092, G Loss: 0.8202528357505798\n",
      "Epoch 8, batch 76 D Loss: 1.4108824729919434, G Loss: 0.7548058032989502\n",
      "Epoch 8, batch 77 D Loss: 1.4782371520996094, G Loss: 0.8234425187110901\n",
      "Epoch 8, batch 78 D Loss: 1.3253750801086426, G Loss: 0.8320053815841675\n",
      "Epoch 8, batch 79 D Loss: 1.674422025680542, G Loss: 0.7380027770996094\n",
      "Epoch 8, batch 80 D Loss: 1.5796663761138916, G Loss: 0.8115687370300293\n",
      "Epoch 8, batch 81 D Loss: 1.5422570705413818, G Loss: 0.8633646965026855\n",
      "Epoch 8, batch 82 D Loss: 1.4668481349945068, G Loss: 0.8399137854576111\n",
      "Epoch 8, batch 83 D Loss: 1.5919945240020752, G Loss: 0.7263486981391907\n",
      "Epoch 8, batch 84 D Loss: 1.4779402017593384, G Loss: 0.8784310221672058\n",
      "Epoch 8, batch 85 D Loss: 1.6697707176208496, G Loss: 0.7670672535896301\n",
      "Epoch 8, batch 86 D Loss: 1.5148828029632568, G Loss: 0.8117550611495972\n",
      "Epoch 8, batch 87 D Loss: 1.2950193881988525, G Loss: 0.838759183883667\n",
      "Epoch 8, batch 88 D Loss: 1.554537057876587, G Loss: 0.776237428188324\n",
      "Epoch 8, batch 89 D Loss: 1.4847584962844849, G Loss: 0.7780718803405762\n",
      "Epoch 8, batch 90 D Loss: 1.5619674921035767, G Loss: 0.8077106475830078\n",
      "Epoch 8, batch 91 D Loss: 1.6034948825836182, G Loss: 0.779496431350708\n",
      "Epoch 8, batch 92 D Loss: 1.3918132781982422, G Loss: 0.833702564239502\n",
      "Epoch 8, batch 93 D Loss: 1.560976266860962, G Loss: 0.7186358571052551\n",
      "Epoch 8, batch 94 D Loss: 1.5947887897491455, G Loss: 0.7420675754547119\n",
      "Epoch 8, batch 95 D Loss: 1.4407525062561035, G Loss: 0.7921898365020752\n",
      "Epoch 8, batch 96 D Loss: 1.4240161180496216, G Loss: 0.7944722771644592\n",
      "Epoch 8, batch 97 D Loss: 1.4288835525512695, G Loss: 0.7659260630607605\n",
      "Epoch 8, batch 98 D Loss: 1.5886280536651611, G Loss: 0.7309300303459167\n",
      "Epoch 8, batch 99 D Loss: 1.680422306060791, G Loss: 0.7375919818878174\n",
      "Epoch 8, batch 100 D Loss: 1.491560935974121, G Loss: 0.7441441416740417\n",
      "Epoch 8, batch 101 D Loss: 1.4285540580749512, G Loss: 0.776926577091217\n",
      "Epoch 8, batch 102 D Loss: 1.4586384296417236, G Loss: 0.7851621508598328\n",
      "Epoch 8, batch 103 D Loss: 1.351203441619873, G Loss: 0.7693408727645874\n",
      "Epoch 8, batch 104 D Loss: 1.3592294454574585, G Loss: 0.8105266094207764\n",
      "Epoch 8, batch 105 D Loss: 1.7060227394104004, G Loss: 0.6629754900932312\n",
      "Epoch 8, batch 106 D Loss: 1.4477839469909668, G Loss: 0.8061458468437195\n",
      "Epoch 8, batch 107 D Loss: 1.4971637725830078, G Loss: 0.730151355266571\n",
      "Epoch 8, batch 108 D Loss: 1.4754993915557861, G Loss: 0.7064439654350281\n",
      "Epoch 8, batch 109 D Loss: 1.4000928401947021, G Loss: 0.8138086199760437\n",
      "Epoch 8, batch 110 D Loss: 1.5241070985794067, G Loss: 0.7545528411865234\n",
      "Epoch 8, batch 111 D Loss: 1.4337444305419922, G Loss: 0.7691781520843506\n",
      "Epoch 8, batch 112 D Loss: 1.4392824172973633, G Loss: 0.8182809352874756\n",
      "Epoch 8, batch 113 D Loss: 1.4220407009124756, G Loss: 0.7556917071342468\n",
      "Epoch 8, batch 114 D Loss: 1.4815714359283447, G Loss: 0.7202679514884949\n",
      "Epoch 8, batch 115 D Loss: 1.4502968788146973, G Loss: 0.7707600593566895\n",
      "Epoch 8, batch 116 D Loss: 1.4614709615707397, G Loss: 0.7367526292800903\n",
      "Epoch 8, batch 117 D Loss: 1.540270209312439, G Loss: 0.7206404805183411\n",
      "Epoch 8, batch 118 D Loss: 1.40059232711792, G Loss: 0.8079062104225159\n",
      "Epoch 8, batch 119 D Loss: 1.507460594177246, G Loss: 0.7506523728370667\n",
      "Epoch 8, batch 120 D Loss: 1.3920438289642334, G Loss: 0.8002259731292725\n",
      "Epoch 8, batch 121 D Loss: 1.383070468902588, G Loss: 0.7851002216339111\n",
      "Epoch 8, batch 122 D Loss: 1.5037198066711426, G Loss: 0.7536390423774719\n",
      "Epoch 8, batch 123 D Loss: 1.400834321975708, G Loss: 0.7776688933372498\n",
      "Epoch 8, batch 124 D Loss: 1.3730429410934448, G Loss: 0.8204423785209656\n",
      "Epoch 8, batch 125 D Loss: 1.519554615020752, G Loss: 0.7391753196716309\n",
      "Epoch 8, batch 126 D Loss: 1.4115538597106934, G Loss: 0.7775242328643799\n",
      "Epoch 8, batch 127 D Loss: 1.4792494773864746, G Loss: 0.754639744758606\n",
      "Epoch 8, batch 128 D Loss: 1.5965558290481567, G Loss: 0.6803733110427856\n",
      "Epoch 8, batch 129 D Loss: 1.3599423170089722, G Loss: 0.8062602281570435\n",
      "Epoch 8, batch 130 D Loss: 1.429187297821045, G Loss: 0.7823236584663391\n",
      "Epoch 8, batch 131 D Loss: 1.3860948085784912, G Loss: 0.7622114419937134\n",
      "Epoch 8, batch 132 D Loss: 1.4219681024551392, G Loss: 0.7711893320083618\n",
      "Epoch 8, batch 133 D Loss: 1.422410488128662, G Loss: 0.7861852049827576\n",
      "Epoch 8, batch 134 D Loss: 1.5088757276535034, G Loss: 0.7485302686691284\n",
      "Epoch 8, batch 135 D Loss: 1.418389081954956, G Loss: 0.7475215196609497\n",
      "Epoch 8, batch 136 D Loss: 1.4489046335220337, G Loss: 0.7610880136489868\n",
      "Epoch 8, batch 137 D Loss: 1.4073801040649414, G Loss: 0.7940131425857544\n",
      "Epoch 8, batch 138 D Loss: 1.4173718690872192, G Loss: 0.8035541772842407\n",
      "Epoch 8, batch 139 D Loss: 1.4101920127868652, G Loss: 0.7709270715713501\n",
      "Epoch 8, batch 140 D Loss: 1.453073263168335, G Loss: 0.7637113928794861\n",
      "Epoch 8, batch 141 D Loss: 1.4356215000152588, G Loss: 0.7785029411315918\n",
      "Epoch 8, batch 142 D Loss: 1.3970952033996582, G Loss: 0.7770110964775085\n",
      "Epoch 8, batch 143 D Loss: 1.3996931314468384, G Loss: 0.7388954162597656\n",
      "Epoch 8, batch 144 D Loss: 1.4312891960144043, G Loss: 0.7566343545913696\n",
      "Epoch 8, batch 145 D Loss: 1.407287836074829, G Loss: 0.8093879818916321\n",
      "Epoch 8, batch 146 D Loss: 1.3556926250457764, G Loss: 0.8020917773246765\n",
      "Epoch 8, batch 147 D Loss: 1.457257866859436, G Loss: 0.7743018269538879\n",
      "Epoch 8, batch 148 D Loss: 1.3674066066741943, G Loss: 0.7905129194259644\n",
      "Epoch 8, batch 149 D Loss: 1.4682447910308838, G Loss: 0.7654029726982117\n",
      "Epoch 8, batch 150 D Loss: 1.3427313566207886, G Loss: 0.804721474647522\n",
      "Epoch 8, batch 151 D Loss: 1.4406015872955322, G Loss: 0.7661190629005432\n",
      "Epoch 8, batch 152 D Loss: 1.4208548069000244, G Loss: 0.7923545837402344\n",
      "Epoch 8, batch 153 D Loss: 1.358950138092041, G Loss: 0.8474293351173401\n",
      "Epoch 8, batch 154 D Loss: 1.3959459066390991, G Loss: 0.8063423037528992\n",
      "Epoch 8, batch 155 D Loss: 1.426790714263916, G Loss: 0.7965347766876221\n",
      "Epoch 8, batch 156 D Loss: 1.3739252090454102, G Loss: 0.8509074449539185\n",
      "Epoch 8, batch 157 D Loss: 1.455712080001831, G Loss: 0.7565558552742004\n",
      "Epoch 8, batch 158 D Loss: 1.4038374423980713, G Loss: 0.7760124206542969\n",
      "Epoch 8, batch 159 D Loss: 1.4282978773117065, G Loss: 0.7909154295921326\n",
      "Epoch 8, batch 160 D Loss: 1.3672518730163574, G Loss: 0.8317080736160278\n",
      "Epoch 8, batch 161 D Loss: 1.391181468963623, G Loss: 0.8304120898246765\n",
      "Epoch 8, batch 162 D Loss: 1.3790068626403809, G Loss: 0.7983756065368652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, batch 163 D Loss: 1.447619915008545, G Loss: 0.782408595085144\n",
      "Epoch 8, batch 164 D Loss: 1.3932645320892334, G Loss: 0.8085852265357971\n",
      "Epoch 8, batch 165 D Loss: 1.4250104427337646, G Loss: 0.7883279323577881\n",
      "Epoch 8, batch 166 D Loss: 1.4257361888885498, G Loss: 0.8038797974586487\n",
      "Epoch 8, batch 167 D Loss: 1.41494619846344, G Loss: 0.8118005990982056\n",
      "Epoch 8, batch 168 D Loss: 1.3826375007629395, G Loss: 0.8005483150482178\n",
      "Epoch 8, batch 169 D Loss: 1.358250617980957, G Loss: 0.7864839434623718\n",
      "Epoch 8, batch 170 D Loss: 1.4001946449279785, G Loss: 0.7659881114959717\n",
      "Epoch 8, batch 171 D Loss: 1.3957746028900146, G Loss: 0.8101127743721008\n",
      "Epoch 8, batch 172 D Loss: 1.4489555358886719, G Loss: 0.7330583930015564\n",
      "Epoch 8, batch 173 D Loss: 1.367250919342041, G Loss: 0.7962866425514221\n",
      "Epoch 8, batch 174 D Loss: 1.3807976245880127, G Loss: 0.773857593536377\n",
      "Epoch 8, batch 175 D Loss: 1.3678879737854004, G Loss: 0.7857595682144165\n",
      "Epoch 8, batch 176 D Loss: 1.4261069297790527, G Loss: 0.7814491391181946\n",
      "Epoch 8, batch 177 D Loss: 1.4067654609680176, G Loss: 0.758283793926239\n",
      "Epoch 8, batch 178 D Loss: 1.3970236778259277, G Loss: 0.7709869146347046\n",
      "Epoch 8, batch 179 D Loss: 1.3452043533325195, G Loss: 0.8147995471954346\n",
      "Epoch 8, batch 180 D Loss: 1.3716813325881958, G Loss: 0.7871006727218628\n",
      "Epoch 8, batch 181 D Loss: 1.3570585250854492, G Loss: 0.7764118909835815\n",
      "Epoch 8, batch 182 D Loss: 1.4467099905014038, G Loss: 0.729037344455719\n",
      "Epoch 8, batch 183 D Loss: 1.4095255136489868, G Loss: 0.7852928638458252\n",
      "Epoch 8, batch 184 D Loss: 1.365875244140625, G Loss: 0.7844434380531311\n",
      "Epoch 8, batch 185 D Loss: 1.4611735343933105, G Loss: 0.7792513370513916\n",
      "Epoch 8, batch 186 D Loss: 1.4395747184753418, G Loss: 0.7396953105926514\n",
      "Epoch 8, batch 187 D Loss: 1.3775930404663086, G Loss: 0.7943950891494751\n",
      "Epoch 8, batch 188 D Loss: 1.3995749950408936, G Loss: 0.7666855454444885\n",
      "Epoch 8, batch 189 D Loss: 1.4060311317443848, G Loss: 0.7477376461029053\n",
      "Epoch 8, batch 190 D Loss: 1.4807113409042358, G Loss: 0.7399523258209229\n",
      "Epoch 8, batch 191 D Loss: 1.3453978300094604, G Loss: 0.764415442943573\n",
      "Epoch 8, batch 192 D Loss: 1.4191054105758667, G Loss: 0.7903094291687012\n",
      "Epoch 8, batch 193 D Loss: 1.409409761428833, G Loss: 0.7715628743171692\n",
      "Epoch 8, batch 194 D Loss: 1.3959895372390747, G Loss: 0.7635347843170166\n",
      "Epoch 8, batch 195 D Loss: 1.3712273836135864, G Loss: 0.8103540539741516\n",
      "Epoch 8, batch 196 D Loss: 1.4512665271759033, G Loss: 0.7204427123069763\n",
      "Epoch 8, batch 197 D Loss: 1.3688244819641113, G Loss: 0.8158027529716492\n",
      "Epoch 8, batch 198 D Loss: 1.4018263816833496, G Loss: 0.7891298532485962\n",
      "Epoch 8, batch 199 D Loss: 1.4584341049194336, G Loss: 0.7688208818435669\n",
      "Epoch 8, batch 200 D Loss: 1.2984483242034912, G Loss: 0.831179678440094\n",
      "Epoch 9, batch 1 D Loss: 1.3676786422729492, G Loss: 0.7693231701850891\n",
      "Epoch 9, batch 2 D Loss: 1.358720064163208, G Loss: 0.8017932772636414\n",
      "Epoch 9, batch 3 D Loss: 1.4182751178741455, G Loss: 0.7731635570526123\n",
      "Epoch 9, batch 4 D Loss: 1.3892381191253662, G Loss: 0.7672378420829773\n",
      "Epoch 9, batch 5 D Loss: 1.4327056407928467, G Loss: 0.7526999115943909\n",
      "Epoch 9, batch 6 D Loss: 1.4461138248443604, G Loss: 0.7883909344673157\n",
      "Epoch 9, batch 7 D Loss: 1.4296462535858154, G Loss: 0.7621618509292603\n",
      "Epoch 9, batch 8 D Loss: 1.4029715061187744, G Loss: 0.7641391754150391\n",
      "Epoch 9, batch 9 D Loss: 1.3759645223617554, G Loss: 0.7919963002204895\n",
      "Epoch 9, batch 10 D Loss: 1.3739020824432373, G Loss: 0.8101447224617004\n",
      "Epoch 9, batch 11 D Loss: 1.3630917072296143, G Loss: 0.8190475702285767\n",
      "Epoch 9, batch 12 D Loss: 1.4182264804840088, G Loss: 0.8049474954605103\n",
      "Epoch 9, batch 13 D Loss: 1.4258824586868286, G Loss: 0.7584827542304993\n",
      "Epoch 9, batch 14 D Loss: 1.3475558757781982, G Loss: 0.7921586632728577\n",
      "Epoch 9, batch 15 D Loss: 1.4132128953933716, G Loss: 0.7518147230148315\n",
      "Epoch 9, batch 16 D Loss: 1.394164800643921, G Loss: 0.7780306935310364\n",
      "Epoch 9, batch 17 D Loss: 1.4096776247024536, G Loss: 0.7592506408691406\n",
      "Epoch 9, batch 18 D Loss: 1.4379637241363525, G Loss: 0.7581724524497986\n",
      "Epoch 9, batch 19 D Loss: 1.3688815832138062, G Loss: 0.7518869638442993\n",
      "Epoch 9, batch 20 D Loss: 1.395345687866211, G Loss: 0.7864967584609985\n",
      "Epoch 9, batch 21 D Loss: 1.3655128479003906, G Loss: 0.7939543724060059\n",
      "Epoch 9, batch 22 D Loss: 1.3976002931594849, G Loss: 0.786917507648468\n",
      "Epoch 9, batch 23 D Loss: 1.512613296508789, G Loss: 0.710206151008606\n",
      "Epoch 9, batch 24 D Loss: 1.463883876800537, G Loss: 0.7953432202339172\n",
      "Epoch 9, batch 25 D Loss: 1.4137284755706787, G Loss: 0.7902771234512329\n",
      "Epoch 9, batch 26 D Loss: 1.4067490100860596, G Loss: 0.7996848821640015\n",
      "Epoch 9, batch 27 D Loss: 1.4322021007537842, G Loss: 0.7779765129089355\n",
      "Epoch 9, batch 28 D Loss: 1.435143232345581, G Loss: 0.7718431949615479\n",
      "Epoch 9, batch 29 D Loss: 1.3897507190704346, G Loss: 0.7935094237327576\n",
      "Epoch 9, batch 30 D Loss: 1.4251892566680908, G Loss: 0.8025417327880859\n",
      "Epoch 9, batch 31 D Loss: 1.410059928894043, G Loss: 0.792732834815979\n",
      "Epoch 9, batch 32 D Loss: 1.4414312839508057, G Loss: 0.7774227261543274\n",
      "Epoch 9, batch 33 D Loss: 1.3872878551483154, G Loss: 0.789002537727356\n",
      "Epoch 9, batch 34 D Loss: 1.4024879932403564, G Loss: 0.8065049052238464\n",
      "Epoch 9, batch 35 D Loss: 1.4015624523162842, G Loss: 0.7652230858802795\n",
      "Epoch 9, batch 36 D Loss: 1.3928927183151245, G Loss: 0.7724224925041199\n",
      "Epoch 9, batch 37 D Loss: 1.4468294382095337, G Loss: 0.7630299925804138\n",
      "Epoch 9, batch 38 D Loss: 1.42910635471344, G Loss: 0.7831926941871643\n",
      "Epoch 9, batch 39 D Loss: 1.43430757522583, G Loss: 0.7440907955169678\n",
      "Epoch 9, batch 40 D Loss: 1.4675514698028564, G Loss: 0.7399750351905823\n",
      "Epoch 9, batch 41 D Loss: 1.4064534902572632, G Loss: 0.7767575979232788\n",
      "Epoch 9, batch 42 D Loss: 1.4453128576278687, G Loss: 0.7731236219406128\n",
      "Epoch 9, batch 43 D Loss: 1.437934398651123, G Loss: 0.7578880190849304\n",
      "Epoch 9, batch 44 D Loss: 1.4293936491012573, G Loss: 0.7723414897918701\n",
      "Epoch 9, batch 45 D Loss: 1.4208707809448242, G Loss: 0.7577160596847534\n",
      "Epoch 9, batch 46 D Loss: 1.436448097229004, G Loss: 0.7435802221298218\n",
      "Epoch 9, batch 47 D Loss: 1.416642665863037, G Loss: 0.7428348660469055\n",
      "Epoch 9, batch 48 D Loss: 1.4242303371429443, G Loss: 0.7625810503959656\n",
      "Epoch 9, batch 49 D Loss: 1.411507248878479, G Loss: 0.7651419043540955\n",
      "Epoch 9, batch 50 D Loss: 1.3680423498153687, G Loss: 0.776216447353363\n",
      "Epoch 9, batch 51 D Loss: 1.4377802610397339, G Loss: 0.7365820407867432\n",
      "Epoch 9, batch 52 D Loss: 1.3951787948608398, G Loss: 0.7510464191436768\n",
      "Epoch 9, batch 53 D Loss: 1.3910913467407227, G Loss: 0.7607539296150208\n",
      "Epoch 9, batch 54 D Loss: 1.4559590816497803, G Loss: 0.743130087852478\n",
      "Epoch 9, batch 55 D Loss: 1.4477670192718506, G Loss: 0.7113488912582397\n",
      "Epoch 9, batch 56 D Loss: 1.414210557937622, G Loss: 0.7294901013374329\n",
      "Epoch 9, batch 57 D Loss: 1.3750137090682983, G Loss: 0.7566502094268799\n",
      "Epoch 9, batch 58 D Loss: 1.430479645729065, G Loss: 0.7278610467910767\n",
      "Epoch 9, batch 59 D Loss: 1.432544231414795, G Loss: 0.7152202725410461\n",
      "Epoch 9, batch 60 D Loss: 1.4293980598449707, G Loss: 0.7375237941741943\n",
      "Epoch 9, batch 61 D Loss: 1.359161138534546, G Loss: 0.7768867015838623\n",
      "Epoch 9, batch 62 D Loss: 1.426950454711914, G Loss: 0.7370274066925049\n",
      "Epoch 9, batch 63 D Loss: 1.351745843887329, G Loss: 0.7780281901359558\n",
      "Epoch 9, batch 64 D Loss: 1.3928463459014893, G Loss: 0.7519465684890747\n",
      "Epoch 9, batch 65 D Loss: 1.3958501815795898, G Loss: 0.7521665692329407\n",
      "Epoch 9, batch 66 D Loss: 1.4050989151000977, G Loss: 0.7592500448226929\n",
      "Epoch 9, batch 67 D Loss: 1.4156337976455688, G Loss: 0.7249956727027893\n",
      "Epoch 9, batch 68 D Loss: 1.4097771644592285, G Loss: 0.7414320111274719\n",
      "Epoch 9, batch 69 D Loss: 1.4024368524551392, G Loss: 0.7515239119529724\n",
      "Epoch 9, batch 70 D Loss: 1.3953818082809448, G Loss: 0.7544782161712646\n",
      "Epoch 9, batch 71 D Loss: 1.4204754829406738, G Loss: 0.7450264692306519\n",
      "Epoch 9, batch 72 D Loss: 1.3990209102630615, G Loss: 0.7521691918373108\n",
      "Epoch 9, batch 73 D Loss: 1.3975822925567627, G Loss: 0.7573959231376648\n",
      "Epoch 9, batch 74 D Loss: 1.4054428339004517, G Loss: 0.7470781803131104\n",
      "Epoch 9, batch 75 D Loss: 1.3923078775405884, G Loss: 0.7384933233261108\n",
      "Epoch 9, batch 76 D Loss: 1.4022530317306519, G Loss: 0.7423661947250366\n",
      "Epoch 9, batch 77 D Loss: 1.3807991743087769, G Loss: 0.7673342227935791\n",
      "Epoch 9, batch 78 D Loss: 1.4077045917510986, G Loss: 0.7517178058624268\n",
      "Epoch 9, batch 79 D Loss: 1.3956817388534546, G Loss: 0.7586654424667358\n",
      "Epoch 9, batch 80 D Loss: 1.3909845352172852, G Loss: 0.7629599571228027\n",
      "Epoch 9, batch 81 D Loss: 1.3908451795578003, G Loss: 0.7599163055419922\n",
      "Epoch 9, batch 82 D Loss: 1.3987239599227905, G Loss: 0.7627519965171814\n",
      "Epoch 9, batch 83 D Loss: 1.3963220119476318, G Loss: 0.7648117542266846\n",
      "Epoch 9, batch 84 D Loss: 1.3979350328445435, G Loss: 0.7531558275222778\n",
      "Epoch 9, batch 85 D Loss: 1.3886833190917969, G Loss: 0.7527258396148682\n",
      "Epoch 9, batch 86 D Loss: 1.394452452659607, G Loss: 0.7678821682929993\n",
      "Epoch 9, batch 87 D Loss: 1.375220775604248, G Loss: 0.765394389629364\n",
      "Epoch 9, batch 88 D Loss: 1.3927669525146484, G Loss: 0.7710826396942139\n",
      "Epoch 9, batch 89 D Loss: 1.4041329622268677, G Loss: 0.7430837154388428\n",
      "Epoch 9, batch 90 D Loss: 1.4052772521972656, G Loss: 0.7455751299858093\n",
      "Epoch 9, batch 91 D Loss: 1.3676598072052002, G Loss: 0.7734435200691223\n",
      "Epoch 9, batch 92 D Loss: 1.3836033344268799, G Loss: 0.7728901505470276\n",
      "Epoch 9, batch 93 D Loss: 1.3830862045288086, G Loss: 0.7735909223556519\n",
      "Epoch 9, batch 94 D Loss: 1.3898119926452637, G Loss: 0.7579703330993652\n",
      "Epoch 9, batch 95 D Loss: 1.384354829788208, G Loss: 0.7630221843719482\n",
      "Epoch 9, batch 96 D Loss: 1.3750073909759521, G Loss: 0.7538106441497803\n",
      "Epoch 9, batch 97 D Loss: 1.3921051025390625, G Loss: 0.757345974445343\n",
      "Epoch 9, batch 98 D Loss: 1.3852778673171997, G Loss: 0.7537501454353333\n",
      "Epoch 9, batch 99 D Loss: 1.3953771591186523, G Loss: 0.7479003071784973\n",
      "Epoch 9, batch 100 D Loss: 1.3856295347213745, G Loss: 0.7565425038337708\n",
      "Epoch 9, batch 101 D Loss: 1.3950457572937012, G Loss: 0.7386373281478882\n",
      "Epoch 9, batch 102 D Loss: 1.378291368484497, G Loss: 0.7503808736801147\n",
      "Epoch 9, batch 103 D Loss: 1.3693864345550537, G Loss: 0.7604866623878479\n",
      "Epoch 9, batch 104 D Loss: 1.3724751472473145, G Loss: 0.754865825176239\n",
      "Epoch 9, batch 105 D Loss: 1.3959623575210571, G Loss: 0.7452953457832336\n",
      "Epoch 9, batch 106 D Loss: 1.3796600103378296, G Loss: 0.764432430267334\n",
      "Epoch 9, batch 107 D Loss: 1.3763980865478516, G Loss: 0.7529805898666382\n",
      "Epoch 9, batch 108 D Loss: 1.3834058046340942, G Loss: 0.7532055377960205\n",
      "Epoch 9, batch 109 D Loss: 1.3728487491607666, G Loss: 0.7547299265861511\n",
      "Epoch 9, batch 110 D Loss: 1.3695213794708252, G Loss: 0.7452508807182312\n",
      "Epoch 9, batch 111 D Loss: 1.3713679313659668, G Loss: 0.7552504539489746\n",
      "Epoch 9, batch 112 D Loss: 1.3827803134918213, G Loss: 0.7485803961753845\n",
      "Epoch 9, batch 113 D Loss: 1.386728286743164, G Loss: 0.7343366742134094\n",
      "Epoch 9, batch 114 D Loss: 1.392622709274292, G Loss: 0.7380045056343079\n",
      "Epoch 9, batch 115 D Loss: 1.3692429065704346, G Loss: 0.7605705857276917\n",
      "Epoch 9, batch 116 D Loss: 1.3518421649932861, G Loss: 0.7621148824691772\n",
      "Epoch 9, batch 117 D Loss: 1.3709394931793213, G Loss: 0.7473053932189941\n",
      "Epoch 9, batch 118 D Loss: 1.3609000444412231, G Loss: 0.7616710066795349\n",
      "Epoch 9, batch 119 D Loss: 1.3749632835388184, G Loss: 0.7639971375465393\n",
      "Epoch 9, batch 120 D Loss: 1.370760440826416, G Loss: 0.7404349446296692\n",
      "Epoch 9, batch 121 D Loss: 1.3596959114074707, G Loss: 0.7586953043937683\n",
      "Epoch 9, batch 122 D Loss: 1.378211259841919, G Loss: 0.741741955280304\n",
      "Epoch 9, batch 123 D Loss: 1.3860453367233276, G Loss: 0.7321295142173767\n",
      "Epoch 9, batch 124 D Loss: 1.3627617359161377, G Loss: 0.7459883093833923\n",
      "Epoch 9, batch 125 D Loss: 1.411773681640625, G Loss: 0.7299588918685913\n",
      "Epoch 9, batch 126 D Loss: 1.3741724491119385, G Loss: 0.7348179817199707\n",
      "Epoch 9, batch 127 D Loss: 1.3769298791885376, G Loss: 0.7440449595451355\n",
      "Epoch 9, batch 128 D Loss: 1.3554906845092773, G Loss: 0.7662901282310486\n",
      "Epoch 9, batch 129 D Loss: 1.3745880126953125, G Loss: 0.7475988864898682\n",
      "Epoch 9, batch 130 D Loss: 1.3731454610824585, G Loss: 0.732076108455658\n",
      "Epoch 9, batch 131 D Loss: 1.3747119903564453, G Loss: 0.7529281377792358\n",
      "Epoch 9, batch 132 D Loss: 1.3706095218658447, G Loss: 0.7329703569412231\n",
      "Epoch 9, batch 133 D Loss: 1.3621572256088257, G Loss: 0.7533706426620483\n",
      "Epoch 9, batch 134 D Loss: 1.3854403495788574, G Loss: 0.7291096448898315\n",
      "Epoch 9, batch 135 D Loss: 1.37057363986969, G Loss: 0.7396326661109924\n",
      "Epoch 9, batch 136 D Loss: 1.388460397720337, G Loss: 0.7258789539337158\n",
      "Epoch 9, batch 137 D Loss: 1.3927267789840698, G Loss: 0.7319761514663696\n",
      "Epoch 9, batch 138 D Loss: 1.373023271560669, G Loss: 0.7287368774414062\n",
      "Epoch 9, batch 139 D Loss: 1.3674293756484985, G Loss: 0.7513951659202576\n",
      "Epoch 9, batch 140 D Loss: 1.3577816486358643, G Loss: 0.739276647567749\n",
      "Epoch 9, batch 141 D Loss: 1.3893896341323853, G Loss: 0.7194852232933044\n",
      "Epoch 9, batch 142 D Loss: 1.4056036472320557, G Loss: 0.7086021304130554\n",
      "Epoch 9, batch 143 D Loss: 1.3722035884857178, G Loss: 0.720640242099762\n",
      "Epoch 9, batch 144 D Loss: 1.371309518814087, G Loss: 0.7337360978126526\n",
      "Epoch 9, batch 145 D Loss: 1.3543446063995361, G Loss: 0.7382460236549377\n",
      "Epoch 9, batch 146 D Loss: 1.3812086582183838, G Loss: 0.708220899105072\n",
      "Epoch 9, batch 147 D Loss: 1.3661963939666748, G Loss: 0.7169060707092285\n",
      "Epoch 9, batch 148 D Loss: 1.3518762588500977, G Loss: 0.7408460974693298\n",
      "Epoch 9, batch 149 D Loss: 1.3805480003356934, G Loss: 0.7132322788238525\n",
      "Epoch 9, batch 150 D Loss: 1.3749613761901855, G Loss: 0.7151925563812256\n",
      "Epoch 9, batch 151 D Loss: 1.3714293241500854, G Loss: 0.7236694097518921\n",
      "Epoch 9, batch 152 D Loss: 1.3627865314483643, G Loss: 0.7044967412948608\n",
      "Epoch 9, batch 153 D Loss: 1.3635681867599487, G Loss: 0.7239391207695007\n",
      "Epoch 9, batch 154 D Loss: 1.3767259120941162, G Loss: 0.7022604942321777\n",
      "Epoch 9, batch 155 D Loss: 1.3887357711791992, G Loss: 0.700103223323822\n",
      "Epoch 9, batch 156 D Loss: 1.3617196083068848, G Loss: 0.7253303527832031\n",
      "Epoch 9, batch 157 D Loss: 1.3952126502990723, G Loss: 0.6958312392234802\n",
      "Epoch 9, batch 158 D Loss: 1.3902640342712402, G Loss: 0.702767550945282\n",
      "Epoch 9, batch 159 D Loss: 1.3720334768295288, G Loss: 0.7207410931587219\n",
      "Epoch 9, batch 160 D Loss: 1.383727788925171, G Loss: 0.7008086442947388\n",
      "Epoch 9, batch 161 D Loss: 1.3653850555419922, G Loss: 0.7023422718048096\n",
      "Epoch 9, batch 162 D Loss: 1.3668643236160278, G Loss: 0.703590989112854\n",
      "Epoch 9, batch 163 D Loss: 1.3658809661865234, G Loss: 0.6984109282493591\n",
      "Epoch 9, batch 164 D Loss: 1.384115219116211, G Loss: 0.7010245323181152\n",
      "Epoch 9, batch 165 D Loss: 1.3885647058486938, G Loss: 0.6909401416778564\n",
      "Epoch 9, batch 166 D Loss: 1.3608474731445312, G Loss: 0.7058643102645874\n",
      "Epoch 9, batch 167 D Loss: 1.3737374544143677, G Loss: 0.6930872201919556\n",
      "Epoch 9, batch 168 D Loss: 1.3787091970443726, G Loss: 0.6793188452720642\n",
      "Epoch 9, batch 169 D Loss: 1.3650354146957397, G Loss: 0.6901035904884338\n",
      "Epoch 9, batch 170 D Loss: 1.381523609161377, G Loss: 0.701473593711853\n",
      "Epoch 9, batch 171 D Loss: 1.3758773803710938, G Loss: 0.6708546280860901\n",
      "Epoch 9, batch 172 D Loss: 1.3650264739990234, G Loss: 0.7064219117164612\n",
      "Epoch 9, batch 173 D Loss: 1.3830525875091553, G Loss: 0.6856579780578613\n",
      "Epoch 9, batch 174 D Loss: 1.3552908897399902, G Loss: 0.7053383588790894\n",
      "Epoch 9, batch 175 D Loss: 1.3700380325317383, G Loss: 0.6799235343933105\n",
      "Epoch 9, batch 176 D Loss: 1.3673896789550781, G Loss: 0.69428551197052\n",
      "Epoch 9, batch 177 D Loss: 1.3996765613555908, G Loss: 0.6744681596755981\n",
      "Epoch 9, batch 178 D Loss: 1.3792533874511719, G Loss: 0.6913511157035828\n",
      "Epoch 9, batch 179 D Loss: 1.34922194480896, G Loss: 0.7107762694358826\n",
      "Epoch 9, batch 180 D Loss: 1.3670473098754883, G Loss: 0.679653525352478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, batch 181 D Loss: 1.3746495246887207, G Loss: 0.7022994756698608\n",
      "Epoch 9, batch 182 D Loss: 1.3622207641601562, G Loss: 0.699138879776001\n",
      "Epoch 9, batch 183 D Loss: 1.3712928295135498, G Loss: 0.7005581855773926\n",
      "Epoch 9, batch 184 D Loss: 1.3637335300445557, G Loss: 0.7060725688934326\n",
      "Epoch 9, batch 185 D Loss: 1.407350778579712, G Loss: 0.6801244616508484\n",
      "Epoch 9, batch 186 D Loss: 1.37533700466156, G Loss: 0.6800739765167236\n",
      "Epoch 9, batch 187 D Loss: 1.3783259391784668, G Loss: 0.6945438385009766\n",
      "Epoch 9, batch 188 D Loss: 1.392681360244751, G Loss: 0.6784714460372925\n",
      "Epoch 9, batch 189 D Loss: 1.3907952308654785, G Loss: 0.6712827086448669\n",
      "Epoch 9, batch 190 D Loss: 1.4118810892105103, G Loss: 0.666167140007019\n",
      "Epoch 9, batch 191 D Loss: 1.3891503810882568, G Loss: 0.6883722543716431\n",
      "Epoch 9, batch 192 D Loss: 1.3634874820709229, G Loss: 0.7155697345733643\n",
      "Epoch 9, batch 193 D Loss: 1.3931773900985718, G Loss: 0.6797676086425781\n",
      "Epoch 9, batch 194 D Loss: 1.375285029411316, G Loss: 0.6872475147247314\n",
      "Epoch 9, batch 195 D Loss: 1.3754987716674805, G Loss: 0.6819027662277222\n",
      "Epoch 9, batch 196 D Loss: 1.3487613201141357, G Loss: 0.706777036190033\n",
      "Epoch 9, batch 197 D Loss: 1.3906731605529785, G Loss: 0.6772193312644958\n",
      "Epoch 9, batch 198 D Loss: 1.3584438562393188, G Loss: 0.7133904099464417\n",
      "Epoch 9, batch 199 D Loss: 1.3647099733352661, G Loss: 0.6818301677703857\n",
      "Epoch 9, batch 200 D Loss: 1.406860113143921, G Loss: 0.6800116896629333\n",
      "Epoch 10, batch 1 D Loss: 1.408745288848877, G Loss: 0.6781237721443176\n",
      "Epoch 10, batch 2 D Loss: 1.3701932430267334, G Loss: 0.6991868615150452\n",
      "Epoch 10, batch 3 D Loss: 1.3947772979736328, G Loss: 0.6862371563911438\n",
      "Epoch 10, batch 4 D Loss: 1.3649399280548096, G Loss: 0.7053266167640686\n",
      "Epoch 10, batch 5 D Loss: 1.3895817995071411, G Loss: 0.672770082950592\n",
      "Epoch 10, batch 6 D Loss: 1.3959672451019287, G Loss: 0.7002661228179932\n",
      "Epoch 10, batch 7 D Loss: 1.38875412940979, G Loss: 0.6945064067840576\n",
      "Epoch 10, batch 8 D Loss: 1.3674780130386353, G Loss: 0.6895350813865662\n",
      "Epoch 10, batch 9 D Loss: 1.3975653648376465, G Loss: 0.6631933450698853\n",
      "Epoch 10, batch 10 D Loss: 1.3759956359863281, G Loss: 0.6920367479324341\n",
      "Epoch 10, batch 11 D Loss: 1.374122142791748, G Loss: 0.7051446437835693\n",
      "Epoch 10, batch 12 D Loss: 1.3694937229156494, G Loss: 0.6998075842857361\n",
      "Epoch 10, batch 13 D Loss: 1.413515567779541, G Loss: 0.6718589067459106\n",
      "Epoch 10, batch 14 D Loss: 1.394636631011963, G Loss: 0.6875571608543396\n",
      "Epoch 10, batch 15 D Loss: 1.3774855136871338, G Loss: 0.6839662790298462\n",
      "Epoch 10, batch 16 D Loss: 1.392332911491394, G Loss: 0.6754005551338196\n",
      "Epoch 10, batch 17 D Loss: 1.3867897987365723, G Loss: 0.6858686804771423\n",
      "Epoch 10, batch 18 D Loss: 1.3701517581939697, G Loss: 0.6894832849502563\n",
      "Epoch 10, batch 19 D Loss: 1.3967998027801514, G Loss: 0.6823814511299133\n",
      "Epoch 10, batch 20 D Loss: 1.3858246803283691, G Loss: 0.6958646178245544\n",
      "Epoch 10, batch 21 D Loss: 1.3933802843093872, G Loss: 0.6711751818656921\n",
      "Epoch 10, batch 22 D Loss: 1.3650496006011963, G Loss: 0.6928958296775818\n",
      "Epoch 10, batch 23 D Loss: 1.3720333576202393, G Loss: 0.6901808977127075\n",
      "Epoch 10, batch 24 D Loss: 1.3980789184570312, G Loss: 0.689139723777771\n",
      "Epoch 10, batch 25 D Loss: 1.4201056957244873, G Loss: 0.6694744229316711\n",
      "Epoch 10, batch 26 D Loss: 1.3596609830856323, G Loss: 0.6795306205749512\n",
      "Epoch 10, batch 27 D Loss: 1.3802008628845215, G Loss: 0.6716974377632141\n",
      "Epoch 10, batch 28 D Loss: 1.3778116703033447, G Loss: 0.6895626783370972\n",
      "Epoch 10, batch 29 D Loss: 1.3846373558044434, G Loss: 0.6859697103500366\n",
      "Epoch 10, batch 30 D Loss: 1.352737545967102, G Loss: 0.6933724880218506\n",
      "Epoch 10, batch 31 D Loss: 1.3710850477218628, G Loss: 0.6765108704566956\n",
      "Epoch 10, batch 32 D Loss: 1.3749258518218994, G Loss: 0.6820642948150635\n",
      "Epoch 10, batch 33 D Loss: 1.3945815563201904, G Loss: 0.6827867031097412\n",
      "Epoch 10, batch 34 D Loss: 1.3760900497436523, G Loss: 0.6808817982673645\n",
      "Epoch 10, batch 35 D Loss: 1.388087272644043, G Loss: 0.6759880781173706\n",
      "Epoch 10, batch 36 D Loss: 1.36293625831604, G Loss: 0.6921613812446594\n",
      "Epoch 10, batch 37 D Loss: 1.3833357095718384, G Loss: 0.6722987294197083\n",
      "Epoch 10, batch 38 D Loss: 1.4131040573120117, G Loss: 0.6649975776672363\n",
      "Epoch 10, batch 39 D Loss: 1.404971957206726, G Loss: 0.6613737344741821\n",
      "Epoch 10, batch 40 D Loss: 1.4057142734527588, G Loss: 0.6714575290679932\n",
      "Epoch 10, batch 41 D Loss: 1.4087873697280884, G Loss: 0.6741746664047241\n",
      "Epoch 10, batch 42 D Loss: 1.403464436531067, G Loss: 0.6638792157173157\n",
      "Epoch 10, batch 43 D Loss: 1.3972246646881104, G Loss: 0.6675045490264893\n",
      "Epoch 10, batch 44 D Loss: 1.390947699546814, G Loss: 0.6851198673248291\n",
      "Epoch 10, batch 45 D Loss: 1.3965753316879272, G Loss: 0.6735739707946777\n",
      "Epoch 10, batch 46 D Loss: 1.3996186256408691, G Loss: 0.6573894023895264\n",
      "Epoch 10, batch 47 D Loss: 1.371869444847107, G Loss: 0.6841078996658325\n",
      "Epoch 10, batch 48 D Loss: 1.4041169881820679, G Loss: 0.67509925365448\n",
      "Epoch 10, batch 49 D Loss: 1.3974307775497437, G Loss: 0.6711227893829346\n",
      "Epoch 10, batch 50 D Loss: 1.386000394821167, G Loss: 0.6779991388320923\n",
      "Epoch 10, batch 51 D Loss: 1.379847764968872, G Loss: 0.6789225935935974\n",
      "Epoch 10, batch 52 D Loss: 1.394803762435913, G Loss: 0.6722080111503601\n",
      "Epoch 10, batch 53 D Loss: 1.3831219673156738, G Loss: 0.6751953959465027\n",
      "Epoch 10, batch 54 D Loss: 1.3732199668884277, G Loss: 0.6803783178329468\n",
      "Epoch 10, batch 55 D Loss: 1.3877265453338623, G Loss: 0.6686791777610779\n",
      "Epoch 10, batch 56 D Loss: 1.3818163871765137, G Loss: 0.6774228811264038\n",
      "Epoch 10, batch 57 D Loss: 1.3731410503387451, G Loss: 0.6776195764541626\n",
      "Epoch 10, batch 58 D Loss: 1.4120874404907227, G Loss: 0.6610870361328125\n",
      "Epoch 10, batch 59 D Loss: 1.386541724205017, G Loss: 0.6759040951728821\n",
      "Epoch 10, batch 60 D Loss: 1.4034403562545776, G Loss: 0.6514082551002502\n",
      "Epoch 10, batch 61 D Loss: 1.3966248035430908, G Loss: 0.6532233953475952\n",
      "Epoch 10, batch 62 D Loss: 1.379826307296753, G Loss: 0.6712847352027893\n",
      "Epoch 10, batch 63 D Loss: 1.3794151544570923, G Loss: 0.6933796405792236\n",
      "Epoch 10, batch 64 D Loss: 1.3584003448486328, G Loss: 0.690001904964447\n",
      "Epoch 10, batch 65 D Loss: 1.4041179418563843, G Loss: 0.667766809463501\n",
      "Epoch 10, batch 66 D Loss: 1.3965129852294922, G Loss: 0.6741567254066467\n",
      "Epoch 10, batch 67 D Loss: 1.3959903717041016, G Loss: 0.6800222992897034\n",
      "Epoch 10, batch 68 D Loss: 1.4005857706069946, G Loss: 0.6758144497871399\n",
      "Epoch 10, batch 69 D Loss: 1.4064903259277344, G Loss: 0.6778561472892761\n",
      "Epoch 10, batch 70 D Loss: 1.4166865348815918, G Loss: 0.659443199634552\n",
      "Epoch 10, batch 71 D Loss: 1.3837783336639404, G Loss: 0.685625433921814\n",
      "Epoch 10, batch 72 D Loss: 1.3843679428100586, G Loss: 0.6848039031028748\n",
      "Epoch 10, batch 73 D Loss: 1.392665982246399, G Loss: 0.6783193945884705\n",
      "Epoch 10, batch 74 D Loss: 1.3843443393707275, G Loss: 0.6752095222473145\n",
      "Epoch 10, batch 75 D Loss: 1.410118818283081, G Loss: 0.6605398058891296\n",
      "Epoch 10, batch 76 D Loss: 1.3804742097854614, G Loss: 0.6796047687530518\n",
      "Epoch 10, batch 77 D Loss: 1.3894445896148682, G Loss: 0.6814830899238586\n",
      "Epoch 10, batch 78 D Loss: 1.3927712440490723, G Loss: 0.6710253953933716\n",
      "Epoch 10, batch 79 D Loss: 1.3802204132080078, G Loss: 0.6802253127098083\n",
      "Epoch 10, batch 80 D Loss: 1.3884599208831787, G Loss: 0.6646143198013306\n",
      "Epoch 10, batch 81 D Loss: 1.3864420652389526, G Loss: 0.6890113949775696\n",
      "Epoch 10, batch 82 D Loss: 1.386733055114746, G Loss: 0.6743220686912537\n",
      "Epoch 10, batch 83 D Loss: 1.3771145343780518, G Loss: 0.6720744967460632\n",
      "Epoch 10, batch 84 D Loss: 1.3821905851364136, G Loss: 0.6825678944587708\n",
      "Epoch 10, batch 85 D Loss: 1.3853099346160889, G Loss: 0.6880332827568054\n",
      "Epoch 10, batch 86 D Loss: 1.3984532356262207, G Loss: 0.6733582019805908\n",
      "Epoch 10, batch 87 D Loss: 1.3823390007019043, G Loss: 0.6796541810035706\n",
      "Epoch 10, batch 88 D Loss: 1.4117751121520996, G Loss: 0.6592227816581726\n",
      "Epoch 10, batch 89 D Loss: 1.387223720550537, G Loss: 0.6726553440093994\n",
      "Epoch 10, batch 90 D Loss: 1.391411304473877, G Loss: 0.6727615594863892\n",
      "Epoch 10, batch 91 D Loss: 1.37382173538208, G Loss: 0.6835645437240601\n",
      "Epoch 10, batch 92 D Loss: 1.3855922222137451, G Loss: 0.6746170520782471\n",
      "Epoch 10, batch 93 D Loss: 1.3490017652511597, G Loss: 0.6879138350486755\n",
      "Epoch 10, batch 94 D Loss: 1.3828917741775513, G Loss: 0.6709904670715332\n",
      "Epoch 10, batch 95 D Loss: 1.384861946105957, G Loss: 0.6805849671363831\n",
      "Epoch 10, batch 96 D Loss: 1.381387710571289, G Loss: 0.6774924397468567\n",
      "Epoch 10, batch 97 D Loss: 1.3868863582611084, G Loss: 0.6798180937767029\n",
      "Epoch 10, batch 98 D Loss: 1.3906528949737549, G Loss: 0.6764639019966125\n",
      "Epoch 10, batch 99 D Loss: 1.3950618505477905, G Loss: 0.6824422478675842\n",
      "Epoch 10, batch 100 D Loss: 1.3764561414718628, G Loss: 0.6821479797363281\n",
      "Epoch 10, batch 101 D Loss: 1.4000228643417358, G Loss: 0.6623027324676514\n",
      "Epoch 10, batch 102 D Loss: 1.3711103200912476, G Loss: 0.6803774833679199\n",
      "Epoch 10, batch 103 D Loss: 1.3630180358886719, G Loss: 0.6912239193916321\n",
      "Epoch 10, batch 104 D Loss: 1.39130437374115, G Loss: 0.6702703833580017\n",
      "Epoch 10, batch 105 D Loss: 1.3824714422225952, G Loss: 0.6831107139587402\n",
      "Epoch 10, batch 106 D Loss: 1.3957781791687012, G Loss: 0.6770463585853577\n",
      "Epoch 10, batch 107 D Loss: 1.3512182235717773, G Loss: 0.7101317048072815\n",
      "Epoch 10, batch 108 D Loss: 1.3990960121154785, G Loss: 0.6683404445648193\n",
      "Epoch 10, batch 109 D Loss: 1.3821192979812622, G Loss: 0.6754610538482666\n",
      "Epoch 10, batch 110 D Loss: 1.3833088874816895, G Loss: 0.6932212114334106\n",
      "Epoch 10, batch 111 D Loss: 1.3786580562591553, G Loss: 0.6819310188293457\n",
      "Epoch 10, batch 112 D Loss: 1.4019386768341064, G Loss: 0.6802418231964111\n",
      "Epoch 10, batch 113 D Loss: 1.3844002485275269, G Loss: 0.6757171154022217\n",
      "Epoch 10, batch 114 D Loss: 1.3815311193466187, G Loss: 0.6897474527359009\n",
      "Epoch 10, batch 115 D Loss: 1.4042646884918213, G Loss: 0.6714288592338562\n",
      "Epoch 10, batch 116 D Loss: 1.391340732574463, G Loss: 0.6604472994804382\n",
      "Epoch 10, batch 117 D Loss: 1.4078919887542725, G Loss: 0.6725701093673706\n",
      "Epoch 10, batch 118 D Loss: 1.3934712409973145, G Loss: 0.6832501292228699\n",
      "Epoch 10, batch 119 D Loss: 1.402704119682312, G Loss: 0.6708269715309143\n",
      "Epoch 10, batch 120 D Loss: 1.4008784294128418, G Loss: 0.6818550229072571\n",
      "Epoch 10, batch 121 D Loss: 1.375185251235962, G Loss: 0.6904999613761902\n",
      "Epoch 10, batch 122 D Loss: 1.3893113136291504, G Loss: 0.6833667755126953\n",
      "Epoch 10, batch 123 D Loss: 1.4023399353027344, G Loss: 0.6860325336456299\n",
      "Epoch 10, batch 124 D Loss: 1.4060306549072266, G Loss: 0.6801193952560425\n",
      "Epoch 10, batch 125 D Loss: 1.3956050872802734, G Loss: 0.691211462020874\n",
      "Epoch 10, batch 126 D Loss: 1.3782490491867065, G Loss: 0.6983705163002014\n",
      "Epoch 10, batch 127 D Loss: 1.396350622177124, G Loss: 0.6901838183403015\n",
      "Epoch 10, batch 128 D Loss: 1.3906097412109375, G Loss: 0.6849806904792786\n",
      "Epoch 10, batch 129 D Loss: 1.3886630535125732, G Loss: 0.6849984526634216\n",
      "Epoch 10, batch 130 D Loss: 1.3866381645202637, G Loss: 0.6961400508880615\n",
      "Epoch 10, batch 131 D Loss: 1.3838905096054077, G Loss: 0.6949226260185242\n",
      "Epoch 10, batch 132 D Loss: 1.3819591999053955, G Loss: 0.6969540119171143\n",
      "Epoch 10, batch 133 D Loss: 1.3918073177337646, G Loss: 0.688057005405426\n",
      "Epoch 10, batch 134 D Loss: 1.3739873170852661, G Loss: 0.6958242654800415\n",
      "Epoch 10, batch 135 D Loss: 1.3856449127197266, G Loss: 0.692625880241394\n",
      "Epoch 10, batch 136 D Loss: 1.3878934383392334, G Loss: 0.692851722240448\n",
      "Epoch 10, batch 137 D Loss: 1.3926329612731934, G Loss: 0.6870964169502258\n",
      "Epoch 10, batch 138 D Loss: 1.3749688863754272, G Loss: 0.6960684061050415\n",
      "Epoch 10, batch 139 D Loss: 1.3825011253356934, G Loss: 0.6933237314224243\n",
      "Epoch 10, batch 140 D Loss: 1.3846497535705566, G Loss: 0.6850430369377136\n",
      "Epoch 10, batch 141 D Loss: 1.3869816064834595, G Loss: 0.6868423223495483\n",
      "Epoch 10, batch 142 D Loss: 1.3839309215545654, G Loss: 0.6850436329841614\n",
      "Epoch 10, batch 143 D Loss: 1.385360836982727, G Loss: 0.684540331363678\n",
      "Epoch 10, batch 144 D Loss: 1.3772810697555542, G Loss: 0.6875659227371216\n",
      "Epoch 10, batch 145 D Loss: 1.3880038261413574, G Loss: 0.6783638000488281\n",
      "Epoch 10, batch 146 D Loss: 1.3858237266540527, G Loss: 0.6823983788490295\n",
      "Epoch 10, batch 147 D Loss: 1.3868083953857422, G Loss: 0.6837108731269836\n",
      "Epoch 10, batch 148 D Loss: 1.385411024093628, G Loss: 0.6805847883224487\n",
      "Epoch 10, batch 149 D Loss: 1.3879876136779785, G Loss: 0.6797019243240356\n",
      "Epoch 10, batch 150 D Loss: 1.3799080848693848, G Loss: 0.6825128793716431\n",
      "Epoch 10, batch 151 D Loss: 1.3719956874847412, G Loss: 0.687879204750061\n",
      "Epoch 10, batch 152 D Loss: 1.3834662437438965, G Loss: 0.6750749349594116\n",
      "Epoch 10, batch 153 D Loss: 1.3902127742767334, G Loss: 0.6802236437797546\n",
      "Epoch 10, batch 154 D Loss: 1.3674921989440918, G Loss: 0.682200014591217\n",
      "Epoch 10, batch 155 D Loss: 1.3855735063552856, G Loss: 0.676755428314209\n",
      "Epoch 10, batch 156 D Loss: 1.3692803382873535, G Loss: 0.6840686798095703\n",
      "Epoch 10, batch 157 D Loss: 1.3809221982955933, G Loss: 0.6779261231422424\n",
      "Epoch 10, batch 158 D Loss: 1.3893733024597168, G Loss: 0.6745017766952515\n",
      "Epoch 10, batch 159 D Loss: 1.3791745901107788, G Loss: 0.6850776672363281\n",
      "Epoch 10, batch 160 D Loss: 1.3748189210891724, G Loss: 0.6850305199623108\n",
      "Epoch 10, batch 161 D Loss: 1.3730480670928955, G Loss: 0.6787010431289673\n",
      "Epoch 10, batch 162 D Loss: 1.3857903480529785, G Loss: 0.6800055503845215\n",
      "Epoch 10, batch 163 D Loss: 1.384291172027588, G Loss: 0.6785385012626648\n",
      "Epoch 10, batch 164 D Loss: 1.3694404363632202, G Loss: 0.6820412278175354\n",
      "Epoch 10, batch 165 D Loss: 1.3800667524337769, G Loss: 0.6863531470298767\n",
      "Epoch 10, batch 166 D Loss: 1.384984016418457, G Loss: 0.6756621599197388\n",
      "Epoch 10, batch 167 D Loss: 1.3653202056884766, G Loss: 0.6815714836120605\n",
      "Epoch 10, batch 168 D Loss: 1.3754539489746094, G Loss: 0.6729502081871033\n",
      "Epoch 10, batch 169 D Loss: 1.3748993873596191, G Loss: 0.676433265209198\n",
      "Epoch 10, batch 170 D Loss: 1.3811992406845093, G Loss: 0.6744393706321716\n",
      "Epoch 10, batch 171 D Loss: 1.3640398979187012, G Loss: 0.6857541799545288\n",
      "Epoch 10, batch 172 D Loss: 1.3700575828552246, G Loss: 0.6720972657203674\n",
      "Epoch 10, batch 173 D Loss: 1.388437032699585, G Loss: 0.6733948588371277\n",
      "Epoch 10, batch 174 D Loss: 1.385817527770996, G Loss: 0.6753000617027283\n",
      "Epoch 10, batch 175 D Loss: 1.3831074237823486, G Loss: 0.6785286664962769\n",
      "Epoch 10, batch 176 D Loss: 1.3765182495117188, G Loss: 0.6770684719085693\n",
      "Epoch 10, batch 177 D Loss: 1.3784512281417847, G Loss: 0.6841866374015808\n",
      "Epoch 10, batch 178 D Loss: 1.3870770931243896, G Loss: 0.6737523078918457\n",
      "Epoch 10, batch 179 D Loss: 1.3903679847717285, G Loss: 0.6719368696212769\n",
      "Epoch 10, batch 180 D Loss: 1.3828003406524658, G Loss: 0.6720852851867676\n",
      "Epoch 10, batch 181 D Loss: 1.3979442119598389, G Loss: 0.6702793836593628\n",
      "Epoch 10, batch 182 D Loss: 1.3865104913711548, G Loss: 0.6717635989189148\n",
      "Epoch 10, batch 183 D Loss: 1.3725090026855469, G Loss: 0.6807093024253845\n",
      "Epoch 10, batch 184 D Loss: 1.3749529123306274, G Loss: 0.6828937530517578\n",
      "Epoch 10, batch 185 D Loss: 1.3809417486190796, G Loss: 0.6792485117912292\n",
      "Epoch 10, batch 186 D Loss: 1.4045391082763672, G Loss: 0.6708952188491821\n",
      "Epoch 10, batch 187 D Loss: 1.3864508867263794, G Loss: 0.6841211915016174\n",
      "Epoch 10, batch 188 D Loss: 1.3663380146026611, G Loss: 0.6779325008392334\n",
      "Epoch 10, batch 189 D Loss: 1.3972134590148926, G Loss: 0.6659483909606934\n",
      "Epoch 10, batch 190 D Loss: 1.3817485570907593, G Loss: 0.6743919253349304\n",
      "Epoch 10, batch 191 D Loss: 1.3765188455581665, G Loss: 0.6878598928451538\n",
      "Epoch 10, batch 192 D Loss: 1.3659448623657227, G Loss: 0.6861408948898315\n",
      "Epoch 10, batch 193 D Loss: 1.3745697736740112, G Loss: 0.6802244782447815\n",
      "Epoch 10, batch 194 D Loss: 1.3815326690673828, G Loss: 0.6819477677345276\n",
      "Epoch 10, batch 195 D Loss: 1.3695707321166992, G Loss: 0.6895869970321655\n",
      "Epoch 10, batch 196 D Loss: 1.3857831954956055, G Loss: 0.6823418140411377\n",
      "Epoch 10, batch 197 D Loss: 1.3721510171890259, G Loss: 0.6901093125343323\n",
      "Epoch 10, batch 198 D Loss: 1.3951045274734497, G Loss: 0.6776430606842041\n",
      "Epoch 10, batch 199 D Loss: 1.3710875511169434, G Loss: 0.6864596605300903\n",
      "Epoch 10, batch 200 D Loss: 1.37044095993042, G Loss: 0.6790688037872314\n",
      "Epoch 11, batch 1 D Loss: 1.3761160373687744, G Loss: 0.6843071579933167\n",
      "Epoch 11, batch 2 D Loss: 1.3890907764434814, G Loss: 0.6800447106361389\n",
      "Epoch 11, batch 3 D Loss: 1.3671895265579224, G Loss: 0.6887075304985046\n",
      "Epoch 11, batch 4 D Loss: 1.3758974075317383, G Loss: 0.6760597825050354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, batch 5 D Loss: 1.3737785816192627, G Loss: 0.6887234449386597\n",
      "Epoch 11, batch 6 D Loss: 1.3681886196136475, G Loss: 0.6902093291282654\n",
      "Epoch 11, batch 7 D Loss: 1.366750717163086, G Loss: 0.7006630897521973\n",
      "Epoch 11, batch 8 D Loss: 1.3861044645309448, G Loss: 0.6903002262115479\n",
      "Epoch 11, batch 9 D Loss: 1.384695291519165, G Loss: 0.6842847466468811\n",
      "Epoch 11, batch 10 D Loss: 1.3700199127197266, G Loss: 0.6988129615783691\n",
      "Epoch 11, batch 11 D Loss: 1.37947416305542, G Loss: 0.6893972158432007\n",
      "Epoch 11, batch 12 D Loss: 1.3730425834655762, G Loss: 0.6922680735588074\n",
      "Epoch 11, batch 13 D Loss: 1.388178825378418, G Loss: 0.686256468296051\n",
      "Epoch 11, batch 14 D Loss: 1.3709139823913574, G Loss: 0.6880569458007812\n",
      "Epoch 11, batch 15 D Loss: 1.3728852272033691, G Loss: 0.7007652521133423\n",
      "Epoch 11, batch 16 D Loss: 1.380434274673462, G Loss: 0.6915512681007385\n",
      "Epoch 11, batch 17 D Loss: 1.368786096572876, G Loss: 0.6887316107749939\n",
      "Epoch 11, batch 18 D Loss: 1.385380506515503, G Loss: 0.6957756280899048\n",
      "Epoch 11, batch 19 D Loss: 1.3695158958435059, G Loss: 0.6921359300613403\n",
      "Epoch 11, batch 20 D Loss: 1.3822296857833862, G Loss: 0.6925520896911621\n",
      "Epoch 11, batch 21 D Loss: 1.3750619888305664, G Loss: 0.6947023868560791\n",
      "Epoch 11, batch 22 D Loss: 1.3883471488952637, G Loss: 0.6870525479316711\n",
      "Epoch 11, batch 23 D Loss: 1.3982901573181152, G Loss: 0.6834011673927307\n",
      "Epoch 11, batch 24 D Loss: 1.384394645690918, G Loss: 0.7074897885322571\n",
      "Epoch 11, batch 25 D Loss: 1.37711501121521, G Loss: 0.7033135890960693\n",
      "Epoch 11, batch 26 D Loss: 1.3648444414138794, G Loss: 0.7063209414482117\n",
      "Epoch 11, batch 27 D Loss: 1.3648560047149658, G Loss: 0.7050601840019226\n",
      "Epoch 11, batch 28 D Loss: 1.374767780303955, G Loss: 0.7075585126876831\n",
      "Epoch 11, batch 29 D Loss: 1.3843631744384766, G Loss: 0.6906067728996277\n",
      "Epoch 11, batch 30 D Loss: 1.4035656452178955, G Loss: 0.6900569796562195\n",
      "Epoch 11, batch 31 D Loss: 1.4003806114196777, G Loss: 0.6866661906242371\n",
      "Epoch 11, batch 32 D Loss: 1.3797677755355835, G Loss: 0.6999204158782959\n",
      "Epoch 11, batch 33 D Loss: 1.389760971069336, G Loss: 0.7046798467636108\n",
      "Epoch 11, batch 34 D Loss: 1.3762738704681396, G Loss: 0.6999801397323608\n",
      "Epoch 11, batch 35 D Loss: 1.3838669061660767, G Loss: 0.6936803460121155\n",
      "Epoch 11, batch 36 D Loss: 1.372527837753296, G Loss: 0.7010006904602051\n",
      "Epoch 11, batch 37 D Loss: 1.384615182876587, G Loss: 0.7135000824928284\n",
      "Epoch 11, batch 38 D Loss: 1.3973946571350098, G Loss: 0.6916093230247498\n",
      "Epoch 11, batch 39 D Loss: 1.3666456937789917, G Loss: 0.6987192630767822\n",
      "Epoch 11, batch 40 D Loss: 1.374110460281372, G Loss: 0.7225223779678345\n",
      "Epoch 11, batch 41 D Loss: 1.3561160564422607, G Loss: 0.7274171710014343\n",
      "Epoch 11, batch 42 D Loss: 1.3696333169937134, G Loss: 0.7158437967300415\n",
      "Epoch 11, batch 43 D Loss: 1.4078898429870605, G Loss: 0.6943507194519043\n",
      "Epoch 11, batch 44 D Loss: 1.398633599281311, G Loss: 0.6971642374992371\n",
      "Epoch 11, batch 45 D Loss: 1.401795506477356, G Loss: 0.7004556059837341\n",
      "Epoch 11, batch 46 D Loss: 1.4119377136230469, G Loss: 0.6910681128501892\n",
      "Epoch 11, batch 47 D Loss: 1.3717683553695679, G Loss: 0.7216246128082275\n",
      "Epoch 11, batch 48 D Loss: 1.3846516609191895, G Loss: 0.7063789367675781\n",
      "Epoch 11, batch 49 D Loss: 1.3892061710357666, G Loss: 0.7051749229431152\n",
      "Epoch 11, batch 50 D Loss: 1.3650591373443604, G Loss: 0.7084268927574158\n",
      "Epoch 11, batch 51 D Loss: 1.3790607452392578, G Loss: 0.7027013897895813\n",
      "Epoch 11, batch 52 D Loss: 1.3745216131210327, G Loss: 0.720855176448822\n",
      "Epoch 11, batch 53 D Loss: 1.3863309621810913, G Loss: 0.714347243309021\n",
      "Epoch 11, batch 54 D Loss: 1.3937897682189941, G Loss: 0.708589494228363\n",
      "Epoch 11, batch 55 D Loss: 1.368309736251831, G Loss: 0.7197679281234741\n",
      "Epoch 11, batch 56 D Loss: 1.387462854385376, G Loss: 0.7050895690917969\n",
      "Epoch 11, batch 57 D Loss: 1.3937212228775024, G Loss: 0.712583601474762\n",
      "Epoch 11, batch 58 D Loss: 1.3458539247512817, G Loss: 0.7235299944877625\n",
      "Epoch 11, batch 59 D Loss: 1.3966550827026367, G Loss: 0.7097756862640381\n",
      "Epoch 11, batch 60 D Loss: 1.3868839740753174, G Loss: 0.7005999088287354\n",
      "Epoch 11, batch 61 D Loss: 1.3821158409118652, G Loss: 0.710116982460022\n",
      "Epoch 11, batch 62 D Loss: 1.3991873264312744, G Loss: 0.6954419612884521\n",
      "Epoch 11, batch 63 D Loss: 1.3912017345428467, G Loss: 0.7135421633720398\n",
      "Epoch 11, batch 64 D Loss: 1.3769129514694214, G Loss: 0.7069050669670105\n",
      "Epoch 11, batch 65 D Loss: 1.3616315126419067, G Loss: 0.7153336405754089\n",
      "Epoch 11, batch 66 D Loss: 1.3812994956970215, G Loss: 0.7071359157562256\n",
      "Epoch 11, batch 67 D Loss: 1.3825162649154663, G Loss: 0.7148861885070801\n",
      "Epoch 11, batch 68 D Loss: 1.389599323272705, G Loss: 0.6991953253746033\n",
      "Epoch 11, batch 69 D Loss: 1.3671207427978516, G Loss: 0.7082816362380981\n",
      "Epoch 11, batch 70 D Loss: 1.3833746910095215, G Loss: 0.7113265991210938\n",
      "Epoch 11, batch 71 D Loss: 1.377737283706665, G Loss: 0.716813325881958\n",
      "Epoch 11, batch 72 D Loss: 1.3607165813446045, G Loss: 0.7241798639297485\n",
      "Epoch 11, batch 73 D Loss: 1.3867051601409912, G Loss: 0.7029735445976257\n",
      "Epoch 11, batch 74 D Loss: 1.3800849914550781, G Loss: 0.7107639908790588\n",
      "Epoch 11, batch 75 D Loss: 1.3929110765457153, G Loss: 0.6982600688934326\n",
      "Epoch 11, batch 76 D Loss: 1.3819571733474731, G Loss: 0.7043855786323547\n",
      "Epoch 11, batch 77 D Loss: 1.3877294063568115, G Loss: 0.6965749263763428\n",
      "Epoch 11, batch 78 D Loss: 1.3825724124908447, G Loss: 0.6938633918762207\n",
      "Epoch 11, batch 79 D Loss: 1.3942333459854126, G Loss: 0.6984044909477234\n",
      "Epoch 11, batch 80 D Loss: 1.3932551145553589, G Loss: 0.7066241502761841\n",
      "Epoch 11, batch 81 D Loss: 1.3860557079315186, G Loss: 0.6993620991706848\n",
      "Epoch 11, batch 82 D Loss: 1.3851288557052612, G Loss: 0.6983932256698608\n",
      "Epoch 11, batch 83 D Loss: 1.3731844425201416, G Loss: 0.7011355757713318\n",
      "Epoch 11, batch 84 D Loss: 1.3692224025726318, G Loss: 0.712145209312439\n",
      "Epoch 11, batch 85 D Loss: 1.394791841506958, G Loss: 0.6943780779838562\n",
      "Epoch 11, batch 86 D Loss: 1.385882019996643, G Loss: 0.6884775757789612\n",
      "Epoch 11, batch 87 D Loss: 1.3862593173980713, G Loss: 0.6948406100273132\n",
      "Epoch 11, batch 88 D Loss: 1.3806148767471313, G Loss: 0.7020426988601685\n",
      "Epoch 11, batch 89 D Loss: 1.3866558074951172, G Loss: 0.7073418498039246\n",
      "Epoch 11, batch 90 D Loss: 1.3690131902694702, G Loss: 0.7017892599105835\n",
      "Epoch 11, batch 91 D Loss: 1.4009767770767212, G Loss: 0.693132758140564\n",
      "Epoch 11, batch 92 D Loss: 1.3825525045394897, G Loss: 0.7112312316894531\n",
      "Epoch 11, batch 93 D Loss: 1.3842693567276, G Loss: 0.692381739616394\n",
      "Epoch 11, batch 94 D Loss: 1.3669524192810059, G Loss: 0.7056621313095093\n",
      "Epoch 11, batch 95 D Loss: 1.3628778457641602, G Loss: 0.7079825401306152\n",
      "Epoch 11, batch 96 D Loss: 1.3916292190551758, G Loss: 0.6977693438529968\n",
      "Epoch 11, batch 97 D Loss: 1.3852587938308716, G Loss: 0.7072131633758545\n",
      "Epoch 11, batch 98 D Loss: 1.3757293224334717, G Loss: 0.7074383497238159\n",
      "Epoch 11, batch 99 D Loss: 1.3864388465881348, G Loss: 0.6994741559028625\n",
      "Epoch 11, batch 100 D Loss: 1.3889079093933105, G Loss: 0.7013339996337891\n",
      "Epoch 11, batch 101 D Loss: 1.388765811920166, G Loss: 0.709545910358429\n",
      "Epoch 11, batch 102 D Loss: 1.3877660036087036, G Loss: 0.7064208388328552\n",
      "Epoch 11, batch 103 D Loss: 1.392758846282959, G Loss: 0.7022712230682373\n",
      "Epoch 11, batch 104 D Loss: 1.383542776107788, G Loss: 0.7053125500679016\n",
      "Epoch 11, batch 105 D Loss: 1.387786865234375, G Loss: 0.702842652797699\n",
      "Epoch 11, batch 106 D Loss: 1.395822286605835, G Loss: 0.7003418207168579\n",
      "Epoch 11, batch 107 D Loss: 1.3799971342086792, G Loss: 0.7159429788589478\n",
      "Epoch 11, batch 108 D Loss: 1.391064167022705, G Loss: 0.7121853828430176\n",
      "Epoch 11, batch 109 D Loss: 1.3805181980133057, G Loss: 0.7161008715629578\n",
      "Epoch 11, batch 110 D Loss: 1.3597261905670166, G Loss: 0.7202789187431335\n",
      "Epoch 11, batch 111 D Loss: 1.3870329856872559, G Loss: 0.708183228969574\n",
      "Epoch 11, batch 112 D Loss: 1.390809178352356, G Loss: 0.7125917673110962\n",
      "Epoch 11, batch 113 D Loss: 1.4039876461029053, G Loss: 0.6975076794624329\n",
      "Epoch 11, batch 114 D Loss: 1.3867433071136475, G Loss: 0.7202972173690796\n",
      "Epoch 11, batch 115 D Loss: 1.3852384090423584, G Loss: 0.7225837111473083\n",
      "Epoch 11, batch 116 D Loss: 1.3958830833435059, G Loss: 0.7098567485809326\n",
      "Epoch 11, batch 117 D Loss: 1.381392240524292, G Loss: 0.7064979672431946\n",
      "Epoch 11, batch 118 D Loss: 1.395934820175171, G Loss: 0.7026067972183228\n",
      "Epoch 11, batch 119 D Loss: 1.3843168020248413, G Loss: 0.7282066941261292\n",
      "Epoch 11, batch 120 D Loss: 1.3656166791915894, G Loss: 0.7307935357093811\n",
      "Epoch 11, batch 121 D Loss: 1.3945565223693848, G Loss: 0.7242037057876587\n",
      "Epoch 11, batch 122 D Loss: 1.3609682321548462, G Loss: 0.7348989248275757\n",
      "Epoch 11, batch 123 D Loss: 1.3939323425292969, G Loss: 0.7314602136611938\n",
      "Epoch 11, batch 124 D Loss: 1.391556739807129, G Loss: 0.7199822068214417\n",
      "Epoch 11, batch 125 D Loss: 1.3828401565551758, G Loss: 0.7268256545066833\n",
      "Epoch 11, batch 126 D Loss: 1.3814717531204224, G Loss: 0.725675642490387\n",
      "Epoch 11, batch 127 D Loss: 1.3889801502227783, G Loss: 0.725006639957428\n",
      "Epoch 11, batch 128 D Loss: 1.412221074104309, G Loss: 0.7088636755943298\n",
      "Epoch 11, batch 129 D Loss: 1.3924243450164795, G Loss: 0.7214657068252563\n",
      "Epoch 11, batch 130 D Loss: 1.391343593597412, G Loss: 0.7165758609771729\n",
      "Epoch 11, batch 131 D Loss: 1.393310308456421, G Loss: 0.7155135869979858\n",
      "Epoch 11, batch 132 D Loss: 1.3847172260284424, G Loss: 0.7226952314376831\n",
      "Epoch 11, batch 133 D Loss: 1.3862597942352295, G Loss: 0.7278508543968201\n",
      "Epoch 11, batch 134 D Loss: 1.3950142860412598, G Loss: 0.7263200283050537\n",
      "Epoch 11, batch 135 D Loss: 1.3880953788757324, G Loss: 0.7211600542068481\n",
      "Epoch 11, batch 136 D Loss: 1.379082202911377, G Loss: 0.7369977831840515\n",
      "Epoch 11, batch 137 D Loss: 1.393860936164856, G Loss: 0.7283241748809814\n",
      "Epoch 11, batch 138 D Loss: 1.3991118669509888, G Loss: 0.7159437537193298\n",
      "Epoch 11, batch 139 D Loss: 1.3892409801483154, G Loss: 0.7242940664291382\n",
      "Epoch 11, batch 140 D Loss: 1.3846969604492188, G Loss: 0.7219296097755432\n",
      "Epoch 11, batch 141 D Loss: 1.385765790939331, G Loss: 0.7334712743759155\n",
      "Epoch 11, batch 142 D Loss: 1.3922545909881592, G Loss: 0.7143327593803406\n",
      "Epoch 11, batch 143 D Loss: 1.3857883214950562, G Loss: 0.7238729000091553\n",
      "Epoch 11, batch 144 D Loss: 1.4001972675323486, G Loss: 0.7299151420593262\n",
      "Epoch 11, batch 145 D Loss: 1.3975751399993896, G Loss: 0.7246922850608826\n",
      "Epoch 11, batch 146 D Loss: 1.3796600103378296, G Loss: 0.7295913696289062\n",
      "Epoch 11, batch 147 D Loss: 1.3750519752502441, G Loss: 0.7369372844696045\n",
      "Epoch 11, batch 148 D Loss: 1.3701980113983154, G Loss: 0.7353634834289551\n",
      "Epoch 11, batch 149 D Loss: 1.3886010646820068, G Loss: 0.7203028202056885\n",
      "Epoch 11, batch 150 D Loss: 1.3795661926269531, G Loss: 0.7291584610939026\n",
      "Epoch 11, batch 151 D Loss: 1.3909492492675781, G Loss: 0.7293409705162048\n",
      "Epoch 11, batch 152 D Loss: 1.3843672275543213, G Loss: 0.7227382063865662\n",
      "Epoch 11, batch 153 D Loss: 1.3678874969482422, G Loss: 0.7373992800712585\n",
      "Epoch 11, batch 154 D Loss: 1.4044888019561768, G Loss: 0.7162074446678162\n",
      "Epoch 11, batch 155 D Loss: 1.3968048095703125, G Loss: 0.718315601348877\n",
      "Epoch 11, batch 156 D Loss: 1.3657282590866089, G Loss: 0.7438962459564209\n",
      "Epoch 11, batch 157 D Loss: 1.3920643329620361, G Loss: 0.7239835262298584\n",
      "Epoch 11, batch 158 D Loss: 1.3807425498962402, G Loss: 0.73590087890625\n",
      "Epoch 11, batch 159 D Loss: 1.3870790004730225, G Loss: 0.7386527061462402\n",
      "Epoch 11, batch 160 D Loss: 1.3780786991119385, G Loss: 0.7339325547218323\n",
      "Epoch 11, batch 161 D Loss: 1.3844490051269531, G Loss: 0.7279744744300842\n",
      "Epoch 11, batch 162 D Loss: 1.3738703727722168, G Loss: 0.7497632503509521\n",
      "Epoch 11, batch 163 D Loss: 1.3868741989135742, G Loss: 0.7258229851722717\n",
      "Epoch 11, batch 164 D Loss: 1.392026662826538, G Loss: 0.7325112223625183\n",
      "Epoch 11, batch 165 D Loss: 1.3948605060577393, G Loss: 0.7311651110649109\n",
      "Epoch 11, batch 166 D Loss: 1.3944450616836548, G Loss: 0.7275916337966919\n",
      "Epoch 11, batch 167 D Loss: 1.3807330131530762, G Loss: 0.7400380969047546\n",
      "Epoch 11, batch 168 D Loss: 1.3975749015808105, G Loss: 0.7227436304092407\n",
      "Epoch 11, batch 169 D Loss: 1.3762037754058838, G Loss: 0.7430856823921204\n",
      "Epoch 11, batch 170 D Loss: 1.390730619430542, G Loss: 0.7280648946762085\n",
      "Epoch 11, batch 171 D Loss: 1.372009515762329, G Loss: 0.7375794053077698\n",
      "Epoch 11, batch 172 D Loss: 1.3993107080459595, G Loss: 0.7298500537872314\n",
      "Epoch 11, batch 173 D Loss: 1.395355224609375, G Loss: 0.7303401231765747\n",
      "Epoch 11, batch 174 D Loss: 1.3807878494262695, G Loss: 0.7270471453666687\n",
      "Epoch 11, batch 175 D Loss: 1.3689110279083252, G Loss: 0.7291281223297119\n",
      "Epoch 11, batch 176 D Loss: 1.3819056749343872, G Loss: 0.72545325756073\n",
      "Epoch 11, batch 177 D Loss: 1.3881816864013672, G Loss: 0.729631245136261\n",
      "Epoch 11, batch 178 D Loss: 1.388325572013855, G Loss: 0.7279854416847229\n",
      "Epoch 11, batch 179 D Loss: 1.3775863647460938, G Loss: 0.7388702630996704\n",
      "Epoch 11, batch 180 D Loss: 1.3825347423553467, G Loss: 0.7253974676132202\n",
      "Epoch 11, batch 181 D Loss: 1.3815784454345703, G Loss: 0.7213943004608154\n",
      "Epoch 11, batch 182 D Loss: 1.3937716484069824, G Loss: 0.7341885566711426\n",
      "Epoch 11, batch 183 D Loss: 1.39015531539917, G Loss: 0.7254974842071533\n",
      "Epoch 11, batch 184 D Loss: 1.392305850982666, G Loss: 0.7284682393074036\n",
      "Epoch 11, batch 185 D Loss: 1.3890562057495117, G Loss: 0.7266870737075806\n",
      "Epoch 11, batch 186 D Loss: 1.390487790107727, G Loss: 0.716979444026947\n",
      "Epoch 11, batch 187 D Loss: 1.3789074420928955, G Loss: 0.7226687669754028\n",
      "Epoch 11, batch 188 D Loss: 1.3860077857971191, G Loss: 0.7160769701004028\n",
      "Epoch 11, batch 189 D Loss: 1.3718490600585938, G Loss: 0.7217437028884888\n",
      "Epoch 11, batch 190 D Loss: 1.4111849069595337, G Loss: 0.6959056258201599\n",
      "Epoch 11, batch 191 D Loss: 1.383113145828247, G Loss: 0.7083563208580017\n",
      "Epoch 11, batch 192 D Loss: 1.384242296218872, G Loss: 0.7011227607727051\n",
      "Epoch 11, batch 193 D Loss: 1.376733422279358, G Loss: 0.7067981958389282\n",
      "Epoch 11, batch 194 D Loss: 1.3716278076171875, G Loss: 0.716564416885376\n",
      "Epoch 11, batch 195 D Loss: 1.379763126373291, G Loss: 0.7157884240150452\n",
      "Epoch 11, batch 196 D Loss: 1.391251802444458, G Loss: 0.7075547575950623\n",
      "Epoch 11, batch 197 D Loss: 1.388349175453186, G Loss: 0.7057053446769714\n",
      "Epoch 11, batch 198 D Loss: 1.3764050006866455, G Loss: 0.7036287188529968\n",
      "Epoch 11, batch 199 D Loss: 1.3912256956100464, G Loss: 0.7078103423118591\n",
      "Epoch 11, batch 200 D Loss: 1.3747870922088623, G Loss: 0.7127375602722168\n",
      "Epoch 12, batch 1 D Loss: 1.386777639389038, G Loss: 0.6993391513824463\n",
      "Epoch 12, batch 2 D Loss: 1.4102351665496826, G Loss: 0.6906999349594116\n",
      "Epoch 12, batch 3 D Loss: 1.3894479274749756, G Loss: 0.697285532951355\n",
      "Epoch 12, batch 4 D Loss: 1.3973917961120605, G Loss: 0.697117030620575\n",
      "Epoch 12, batch 5 D Loss: 1.3889610767364502, G Loss: 0.6950279474258423\n",
      "Epoch 12, batch 6 D Loss: 1.3791412115097046, G Loss: 0.7056158185005188\n",
      "Epoch 12, batch 7 D Loss: 1.3947933912277222, G Loss: 0.6932430267333984\n",
      "Epoch 12, batch 8 D Loss: 1.3886375427246094, G Loss: 0.6933025121688843\n",
      "Epoch 12, batch 9 D Loss: 1.3769056797027588, G Loss: 0.702528715133667\n",
      "Epoch 12, batch 10 D Loss: 1.3954565525054932, G Loss: 0.6866233348846436\n",
      "Epoch 12, batch 11 D Loss: 1.3785557746887207, G Loss: 0.6900837421417236\n",
      "Epoch 12, batch 12 D Loss: 1.3807518482208252, G Loss: 0.6887755393981934\n",
      "Epoch 12, batch 13 D Loss: 1.384493350982666, G Loss: 0.6970144510269165\n",
      "Epoch 12, batch 14 D Loss: 1.3916574716567993, G Loss: 0.6938533186912537\n",
      "Epoch 12, batch 15 D Loss: 1.3719964027404785, G Loss: 0.7064241170883179\n",
      "Epoch 12, batch 16 D Loss: 1.377589225769043, G Loss: 0.7001950740814209\n",
      "Epoch 12, batch 17 D Loss: 1.3899726867675781, G Loss: 0.69162917137146\n",
      "Epoch 12, batch 18 D Loss: 1.37153959274292, G Loss: 0.6998715996742249\n",
      "Epoch 12, batch 19 D Loss: 1.3745826482772827, G Loss: 0.7045459151268005\n",
      "Epoch 12, batch 20 D Loss: 1.3852266073226929, G Loss: 0.6990416049957275\n",
      "Epoch 12, batch 21 D Loss: 1.3573262691497803, G Loss: 0.7196003794670105\n",
      "Epoch 12, batch 22 D Loss: 1.4054690599441528, G Loss: 0.6948727965354919\n",
      "Epoch 12, batch 23 D Loss: 1.3871512413024902, G Loss: 0.7005302309989929\n",
      "Epoch 12, batch 24 D Loss: 1.3701801300048828, G Loss: 0.7117849588394165\n",
      "Epoch 12, batch 25 D Loss: 1.3904885053634644, G Loss: 0.682951807975769\n",
      "Epoch 12, batch 26 D Loss: 1.3840539455413818, G Loss: 0.6966372728347778\n",
      "Epoch 12, batch 27 D Loss: 1.3895573616027832, G Loss: 0.6909875273704529\n",
      "Epoch 12, batch 28 D Loss: 1.4026591777801514, G Loss: 0.682475745677948\n",
      "Epoch 12, batch 29 D Loss: 1.3882416486740112, G Loss: 0.6984118819236755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, batch 30 D Loss: 1.3675755262374878, G Loss: 0.7039312124252319\n",
      "Epoch 12, batch 31 D Loss: 1.4028067588806152, G Loss: 0.6811433434486389\n",
      "Epoch 12, batch 32 D Loss: 1.373284101486206, G Loss: 0.6884689927101135\n",
      "Epoch 12, batch 33 D Loss: 1.3842040300369263, G Loss: 0.6950704455375671\n",
      "Epoch 12, batch 34 D Loss: 1.3883898258209229, G Loss: 0.6908689737319946\n",
      "Epoch 12, batch 35 D Loss: 1.3884129524230957, G Loss: 0.6939531564712524\n",
      "Epoch 12, batch 36 D Loss: 1.3961143493652344, G Loss: 0.6811948418617249\n",
      "Epoch 12, batch 37 D Loss: 1.400413990020752, G Loss: 0.6838961839675903\n",
      "Epoch 12, batch 38 D Loss: 1.4002492427825928, G Loss: 0.6845964193344116\n",
      "Epoch 12, batch 39 D Loss: 1.3990771770477295, G Loss: 0.6838242411613464\n",
      "Epoch 12, batch 40 D Loss: 1.4045801162719727, G Loss: 0.6861749887466431\n",
      "Epoch 12, batch 41 D Loss: 1.3936150074005127, G Loss: 0.6846920251846313\n",
      "Epoch 12, batch 42 D Loss: 1.3830432891845703, G Loss: 0.6935765147209167\n",
      "Epoch 12, batch 43 D Loss: 1.395723581314087, G Loss: 0.6784535050392151\n",
      "Epoch 12, batch 44 D Loss: 1.3875572681427002, G Loss: 0.6932582855224609\n",
      "Epoch 12, batch 45 D Loss: 1.3803250789642334, G Loss: 0.6929690837860107\n",
      "Epoch 12, batch 46 D Loss: 1.3923029899597168, G Loss: 0.6867712140083313\n",
      "Epoch 12, batch 47 D Loss: 1.3830662965774536, G Loss: 0.6764182448387146\n",
      "Epoch 12, batch 48 D Loss: 1.3775591850280762, G Loss: 0.6900022625923157\n",
      "Epoch 12, batch 49 D Loss: 1.393652319908142, G Loss: 0.6749855279922485\n",
      "Epoch 12, batch 50 D Loss: 1.3781182765960693, G Loss: 0.6823087334632874\n",
      "Epoch 12, batch 51 D Loss: 1.3930606842041016, G Loss: 0.6762617230415344\n",
      "Epoch 12, batch 52 D Loss: 1.3882558345794678, G Loss: 0.6770747900009155\n",
      "Epoch 12, batch 53 D Loss: 1.3960909843444824, G Loss: 0.6780686378479004\n",
      "Epoch 12, batch 54 D Loss: 1.3898186683654785, G Loss: 0.6818321347236633\n",
      "Epoch 12, batch 55 D Loss: 1.3739678859710693, G Loss: 0.6909831166267395\n",
      "Epoch 12, batch 56 D Loss: 1.3773796558380127, G Loss: 0.6953567266464233\n",
      "Epoch 12, batch 57 D Loss: 1.3798167705535889, G Loss: 0.6894607543945312\n",
      "Epoch 12, batch 58 D Loss: 1.3903589248657227, G Loss: 0.6858705878257751\n",
      "Epoch 12, batch 59 D Loss: 1.37953519821167, G Loss: 0.6787482500076294\n",
      "Epoch 12, batch 60 D Loss: 1.4062633514404297, G Loss: 0.6754453182220459\n",
      "Epoch 12, batch 61 D Loss: 1.3955957889556885, G Loss: 0.6903665661811829\n",
      "Epoch 12, batch 62 D Loss: 1.3838332891464233, G Loss: 0.68471360206604\n",
      "Epoch 12, batch 63 D Loss: 1.388892650604248, G Loss: 0.684645414352417\n",
      "Epoch 12, batch 64 D Loss: 1.3704807758331299, G Loss: 0.6985728740692139\n",
      "Epoch 12, batch 65 D Loss: 1.3976166248321533, G Loss: 0.6793864369392395\n",
      "Epoch 12, batch 66 D Loss: 1.3808763027191162, G Loss: 0.6946579813957214\n",
      "Epoch 12, batch 67 D Loss: 1.3860615491867065, G Loss: 0.6791435480117798\n",
      "Epoch 12, batch 68 D Loss: 1.3910061120986938, G Loss: 0.6813744902610779\n",
      "Epoch 12, batch 69 D Loss: 1.3959978818893433, G Loss: 0.6769586205482483\n",
      "Epoch 12, batch 70 D Loss: 1.3920491933822632, G Loss: 0.6828978657722473\n",
      "Epoch 12, batch 71 D Loss: 1.3792988061904907, G Loss: 0.6975013613700867\n",
      "Epoch 12, batch 72 D Loss: 1.403338074684143, G Loss: 0.6702365279197693\n",
      "Epoch 12, batch 73 D Loss: 1.3820726871490479, G Loss: 0.686984121799469\n",
      "Epoch 12, batch 74 D Loss: 1.3940876722335815, G Loss: 0.6743171215057373\n",
      "Epoch 12, batch 75 D Loss: 1.3791569471359253, G Loss: 0.6864680647850037\n",
      "Epoch 12, batch 76 D Loss: 1.397472858428955, G Loss: 0.6752567291259766\n",
      "Epoch 12, batch 77 D Loss: 1.393997311592102, G Loss: 0.6801638603210449\n",
      "Epoch 12, batch 78 D Loss: 1.3820064067840576, G Loss: 0.6796297430992126\n",
      "Epoch 12, batch 79 D Loss: 1.3754912614822388, G Loss: 0.6865360736846924\n",
      "Epoch 12, batch 80 D Loss: 1.395965576171875, G Loss: 0.6723101139068604\n",
      "Epoch 12, batch 81 D Loss: 1.3762938976287842, G Loss: 0.6714720129966736\n",
      "Epoch 12, batch 82 D Loss: 1.3792122602462769, G Loss: 0.6750990152359009\n",
      "Epoch 12, batch 83 D Loss: 1.3992321491241455, G Loss: 0.667613685131073\n",
      "Epoch 12, batch 84 D Loss: 1.3871397972106934, G Loss: 0.669100821018219\n",
      "Epoch 12, batch 85 D Loss: 1.4112532138824463, G Loss: 0.670114278793335\n",
      "Epoch 12, batch 86 D Loss: 1.3821156024932861, G Loss: 0.6740736365318298\n",
      "Epoch 12, batch 87 D Loss: 1.389339804649353, G Loss: 0.6699051856994629\n",
      "Epoch 12, batch 88 D Loss: 1.3840205669403076, G Loss: 0.6733518242835999\n",
      "Epoch 12, batch 89 D Loss: 1.389451503753662, G Loss: 0.6718182563781738\n",
      "Epoch 12, batch 90 D Loss: 1.385966420173645, G Loss: 0.6749823093414307\n",
      "Epoch 12, batch 91 D Loss: 1.3892306089401245, G Loss: 0.6688162088394165\n",
      "Epoch 12, batch 92 D Loss: 1.3935329914093018, G Loss: 0.6715337634086609\n",
      "Epoch 12, batch 93 D Loss: 1.3978805541992188, G Loss: 0.6691671013832092\n",
      "Epoch 12, batch 94 D Loss: 1.3916219472885132, G Loss: 0.6775314807891846\n",
      "Epoch 12, batch 95 D Loss: 1.3864383697509766, G Loss: 0.6841819882392883\n",
      "Epoch 12, batch 96 D Loss: 1.3879644870758057, G Loss: 0.6799542903900146\n",
      "Epoch 12, batch 97 D Loss: 1.406890630722046, G Loss: 0.6676660180091858\n",
      "Epoch 12, batch 98 D Loss: 1.3880834579467773, G Loss: 0.669816792011261\n",
      "Epoch 12, batch 99 D Loss: 1.390399694442749, G Loss: 0.6875708699226379\n",
      "Epoch 12, batch 100 D Loss: 1.3934576511383057, G Loss: 0.6847044229507446\n",
      "Epoch 12, batch 101 D Loss: 1.3932743072509766, G Loss: 0.692330539226532\n",
      "Epoch 12, batch 102 D Loss: 1.3851284980773926, G Loss: 0.6820163130760193\n",
      "Epoch 12, batch 103 D Loss: 1.3908393383026123, G Loss: 0.687207818031311\n",
      "Epoch 12, batch 104 D Loss: 1.3971593379974365, G Loss: 0.6816872358322144\n",
      "Epoch 12, batch 105 D Loss: 1.3778207302093506, G Loss: 0.6952375769615173\n",
      "Epoch 12, batch 106 D Loss: 1.3941316604614258, G Loss: 0.6848433017730713\n",
      "Epoch 12, batch 107 D Loss: 1.372497320175171, G Loss: 0.702434778213501\n",
      "Epoch 12, batch 108 D Loss: 1.3955326080322266, G Loss: 0.691021203994751\n",
      "Epoch 12, batch 109 D Loss: 1.3790684938430786, G Loss: 0.6897399425506592\n",
      "Epoch 12, batch 110 D Loss: 1.395972728729248, G Loss: 0.6865913271903992\n",
      "Epoch 12, batch 111 D Loss: 1.3760008811950684, G Loss: 0.7077385783195496\n",
      "Epoch 12, batch 112 D Loss: 1.4008729457855225, G Loss: 0.686957836151123\n",
      "Epoch 12, batch 113 D Loss: 1.3963136672973633, G Loss: 0.69231778383255\n",
      "Epoch 12, batch 114 D Loss: 1.3891226053237915, G Loss: 0.6985548138618469\n",
      "Epoch 12, batch 115 D Loss: 1.3962640762329102, G Loss: 0.6905065178871155\n",
      "Epoch 12, batch 116 D Loss: 1.3883185386657715, G Loss: 0.6882283091545105\n",
      "Epoch 12, batch 117 D Loss: 1.3933663368225098, G Loss: 0.6882016062736511\n",
      "Epoch 12, batch 118 D Loss: 1.4007846117019653, G Loss: 0.6764663457870483\n",
      "Epoch 12, batch 119 D Loss: 1.394516944885254, G Loss: 0.6850090026855469\n",
      "Epoch 12, batch 120 D Loss: 1.379256248474121, G Loss: 0.6900098323822021\n",
      "Epoch 12, batch 121 D Loss: 1.3993064165115356, G Loss: 0.6888988018035889\n",
      "Epoch 12, batch 122 D Loss: 1.3863136768341064, G Loss: 0.6841216087341309\n",
      "Epoch 12, batch 123 D Loss: 1.3901078701019287, G Loss: 0.6881812810897827\n",
      "Epoch 12, batch 124 D Loss: 1.3822394609451294, G Loss: 0.6884134411811829\n",
      "Epoch 12, batch 125 D Loss: 1.3739893436431885, G Loss: 0.6998000144958496\n",
      "Epoch 12, batch 126 D Loss: 1.3802648782730103, G Loss: 0.6905808448791504\n",
      "Epoch 12, batch 127 D Loss: 1.3901139497756958, G Loss: 0.6799935698509216\n",
      "Epoch 12, batch 128 D Loss: 1.3888930082321167, G Loss: 0.6940263509750366\n",
      "Epoch 12, batch 129 D Loss: 1.389341115951538, G Loss: 0.690036952495575\n",
      "Epoch 12, batch 130 D Loss: 1.382804036140442, G Loss: 0.696525514125824\n",
      "Epoch 12, batch 131 D Loss: 1.3851873874664307, G Loss: 0.695102870464325\n",
      "Epoch 12, batch 132 D Loss: 1.4052929878234863, G Loss: 0.6891147494316101\n",
      "Epoch 12, batch 133 D Loss: 1.3933866024017334, G Loss: 0.6931324005126953\n",
      "Epoch 12, batch 134 D Loss: 1.375894546508789, G Loss: 0.7021408081054688\n",
      "Epoch 12, batch 135 D Loss: 1.3892734050750732, G Loss: 0.6939361691474915\n",
      "Epoch 12, batch 136 D Loss: 1.4020720720291138, G Loss: 0.689369797706604\n",
      "Epoch 12, batch 137 D Loss: 1.3901071548461914, G Loss: 0.6909927129745483\n",
      "Epoch 12, batch 138 D Loss: 1.3848776817321777, G Loss: 0.698548436164856\n",
      "Epoch 12, batch 139 D Loss: 1.378434658050537, G Loss: 0.7049228549003601\n",
      "Epoch 12, batch 140 D Loss: 1.381477952003479, G Loss: 0.6973024606704712\n",
      "Epoch 12, batch 141 D Loss: 1.3836308717727661, G Loss: 0.6986716985702515\n",
      "Epoch 12, batch 142 D Loss: 1.3783555030822754, G Loss: 0.701572597026825\n",
      "Epoch 12, batch 143 D Loss: 1.375173807144165, G Loss: 0.6974979639053345\n",
      "Epoch 12, batch 144 D Loss: 1.3975799083709717, G Loss: 0.6926065683364868\n",
      "Epoch 12, batch 145 D Loss: 1.3996167182922363, G Loss: 0.6829891800880432\n",
      "Epoch 12, batch 146 D Loss: 1.3987102508544922, G Loss: 0.6989765167236328\n",
      "Epoch 12, batch 147 D Loss: 1.4022494554519653, G Loss: 0.6941236853599548\n",
      "Epoch 12, batch 148 D Loss: 1.3923258781433105, G Loss: 0.6944963932037354\n",
      "Epoch 12, batch 149 D Loss: 1.394707441329956, G Loss: 0.6954134106636047\n",
      "Epoch 12, batch 150 D Loss: 1.3768329620361328, G Loss: 0.6975639462471008\n",
      "Epoch 12, batch 151 D Loss: 1.397313117980957, G Loss: 0.6881868243217468\n",
      "Epoch 12, batch 152 D Loss: 1.3785916566848755, G Loss: 0.7067997455596924\n",
      "Epoch 12, batch 153 D Loss: 1.390958547592163, G Loss: 0.6929624080657959\n",
      "Epoch 12, batch 154 D Loss: 1.3911495208740234, G Loss: 0.6990684270858765\n",
      "Epoch 12, batch 155 D Loss: 1.3815760612487793, G Loss: 0.6998377442359924\n",
      "Epoch 12, batch 156 D Loss: 1.4068899154663086, G Loss: 0.6801353693008423\n",
      "Epoch 12, batch 157 D Loss: 1.3958505392074585, G Loss: 0.6879368424415588\n",
      "Epoch 12, batch 158 D Loss: 1.4054040908813477, G Loss: 0.6877601146697998\n",
      "Epoch 12, batch 159 D Loss: 1.3915272951126099, G Loss: 0.6892333030700684\n",
      "Epoch 12, batch 160 D Loss: 1.3922889232635498, G Loss: 0.6900784373283386\n",
      "Epoch 12, batch 161 D Loss: 1.3900600671768188, G Loss: 0.6922119855880737\n",
      "Epoch 12, batch 162 D Loss: 1.384387731552124, G Loss: 0.6884413361549377\n",
      "Epoch 12, batch 163 D Loss: 1.3869752883911133, G Loss: 0.6859709024429321\n",
      "Epoch 12, batch 164 D Loss: 1.3858156204223633, G Loss: 0.6846863031387329\n",
      "Epoch 12, batch 165 D Loss: 1.3903266191482544, G Loss: 0.6845101118087769\n",
      "Epoch 12, batch 166 D Loss: 1.379360318183899, G Loss: 0.683613657951355\n",
      "Epoch 12, batch 167 D Loss: 1.3840241432189941, G Loss: 0.6856709122657776\n",
      "Epoch 12, batch 168 D Loss: 1.3958908319473267, G Loss: 0.6748073697090149\n",
      "Epoch 12, batch 169 D Loss: 1.3908755779266357, G Loss: 0.6847560405731201\n",
      "Epoch 12, batch 170 D Loss: 1.3803082704544067, G Loss: 0.689534604549408\n",
      "Epoch 12, batch 171 D Loss: 1.3826602697372437, G Loss: 0.6858057975769043\n",
      "Epoch 12, batch 172 D Loss: 1.3902649879455566, G Loss: 0.6780595183372498\n",
      "Epoch 12, batch 173 D Loss: 1.3865866661071777, G Loss: 0.6771989464759827\n",
      "Epoch 12, batch 174 D Loss: 1.3776544332504272, G Loss: 0.6879443526268005\n",
      "Epoch 12, batch 175 D Loss: 1.3810927867889404, G Loss: 0.6744787096977234\n",
      "Epoch 12, batch 176 D Loss: 1.3788355588912964, G Loss: 0.6876062154769897\n",
      "Epoch 12, batch 177 D Loss: 1.4077990055084229, G Loss: 0.663877546787262\n",
      "Epoch 12, batch 178 D Loss: 1.3893449306488037, G Loss: 0.6788081526756287\n",
      "Epoch 12, batch 179 D Loss: 1.3929792642593384, G Loss: 0.6735220551490784\n",
      "Epoch 12, batch 180 D Loss: 1.3941254615783691, G Loss: 0.6835237741470337\n",
      "Epoch 12, batch 181 D Loss: 1.3974251747131348, G Loss: 0.6698485612869263\n",
      "Epoch 12, batch 182 D Loss: 1.3931069374084473, G Loss: 0.6828272342681885\n",
      "Epoch 12, batch 183 D Loss: 1.4003002643585205, G Loss: 0.6699647307395935\n",
      "Epoch 12, batch 184 D Loss: 1.3958903551101685, G Loss: 0.6695648431777954\n",
      "Epoch 12, batch 185 D Loss: 1.385190725326538, G Loss: 0.6866664290428162\n",
      "Epoch 12, batch 186 D Loss: 1.390862226486206, G Loss: 0.6793932318687439\n",
      "Epoch 12, batch 187 D Loss: 1.3869268894195557, G Loss: 0.6786049008369446\n",
      "Epoch 12, batch 188 D Loss: 1.3812520503997803, G Loss: 0.689441978931427\n",
      "Epoch 12, batch 189 D Loss: 1.3799128532409668, G Loss: 0.686313271522522\n",
      "Epoch 12, batch 190 D Loss: 1.3765076398849487, G Loss: 0.6828640103340149\n",
      "Epoch 12, batch 191 D Loss: 1.386594533920288, G Loss: 0.6837846636772156\n",
      "Epoch 12, batch 192 D Loss: 1.3794530630111694, G Loss: 0.68761146068573\n",
      "Epoch 12, batch 193 D Loss: 1.3941905498504639, G Loss: 0.675591230392456\n",
      "Epoch 12, batch 194 D Loss: 1.3804439306259155, G Loss: 0.6842921376228333\n",
      "Epoch 12, batch 195 D Loss: 1.397766351699829, G Loss: 0.6774132251739502\n",
      "Epoch 12, batch 196 D Loss: 1.3960106372833252, G Loss: 0.6817786693572998\n",
      "Epoch 12, batch 197 D Loss: 1.3862810134887695, G Loss: 0.6845090389251709\n",
      "Epoch 12, batch 198 D Loss: 1.3796536922454834, G Loss: 0.6816444396972656\n",
      "Epoch 12, batch 199 D Loss: 1.3948909044265747, G Loss: 0.6800404191017151\n",
      "Epoch 12, batch 200 D Loss: 1.3890326023101807, G Loss: 0.6829519867897034\n",
      "Epoch 13, batch 1 D Loss: 1.3935827016830444, G Loss: 0.6824033856391907\n",
      "Epoch 13, batch 2 D Loss: 1.3968983888626099, G Loss: 0.681489109992981\n",
      "Epoch 13, batch 3 D Loss: 1.381697177886963, G Loss: 0.6846838593482971\n",
      "Epoch 13, batch 4 D Loss: 1.392827033996582, G Loss: 0.6768745183944702\n",
      "Epoch 13, batch 5 D Loss: 1.3905194997787476, G Loss: 0.6853368878364563\n",
      "Epoch 13, batch 6 D Loss: 1.3749754428863525, G Loss: 0.6952968835830688\n",
      "Epoch 13, batch 7 D Loss: 1.3826098442077637, G Loss: 0.6984078288078308\n",
      "Epoch 13, batch 8 D Loss: 1.3927505016326904, G Loss: 0.6842166781425476\n",
      "Epoch 13, batch 9 D Loss: 1.3908677101135254, G Loss: 0.6896942853927612\n",
      "Epoch 13, batch 10 D Loss: 1.379866361618042, G Loss: 0.6954313516616821\n",
      "Epoch 13, batch 11 D Loss: 1.3860292434692383, G Loss: 0.6928211450576782\n",
      "Epoch 13, batch 12 D Loss: 1.3977594375610352, G Loss: 0.689727246761322\n",
      "Epoch 13, batch 13 D Loss: 1.3965388536453247, G Loss: 0.6948132514953613\n",
      "Epoch 13, batch 14 D Loss: 1.4051754474639893, G Loss: 0.695895254611969\n",
      "Epoch 13, batch 15 D Loss: 1.3803975582122803, G Loss: 0.7062422037124634\n",
      "Epoch 13, batch 16 D Loss: 1.4020063877105713, G Loss: 0.6921999454498291\n",
      "Epoch 13, batch 17 D Loss: 1.3896089792251587, G Loss: 0.6922312378883362\n",
      "Epoch 13, batch 18 D Loss: 1.3816826343536377, G Loss: 0.6918702721595764\n",
      "Epoch 13, batch 19 D Loss: 1.3790875673294067, G Loss: 0.7031610608100891\n",
      "Epoch 13, batch 20 D Loss: 1.3910386562347412, G Loss: 0.6971408128738403\n",
      "Epoch 13, batch 21 D Loss: 1.3823280334472656, G Loss: 0.6963098645210266\n",
      "Epoch 13, batch 22 D Loss: 1.3854737281799316, G Loss: 0.6975573897361755\n",
      "Epoch 13, batch 23 D Loss: 1.3899421691894531, G Loss: 0.7044899463653564\n",
      "Epoch 13, batch 24 D Loss: 1.3909422159194946, G Loss: 0.6987389922142029\n",
      "Epoch 13, batch 25 D Loss: 1.4090993404388428, G Loss: 0.6862095594406128\n",
      "Epoch 13, batch 26 D Loss: 1.3967199325561523, G Loss: 0.69267737865448\n",
      "Epoch 13, batch 27 D Loss: 1.3823747634887695, G Loss: 0.7007231116294861\n",
      "Epoch 13, batch 28 D Loss: 1.3883634805679321, G Loss: 0.688882052898407\n",
      "Epoch 13, batch 29 D Loss: 1.3737950325012207, G Loss: 0.6998295783996582\n",
      "Epoch 13, batch 30 D Loss: 1.3937121629714966, G Loss: 0.6840402483940125\n",
      "Epoch 13, batch 31 D Loss: 1.3906190395355225, G Loss: 0.6861891150474548\n",
      "Epoch 13, batch 32 D Loss: 1.395707130432129, G Loss: 0.6944054961204529\n",
      "Epoch 13, batch 33 D Loss: 1.3833427429199219, G Loss: 0.6956896781921387\n",
      "Epoch 13, batch 34 D Loss: 1.396543025970459, G Loss: 0.6923956274986267\n",
      "Epoch 13, batch 35 D Loss: 1.3900063037872314, G Loss: 0.6921144127845764\n",
      "Epoch 13, batch 36 D Loss: 1.3850305080413818, G Loss: 0.6956042647361755\n",
      "Epoch 13, batch 37 D Loss: 1.381582498550415, G Loss: 0.7052108645439148\n",
      "Epoch 13, batch 38 D Loss: 1.3913531303405762, G Loss: 0.6956634521484375\n",
      "Epoch 13, batch 39 D Loss: 1.38055419921875, G Loss: 0.7042802572250366\n",
      "Epoch 13, batch 40 D Loss: 1.39937424659729, G Loss: 0.6901472210884094\n",
      "Epoch 13, batch 41 D Loss: 1.3842408657073975, G Loss: 0.6961095929145813\n",
      "Epoch 13, batch 42 D Loss: 1.403091311454773, G Loss: 0.6853227019309998\n",
      "Epoch 13, batch 43 D Loss: 1.3810251951217651, G Loss: 0.6982983946800232\n",
      "Epoch 13, batch 44 D Loss: 1.3927301168441772, G Loss: 0.6846904754638672\n",
      "Epoch 13, batch 45 D Loss: 1.3912267684936523, G Loss: 0.6860021352767944\n",
      "Epoch 13, batch 46 D Loss: 1.3874289989471436, G Loss: 0.6958361268043518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, batch 47 D Loss: 1.3860753774642944, G Loss: 0.6890275478363037\n",
      "Epoch 13, batch 48 D Loss: 1.3814970254898071, G Loss: 0.6944282650947571\n",
      "Epoch 13, batch 49 D Loss: 1.3955833911895752, G Loss: 0.6914305090904236\n",
      "Epoch 13, batch 50 D Loss: 1.3837311267852783, G Loss: 0.6914365887641907\n",
      "Epoch 13, batch 51 D Loss: 1.4007775783538818, G Loss: 0.6911914348602295\n",
      "Epoch 13, batch 52 D Loss: 1.3827223777770996, G Loss: 0.6910886168479919\n",
      "Epoch 13, batch 53 D Loss: 1.375884771347046, G Loss: 0.6933788061141968\n",
      "Epoch 13, batch 54 D Loss: 1.3800925016403198, G Loss: 0.6842111349105835\n",
      "Epoch 13, batch 55 D Loss: 1.397340178489685, G Loss: 0.6825027465820312\n",
      "Epoch 13, batch 56 D Loss: 1.381695032119751, G Loss: 0.6853163838386536\n",
      "Epoch 13, batch 57 D Loss: 1.3972985744476318, G Loss: 0.6752703189849854\n",
      "Epoch 13, batch 58 D Loss: 1.3882067203521729, G Loss: 0.6801426410675049\n",
      "Epoch 13, batch 59 D Loss: 1.3966138362884521, G Loss: 0.6731588840484619\n",
      "Epoch 13, batch 60 D Loss: 1.3784401416778564, G Loss: 0.6767322421073914\n",
      "Epoch 13, batch 61 D Loss: 1.3750550746917725, G Loss: 0.6826087832450867\n",
      "Epoch 13, batch 62 D Loss: 1.3994935750961304, G Loss: 0.666231095790863\n",
      "Epoch 13, batch 63 D Loss: 1.394625186920166, G Loss: 0.668586015701294\n",
      "Epoch 13, batch 64 D Loss: 1.3879773616790771, G Loss: 0.6678043603897095\n",
      "Epoch 13, batch 65 D Loss: 1.3747663497924805, G Loss: 0.6719254851341248\n",
      "Epoch 13, batch 66 D Loss: 1.3890644311904907, G Loss: 0.6679595708847046\n",
      "Epoch 13, batch 67 D Loss: 1.3960027694702148, G Loss: 0.6640952229499817\n",
      "Epoch 13, batch 68 D Loss: 1.3823027610778809, G Loss: 0.6736220717430115\n",
      "Epoch 13, batch 69 D Loss: 1.3863246440887451, G Loss: 0.6701787710189819\n",
      "Epoch 13, batch 70 D Loss: 1.3903937339782715, G Loss: 0.6751707196235657\n",
      "Epoch 13, batch 71 D Loss: 1.386486530303955, G Loss: 0.6743104457855225\n",
      "Epoch 13, batch 72 D Loss: 1.3881038427352905, G Loss: 0.6711483597755432\n",
      "Epoch 13, batch 73 D Loss: 1.3876450061798096, G Loss: 0.6739139556884766\n",
      "Epoch 13, batch 74 D Loss: 1.3847434520721436, G Loss: 0.6836802959442139\n",
      "Epoch 13, batch 75 D Loss: 1.3852918148040771, G Loss: 0.6794332265853882\n",
      "Epoch 13, batch 76 D Loss: 1.3810477256774902, G Loss: 0.6867715716362\n",
      "Epoch 13, batch 77 D Loss: 1.3843224048614502, G Loss: 0.6823079586029053\n",
      "Epoch 13, batch 78 D Loss: 1.391510248184204, G Loss: 0.6850512623786926\n",
      "Epoch 13, batch 79 D Loss: 1.3943191766738892, G Loss: 0.6862277984619141\n",
      "Epoch 13, batch 80 D Loss: 1.3820652961730957, G Loss: 0.6923397779464722\n",
      "Epoch 13, batch 81 D Loss: 1.3903183937072754, G Loss: 0.6938565969467163\n",
      "Epoch 13, batch 82 D Loss: 1.3860669136047363, G Loss: 0.6918424963951111\n",
      "Epoch 13, batch 83 D Loss: 1.3837089538574219, G Loss: 0.6940847635269165\n",
      "Epoch 13, batch 84 D Loss: 1.3772900104522705, G Loss: 0.6989408135414124\n",
      "Epoch 13, batch 85 D Loss: 1.398367166519165, G Loss: 0.6916233897209167\n",
      "Epoch 13, batch 86 D Loss: 1.3842878341674805, G Loss: 0.7006433010101318\n",
      "Epoch 13, batch 87 D Loss: 1.388540267944336, G Loss: 0.6995372772216797\n",
      "Epoch 13, batch 88 D Loss: 1.3825182914733887, G Loss: 0.7021652460098267\n",
      "Epoch 13, batch 89 D Loss: 1.382680058479309, G Loss: 0.7041933536529541\n",
      "Epoch 13, batch 90 D Loss: 1.4045671224594116, G Loss: 0.6946378350257874\n",
      "Epoch 13, batch 91 D Loss: 1.3764253854751587, G Loss: 0.7021086812019348\n",
      "Epoch 13, batch 92 D Loss: 1.3793909549713135, G Loss: 0.7059391736984253\n",
      "Epoch 13, batch 93 D Loss: 1.3938720226287842, G Loss: 0.696105420589447\n",
      "Epoch 13, batch 94 D Loss: 1.3856263160705566, G Loss: 0.6956891417503357\n",
      "Epoch 13, batch 95 D Loss: 1.3806779384613037, G Loss: 0.6976385712623596\n",
      "Epoch 13, batch 96 D Loss: 1.3839011192321777, G Loss: 0.7007086873054504\n",
      "Epoch 13, batch 97 D Loss: 1.385980486869812, G Loss: 0.7012072205543518\n",
      "Epoch 13, batch 98 D Loss: 1.385637879371643, G Loss: 0.695701539516449\n",
      "Epoch 13, batch 99 D Loss: 1.3870422840118408, G Loss: 0.6927170753479004\n",
      "Epoch 13, batch 100 D Loss: 1.3811122179031372, G Loss: 0.6927692890167236\n",
      "Epoch 13, batch 101 D Loss: 1.3794522285461426, G Loss: 0.6870898604393005\n",
      "Epoch 13, batch 102 D Loss: 1.394029140472412, G Loss: 0.6856513023376465\n",
      "Epoch 13, batch 103 D Loss: 1.3931686878204346, G Loss: 0.691956102848053\n",
      "Epoch 13, batch 104 D Loss: 1.4025349617004395, G Loss: 0.6805208325386047\n",
      "Epoch 13, batch 105 D Loss: 1.3844870328903198, G Loss: 0.6961166262626648\n",
      "Epoch 13, batch 106 D Loss: 1.3619539737701416, G Loss: 0.6888929605484009\n",
      "Epoch 13, batch 107 D Loss: 1.379119873046875, G Loss: 0.6942395567893982\n",
      "Epoch 13, batch 108 D Loss: 1.4041098356246948, G Loss: 0.6846688985824585\n",
      "Epoch 13, batch 109 D Loss: 1.3886284828186035, G Loss: 0.6844030618667603\n",
      "Epoch 13, batch 110 D Loss: 1.3847055435180664, G Loss: 0.6900091767311096\n",
      "Epoch 13, batch 111 D Loss: 1.3990991115570068, G Loss: 0.6805478930473328\n",
      "Epoch 13, batch 112 D Loss: 1.4025851488113403, G Loss: 0.6834375262260437\n",
      "Epoch 13, batch 113 D Loss: 1.4107177257537842, G Loss: 0.6752338409423828\n",
      "Epoch 13, batch 114 D Loss: 1.3811743259429932, G Loss: 0.6922895908355713\n",
      "Epoch 13, batch 115 D Loss: 1.3873562812805176, G Loss: 0.6953244805335999\n",
      "Epoch 13, batch 116 D Loss: 1.3997704982757568, G Loss: 0.6865088939666748\n",
      "Epoch 13, batch 117 D Loss: 1.3886871337890625, G Loss: 0.694786787033081\n",
      "Epoch 13, batch 118 D Loss: 1.389082670211792, G Loss: 0.688920259475708\n",
      "Epoch 13, batch 119 D Loss: 1.3809326887130737, G Loss: 0.6935577392578125\n",
      "Epoch 13, batch 120 D Loss: 1.3827950954437256, G Loss: 0.695569634437561\n",
      "Epoch 13, batch 121 D Loss: 1.3856968879699707, G Loss: 0.6908531785011292\n",
      "Epoch 13, batch 122 D Loss: 1.3992359638214111, G Loss: 0.685108482837677\n",
      "Epoch 13, batch 123 D Loss: 1.4015717506408691, G Loss: 0.695100724697113\n",
      "Epoch 13, batch 124 D Loss: 1.39604914188385, G Loss: 0.6857638359069824\n",
      "Epoch 13, batch 125 D Loss: 1.3940156698226929, G Loss: 0.6890051960945129\n",
      "Epoch 13, batch 126 D Loss: 1.3876662254333496, G Loss: 0.698721706867218\n",
      "Epoch 13, batch 127 D Loss: 1.39139723777771, G Loss: 0.6943688988685608\n",
      "Epoch 13, batch 128 D Loss: 1.409874439239502, G Loss: 0.6862378120422363\n",
      "Epoch 13, batch 129 D Loss: 1.3890663385391235, G Loss: 0.6997616291046143\n",
      "Epoch 13, batch 130 D Loss: 1.3929752111434937, G Loss: 0.6917985677719116\n",
      "Epoch 13, batch 131 D Loss: 1.3767521381378174, G Loss: 0.6961985230445862\n",
      "Epoch 13, batch 132 D Loss: 1.4145216941833496, G Loss: 0.6900883316993713\n",
      "Epoch 13, batch 133 D Loss: 1.3918795585632324, G Loss: 0.6895323991775513\n",
      "Epoch 13, batch 134 D Loss: 1.388706922531128, G Loss: 0.6921935081481934\n",
      "Epoch 13, batch 135 D Loss: 1.3902060985565186, G Loss: 0.6888559460639954\n",
      "Epoch 13, batch 136 D Loss: 1.3867143392562866, G Loss: 0.6896424889564514\n",
      "Epoch 13, batch 137 D Loss: 1.384727954864502, G Loss: 0.6986061930656433\n",
      "Epoch 13, batch 138 D Loss: 1.3965214490890503, G Loss: 0.6872894167900085\n",
      "Epoch 13, batch 139 D Loss: 1.3836077451705933, G Loss: 0.6929295063018799\n",
      "Epoch 13, batch 140 D Loss: 1.4069194793701172, G Loss: 0.6775311827659607\n",
      "Epoch 13, batch 141 D Loss: 1.4045617580413818, G Loss: 0.6778460144996643\n",
      "Epoch 13, batch 142 D Loss: 1.3920750617980957, G Loss: 0.6852512955665588\n",
      "Epoch 13, batch 143 D Loss: 1.3895821571350098, G Loss: 0.6871345639228821\n",
      "Epoch 13, batch 144 D Loss: 1.3981252908706665, G Loss: 0.6742761135101318\n",
      "Epoch 13, batch 145 D Loss: 1.401328682899475, G Loss: 0.682708740234375\n",
      "Epoch 13, batch 146 D Loss: 1.392949104309082, G Loss: 0.6923103332519531\n",
      "Epoch 13, batch 147 D Loss: 1.392547369003296, G Loss: 0.6946512460708618\n",
      "Epoch 13, batch 148 D Loss: 1.388777494430542, G Loss: 0.6967231631278992\n",
      "Epoch 13, batch 149 D Loss: 1.3828706741333008, G Loss: 0.6896849274635315\n",
      "Epoch 13, batch 150 D Loss: 1.398784875869751, G Loss: 0.6916747093200684\n",
      "Epoch 13, batch 151 D Loss: 1.3968806266784668, G Loss: 0.6955493092536926\n",
      "Epoch 13, batch 152 D Loss: 1.3941056728363037, G Loss: 0.6947751641273499\n",
      "Epoch 13, batch 153 D Loss: 1.398097038269043, G Loss: 0.6934123039245605\n",
      "Epoch 13, batch 154 D Loss: 1.3912122249603271, G Loss: 0.7062334418296814\n",
      "Epoch 13, batch 155 D Loss: 1.3923609256744385, G Loss: 0.6994986534118652\n",
      "Epoch 13, batch 156 D Loss: 1.3824223279953003, G Loss: 0.7098697423934937\n",
      "Epoch 13, batch 157 D Loss: 1.383049488067627, G Loss: 0.7072729468345642\n",
      "Epoch 13, batch 158 D Loss: 1.3821852207183838, G Loss: 0.707584023475647\n",
      "Epoch 13, batch 159 D Loss: 1.3939213752746582, G Loss: 0.6999859809875488\n",
      "Epoch 13, batch 160 D Loss: 1.3970143795013428, G Loss: 0.7074776291847229\n",
      "Epoch 13, batch 161 D Loss: 1.3927698135375977, G Loss: 0.7048838138580322\n",
      "Epoch 13, batch 162 D Loss: 1.3843004703521729, G Loss: 0.7035019397735596\n",
      "Epoch 13, batch 163 D Loss: 1.3891650438308716, G Loss: 0.7079411149024963\n",
      "Epoch 13, batch 164 D Loss: 1.4017305374145508, G Loss: 0.7058062553405762\n",
      "Epoch 13, batch 165 D Loss: 1.382920265197754, G Loss: 0.710612416267395\n",
      "Epoch 13, batch 166 D Loss: 1.3798081874847412, G Loss: 0.7168382406234741\n",
      "Epoch 13, batch 167 D Loss: 1.3973290920257568, G Loss: 0.7045408487319946\n",
      "Epoch 13, batch 168 D Loss: 1.383716344833374, G Loss: 0.7114799618721008\n",
      "Epoch 13, batch 169 D Loss: 1.383608102798462, G Loss: 0.7097675800323486\n",
      "Epoch 13, batch 170 D Loss: 1.3955357074737549, G Loss: 0.7024475336074829\n",
      "Epoch 13, batch 171 D Loss: 1.4000768661499023, G Loss: 0.6994964480400085\n",
      "Epoch 13, batch 172 D Loss: 1.388153314590454, G Loss: 0.7065330743789673\n",
      "Epoch 13, batch 173 D Loss: 1.392507791519165, G Loss: 0.7051635980606079\n",
      "Epoch 13, batch 174 D Loss: 1.380372166633606, G Loss: 0.7158259749412537\n",
      "Epoch 13, batch 175 D Loss: 1.3950366973876953, G Loss: 0.6966056823730469\n",
      "Epoch 13, batch 176 D Loss: 1.3935043811798096, G Loss: 0.7055342197418213\n",
      "Epoch 13, batch 177 D Loss: 1.3759280443191528, G Loss: 0.7162069082260132\n",
      "Epoch 13, batch 178 D Loss: 1.3956503868103027, G Loss: 0.703525722026825\n",
      "Epoch 13, batch 179 D Loss: 1.386871576309204, G Loss: 0.7074295282363892\n",
      "Epoch 13, batch 180 D Loss: 1.388601541519165, G Loss: 0.7038215398788452\n",
      "Epoch 13, batch 181 D Loss: 1.3935542106628418, G Loss: 0.7003405690193176\n",
      "Epoch 13, batch 182 D Loss: 1.3864688873291016, G Loss: 0.7073078751564026\n",
      "Epoch 13, batch 183 D Loss: 1.3963494300842285, G Loss: 0.6994011402130127\n",
      "Epoch 13, batch 184 D Loss: 1.3958606719970703, G Loss: 0.7019245624542236\n",
      "Epoch 13, batch 185 D Loss: 1.389160394668579, G Loss: 0.7072942852973938\n",
      "Epoch 13, batch 186 D Loss: 1.3791807889938354, G Loss: 0.7092138528823853\n",
      "Epoch 13, batch 187 D Loss: 1.385500192642212, G Loss: 0.7060569524765015\n",
      "Epoch 13, batch 188 D Loss: 1.3934245109558105, G Loss: 0.7007319927215576\n",
      "Epoch 13, batch 189 D Loss: 1.3790147304534912, G Loss: 0.707747220993042\n",
      "Epoch 13, batch 190 D Loss: 1.3927929401397705, G Loss: 0.7013121843338013\n",
      "Epoch 13, batch 191 D Loss: 1.3907676935195923, G Loss: 0.69928377866745\n",
      "Epoch 13, batch 192 D Loss: 1.3810453414916992, G Loss: 0.7034403085708618\n",
      "Epoch 13, batch 193 D Loss: 1.3877222537994385, G Loss: 0.7048642635345459\n",
      "Epoch 13, batch 194 D Loss: 1.3895375728607178, G Loss: 0.7015070915222168\n",
      "Epoch 13, batch 195 D Loss: 1.3879196643829346, G Loss: 0.7038469910621643\n",
      "Epoch 13, batch 196 D Loss: 1.3836371898651123, G Loss: 0.7082637548446655\n",
      "Epoch 13, batch 197 D Loss: 1.396202564239502, G Loss: 0.6981925368309021\n",
      "Epoch 13, batch 198 D Loss: 1.399775743484497, G Loss: 0.6929256319999695\n",
      "Epoch 13, batch 199 D Loss: 1.3910616636276245, G Loss: 0.698995053768158\n",
      "Epoch 13, batch 200 D Loss: 1.3937056064605713, G Loss: 0.6996881365776062\n",
      "Epoch 14, batch 1 D Loss: 1.3939539194107056, G Loss: 0.6970012187957764\n",
      "Epoch 14, batch 2 D Loss: 1.3848851919174194, G Loss: 0.69987952709198\n",
      "Epoch 14, batch 3 D Loss: 1.3927799463272095, G Loss: 0.6938852667808533\n",
      "Epoch 14, batch 4 D Loss: 1.394836664199829, G Loss: 0.6920786499977112\n",
      "Epoch 14, batch 5 D Loss: 1.3921481370925903, G Loss: 0.6939248442649841\n",
      "Epoch 14, batch 6 D Loss: 1.3888874053955078, G Loss: 0.6948692798614502\n",
      "Epoch 14, batch 7 D Loss: 1.3841540813446045, G Loss: 0.6978343725204468\n",
      "Epoch 14, batch 8 D Loss: 1.3817214965820312, G Loss: 0.6991813778877258\n",
      "Epoch 14, batch 9 D Loss: 1.3900537490844727, G Loss: 0.6954964995384216\n",
      "Epoch 14, batch 10 D Loss: 1.3870044946670532, G Loss: 0.6909308433532715\n",
      "Epoch 14, batch 11 D Loss: 1.3921716213226318, G Loss: 0.6874110698699951\n",
      "Epoch 14, batch 12 D Loss: 1.387606143951416, G Loss: 0.690883994102478\n",
      "Epoch 14, batch 13 D Loss: 1.3871815204620361, G Loss: 0.6940712928771973\n",
      "Epoch 14, batch 14 D Loss: 1.3909721374511719, G Loss: 0.6873046159744263\n",
      "Epoch 14, batch 15 D Loss: 1.3848689794540405, G Loss: 0.6908342242240906\n",
      "Epoch 14, batch 16 D Loss: 1.3865432739257812, G Loss: 0.6879266500473022\n",
      "Epoch 14, batch 17 D Loss: 1.377870798110962, G Loss: 0.6939346790313721\n",
      "Epoch 14, batch 18 D Loss: 1.3884241580963135, G Loss: 0.6876991391181946\n",
      "Epoch 14, batch 19 D Loss: 1.3751912117004395, G Loss: 0.7021582126617432\n",
      "Epoch 14, batch 20 D Loss: 1.3973257541656494, G Loss: 0.6843879222869873\n",
      "Epoch 14, batch 21 D Loss: 1.3932044506072998, G Loss: 0.6916062831878662\n",
      "Epoch 14, batch 22 D Loss: 1.3861892223358154, G Loss: 0.6948257684707642\n",
      "Epoch 14, batch 23 D Loss: 1.3969693183898926, G Loss: 0.6888924241065979\n",
      "Epoch 14, batch 24 D Loss: 1.3995976448059082, G Loss: 0.685347318649292\n",
      "Epoch 14, batch 25 D Loss: 1.3845621347427368, G Loss: 0.6960367560386658\n",
      "Epoch 14, batch 26 D Loss: 1.38139009475708, G Loss: 0.6956178545951843\n",
      "Epoch 14, batch 27 D Loss: 1.391620397567749, G Loss: 0.6896153092384338\n",
      "Epoch 14, batch 28 D Loss: 1.388627052307129, G Loss: 0.6901958584785461\n",
      "Epoch 14, batch 29 D Loss: 1.3871740102767944, G Loss: 0.6933445930480957\n",
      "Epoch 14, batch 30 D Loss: 1.3917299509048462, G Loss: 0.6961680054664612\n",
      "Epoch 14, batch 31 D Loss: 1.3869831562042236, G Loss: 0.6959859728813171\n",
      "Epoch 14, batch 32 D Loss: 1.3944602012634277, G Loss: 0.6938797235488892\n",
      "Epoch 14, batch 33 D Loss: 1.388340950012207, G Loss: 0.695991575717926\n",
      "Epoch 14, batch 34 D Loss: 1.3851666450500488, G Loss: 0.6989591717720032\n",
      "Epoch 14, batch 35 D Loss: 1.382234811782837, G Loss: 0.6969922780990601\n",
      "Epoch 14, batch 36 D Loss: 1.386831283569336, G Loss: 0.7003158330917358\n",
      "Epoch 14, batch 37 D Loss: 1.3864445686340332, G Loss: 0.7010535597801208\n",
      "Epoch 14, batch 38 D Loss: 1.3804209232330322, G Loss: 0.7037443518638611\n",
      "Epoch 14, batch 39 D Loss: 1.3879982233047485, G Loss: 0.6989966630935669\n",
      "Epoch 14, batch 40 D Loss: 1.3864010572433472, G Loss: 0.7001702189445496\n",
      "Epoch 14, batch 41 D Loss: 1.3794969320297241, G Loss: 0.7019262909889221\n",
      "Epoch 14, batch 42 D Loss: 1.3852181434631348, G Loss: 0.7006689310073853\n",
      "Epoch 14, batch 43 D Loss: 1.3851978778839111, G Loss: 0.7023606896400452\n",
      "Epoch 14, batch 44 D Loss: 1.3861021995544434, G Loss: 0.7011367082595825\n",
      "Epoch 14, batch 45 D Loss: 1.3860764503479004, G Loss: 0.6976637244224548\n",
      "Epoch 14, batch 46 D Loss: 1.3869723081588745, G Loss: 0.7006414532661438\n",
      "Epoch 14, batch 47 D Loss: 1.3801372051239014, G Loss: 0.705650269985199\n",
      "Epoch 14, batch 48 D Loss: 1.3987904787063599, G Loss: 0.6942437887191772\n",
      "Epoch 14, batch 49 D Loss: 1.3932037353515625, G Loss: 0.6964324712753296\n",
      "Epoch 14, batch 50 D Loss: 1.3850358724594116, G Loss: 0.7042607665061951\n",
      "Epoch 14, batch 51 D Loss: 1.3865158557891846, G Loss: 0.7048711180686951\n",
      "Epoch 14, batch 52 D Loss: 1.390498399734497, G Loss: 0.6988875865936279\n",
      "Epoch 14, batch 53 D Loss: 1.3847424983978271, G Loss: 0.6972090005874634\n",
      "Epoch 14, batch 54 D Loss: 1.389160394668579, G Loss: 0.6914919018745422\n",
      "Epoch 14, batch 55 D Loss: 1.3778586387634277, G Loss: 0.6938887238502502\n",
      "Epoch 14, batch 56 D Loss: 1.3717961311340332, G Loss: 0.7030426859855652\n",
      "Epoch 14, batch 57 D Loss: 1.3890748023986816, G Loss: 0.6949248313903809\n",
      "Epoch 14, batch 58 D Loss: 1.391627311706543, G Loss: 0.696777880191803\n",
      "Epoch 14, batch 59 D Loss: 1.3807792663574219, G Loss: 0.6910009980201721\n",
      "Epoch 14, batch 60 D Loss: 1.3906450271606445, G Loss: 0.6877721548080444\n",
      "Epoch 14, batch 61 D Loss: 1.387008786201477, G Loss: 0.6931214332580566\n",
      "Epoch 14, batch 62 D Loss: 1.3886144161224365, G Loss: 0.695738673210144\n",
      "Epoch 14, batch 63 D Loss: 1.3830561637878418, G Loss: 0.6865696907043457\n",
      "Epoch 14, batch 64 D Loss: 1.38456130027771, G Loss: 0.6920793056488037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, batch 65 D Loss: 1.3899147510528564, G Loss: 0.690490186214447\n",
      "Epoch 14, batch 66 D Loss: 1.3925933837890625, G Loss: 0.684410572052002\n",
      "Epoch 14, batch 67 D Loss: 1.3954005241394043, G Loss: 0.6893147230148315\n",
      "Epoch 14, batch 68 D Loss: 1.3823819160461426, G Loss: 0.6866522431373596\n",
      "Epoch 14, batch 69 D Loss: 1.3861514329910278, G Loss: 0.6916319131851196\n",
      "Epoch 14, batch 70 D Loss: 1.3852756023406982, G Loss: 0.693418562412262\n",
      "Epoch 14, batch 71 D Loss: 1.3841971158981323, G Loss: 0.6981274485588074\n",
      "Epoch 14, batch 72 D Loss: 1.3898476362228394, G Loss: 0.6887770891189575\n",
      "Epoch 14, batch 73 D Loss: 1.391028881072998, G Loss: 0.6903858184814453\n",
      "Epoch 14, batch 74 D Loss: 1.383850336074829, G Loss: 0.6892175078392029\n",
      "Epoch 14, batch 75 D Loss: 1.3859580755233765, G Loss: 0.6975947618484497\n",
      "Epoch 14, batch 76 D Loss: 1.3772398233413696, G Loss: 0.6963592767715454\n",
      "Epoch 14, batch 77 D Loss: 1.3824543952941895, G Loss: 0.6935328841209412\n",
      "Epoch 14, batch 78 D Loss: 1.3808705806732178, G Loss: 0.6999591588973999\n",
      "Epoch 14, batch 79 D Loss: 1.3808339834213257, G Loss: 0.6989789009094238\n",
      "Epoch 14, batch 80 D Loss: 1.3852618932724, G Loss: 0.693564772605896\n",
      "Epoch 14, batch 81 D Loss: 1.3853213787078857, G Loss: 0.697434663772583\n",
      "Epoch 14, batch 82 D Loss: 1.3998488187789917, G Loss: 0.6869250535964966\n",
      "Epoch 14, batch 83 D Loss: 1.3768813610076904, G Loss: 0.6935279965400696\n",
      "Epoch 14, batch 84 D Loss: 1.3886945247650146, G Loss: 0.6924643516540527\n",
      "Epoch 14, batch 85 D Loss: 1.3974218368530273, G Loss: 0.6969675421714783\n",
      "Epoch 14, batch 86 D Loss: 1.3823400735855103, G Loss: 0.6933951377868652\n",
      "Epoch 14, batch 87 D Loss: 1.382557988166809, G Loss: 0.6927825212478638\n",
      "Epoch 14, batch 88 D Loss: 1.389413595199585, G Loss: 0.6885949969291687\n",
      "Epoch 14, batch 89 D Loss: 1.4005646705627441, G Loss: 0.6797346472740173\n",
      "Epoch 14, batch 90 D Loss: 1.3973486423492432, G Loss: 0.6775676012039185\n",
      "Epoch 14, batch 91 D Loss: 1.3868365287780762, G Loss: 0.6862277984619141\n",
      "Epoch 14, batch 92 D Loss: 1.3786306381225586, G Loss: 0.6917398571968079\n",
      "Epoch 14, batch 93 D Loss: 1.3874084949493408, G Loss: 0.6825487613677979\n",
      "Epoch 14, batch 94 D Loss: 1.374056339263916, G Loss: 0.6912022233009338\n",
      "Epoch 14, batch 95 D Loss: 1.3754701614379883, G Loss: 0.6901462078094482\n",
      "Epoch 14, batch 96 D Loss: 1.3842647075653076, G Loss: 0.6830457448959351\n",
      "Epoch 14, batch 97 D Loss: 1.384427547454834, G Loss: 0.6861435174942017\n",
      "Epoch 14, batch 98 D Loss: 1.3925127983093262, G Loss: 0.6857843995094299\n",
      "Epoch 14, batch 99 D Loss: 1.3968265056610107, G Loss: 0.6788998246192932\n",
      "Epoch 14, batch 100 D Loss: 1.3950881958007812, G Loss: 0.6869798302650452\n",
      "Epoch 14, batch 101 D Loss: 1.390299677848816, G Loss: 0.6769468784332275\n",
      "Epoch 14, batch 102 D Loss: 1.3856679201126099, G Loss: 0.68630051612854\n",
      "Epoch 14, batch 103 D Loss: 1.3761560916900635, G Loss: 0.6842416524887085\n",
      "Epoch 14, batch 104 D Loss: 1.397857904434204, G Loss: 0.6702396273612976\n",
      "Epoch 14, batch 105 D Loss: 1.3738770484924316, G Loss: 0.6898820400238037\n",
      "Epoch 14, batch 106 D Loss: 1.3973362445831299, G Loss: 0.6776992082595825\n",
      "Epoch 14, batch 107 D Loss: 1.3980846405029297, G Loss: 0.6733515858650208\n",
      "Epoch 14, batch 108 D Loss: 1.3747234344482422, G Loss: 0.6835124492645264\n",
      "Epoch 14, batch 109 D Loss: 1.393993616104126, G Loss: 0.6762995719909668\n",
      "Epoch 14, batch 110 D Loss: 1.3824808597564697, G Loss: 0.6826815605163574\n",
      "Epoch 14, batch 111 D Loss: 1.3979549407958984, G Loss: 0.6830766201019287\n",
      "Epoch 14, batch 112 D Loss: 1.3845698833465576, G Loss: 0.6873291730880737\n",
      "Epoch 14, batch 113 D Loss: 1.3734699487686157, G Loss: 0.688769519329071\n",
      "Epoch 14, batch 114 D Loss: 1.3845512866973877, G Loss: 0.6817986965179443\n",
      "Epoch 14, batch 115 D Loss: 1.3940112590789795, G Loss: 0.6805885434150696\n",
      "Epoch 14, batch 116 D Loss: 1.395061731338501, G Loss: 0.680260419845581\n",
      "Epoch 14, batch 117 D Loss: 1.390369176864624, G Loss: 0.6804277300834656\n",
      "Epoch 14, batch 118 D Loss: 1.3992456197738647, G Loss: 0.6747888922691345\n",
      "Epoch 14, batch 119 D Loss: 1.386118769645691, G Loss: 0.6846007704734802\n",
      "Epoch 14, batch 120 D Loss: 1.3870949745178223, G Loss: 0.6884094476699829\n",
      "Epoch 14, batch 121 D Loss: 1.3905725479125977, G Loss: 0.6870934963226318\n",
      "Epoch 14, batch 122 D Loss: 1.3903589248657227, G Loss: 0.6909334063529968\n",
      "Epoch 14, batch 123 D Loss: 1.3862336874008179, G Loss: 0.6937183141708374\n",
      "Epoch 14, batch 124 D Loss: 1.3790535926818848, G Loss: 0.6962968707084656\n",
      "Epoch 14, batch 125 D Loss: 1.3892409801483154, G Loss: 0.6916226744651794\n",
      "Epoch 14, batch 126 D Loss: 1.3809632062911987, G Loss: 0.6940773129463196\n",
      "Epoch 14, batch 127 D Loss: 1.3953795433044434, G Loss: 0.687410295009613\n",
      "Epoch 14, batch 128 D Loss: 1.3987908363342285, G Loss: 0.6903032064437866\n",
      "Epoch 14, batch 129 D Loss: 1.386152982711792, G Loss: 0.6945919990539551\n",
      "Epoch 14, batch 130 D Loss: 1.3918440341949463, G Loss: 0.6955984234809875\n",
      "Epoch 14, batch 131 D Loss: 1.3908147811889648, G Loss: 0.6904733180999756\n",
      "Epoch 14, batch 132 D Loss: 1.3821978569030762, G Loss: 0.6956535577774048\n",
      "Epoch 14, batch 133 D Loss: 1.3899405002593994, G Loss: 0.6944830417633057\n",
      "Epoch 14, batch 134 D Loss: 1.3862941265106201, G Loss: 0.6933900713920593\n",
      "Epoch 14, batch 135 D Loss: 1.387007474899292, G Loss: 0.6957656145095825\n",
      "Epoch 14, batch 136 D Loss: 1.3888566493988037, G Loss: 0.6928197741508484\n",
      "Epoch 14, batch 137 D Loss: 1.3832652568817139, G Loss: 0.6986967325210571\n",
      "Epoch 14, batch 138 D Loss: 1.3826876878738403, G Loss: 0.6942236423492432\n",
      "Epoch 14, batch 139 D Loss: 1.38250732421875, G Loss: 0.6959779262542725\n",
      "Epoch 14, batch 140 D Loss: 1.38547682762146, G Loss: 0.6953666806221008\n",
      "Epoch 14, batch 141 D Loss: 1.3873802423477173, G Loss: 0.6972474455833435\n",
      "Epoch 14, batch 142 D Loss: 1.3871467113494873, G Loss: 0.6933075189590454\n",
      "Epoch 14, batch 143 D Loss: 1.3847702741622925, G Loss: 0.6942038536071777\n",
      "Epoch 14, batch 144 D Loss: 1.386283040046692, G Loss: 0.6949034333229065\n",
      "Epoch 14, batch 145 D Loss: 1.3921213150024414, G Loss: 0.689863920211792\n",
      "Epoch 14, batch 146 D Loss: 1.3785429000854492, G Loss: 0.701821506023407\n",
      "Epoch 14, batch 147 D Loss: 1.3875806331634521, G Loss: 0.6960422992706299\n",
      "Epoch 14, batch 148 D Loss: 1.3819782733917236, G Loss: 0.695354163646698\n",
      "Epoch 14, batch 149 D Loss: 1.3901387453079224, G Loss: 0.694501519203186\n",
      "Epoch 14, batch 150 D Loss: 1.384004831314087, G Loss: 0.6968734264373779\n",
      "Epoch 14, batch 151 D Loss: 1.3890990018844604, G Loss: 0.6924822926521301\n",
      "Epoch 14, batch 152 D Loss: 1.3882522583007812, G Loss: 0.694296658039093\n",
      "Epoch 14, batch 153 D Loss: 1.3867557048797607, G Loss: 0.6971074938774109\n",
      "Epoch 14, batch 154 D Loss: 1.3805792331695557, G Loss: 0.6951292157173157\n",
      "Epoch 14, batch 155 D Loss: 1.3827476501464844, G Loss: 0.6923201084136963\n",
      "Epoch 14, batch 156 D Loss: 1.3902314901351929, G Loss: 0.695106029510498\n",
      "Epoch 14, batch 157 D Loss: 1.3884135484695435, G Loss: 0.6878342628479004\n",
      "Epoch 14, batch 158 D Loss: 1.39015793800354, G Loss: 0.6914266347885132\n",
      "Epoch 14, batch 159 D Loss: 1.3743687868118286, G Loss: 0.7067329287528992\n",
      "Epoch 14, batch 160 D Loss: 1.4006030559539795, G Loss: 0.6858083605766296\n",
      "Epoch 14, batch 161 D Loss: 1.383313775062561, G Loss: 0.6896185874938965\n",
      "Epoch 14, batch 162 D Loss: 1.3750882148742676, G Loss: 0.6996658444404602\n",
      "Epoch 14, batch 163 D Loss: 1.3782901763916016, G Loss: 0.6964165568351746\n",
      "Epoch 14, batch 164 D Loss: 1.3883957862854004, G Loss: 0.6921778917312622\n",
      "Epoch 14, batch 165 D Loss: 1.3828413486480713, G Loss: 0.6908776760101318\n",
      "Epoch 14, batch 166 D Loss: 1.390195369720459, G Loss: 0.6894955635070801\n",
      "Epoch 14, batch 167 D Loss: 1.380523681640625, G Loss: 0.6918216943740845\n",
      "Epoch 14, batch 168 D Loss: 1.3876631259918213, G Loss: 0.6894336938858032\n",
      "Epoch 14, batch 169 D Loss: 1.3893277645111084, G Loss: 0.6934424638748169\n",
      "Epoch 14, batch 170 D Loss: 1.3805286884307861, G Loss: 0.6925207376480103\n",
      "Epoch 14, batch 171 D Loss: 1.3912603855133057, G Loss: 0.6902868747711182\n",
      "Epoch 14, batch 172 D Loss: 1.3896510601043701, G Loss: 0.6838564276695251\n",
      "Epoch 14, batch 173 D Loss: 1.382901668548584, G Loss: 0.6901549696922302\n",
      "Epoch 14, batch 174 D Loss: 1.3840227127075195, G Loss: 0.692356288433075\n",
      "Epoch 14, batch 175 D Loss: 1.3727457523345947, G Loss: 0.6971634030342102\n",
      "Epoch 14, batch 176 D Loss: 1.3813295364379883, G Loss: 0.692444384098053\n",
      "Epoch 14, batch 177 D Loss: 1.3870010375976562, G Loss: 0.6858014464378357\n",
      "Epoch 14, batch 178 D Loss: 1.3827868700027466, G Loss: 0.6883175373077393\n",
      "Epoch 14, batch 179 D Loss: 1.3798964023590088, G Loss: 0.6965934038162231\n",
      "Epoch 14, batch 180 D Loss: 1.3862309455871582, G Loss: 0.6908059120178223\n",
      "Epoch 14, batch 181 D Loss: 1.3836567401885986, G Loss: 0.6973369121551514\n",
      "Epoch 14, batch 182 D Loss: 1.3823161125183105, G Loss: 0.6978429555892944\n",
      "Epoch 14, batch 183 D Loss: 1.3775790929794312, G Loss: 0.7035786509513855\n",
      "Epoch 14, batch 184 D Loss: 1.3789849281311035, G Loss: 0.7031948566436768\n",
      "Epoch 14, batch 185 D Loss: 1.3854777812957764, G Loss: 0.7064786553382874\n",
      "Epoch 14, batch 186 D Loss: 1.3976421356201172, G Loss: 0.693558931350708\n",
      "Epoch 14, batch 187 D Loss: 1.3726491928100586, G Loss: 0.70414137840271\n",
      "Epoch 14, batch 188 D Loss: 1.3859302997589111, G Loss: 0.7057701349258423\n",
      "Epoch 14, batch 189 D Loss: 1.3968534469604492, G Loss: 0.6953608989715576\n",
      "Epoch 14, batch 190 D Loss: 1.3755676746368408, G Loss: 0.7123715281486511\n",
      "Epoch 14, batch 191 D Loss: 1.3893983364105225, G Loss: 0.7041253447532654\n",
      "Epoch 14, batch 192 D Loss: 1.386720061302185, G Loss: 0.710795521736145\n",
      "Epoch 14, batch 193 D Loss: 1.3709309101104736, G Loss: 0.7204983234405518\n",
      "Epoch 14, batch 194 D Loss: 1.389009952545166, G Loss: 0.7049427628517151\n",
      "Epoch 14, batch 195 D Loss: 1.3725066184997559, G Loss: 0.7189257144927979\n",
      "Epoch 14, batch 196 D Loss: 1.375999927520752, G Loss: 0.7025463581085205\n",
      "Epoch 14, batch 197 D Loss: 1.3755794763565063, G Loss: 0.7079682946205139\n",
      "Epoch 14, batch 198 D Loss: 1.3857784271240234, G Loss: 0.7020435929298401\n",
      "Epoch 14, batch 199 D Loss: 1.3898885250091553, G Loss: 0.7017642855644226\n",
      "Epoch 14, batch 200 D Loss: 1.389697790145874, G Loss: 0.704721212387085\n",
      "Epoch 15, batch 1 D Loss: 1.3909337520599365, G Loss: 0.691978394985199\n",
      "Epoch 15, batch 2 D Loss: 1.3733720779418945, G Loss: 0.7113626003265381\n",
      "Epoch 15, batch 3 D Loss: 1.391261100769043, G Loss: 0.7113863229751587\n",
      "Epoch 15, batch 4 D Loss: 1.3923008441925049, G Loss: 0.7105241417884827\n",
      "Epoch 15, batch 5 D Loss: 1.3705518245697021, G Loss: 0.7184266448020935\n",
      "Epoch 15, batch 6 D Loss: 1.3834271430969238, G Loss: 0.7029045224189758\n",
      "Epoch 15, batch 7 D Loss: 1.3809545040130615, G Loss: 0.7192346453666687\n",
      "Epoch 15, batch 8 D Loss: 1.3917763233184814, G Loss: 0.708658754825592\n",
      "Epoch 15, batch 9 D Loss: 1.3886799812316895, G Loss: 0.6949414610862732\n",
      "Epoch 15, batch 10 D Loss: 1.3790626525878906, G Loss: 0.7099745869636536\n",
      "Epoch 15, batch 11 D Loss: 1.3645997047424316, G Loss: 0.7128679752349854\n",
      "Epoch 15, batch 12 D Loss: 1.400554895401001, G Loss: 0.7001432180404663\n",
      "Epoch 15, batch 13 D Loss: 1.3652958869934082, G Loss: 0.7126275897026062\n",
      "Epoch 15, batch 14 D Loss: 1.394980549812317, G Loss: 0.6955974102020264\n",
      "Epoch 15, batch 15 D Loss: 1.393970012664795, G Loss: 0.6966182589530945\n",
      "Epoch 15, batch 16 D Loss: 1.393791913986206, G Loss: 0.7010381817817688\n",
      "Epoch 15, batch 17 D Loss: 1.3910781145095825, G Loss: 0.6910092234611511\n",
      "Epoch 15, batch 18 D Loss: 1.3993397951126099, G Loss: 0.6922880411148071\n",
      "Epoch 15, batch 19 D Loss: 1.386096477508545, G Loss: 0.6935550570487976\n",
      "Epoch 15, batch 20 D Loss: 1.3881573677062988, G Loss: 0.6949388384819031\n",
      "Epoch 15, batch 21 D Loss: 1.3882818222045898, G Loss: 0.6951782703399658\n",
      "Epoch 15, batch 22 D Loss: 1.3793108463287354, G Loss: 0.7015277743339539\n",
      "Epoch 15, batch 23 D Loss: 1.3928282260894775, G Loss: 0.6884642243385315\n",
      "Epoch 15, batch 24 D Loss: 1.3924899101257324, G Loss: 0.6945521831512451\n",
      "Epoch 15, batch 25 D Loss: 1.3896223306655884, G Loss: 0.7001711130142212\n",
      "Epoch 15, batch 26 D Loss: 1.3958228826522827, G Loss: 0.6902422904968262\n",
      "Epoch 15, batch 27 D Loss: 1.387312412261963, G Loss: 0.6955838203430176\n",
      "Epoch 15, batch 28 D Loss: 1.385228157043457, G Loss: 0.6917369365692139\n",
      "Epoch 15, batch 29 D Loss: 1.3987081050872803, G Loss: 0.6917939782142639\n",
      "Epoch 15, batch 30 D Loss: 1.3828175067901611, G Loss: 0.6958666443824768\n",
      "Epoch 15, batch 31 D Loss: 1.3873226642608643, G Loss: 0.6904489398002625\n",
      "Epoch 15, batch 32 D Loss: 1.3857333660125732, G Loss: 0.6910252571105957\n",
      "Epoch 15, batch 33 D Loss: 1.3856390714645386, G Loss: 0.6883252859115601\n",
      "Epoch 15, batch 34 D Loss: 1.3830838203430176, G Loss: 0.6884718537330627\n",
      "Epoch 15, batch 35 D Loss: 1.38067626953125, G Loss: 0.6891055107116699\n",
      "Epoch 15, batch 36 D Loss: 1.387970209121704, G Loss: 0.6806271076202393\n",
      "Epoch 15, batch 37 D Loss: 1.3802127838134766, G Loss: 0.6930872201919556\n",
      "Epoch 15, batch 38 D Loss: 1.390166997909546, G Loss: 0.68894362449646\n",
      "Epoch 15, batch 39 D Loss: 1.3825407028198242, G Loss: 0.6871029734611511\n",
      "Epoch 15, batch 40 D Loss: 1.3787438869476318, G Loss: 0.6912745833396912\n",
      "Epoch 15, batch 41 D Loss: 1.3863282203674316, G Loss: 0.6900416612625122\n",
      "Epoch 15, batch 42 D Loss: 1.3891960382461548, G Loss: 0.6838464140892029\n",
      "Epoch 15, batch 43 D Loss: 1.388221025466919, G Loss: 0.6854937672615051\n",
      "Epoch 15, batch 44 D Loss: 1.3806641101837158, G Loss: 0.6915709972381592\n",
      "Epoch 15, batch 45 D Loss: 1.3912734985351562, G Loss: 0.6773003339767456\n",
      "Epoch 15, batch 46 D Loss: 1.3905770778656006, G Loss: 0.6820976138114929\n",
      "Epoch 15, batch 47 D Loss: 1.3866188526153564, G Loss: 0.6813541650772095\n",
      "Epoch 15, batch 48 D Loss: 1.3961020708084106, G Loss: 0.6723425388336182\n",
      "Epoch 15, batch 49 D Loss: 1.3785656690597534, G Loss: 0.6778500080108643\n",
      "Epoch 15, batch 50 D Loss: 1.383739709854126, G Loss: 0.6805754899978638\n",
      "Epoch 15, batch 51 D Loss: 1.384800672531128, G Loss: 0.6762092113494873\n",
      "Epoch 15, batch 52 D Loss: 1.3843914270401, G Loss: 0.6788491010665894\n",
      "Epoch 15, batch 53 D Loss: 1.3807430267333984, G Loss: 0.6785209774971008\n",
      "Epoch 15, batch 54 D Loss: 1.3845751285552979, G Loss: 0.6788656711578369\n",
      "Epoch 15, batch 55 D Loss: 1.380298376083374, G Loss: 0.6761074662208557\n",
      "Epoch 15, batch 56 D Loss: 1.3887739181518555, G Loss: 0.6728423237800598\n",
      "Epoch 15, batch 57 D Loss: 1.3873157501220703, G Loss: 0.6711622476577759\n",
      "Epoch 15, batch 58 D Loss: 1.3874913454055786, G Loss: 0.676816463470459\n",
      "Epoch 15, batch 59 D Loss: 1.3898208141326904, G Loss: 0.6749014258384705\n",
      "Epoch 15, batch 60 D Loss: 1.391500473022461, G Loss: 0.6701294779777527\n",
      "Epoch 15, batch 61 D Loss: 1.3723939657211304, G Loss: 0.6881707906723022\n",
      "Epoch 15, batch 62 D Loss: 1.37791109085083, G Loss: 0.6743881702423096\n",
      "Epoch 15, batch 63 D Loss: 1.392964243888855, G Loss: 0.6737713813781738\n",
      "Epoch 15, batch 64 D Loss: 1.3847825527191162, G Loss: 0.6787945628166199\n",
      "Epoch 15, batch 65 D Loss: 1.3938006162643433, G Loss: 0.6777681112289429\n",
      "Epoch 15, batch 66 D Loss: 1.390242338180542, G Loss: 0.6910871863365173\n",
      "Epoch 15, batch 67 D Loss: 1.3788533210754395, G Loss: 0.6819645166397095\n",
      "Epoch 15, batch 68 D Loss: 1.3945908546447754, G Loss: 0.688187837600708\n",
      "Epoch 15, batch 69 D Loss: 1.38779616355896, G Loss: 0.6880477666854858\n",
      "Epoch 15, batch 70 D Loss: 1.379075050354004, G Loss: 0.689182460308075\n",
      "Epoch 15, batch 71 D Loss: 1.3821830749511719, G Loss: 0.6842573285102844\n",
      "Epoch 15, batch 72 D Loss: 1.3728091716766357, G Loss: 0.6979250311851501\n",
      "Epoch 15, batch 73 D Loss: 1.3912394046783447, G Loss: 0.6884329319000244\n",
      "Epoch 15, batch 74 D Loss: 1.375920295715332, G Loss: 0.6882644891738892\n",
      "Epoch 15, batch 75 D Loss: 1.3687835931777954, G Loss: 0.7009990811347961\n",
      "Epoch 15, batch 76 D Loss: 1.3594545125961304, G Loss: 0.7038671374320984\n",
      "Epoch 15, batch 77 D Loss: 1.386906623840332, G Loss: 0.6810458898544312\n",
      "Epoch 15, batch 78 D Loss: 1.3906223773956299, G Loss: 0.6746327877044678\n",
      "Epoch 15, batch 79 D Loss: 1.3889716863632202, G Loss: 0.6871516704559326\n",
      "Epoch 15, batch 80 D Loss: 1.3870506286621094, G Loss: 0.688740611076355\n",
      "Epoch 15, batch 81 D Loss: 1.3834078311920166, G Loss: 0.6858386993408203\n",
      "Epoch 15, batch 82 D Loss: 1.3742902278900146, G Loss: 0.6854307055473328\n",
      "Epoch 15, batch 83 D Loss: 1.3796887397766113, G Loss: 0.6882392764091492\n",
      "Epoch 15, batch 84 D Loss: 1.3932757377624512, G Loss: 0.679623007774353\n",
      "Epoch 15, batch 85 D Loss: 1.38218355178833, G Loss: 0.6788975596427917\n",
      "Epoch 15, batch 86 D Loss: 1.401551604270935, G Loss: 0.679227352142334\n",
      "Epoch 15, batch 87 D Loss: 1.379500389099121, G Loss: 0.6859815716743469\n",
      "Epoch 15, batch 88 D Loss: 1.3812413215637207, G Loss: 0.687204897403717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, batch 89 D Loss: 1.3688323497772217, G Loss: 0.6970729231834412\n",
      "Epoch 15, batch 90 D Loss: 1.3741540908813477, G Loss: 0.6873353719711304\n",
      "Epoch 15, batch 91 D Loss: 1.3872461318969727, G Loss: 0.6786409616470337\n",
      "Epoch 15, batch 92 D Loss: 1.4024031162261963, G Loss: 0.6850292682647705\n",
      "Epoch 15, batch 93 D Loss: 1.3793177604675293, G Loss: 0.6989519596099854\n",
      "Epoch 15, batch 94 D Loss: 1.3556512594223022, G Loss: 0.7057468295097351\n",
      "Epoch 15, batch 95 D Loss: 1.3755593299865723, G Loss: 0.6976182460784912\n",
      "Epoch 15, batch 96 D Loss: 1.3728220462799072, G Loss: 0.7044748067855835\n",
      "Epoch 15, batch 97 D Loss: 1.3953663110733032, G Loss: 0.6819367408752441\n",
      "Epoch 15, batch 98 D Loss: 1.373387098312378, G Loss: 0.6870204210281372\n",
      "Epoch 15, batch 99 D Loss: 1.391627311706543, G Loss: 0.6789696216583252\n",
      "Epoch 15, batch 100 D Loss: 1.3666157722473145, G Loss: 0.6902226209640503\n",
      "Epoch 15, batch 101 D Loss: 1.3816049098968506, G Loss: 0.6826656460762024\n",
      "Epoch 15, batch 102 D Loss: 1.4049711227416992, G Loss: 0.6996843218803406\n",
      "Epoch 15, batch 103 D Loss: 1.4030964374542236, G Loss: 0.7010191082954407\n",
      "Epoch 15, batch 104 D Loss: 1.403735876083374, G Loss: 0.6948451995849609\n",
      "Epoch 15, batch 105 D Loss: 1.3916358947753906, G Loss: 0.7025613188743591\n",
      "Epoch 15, batch 106 D Loss: 1.378186821937561, G Loss: 0.7060565948486328\n",
      "Epoch 15, batch 107 D Loss: 1.3919637203216553, G Loss: 0.6874926686286926\n",
      "Epoch 15, batch 108 D Loss: 1.4031498432159424, G Loss: 0.6827943921089172\n",
      "Epoch 15, batch 109 D Loss: 1.3871910572052002, G Loss: 0.6873764991760254\n",
      "Epoch 15, batch 110 D Loss: 1.3800612688064575, G Loss: 0.6978740692138672\n",
      "Epoch 15, batch 111 D Loss: 1.3717899322509766, G Loss: 0.7014802694320679\n",
      "Epoch 15, batch 112 D Loss: 1.3885966539382935, G Loss: 0.6961286664009094\n",
      "Epoch 15, batch 113 D Loss: 1.3939976692199707, G Loss: 0.6899809837341309\n",
      "Epoch 15, batch 114 D Loss: 1.372586965560913, G Loss: 0.691352903842926\n",
      "Epoch 15, batch 115 D Loss: 1.3871653079986572, G Loss: 0.6946923732757568\n",
      "Epoch 15, batch 116 D Loss: 1.3888078927993774, G Loss: 0.6889528632164001\n",
      "Epoch 15, batch 117 D Loss: 1.3961756229400635, G Loss: 0.6941854357719421\n",
      "Epoch 15, batch 118 D Loss: 1.3835407495498657, G Loss: 0.6975305080413818\n",
      "Epoch 15, batch 119 D Loss: 1.3764281272888184, G Loss: 0.6919288039207458\n",
      "Epoch 15, batch 120 D Loss: 1.3774317502975464, G Loss: 0.6968181729316711\n",
      "Epoch 15, batch 121 D Loss: 1.3791855573654175, G Loss: 0.6962800621986389\n",
      "Epoch 15, batch 122 D Loss: 1.3780088424682617, G Loss: 0.6997091770172119\n",
      "Epoch 15, batch 123 D Loss: 1.3963452577590942, G Loss: 0.6838855147361755\n",
      "Epoch 15, batch 124 D Loss: 1.3726215362548828, G Loss: 0.7016876339912415\n",
      "Epoch 15, batch 125 D Loss: 1.4000321626663208, G Loss: 0.6841641068458557\n",
      "Epoch 15, batch 126 D Loss: 1.4101734161376953, G Loss: 0.6871627569198608\n",
      "Epoch 15, batch 127 D Loss: 1.375001311302185, G Loss: 0.6946280598640442\n",
      "Epoch 15, batch 128 D Loss: 1.379136085510254, G Loss: 0.7013862729072571\n",
      "Epoch 15, batch 129 D Loss: 1.3774267435073853, G Loss: 0.7002681493759155\n",
      "Epoch 15, batch 130 D Loss: 1.3910739421844482, G Loss: 0.7073397636413574\n",
      "Epoch 15, batch 131 D Loss: 1.3929892778396606, G Loss: 0.697032630443573\n",
      "Epoch 15, batch 132 D Loss: 1.3955497741699219, G Loss: 0.6942979693412781\n",
      "Epoch 15, batch 133 D Loss: 1.3903441429138184, G Loss: 0.6971142292022705\n",
      "Epoch 15, batch 134 D Loss: 1.3624987602233887, G Loss: 0.7031283378601074\n",
      "Epoch 15, batch 135 D Loss: 1.38564133644104, G Loss: 0.6970219612121582\n",
      "Epoch 15, batch 136 D Loss: 1.3851149082183838, G Loss: 0.6942629218101501\n",
      "Epoch 15, batch 137 D Loss: 1.379882574081421, G Loss: 0.6938008666038513\n",
      "Epoch 15, batch 138 D Loss: 1.3967950344085693, G Loss: 0.6804744601249695\n",
      "Epoch 15, batch 139 D Loss: 1.3790562152862549, G Loss: 0.6893815398216248\n",
      "Epoch 15, batch 140 D Loss: 1.3809428215026855, G Loss: 0.6853234171867371\n",
      "Epoch 15, batch 141 D Loss: 1.381052017211914, G Loss: 0.6836766600608826\n",
      "Epoch 15, batch 142 D Loss: 1.3722647428512573, G Loss: 0.690004289150238\n",
      "Epoch 15, batch 143 D Loss: 1.3768372535705566, G Loss: 0.684161365032196\n",
      "Epoch 15, batch 144 D Loss: 1.3845055103302002, G Loss: 0.685219943523407\n",
      "Epoch 15, batch 145 D Loss: 1.3912104368209839, G Loss: 0.6772559285163879\n",
      "Epoch 15, batch 146 D Loss: 1.3851196765899658, G Loss: 0.6843553781509399\n",
      "Epoch 15, batch 147 D Loss: 1.3937395811080933, G Loss: 0.6826345920562744\n",
      "Epoch 15, batch 148 D Loss: 1.378354787826538, G Loss: 0.693606972694397\n",
      "Epoch 15, batch 149 D Loss: 1.3984311819076538, G Loss: 0.6817314028739929\n",
      "Epoch 15, batch 150 D Loss: 1.383800983428955, G Loss: 0.6909054517745972\n",
      "Epoch 15, batch 151 D Loss: 1.3816781044006348, G Loss: 0.6974121332168579\n",
      "Epoch 15, batch 152 D Loss: 1.3852719068527222, G Loss: 0.691562294960022\n",
      "Epoch 15, batch 153 D Loss: 1.3700029850006104, G Loss: 0.6983351111412048\n",
      "Epoch 15, batch 154 D Loss: 1.3788325786590576, G Loss: 0.695044994354248\n",
      "Epoch 15, batch 155 D Loss: 1.3930511474609375, G Loss: 0.6941294074058533\n",
      "Epoch 15, batch 156 D Loss: 1.4026634693145752, G Loss: 0.6797871589660645\n",
      "Epoch 15, batch 157 D Loss: 1.383603572845459, G Loss: 0.6927903890609741\n",
      "Epoch 15, batch 158 D Loss: 1.3878865242004395, G Loss: 0.7011245489120483\n",
      "Epoch 15, batch 159 D Loss: 1.3811142444610596, G Loss: 0.7070901989936829\n",
      "Epoch 15, batch 160 D Loss: 1.3723223209381104, G Loss: 0.71697598695755\n",
      "Epoch 15, batch 161 D Loss: 1.389061450958252, G Loss: 0.696671724319458\n",
      "Epoch 15, batch 162 D Loss: 1.3880629539489746, G Loss: 0.7051067352294922\n",
      "Epoch 15, batch 163 D Loss: 1.3814201354980469, G Loss: 0.7025918364524841\n",
      "Epoch 15, batch 164 D Loss: 1.383371114730835, G Loss: 0.7067005634307861\n",
      "Epoch 15, batch 165 D Loss: 1.3729443550109863, G Loss: 0.7106205821037292\n",
      "Epoch 15, batch 166 D Loss: 1.3879189491271973, G Loss: 0.7131215929985046\n",
      "Epoch 15, batch 167 D Loss: 1.385743498802185, G Loss: 0.7042292952537537\n",
      "Epoch 15, batch 168 D Loss: 1.372776746749878, G Loss: 0.7188241481781006\n",
      "Epoch 15, batch 169 D Loss: 1.4065313339233398, G Loss: 0.6957353353500366\n",
      "Epoch 15, batch 170 D Loss: 1.3875617980957031, G Loss: 0.7100555896759033\n",
      "Epoch 15, batch 171 D Loss: 1.3857085704803467, G Loss: 0.7069135904312134\n",
      "Epoch 15, batch 172 D Loss: 1.3636651039123535, G Loss: 0.7177006602287292\n",
      "Epoch 15, batch 173 D Loss: 1.3672232627868652, G Loss: 0.7120616436004639\n",
      "Epoch 15, batch 174 D Loss: 1.3968377113342285, G Loss: 0.6938995122909546\n",
      "Epoch 15, batch 175 D Loss: 1.3768486976623535, G Loss: 0.6971971392631531\n",
      "Epoch 15, batch 176 D Loss: 1.3954037427902222, G Loss: 0.7048476338386536\n",
      "Epoch 15, batch 177 D Loss: 1.3839008808135986, G Loss: 0.7079179883003235\n",
      "Epoch 15, batch 178 D Loss: 1.390434741973877, G Loss: 0.7000154852867126\n",
      "Epoch 15, batch 179 D Loss: 1.4092438220977783, G Loss: 0.6866800785064697\n",
      "Epoch 15, batch 180 D Loss: 1.41403067111969, G Loss: 0.6722732782363892\n",
      "Epoch 15, batch 181 D Loss: 1.3951690196990967, G Loss: 0.695844292640686\n",
      "Epoch 15, batch 182 D Loss: 1.3658266067504883, G Loss: 0.6995407938957214\n",
      "Epoch 15, batch 183 D Loss: 1.3857460021972656, G Loss: 0.6947677731513977\n",
      "Epoch 15, batch 184 D Loss: 1.3868589401245117, G Loss: 0.7101291418075562\n",
      "Epoch 15, batch 185 D Loss: 1.4093194007873535, G Loss: 0.6941689252853394\n",
      "Epoch 15, batch 186 D Loss: 1.395864486694336, G Loss: 0.6910760998725891\n",
      "Epoch 15, batch 187 D Loss: 1.3902956247329712, G Loss: 0.7035393714904785\n",
      "Epoch 15, batch 188 D Loss: 1.3945916891098022, G Loss: 0.6902039051055908\n",
      "Epoch 15, batch 189 D Loss: 1.384718894958496, G Loss: 0.7081682682037354\n",
      "Epoch 15, batch 190 D Loss: 1.3771824836730957, G Loss: 0.7094837427139282\n",
      "Epoch 15, batch 191 D Loss: 1.4002070426940918, G Loss: 0.6938686370849609\n",
      "Epoch 15, batch 192 D Loss: 1.4037668704986572, G Loss: 0.6848831176757812\n",
      "Epoch 15, batch 193 D Loss: 1.3962492942810059, G Loss: 0.703249990940094\n",
      "Epoch 15, batch 194 D Loss: 1.36906099319458, G Loss: 0.7170040607452393\n",
      "Epoch 15, batch 195 D Loss: 1.3944437503814697, G Loss: 0.7043670415878296\n",
      "Epoch 15, batch 196 D Loss: 1.3954120874404907, G Loss: 0.6981004476547241\n",
      "Epoch 15, batch 197 D Loss: 1.3812181949615479, G Loss: 0.7040907144546509\n",
      "Epoch 15, batch 198 D Loss: 1.381423830986023, G Loss: 0.7049830555915833\n",
      "Epoch 15, batch 199 D Loss: 1.3887338638305664, G Loss: 0.7048998475074768\n",
      "Epoch 15, batch 200 D Loss: 1.374391794204712, G Loss: 0.7091454267501831\n",
      "Epoch 16, batch 1 D Loss: 1.3743071556091309, G Loss: 0.700034499168396\n",
      "Epoch 16, batch 2 D Loss: 1.3884015083312988, G Loss: 0.6969465613365173\n",
      "Epoch 16, batch 3 D Loss: 1.3729300498962402, G Loss: 0.7166510224342346\n",
      "Epoch 16, batch 4 D Loss: 1.386061668395996, G Loss: 0.6985070109367371\n",
      "Epoch 16, batch 5 D Loss: 1.3844660520553589, G Loss: 0.7063258290290833\n",
      "Epoch 16, batch 6 D Loss: 1.3920361995697021, G Loss: 0.7002937197685242\n",
      "Epoch 16, batch 7 D Loss: 1.3934623003005981, G Loss: 0.7065631151199341\n",
      "Epoch 16, batch 8 D Loss: 1.3897747993469238, G Loss: 0.7049288153648376\n",
      "Epoch 16, batch 9 D Loss: 1.38193941116333, G Loss: 0.7101452350616455\n",
      "Epoch 16, batch 10 D Loss: 1.3890597820281982, G Loss: 0.709075927734375\n",
      "Epoch 16, batch 11 D Loss: 1.3886151313781738, G Loss: 0.7070475816726685\n",
      "Epoch 16, batch 12 D Loss: 1.3925211429595947, G Loss: 0.7029312252998352\n",
      "Epoch 16, batch 13 D Loss: 1.3908326625823975, G Loss: 0.7170993089675903\n",
      "Epoch 16, batch 14 D Loss: 1.3963227272033691, G Loss: 0.7155703902244568\n",
      "Epoch 16, batch 15 D Loss: 1.3997929096221924, G Loss: 0.7079121470451355\n",
      "Epoch 16, batch 16 D Loss: 1.3834724426269531, G Loss: 0.7222459316253662\n",
      "Epoch 16, batch 17 D Loss: 1.377510905265808, G Loss: 0.7242233157157898\n",
      "Epoch 16, batch 18 D Loss: 1.3940372467041016, G Loss: 0.7109085321426392\n",
      "Epoch 16, batch 19 D Loss: 1.3722290992736816, G Loss: 0.7276233434677124\n",
      "Epoch 16, batch 20 D Loss: 1.3852407932281494, G Loss: 0.7216956615447998\n",
      "Epoch 16, batch 21 D Loss: 1.376829981803894, G Loss: 0.7312468886375427\n",
      "Epoch 16, batch 22 D Loss: 1.385474681854248, G Loss: 0.7212202548980713\n",
      "Epoch 16, batch 23 D Loss: 1.3743982315063477, G Loss: 0.731637179851532\n",
      "Epoch 16, batch 24 D Loss: 1.3849380016326904, G Loss: 0.7230685949325562\n",
      "Epoch 16, batch 25 D Loss: 1.3976469039916992, G Loss: 0.721618115901947\n",
      "Epoch 16, batch 26 D Loss: 1.381314754486084, G Loss: 0.7144538760185242\n",
      "Epoch 16, batch 27 D Loss: 1.389143466949463, G Loss: 0.7205118536949158\n",
      "Epoch 16, batch 28 D Loss: 1.3847079277038574, G Loss: 0.7193760871887207\n",
      "Epoch 16, batch 29 D Loss: 1.3829089403152466, G Loss: 0.7254531383514404\n",
      "Epoch 16, batch 30 D Loss: 1.364262342453003, G Loss: 0.7302395701408386\n",
      "Epoch 16, batch 31 D Loss: 1.3743000030517578, G Loss: 0.7205682396888733\n",
      "Epoch 16, batch 32 D Loss: 1.3859704732894897, G Loss: 0.7223600745201111\n",
      "Epoch 16, batch 33 D Loss: 1.3813750743865967, G Loss: 0.7158063650131226\n",
      "Epoch 16, batch 34 D Loss: 1.3781529664993286, G Loss: 0.7113988995552063\n",
      "Epoch 16, batch 35 D Loss: 1.3895151615142822, G Loss: 0.7086306810379028\n",
      "Epoch 16, batch 36 D Loss: 1.39667809009552, G Loss: 0.707857072353363\n",
      "Epoch 16, batch 37 D Loss: 1.3798679113388062, G Loss: 0.7147734761238098\n",
      "Epoch 16, batch 38 D Loss: 1.3839919567108154, G Loss: 0.715565025806427\n",
      "Epoch 16, batch 39 D Loss: 1.3824015855789185, G Loss: 0.7080644369125366\n",
      "Epoch 16, batch 40 D Loss: 1.3947820663452148, G Loss: 0.7006516456604004\n",
      "Epoch 16, batch 41 D Loss: 1.3683733940124512, G Loss: 0.7216562032699585\n",
      "Epoch 16, batch 42 D Loss: 1.3882719278335571, G Loss: 0.7015893459320068\n",
      "Epoch 16, batch 43 D Loss: 1.4022767543792725, G Loss: 0.7028039693832397\n",
      "Epoch 16, batch 44 D Loss: 1.4079053401947021, G Loss: 0.6936759948730469\n",
      "Epoch 16, batch 45 D Loss: 1.3898241519927979, G Loss: 0.6981673240661621\n",
      "Epoch 16, batch 46 D Loss: 1.404028296470642, G Loss: 0.6968523263931274\n",
      "Epoch 16, batch 47 D Loss: 1.3981761932373047, G Loss: 0.7054448127746582\n",
      "Epoch 16, batch 48 D Loss: 1.402353286743164, G Loss: 0.7002797722816467\n",
      "Epoch 16, batch 49 D Loss: 1.395329236984253, G Loss: 0.6984820365905762\n",
      "Epoch 16, batch 50 D Loss: 1.381371259689331, G Loss: 0.6994161009788513\n",
      "Epoch 16, batch 51 D Loss: 1.3870351314544678, G Loss: 0.6968642473220825\n",
      "Epoch 16, batch 52 D Loss: 1.3949172496795654, G Loss: 0.6924569606781006\n",
      "Epoch 16, batch 53 D Loss: 1.395226240158081, G Loss: 0.6943615078926086\n",
      "Epoch 16, batch 54 D Loss: 1.3833839893341064, G Loss: 0.6920619010925293\n",
      "Epoch 16, batch 55 D Loss: 1.3822513818740845, G Loss: 0.6944559216499329\n",
      "Epoch 16, batch 56 D Loss: 1.3843836784362793, G Loss: 0.6902070641517639\n",
      "Epoch 16, batch 57 D Loss: 1.3811453580856323, G Loss: 0.6953109502792358\n",
      "Epoch 16, batch 58 D Loss: 1.3797788619995117, G Loss: 0.6907657384872437\n",
      "Epoch 16, batch 59 D Loss: 1.3932039737701416, G Loss: 0.684700608253479\n",
      "Epoch 16, batch 60 D Loss: 1.382419466972351, G Loss: 0.6965487003326416\n",
      "Epoch 16, batch 61 D Loss: 1.3768707513809204, G Loss: 0.6972224712371826\n",
      "Epoch 16, batch 62 D Loss: 1.4040563106536865, G Loss: 0.6857430934906006\n",
      "Epoch 16, batch 63 D Loss: 1.3830957412719727, G Loss: 0.6976762413978577\n",
      "Epoch 16, batch 64 D Loss: 1.3843789100646973, G Loss: 0.6875526309013367\n",
      "Epoch 16, batch 65 D Loss: 1.3880877494812012, G Loss: 0.6872817277908325\n",
      "Epoch 16, batch 66 D Loss: 1.395301342010498, G Loss: 0.682604193687439\n",
      "Epoch 16, batch 67 D Loss: 1.3793354034423828, G Loss: 0.6974542140960693\n",
      "Epoch 16, batch 68 D Loss: 1.3841662406921387, G Loss: 0.6873403787612915\n",
      "Epoch 16, batch 69 D Loss: 1.3766369819641113, G Loss: 0.6888840198516846\n",
      "Epoch 16, batch 70 D Loss: 1.3788409233093262, G Loss: 0.687237024307251\n",
      "Epoch 16, batch 71 D Loss: 1.379209280014038, G Loss: 0.6911689043045044\n",
      "Epoch 16, batch 72 D Loss: 1.3812358379364014, G Loss: 0.6773501038551331\n",
      "Epoch 16, batch 73 D Loss: 1.3803224563598633, G Loss: 0.6815763711929321\n",
      "Epoch 16, batch 74 D Loss: 1.3875467777252197, G Loss: 0.6796788573265076\n",
      "Epoch 16, batch 75 D Loss: 1.3847661018371582, G Loss: 0.6706932187080383\n",
      "Epoch 16, batch 76 D Loss: 1.372753381729126, G Loss: 0.683656632900238\n",
      "Epoch 16, batch 77 D Loss: 1.3770020008087158, G Loss: 0.6741475462913513\n",
      "Epoch 16, batch 78 D Loss: 1.3926773071289062, G Loss: 0.6747392416000366\n",
      "Epoch 16, batch 79 D Loss: 1.3972375392913818, G Loss: 0.6681125164031982\n",
      "Epoch 16, batch 80 D Loss: 1.3798434734344482, G Loss: 0.6837531924247742\n",
      "Epoch 16, batch 81 D Loss: 1.377062439918518, G Loss: 0.6920846700668335\n",
      "Epoch 16, batch 82 D Loss: 1.3869776725769043, G Loss: 0.6808844208717346\n",
      "Epoch 16, batch 83 D Loss: 1.3920459747314453, G Loss: 0.6767720580101013\n",
      "Epoch 16, batch 84 D Loss: 1.3803229331970215, G Loss: 0.6804495453834534\n",
      "Epoch 16, batch 85 D Loss: 1.3961691856384277, G Loss: 0.6700061559677124\n",
      "Epoch 16, batch 86 D Loss: 1.3727270364761353, G Loss: 0.6842657327651978\n",
      "Epoch 16, batch 87 D Loss: 1.3883233070373535, G Loss: 0.686947226524353\n",
      "Epoch 16, batch 88 D Loss: 1.382716417312622, G Loss: 0.684013307094574\n",
      "Epoch 16, batch 89 D Loss: 1.3896549940109253, G Loss: 0.6807057857513428\n",
      "Epoch 16, batch 90 D Loss: 1.3791993856430054, G Loss: 0.6833980083465576\n",
      "Epoch 16, batch 91 D Loss: 1.3921432495117188, G Loss: 0.6747994422912598\n",
      "Epoch 16, batch 92 D Loss: 1.387425422668457, G Loss: 0.6833417415618896\n",
      "Epoch 16, batch 93 D Loss: 1.3882179260253906, G Loss: 0.6875706315040588\n",
      "Epoch 16, batch 94 D Loss: 1.3896715641021729, G Loss: 0.6782351732254028\n",
      "Epoch 16, batch 95 D Loss: 1.3795948028564453, G Loss: 0.693959653377533\n",
      "Epoch 16, batch 96 D Loss: 1.389860987663269, G Loss: 0.6897545456886292\n",
      "Epoch 16, batch 97 D Loss: 1.3966214656829834, G Loss: 0.6855139136314392\n",
      "Epoch 16, batch 98 D Loss: 1.387472152709961, G Loss: 0.7000108361244202\n",
      "Epoch 16, batch 99 D Loss: 1.3968032598495483, G Loss: 0.6867433786392212\n",
      "Epoch 16, batch 100 D Loss: 1.401125431060791, G Loss: 0.684420645236969\n",
      "Epoch 16, batch 101 D Loss: 1.3982985019683838, G Loss: 0.6871305704116821\n",
      "Epoch 16, batch 102 D Loss: 1.3848402500152588, G Loss: 0.6925387382507324\n",
      "Epoch 16, batch 103 D Loss: 1.384981393814087, G Loss: 0.6935026049613953\n",
      "Epoch 16, batch 104 D Loss: 1.3842917680740356, G Loss: 0.6887348294258118\n",
      "Epoch 16, batch 105 D Loss: 1.3884975910186768, G Loss: 0.6917433738708496\n",
      "Epoch 16, batch 106 D Loss: 1.378479242324829, G Loss: 0.6855176687240601\n",
      "Epoch 16, batch 107 D Loss: 1.3873543739318848, G Loss: 0.6893918514251709\n",
      "Epoch 16, batch 108 D Loss: 1.3876540660858154, G Loss: 0.6824958324432373\n",
      "Epoch 16, batch 109 D Loss: 1.3894338607788086, G Loss: 0.6880128979682922\n",
      "Epoch 16, batch 110 D Loss: 1.3778228759765625, G Loss: 0.6893932819366455\n",
      "Epoch 16, batch 111 D Loss: 1.389431118965149, G Loss: 0.6826726794242859\n",
      "Epoch 16, batch 112 D Loss: 1.3821284770965576, G Loss: 0.6847798228263855\n",
      "Epoch 16, batch 113 D Loss: 1.3806841373443604, G Loss: 0.6891221404075623\n",
      "Epoch 16, batch 114 D Loss: 1.3807858228683472, G Loss: 0.6841825246810913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, batch 115 D Loss: 1.3985824584960938, G Loss: 0.6785550117492676\n",
      "Epoch 16, batch 116 D Loss: 1.4062798023223877, G Loss: 0.6756880283355713\n",
      "Epoch 16, batch 117 D Loss: 1.382962703704834, G Loss: 0.6806958913803101\n",
      "Epoch 16, batch 118 D Loss: 1.393022060394287, G Loss: 0.6838079690933228\n",
      "Epoch 16, batch 119 D Loss: 1.3902325630187988, G Loss: 0.6823142766952515\n",
      "Epoch 16, batch 120 D Loss: 1.3833221197128296, G Loss: 0.6896375417709351\n",
      "Epoch 16, batch 121 D Loss: 1.385123610496521, G Loss: 0.6905288696289062\n",
      "Epoch 16, batch 122 D Loss: 1.3990519046783447, G Loss: 0.6772218942642212\n",
      "Epoch 16, batch 123 D Loss: 1.381378412246704, G Loss: 0.6836614012718201\n",
      "Epoch 16, batch 124 D Loss: 1.3945294618606567, G Loss: 0.6761320233345032\n",
      "Epoch 16, batch 125 D Loss: 1.3895255327224731, G Loss: 0.6798017024993896\n",
      "Epoch 16, batch 126 D Loss: 1.381039023399353, G Loss: 0.6822422742843628\n",
      "Epoch 16, batch 127 D Loss: 1.3791310787200928, G Loss: 0.6880684494972229\n",
      "Epoch 16, batch 128 D Loss: 1.3758108615875244, G Loss: 0.6875994205474854\n",
      "Epoch 16, batch 129 D Loss: 1.3912484645843506, G Loss: 0.6742313504219055\n",
      "Epoch 16, batch 130 D Loss: 1.386100172996521, G Loss: 0.6845085620880127\n",
      "Epoch 16, batch 131 D Loss: 1.3833286762237549, G Loss: 0.6809044480323792\n",
      "Epoch 16, batch 132 D Loss: 1.3889143466949463, G Loss: 0.6801740527153015\n",
      "Epoch 16, batch 133 D Loss: 1.391538143157959, G Loss: 0.6778013706207275\n",
      "Epoch 16, batch 134 D Loss: 1.3890700340270996, G Loss: 0.6765708923339844\n",
      "Epoch 16, batch 135 D Loss: 1.3808093070983887, G Loss: 0.6854342818260193\n",
      "Epoch 16, batch 136 D Loss: 1.3875858783721924, G Loss: 0.6715540885925293\n",
      "Epoch 16, batch 137 D Loss: 1.3919919729232788, G Loss: 0.670501708984375\n",
      "Epoch 16, batch 138 D Loss: 1.380864143371582, G Loss: 0.6896641254425049\n",
      "Epoch 16, batch 139 D Loss: 1.3851416110992432, G Loss: 0.6812913417816162\n",
      "Epoch 16, batch 140 D Loss: 1.393669605255127, G Loss: 0.6837953925132751\n",
      "Epoch 16, batch 141 D Loss: 1.3860106468200684, G Loss: 0.6830032467842102\n",
      "Epoch 16, batch 142 D Loss: 1.3880820274353027, G Loss: 0.6823021769523621\n",
      "Epoch 16, batch 143 D Loss: 1.384016990661621, G Loss: 0.6919206380844116\n",
      "Epoch 16, batch 144 D Loss: 1.3898391723632812, G Loss: 0.6753008365631104\n",
      "Epoch 16, batch 145 D Loss: 1.375282645225525, G Loss: 0.6859934329986572\n",
      "Epoch 16, batch 146 D Loss: 1.3893216848373413, G Loss: 0.6836494207382202\n",
      "Epoch 16, batch 147 D Loss: 1.3864601850509644, G Loss: 0.6844615936279297\n",
      "Epoch 16, batch 148 D Loss: 1.393473505973816, G Loss: 0.6783256530761719\n",
      "Epoch 16, batch 149 D Loss: 1.3892446756362915, G Loss: 0.6818506717681885\n",
      "Epoch 16, batch 150 D Loss: 1.3814141750335693, G Loss: 0.6933658123016357\n",
      "Epoch 16, batch 151 D Loss: 1.388214111328125, G Loss: 0.6894037127494812\n",
      "Epoch 16, batch 152 D Loss: 1.3834419250488281, G Loss: 0.6921629905700684\n",
      "Epoch 16, batch 153 D Loss: 1.3895797729492188, G Loss: 0.6937490105628967\n",
      "Epoch 16, batch 154 D Loss: 1.3909493684768677, G Loss: 0.6908715963363647\n",
      "Epoch 16, batch 155 D Loss: 1.3998785018920898, G Loss: 0.6905937194824219\n",
      "Epoch 16, batch 156 D Loss: 1.4067535400390625, G Loss: 0.6857236623764038\n",
      "Epoch 16, batch 157 D Loss: 1.3915905952453613, G Loss: 0.6974640488624573\n",
      "Epoch 16, batch 158 D Loss: 1.3867371082305908, G Loss: 0.6940116286277771\n",
      "Epoch 16, batch 159 D Loss: 1.3871891498565674, G Loss: 0.6935184001922607\n",
      "Epoch 16, batch 160 D Loss: 1.3763856887817383, G Loss: 0.7051664590835571\n",
      "Epoch 16, batch 161 D Loss: 1.3824207782745361, G Loss: 0.6905621886253357\n",
      "Epoch 16, batch 162 D Loss: 1.381917953491211, G Loss: 0.6972104907035828\n",
      "Epoch 16, batch 163 D Loss: 1.388519048690796, G Loss: 0.694887638092041\n",
      "Epoch 16, batch 164 D Loss: 1.3906331062316895, G Loss: 0.6996285915374756\n",
      "Epoch 16, batch 165 D Loss: 1.3853617906570435, G Loss: 0.697243332862854\n",
      "Epoch 16, batch 166 D Loss: 1.3873345851898193, G Loss: 0.6959429979324341\n",
      "Epoch 16, batch 167 D Loss: 1.401869773864746, G Loss: 0.6854147911071777\n",
      "Epoch 16, batch 168 D Loss: 1.391663670539856, G Loss: 0.6907419562339783\n",
      "Epoch 16, batch 169 D Loss: 1.3845481872558594, G Loss: 0.6933714151382446\n",
      "Epoch 16, batch 170 D Loss: 1.3931770324707031, G Loss: 0.6939225792884827\n",
      "Epoch 16, batch 171 D Loss: 1.3814749717712402, G Loss: 0.7014148831367493\n",
      "Epoch 16, batch 172 D Loss: 1.3894152641296387, G Loss: 0.6895696520805359\n",
      "Epoch 16, batch 173 D Loss: 1.3820500373840332, G Loss: 0.6970726251602173\n",
      "Epoch 16, batch 174 D Loss: 1.3849520683288574, G Loss: 0.693784773349762\n",
      "Epoch 16, batch 175 D Loss: 1.385984182357788, G Loss: 0.6952519416809082\n",
      "Epoch 16, batch 176 D Loss: 1.3945783376693726, G Loss: 0.6883878111839294\n",
      "Epoch 16, batch 177 D Loss: 1.3880422115325928, G Loss: 0.692821741104126\n",
      "Epoch 16, batch 178 D Loss: 1.3767378330230713, G Loss: 0.7032523155212402\n",
      "Epoch 16, batch 179 D Loss: 1.3917429447174072, G Loss: 0.6984236836433411\n",
      "Epoch 16, batch 180 D Loss: 1.3959829807281494, G Loss: 0.6944931149482727\n",
      "Epoch 16, batch 181 D Loss: 1.3708882331848145, G Loss: 0.7114585041999817\n",
      "Epoch 16, batch 182 D Loss: 1.3987751007080078, G Loss: 0.6975913047790527\n",
      "Epoch 16, batch 183 D Loss: 1.3876662254333496, G Loss: 0.7118961215019226\n",
      "Epoch 16, batch 184 D Loss: 1.3764643669128418, G Loss: 0.7099061012268066\n",
      "Epoch 16, batch 185 D Loss: 1.3837995529174805, G Loss: 0.7081578969955444\n",
      "Epoch 16, batch 186 D Loss: 1.3840606212615967, G Loss: 0.7051576972007751\n",
      "Epoch 16, batch 187 D Loss: 1.3852157592773438, G Loss: 0.7130931615829468\n",
      "Epoch 16, batch 188 D Loss: 1.382020354270935, G Loss: 0.7035365104675293\n",
      "Epoch 16, batch 189 D Loss: 1.3887829780578613, G Loss: 0.700604259967804\n",
      "Epoch 16, batch 190 D Loss: 1.4088518619537354, G Loss: 0.7063882350921631\n",
      "Epoch 16, batch 191 D Loss: 1.3829295635223389, G Loss: 0.7060016393661499\n",
      "Epoch 16, batch 192 D Loss: 1.3679966926574707, G Loss: 0.725324809551239\n",
      "Epoch 16, batch 193 D Loss: 1.3875491619110107, G Loss: 0.7117639780044556\n",
      "Epoch 16, batch 194 D Loss: 1.3886548280715942, G Loss: 0.7107506394386292\n",
      "Epoch 16, batch 195 D Loss: 1.3898539543151855, G Loss: 0.70173180103302\n",
      "Epoch 16, batch 196 D Loss: 1.3802379369735718, G Loss: 0.7056693434715271\n",
      "Epoch 16, batch 197 D Loss: 1.3806617259979248, G Loss: 0.7061029672622681\n",
      "Epoch 16, batch 198 D Loss: 1.3877581357955933, G Loss: 0.7044904232025146\n",
      "Epoch 16, batch 199 D Loss: 1.3681087493896484, G Loss: 0.7024649977684021\n",
      "Epoch 16, batch 200 D Loss: 1.3754216432571411, G Loss: 0.697143018245697\n",
      "Epoch 17, batch 1 D Loss: 1.3691800832748413, G Loss: 0.7077687978744507\n",
      "Epoch 17, batch 2 D Loss: 1.377246379852295, G Loss: 0.7085832357406616\n",
      "Epoch 17, batch 3 D Loss: 1.3692045211791992, G Loss: 0.7090995907783508\n",
      "Epoch 17, batch 4 D Loss: 1.3915214538574219, G Loss: 0.6991790533065796\n",
      "Epoch 17, batch 5 D Loss: 1.369842529296875, G Loss: 0.7174709439277649\n",
      "Epoch 17, batch 6 D Loss: 1.376248836517334, G Loss: 0.7090025544166565\n",
      "Epoch 17, batch 7 D Loss: 1.367354154586792, G Loss: 0.7110757231712341\n",
      "Epoch 17, batch 8 D Loss: 1.3973119258880615, G Loss: 0.6956378221511841\n",
      "Epoch 17, batch 9 D Loss: 1.3876960277557373, G Loss: 0.6891840100288391\n",
      "Epoch 17, batch 10 D Loss: 1.3794212341308594, G Loss: 0.7070222496986389\n",
      "Epoch 17, batch 11 D Loss: 1.375617504119873, G Loss: 0.7030930519104004\n",
      "Epoch 17, batch 12 D Loss: 1.3930981159210205, G Loss: 0.6944248080253601\n",
      "Epoch 17, batch 13 D Loss: 1.3910703659057617, G Loss: 0.7028616070747375\n",
      "Epoch 17, batch 14 D Loss: 1.3952091932296753, G Loss: 0.6916951537132263\n",
      "Epoch 17, batch 15 D Loss: 1.3891667127609253, G Loss: 0.6982048153877258\n",
      "Epoch 17, batch 16 D Loss: 1.3792669773101807, G Loss: 0.7076298594474792\n",
      "Epoch 17, batch 17 D Loss: 1.3782401084899902, G Loss: 0.7162995338439941\n",
      "Epoch 17, batch 18 D Loss: 1.386605978012085, G Loss: 0.7023117542266846\n",
      "Epoch 17, batch 19 D Loss: 1.3802926540374756, G Loss: 0.7093472480773926\n",
      "Epoch 17, batch 20 D Loss: 1.4008163213729858, G Loss: 0.7144718766212463\n",
      "Epoch 17, batch 21 D Loss: 1.3746006488800049, G Loss: 0.7099589705467224\n",
      "Epoch 17, batch 22 D Loss: 1.3777117729187012, G Loss: 0.7192570567131042\n",
      "Epoch 17, batch 23 D Loss: 1.3905904293060303, G Loss: 0.7034273743629456\n",
      "Epoch 17, batch 24 D Loss: 1.3910741806030273, G Loss: 0.7087296843528748\n",
      "Epoch 17, batch 25 D Loss: 1.3730857372283936, G Loss: 0.708901047706604\n",
      "Epoch 17, batch 26 D Loss: 1.3931877613067627, G Loss: 0.7036236524581909\n",
      "Epoch 17, batch 27 D Loss: 1.4206993579864502, G Loss: 0.7027405500411987\n",
      "Epoch 17, batch 28 D Loss: 1.3956265449523926, G Loss: 0.70585036277771\n",
      "Epoch 17, batch 29 D Loss: 1.3806686401367188, G Loss: 0.7056536674499512\n",
      "Epoch 17, batch 30 D Loss: 1.3718478679656982, G Loss: 0.7021870613098145\n",
      "Epoch 17, batch 31 D Loss: 1.3812127113342285, G Loss: 0.696129322052002\n",
      "Epoch 17, batch 32 D Loss: 1.3721637725830078, G Loss: 0.7104470729827881\n",
      "Epoch 17, batch 33 D Loss: 1.388226866722107, G Loss: 0.702415943145752\n",
      "Epoch 17, batch 34 D Loss: 1.379011869430542, G Loss: 0.702437162399292\n",
      "Epoch 17, batch 35 D Loss: 1.4101697206497192, G Loss: 0.6945227980613708\n",
      "Epoch 17, batch 36 D Loss: 1.3590508699417114, G Loss: 0.7087364196777344\n",
      "Epoch 17, batch 37 D Loss: 1.3844530582427979, G Loss: 0.7011568546295166\n",
      "Epoch 17, batch 38 D Loss: 1.4008276462554932, G Loss: 0.6922081112861633\n",
      "Epoch 17, batch 39 D Loss: 1.4049502611160278, G Loss: 0.6815416216850281\n",
      "Epoch 17, batch 40 D Loss: 1.3886862993240356, G Loss: 0.696530818939209\n",
      "Epoch 17, batch 41 D Loss: 1.3914878368377686, G Loss: 0.6975433230400085\n",
      "Epoch 17, batch 42 D Loss: 1.3833571672439575, G Loss: 0.6920387148857117\n",
      "Epoch 17, batch 43 D Loss: 1.3883122205734253, G Loss: 0.6812642812728882\n",
      "Epoch 17, batch 44 D Loss: 1.4023771286010742, G Loss: 0.6908751726150513\n",
      "Epoch 17, batch 45 D Loss: 1.3795629739761353, G Loss: 0.6880842447280884\n",
      "Epoch 17, batch 46 D Loss: 1.39072847366333, G Loss: 0.6817867755889893\n",
      "Epoch 17, batch 47 D Loss: 1.403520107269287, G Loss: 0.6797498464584351\n",
      "Epoch 17, batch 48 D Loss: 1.3886134624481201, G Loss: 0.6872371435165405\n",
      "Epoch 17, batch 49 D Loss: 1.3914577960968018, G Loss: 0.6981133818626404\n",
      "Epoch 17, batch 50 D Loss: 1.3887850046157837, G Loss: 0.7023808360099792\n",
      "Epoch 17, batch 51 D Loss: 1.3956341743469238, G Loss: 0.6822291612625122\n",
      "Epoch 17, batch 52 D Loss: 1.397674798965454, G Loss: 0.6841599345207214\n",
      "Epoch 17, batch 53 D Loss: 1.3868460655212402, G Loss: 0.6869237422943115\n",
      "Epoch 17, batch 54 D Loss: 1.4199556112289429, G Loss: 0.6758676767349243\n",
      "Epoch 17, batch 55 D Loss: 1.3975145816802979, G Loss: 0.676900327205658\n",
      "Epoch 17, batch 56 D Loss: 1.3897267580032349, G Loss: 0.6883650422096252\n",
      "Epoch 17, batch 57 D Loss: 1.3949332237243652, G Loss: 0.6722393035888672\n",
      "Epoch 17, batch 58 D Loss: 1.4018478393554688, G Loss: 0.6795340776443481\n",
      "Epoch 17, batch 59 D Loss: 1.3892199993133545, G Loss: 0.6811633110046387\n",
      "Epoch 17, batch 60 D Loss: 1.3900153636932373, G Loss: 0.6831018924713135\n",
      "Epoch 17, batch 61 D Loss: 1.3955788612365723, G Loss: 0.6812230944633484\n",
      "Epoch 17, batch 62 D Loss: 1.4070711135864258, G Loss: 0.6713510155677795\n",
      "Epoch 17, batch 63 D Loss: 1.3882088661193848, G Loss: 0.6891283988952637\n",
      "Epoch 17, batch 64 D Loss: 1.3908222913742065, G Loss: 0.6817032098770142\n",
      "Epoch 17, batch 65 D Loss: 1.3808763027191162, G Loss: 0.6887450218200684\n",
      "Epoch 17, batch 66 D Loss: 1.3900747299194336, G Loss: 0.6866624355316162\n",
      "Epoch 17, batch 67 D Loss: 1.3914837837219238, G Loss: 0.691698431968689\n",
      "Epoch 17, batch 68 D Loss: 1.3911795616149902, G Loss: 0.6958351135253906\n",
      "Epoch 17, batch 69 D Loss: 1.396714687347412, G Loss: 0.6976373195648193\n",
      "Epoch 17, batch 70 D Loss: 1.3810689449310303, G Loss: 0.7054110169410706\n",
      "Epoch 17, batch 71 D Loss: 1.3909924030303955, G Loss: 0.7033097743988037\n",
      "Epoch 17, batch 72 D Loss: 1.369307279586792, G Loss: 0.7228819131851196\n",
      "Epoch 17, batch 73 D Loss: 1.4108387231826782, G Loss: 0.6934576630592346\n",
      "Epoch 17, batch 74 D Loss: 1.3975656032562256, G Loss: 0.7029373645782471\n",
      "Epoch 17, batch 75 D Loss: 1.3901896476745605, G Loss: 0.7089075446128845\n",
      "Epoch 17, batch 76 D Loss: 1.3900254964828491, G Loss: 0.7086341381072998\n",
      "Epoch 17, batch 77 D Loss: 1.3889286518096924, G Loss: 0.7035783529281616\n",
      "Epoch 17, batch 78 D Loss: 1.3902853727340698, G Loss: 0.7089521884918213\n",
      "Epoch 17, batch 79 D Loss: 1.3746711015701294, G Loss: 0.7156320810317993\n",
      "Epoch 17, batch 80 D Loss: 1.393022060394287, G Loss: 0.7129149436950684\n",
      "Epoch 17, batch 81 D Loss: 1.3784561157226562, G Loss: 0.7154600620269775\n",
      "Epoch 17, batch 82 D Loss: 1.3894140720367432, G Loss: 0.7169124484062195\n",
      "Epoch 17, batch 83 D Loss: 1.3830804824829102, G Loss: 0.7125707864761353\n",
      "Epoch 17, batch 84 D Loss: 1.3813046216964722, G Loss: 0.71886146068573\n",
      "Epoch 17, batch 85 D Loss: 1.3798437118530273, G Loss: 0.7136443257331848\n",
      "Epoch 17, batch 86 D Loss: 1.3897371292114258, G Loss: 0.7125557065010071\n",
      "Epoch 17, batch 87 D Loss: 1.3875644207000732, G Loss: 0.7080419063568115\n",
      "Epoch 17, batch 88 D Loss: 1.3898365497589111, G Loss: 0.7065848708152771\n",
      "Epoch 17, batch 89 D Loss: 1.361467719078064, G Loss: 0.7170014381408691\n",
      "Epoch 17, batch 90 D Loss: 1.3864881992340088, G Loss: 0.6951702833175659\n",
      "Epoch 17, batch 91 D Loss: 1.4004260301589966, G Loss: 0.6884476542472839\n",
      "Epoch 17, batch 92 D Loss: 1.3960456848144531, G Loss: 0.7021396160125732\n",
      "Epoch 17, batch 93 D Loss: 1.4069663286209106, G Loss: 0.6820448040962219\n",
      "Epoch 17, batch 94 D Loss: 1.382952332496643, G Loss: 0.7057855129241943\n",
      "Epoch 17, batch 95 D Loss: 1.390481948852539, G Loss: 0.6914605498313904\n",
      "Epoch 17, batch 96 D Loss: 1.3612456321716309, G Loss: 0.7119409441947937\n",
      "Epoch 17, batch 97 D Loss: 1.3818236589431763, G Loss: 0.6987160444259644\n",
      "Epoch 17, batch 98 D Loss: 1.36576509475708, G Loss: 0.694654107093811\n",
      "Epoch 17, batch 99 D Loss: 1.361852765083313, G Loss: 0.7135254144668579\n",
      "Epoch 17, batch 100 D Loss: 1.4025065898895264, G Loss: 0.6832891702651978\n",
      "Epoch 17, batch 101 D Loss: 1.387773871421814, G Loss: 0.6979449987411499\n",
      "Epoch 17, batch 102 D Loss: 1.3841056823730469, G Loss: 0.6977732181549072\n",
      "Epoch 17, batch 103 D Loss: 1.4045820236206055, G Loss: 0.6898467540740967\n",
      "Epoch 17, batch 104 D Loss: 1.3987231254577637, G Loss: 0.6891992092132568\n",
      "Epoch 17, batch 105 D Loss: 1.3692262172698975, G Loss: 0.7018596529960632\n",
      "Epoch 17, batch 106 D Loss: 1.379677414894104, G Loss: 0.6970680952072144\n",
      "Epoch 17, batch 107 D Loss: 1.380676507949829, G Loss: 0.6948320269584656\n",
      "Epoch 17, batch 108 D Loss: 1.404935359954834, G Loss: 0.6816928386688232\n",
      "Epoch 17, batch 109 D Loss: 1.3800170421600342, G Loss: 0.6915335655212402\n",
      "Epoch 17, batch 110 D Loss: 1.3632428646087646, G Loss: 0.7077080011367798\n",
      "Epoch 17, batch 111 D Loss: 1.4082367420196533, G Loss: 0.6808474659919739\n",
      "Epoch 17, batch 112 D Loss: 1.394060730934143, G Loss: 0.6975899338722229\n",
      "Epoch 17, batch 113 D Loss: 1.3837889432907104, G Loss: 0.6883820295333862\n",
      "Epoch 17, batch 114 D Loss: 1.3877975940704346, G Loss: 0.7063187956809998\n",
      "Epoch 17, batch 115 D Loss: 1.377914309501648, G Loss: 0.7056264281272888\n",
      "Epoch 17, batch 116 D Loss: 1.378674030303955, G Loss: 0.7001199126243591\n",
      "Epoch 17, batch 117 D Loss: 1.379956841468811, G Loss: 0.698512852191925\n",
      "Epoch 17, batch 118 D Loss: 1.3735520839691162, G Loss: 0.7165805101394653\n",
      "Epoch 17, batch 119 D Loss: 1.3897168636322021, G Loss: 0.696668803691864\n",
      "Epoch 17, batch 120 D Loss: 1.3840203285217285, G Loss: 0.7040495872497559\n",
      "Epoch 17, batch 121 D Loss: 1.3876454830169678, G Loss: 0.7028887271881104\n",
      "Epoch 17, batch 122 D Loss: 1.3917531967163086, G Loss: 0.7063770890235901\n",
      "Epoch 17, batch 123 D Loss: 1.3876502513885498, G Loss: 0.7097851634025574\n",
      "Epoch 17, batch 124 D Loss: 1.4071202278137207, G Loss: 0.6904274821281433\n",
      "Epoch 17, batch 125 D Loss: 1.4006366729736328, G Loss: 0.6909278035163879\n",
      "Epoch 17, batch 126 D Loss: 1.3758095502853394, G Loss: 0.7004397511482239\n",
      "Epoch 17, batch 127 D Loss: 1.3838932514190674, G Loss: 0.6993455290794373\n",
      "Epoch 17, batch 128 D Loss: 1.3843668699264526, G Loss: 0.6961121559143066\n",
      "Epoch 17, batch 129 D Loss: 1.3706936836242676, G Loss: 0.7003836035728455\n",
      "Epoch 17, batch 130 D Loss: 1.3815289735794067, G Loss: 0.6992340087890625\n",
      "Epoch 17, batch 131 D Loss: 1.3759979009628296, G Loss: 0.7049520611763\n",
      "Epoch 17, batch 132 D Loss: 1.3810862302780151, G Loss: 0.7051200866699219\n",
      "Epoch 17, batch 133 D Loss: 1.3917317390441895, G Loss: 0.6873634457588196\n",
      "Epoch 17, batch 134 D Loss: 1.3775238990783691, G Loss: 0.7032808065414429\n",
      "Epoch 17, batch 135 D Loss: 1.400194525718689, G Loss: 0.6889891624450684\n",
      "Epoch 17, batch 136 D Loss: 1.389117956161499, G Loss: 0.6981157660484314\n",
      "Epoch 17, batch 137 D Loss: 1.3990148305892944, G Loss: 0.6966422200202942\n",
      "Epoch 17, batch 138 D Loss: 1.3804023265838623, G Loss: 0.7055298686027527\n",
      "Epoch 17, batch 139 D Loss: 1.3847726583480835, G Loss: 0.6959474682807922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, batch 140 D Loss: 1.3983135223388672, G Loss: 0.6884730458259583\n",
      "Epoch 17, batch 141 D Loss: 1.3711683750152588, G Loss: 0.7036325931549072\n",
      "Epoch 17, batch 142 D Loss: 1.3983821868896484, G Loss: 0.6948503255844116\n",
      "Epoch 17, batch 143 D Loss: 1.3838081359863281, G Loss: 0.695204496383667\n",
      "Epoch 17, batch 144 D Loss: 1.4014067649841309, G Loss: 0.6913754940032959\n",
      "Epoch 17, batch 145 D Loss: 1.4026057720184326, G Loss: 0.6869957447052002\n",
      "Epoch 17, batch 146 D Loss: 1.4052743911743164, G Loss: 0.6829322576522827\n",
      "Epoch 17, batch 147 D Loss: 1.4058325290679932, G Loss: 0.6838119626045227\n",
      "Epoch 17, batch 148 D Loss: 1.383162498474121, G Loss: 0.6891958713531494\n",
      "Epoch 17, batch 149 D Loss: 1.3763127326965332, G Loss: 0.7050561308860779\n",
      "Epoch 17, batch 150 D Loss: 1.382301926612854, G Loss: 0.6975547075271606\n",
      "Epoch 17, batch 151 D Loss: 1.3738799095153809, G Loss: 0.7032350301742554\n",
      "Epoch 17, batch 152 D Loss: 1.3835618495941162, G Loss: 0.6950490474700928\n",
      "Epoch 17, batch 153 D Loss: 1.3792064189910889, G Loss: 0.6949130892753601\n",
      "Epoch 17, batch 154 D Loss: 1.3916518688201904, G Loss: 0.6832728385925293\n",
      "Epoch 17, batch 155 D Loss: 1.3929469585418701, G Loss: 0.6898687481880188\n",
      "Epoch 17, batch 156 D Loss: 1.3866691589355469, G Loss: 0.6928707957267761\n",
      "Epoch 17, batch 157 D Loss: 1.3940832614898682, G Loss: 0.6775556802749634\n",
      "Epoch 17, batch 158 D Loss: 1.380165696144104, G Loss: 0.6856343150138855\n",
      "Epoch 17, batch 159 D Loss: 1.3780765533447266, G Loss: 0.6947247385978699\n",
      "Epoch 17, batch 160 D Loss: 1.3873140811920166, G Loss: 0.6862738728523254\n",
      "Epoch 17, batch 161 D Loss: 1.3803941011428833, G Loss: 0.6941993832588196\n",
      "Epoch 17, batch 162 D Loss: 1.3854098320007324, G Loss: 0.6916065216064453\n",
      "Epoch 17, batch 163 D Loss: 1.3726816177368164, G Loss: 0.6927888989448547\n",
      "Epoch 17, batch 164 D Loss: 1.3833969831466675, G Loss: 0.6846010088920593\n",
      "Epoch 17, batch 165 D Loss: 1.3838554620742798, G Loss: 0.6933830976486206\n",
      "Epoch 17, batch 166 D Loss: 1.3932729959487915, G Loss: 0.680013120174408\n",
      "Epoch 17, batch 167 D Loss: 1.3860657215118408, G Loss: 0.6952070593833923\n",
      "Epoch 17, batch 168 D Loss: 1.4030110836029053, G Loss: 0.6823863387107849\n",
      "Epoch 17, batch 169 D Loss: 1.3983702659606934, G Loss: 0.680910050868988\n",
      "Epoch 17, batch 170 D Loss: 1.3960886001586914, G Loss: 0.6845771670341492\n",
      "Epoch 17, batch 171 D Loss: 1.3861069679260254, G Loss: 0.6867836117744446\n",
      "Epoch 17, batch 172 D Loss: 1.3752411603927612, G Loss: 0.6937635540962219\n",
      "Epoch 17, batch 173 D Loss: 1.392045021057129, G Loss: 0.6833343505859375\n",
      "Epoch 17, batch 174 D Loss: 1.4020591974258423, G Loss: 0.6887014508247375\n",
      "Epoch 17, batch 175 D Loss: 1.382544755935669, G Loss: 0.6939301490783691\n",
      "Epoch 17, batch 176 D Loss: 1.3920493125915527, G Loss: 0.682375431060791\n",
      "Epoch 17, batch 177 D Loss: 1.3853223323822021, G Loss: 0.6897883415222168\n",
      "Epoch 17, batch 178 D Loss: 1.4017199277877808, G Loss: 0.6781899929046631\n",
      "Epoch 17, batch 179 D Loss: 1.3929872512817383, G Loss: 0.6916673183441162\n",
      "Epoch 17, batch 180 D Loss: 1.3858484029769897, G Loss: 0.6864216327667236\n",
      "Epoch 17, batch 181 D Loss: 1.3981263637542725, G Loss: 0.6849414110183716\n",
      "Epoch 17, batch 182 D Loss: 1.3926236629486084, G Loss: 0.6816598773002625\n",
      "Epoch 17, batch 183 D Loss: 1.3914217948913574, G Loss: 0.6927682757377625\n",
      "Epoch 17, batch 184 D Loss: 1.3989932537078857, G Loss: 0.687389612197876\n",
      "Epoch 17, batch 185 D Loss: 1.383176326751709, G Loss: 0.7006696462631226\n",
      "Epoch 17, batch 186 D Loss: 1.383678674697876, G Loss: 0.699262261390686\n",
      "Epoch 17, batch 187 D Loss: 1.3946974277496338, G Loss: 0.7008554339408875\n",
      "Epoch 17, batch 188 D Loss: 1.395435094833374, G Loss: 0.6930993795394897\n",
      "Epoch 17, batch 189 D Loss: 1.3904043436050415, G Loss: 0.702368974685669\n",
      "Epoch 17, batch 190 D Loss: 1.3882136344909668, G Loss: 0.7050013542175293\n",
      "Epoch 17, batch 191 D Loss: 1.3896660804748535, G Loss: 0.6995947957038879\n",
      "Epoch 17, batch 192 D Loss: 1.3968470096588135, G Loss: 0.702605128288269\n",
      "Epoch 17, batch 193 D Loss: 1.3830084800720215, G Loss: 0.7012122273445129\n",
      "Epoch 17, batch 194 D Loss: 1.3867313861846924, G Loss: 0.6986963152885437\n",
      "Epoch 17, batch 195 D Loss: 1.3909016847610474, G Loss: 0.6967898011207581\n",
      "Epoch 17, batch 196 D Loss: 1.391668438911438, G Loss: 0.6959226131439209\n",
      "Epoch 17, batch 197 D Loss: 1.3893718719482422, G Loss: 0.6917703747749329\n",
      "Epoch 17, batch 198 D Loss: 1.3980462551116943, G Loss: 0.6912280917167664\n",
      "Epoch 17, batch 199 D Loss: 1.39699387550354, G Loss: 0.6974772810935974\n",
      "Epoch 17, batch 200 D Loss: 1.394604206085205, G Loss: 0.6911624073982239\n",
      "Epoch 18, batch 1 D Loss: 1.391141414642334, G Loss: 0.6938271522521973\n",
      "Epoch 18, batch 2 D Loss: 1.3890153169631958, G Loss: 0.6913055181503296\n",
      "Epoch 18, batch 3 D Loss: 1.391737937927246, G Loss: 0.6874719262123108\n",
      "Epoch 18, batch 4 D Loss: 1.392613172531128, G Loss: 0.6871334910392761\n",
      "Epoch 18, batch 5 D Loss: 1.3948283195495605, G Loss: 0.6904996633529663\n",
      "Epoch 18, batch 6 D Loss: 1.3857747316360474, G Loss: 0.6896376609802246\n",
      "Epoch 18, batch 7 D Loss: 1.3945763111114502, G Loss: 0.6882169842720032\n",
      "Epoch 18, batch 8 D Loss: 1.3974353075027466, G Loss: 0.6813713908195496\n",
      "Epoch 18, batch 9 D Loss: 1.3858397006988525, G Loss: 0.6862647533416748\n",
      "Epoch 18, batch 10 D Loss: 1.3850728273391724, G Loss: 0.6868719458580017\n",
      "Epoch 18, batch 11 D Loss: 1.3899699449539185, G Loss: 0.6871969103813171\n",
      "Epoch 18, batch 12 D Loss: 1.384472131729126, G Loss: 0.6916963458061218\n",
      "Epoch 18, batch 13 D Loss: 1.3827402591705322, G Loss: 0.6857211589813232\n",
      "Epoch 18, batch 14 D Loss: 1.3857567310333252, G Loss: 0.6826841235160828\n",
      "Epoch 18, batch 15 D Loss: 1.3856174945831299, G Loss: 0.685234785079956\n",
      "Epoch 18, batch 16 D Loss: 1.3841897249221802, G Loss: 0.6862432360649109\n",
      "Epoch 18, batch 17 D Loss: 1.3900351524353027, G Loss: 0.6888185143470764\n",
      "Epoch 18, batch 18 D Loss: 1.3897125720977783, G Loss: 0.6865973472595215\n",
      "Epoch 18, batch 19 D Loss: 1.395003080368042, G Loss: 0.681712806224823\n",
      "Epoch 18, batch 20 D Loss: 1.3811309337615967, G Loss: 0.6953465938568115\n",
      "Epoch 18, batch 21 D Loss: 1.3750864267349243, G Loss: 0.6967929005622864\n",
      "Epoch 18, batch 22 D Loss: 1.3866100311279297, G Loss: 0.6941229104995728\n",
      "Epoch 18, batch 23 D Loss: 1.3767247200012207, G Loss: 0.6933361887931824\n",
      "Epoch 18, batch 24 D Loss: 1.389113187789917, G Loss: 0.691170334815979\n",
      "Epoch 18, batch 25 D Loss: 1.3822503089904785, G Loss: 0.6975794434547424\n",
      "Epoch 18, batch 26 D Loss: 1.3805327415466309, G Loss: 0.6812768578529358\n",
      "Epoch 18, batch 27 D Loss: 1.3998030424118042, G Loss: 0.6786672472953796\n",
      "Epoch 18, batch 28 D Loss: 1.3983073234558105, G Loss: 0.685249388217926\n",
      "Epoch 18, batch 29 D Loss: 1.3918755054473877, G Loss: 0.6861481666564941\n",
      "Epoch 18, batch 30 D Loss: 1.3861238956451416, G Loss: 0.6900702714920044\n",
      "Epoch 18, batch 31 D Loss: 1.4173879623413086, G Loss: 0.6752145290374756\n",
      "Epoch 18, batch 32 D Loss: 1.3824548721313477, G Loss: 0.6945436000823975\n",
      "Epoch 18, batch 33 D Loss: 1.3887660503387451, G Loss: 0.678718090057373\n",
      "Epoch 18, batch 34 D Loss: 1.37624192237854, G Loss: 0.6950751543045044\n",
      "Epoch 18, batch 35 D Loss: 1.3856565952301025, G Loss: 0.6854504346847534\n",
      "Epoch 18, batch 36 D Loss: 1.3897364139556885, G Loss: 0.6867367625236511\n",
      "Epoch 18, batch 37 D Loss: 1.3818395137786865, G Loss: 0.6885035634040833\n",
      "Epoch 18, batch 38 D Loss: 1.3865255117416382, G Loss: 0.6874915957450867\n",
      "Epoch 18, batch 39 D Loss: 1.3887276649475098, G Loss: 0.6944705247879028\n",
      "Epoch 18, batch 40 D Loss: 1.383009672164917, G Loss: 0.6879326701164246\n",
      "Epoch 18, batch 41 D Loss: 1.3813148736953735, G Loss: 0.6914844512939453\n",
      "Epoch 18, batch 42 D Loss: 1.3852633237838745, G Loss: 0.6858915686607361\n",
      "Epoch 18, batch 43 D Loss: 1.3800066709518433, G Loss: 0.6879338026046753\n",
      "Epoch 18, batch 44 D Loss: 1.382554292678833, G Loss: 0.6864820718765259\n",
      "Epoch 18, batch 45 D Loss: 1.3770158290863037, G Loss: 0.6860648989677429\n",
      "Epoch 18, batch 46 D Loss: 1.3815284967422485, G Loss: 0.6862967014312744\n",
      "Epoch 18, batch 47 D Loss: 1.3892827033996582, G Loss: 0.6811267137527466\n",
      "Epoch 18, batch 48 D Loss: 1.377284288406372, G Loss: 0.6842724084854126\n",
      "Epoch 18, batch 49 D Loss: 1.376139760017395, G Loss: 0.6826242208480835\n",
      "Epoch 18, batch 50 D Loss: 1.3921453952789307, G Loss: 0.6795430779457092\n",
      "Epoch 18, batch 51 D Loss: 1.385680913925171, G Loss: 0.6848706603050232\n",
      "Epoch 18, batch 52 D Loss: 1.3750700950622559, G Loss: 0.687997579574585\n",
      "Epoch 18, batch 53 D Loss: 1.385195016860962, G Loss: 0.6833664178848267\n",
      "Epoch 18, batch 54 D Loss: 1.3820959329605103, G Loss: 0.6832956671714783\n",
      "Epoch 18, batch 55 D Loss: 1.3756808042526245, G Loss: 0.6825315952301025\n",
      "Epoch 18, batch 56 D Loss: 1.391817569732666, G Loss: 0.6883463859558105\n",
      "Epoch 18, batch 57 D Loss: 1.382753849029541, G Loss: 0.6895898580551147\n",
      "Epoch 18, batch 58 D Loss: 1.3908045291900635, G Loss: 0.6908454298973083\n",
      "Epoch 18, batch 59 D Loss: 1.3791722059249878, G Loss: 0.6872695088386536\n",
      "Epoch 18, batch 60 D Loss: 1.382265567779541, G Loss: 0.6894749999046326\n",
      "Epoch 18, batch 61 D Loss: 1.3758320808410645, G Loss: 0.693906843662262\n",
      "Epoch 18, batch 62 D Loss: 1.386976718902588, G Loss: 0.6854062676429749\n",
      "Epoch 18, batch 63 D Loss: 1.3842110633850098, G Loss: 0.6876014471054077\n",
      "Epoch 18, batch 64 D Loss: 1.3670344352722168, G Loss: 0.6890798807144165\n",
      "Epoch 18, batch 65 D Loss: 1.3992481231689453, G Loss: 0.6770043969154358\n",
      "Epoch 18, batch 66 D Loss: 1.4047856330871582, G Loss: 0.6804502010345459\n",
      "Epoch 18, batch 67 D Loss: 1.4081940650939941, G Loss: 0.6810894012451172\n",
      "Epoch 18, batch 68 D Loss: 1.3973073959350586, G Loss: 0.6773632764816284\n",
      "Epoch 18, batch 69 D Loss: 1.3769214153289795, G Loss: 0.6893441677093506\n",
      "Epoch 18, batch 70 D Loss: 1.400757074356079, G Loss: 0.691295862197876\n",
      "Epoch 18, batch 71 D Loss: 1.386565923690796, G Loss: 0.6893425583839417\n",
      "Epoch 18, batch 72 D Loss: 1.3718448877334595, G Loss: 0.7014157176017761\n",
      "Epoch 18, batch 73 D Loss: 1.384062647819519, G Loss: 0.6958292126655579\n",
      "Epoch 18, batch 74 D Loss: 1.382864236831665, G Loss: 0.6913642883300781\n",
      "Epoch 18, batch 75 D Loss: 1.3863554000854492, G Loss: 0.6942827701568604\n",
      "Epoch 18, batch 76 D Loss: 1.4071109294891357, G Loss: 0.6770395040512085\n",
      "Epoch 18, batch 77 D Loss: 1.395599126815796, G Loss: 0.6942179799079895\n",
      "Epoch 18, batch 78 D Loss: 1.3858250379562378, G Loss: 0.7035951018333435\n",
      "Epoch 18, batch 79 D Loss: 1.3888659477233887, G Loss: 0.6977083086967468\n",
      "Epoch 18, batch 80 D Loss: 1.3704211711883545, G Loss: 0.7043942809104919\n",
      "Epoch 18, batch 81 D Loss: 1.3735363483428955, G Loss: 0.6995368003845215\n",
      "Epoch 18, batch 82 D Loss: 1.3954598903656006, G Loss: 0.6858845353126526\n",
      "Epoch 18, batch 83 D Loss: 1.3848140239715576, G Loss: 0.6965287923812866\n",
      "Epoch 18, batch 84 D Loss: 1.399416208267212, G Loss: 0.6945985555648804\n",
      "Epoch 18, batch 85 D Loss: 1.3910056352615356, G Loss: 0.6937803626060486\n",
      "Epoch 18, batch 86 D Loss: 1.397310495376587, G Loss: 0.6966776847839355\n",
      "Epoch 18, batch 87 D Loss: 1.4019296169281006, G Loss: 0.6936817169189453\n",
      "Epoch 18, batch 88 D Loss: 1.3732552528381348, G Loss: 0.7121208906173706\n",
      "Epoch 18, batch 89 D Loss: 1.38961660861969, G Loss: 0.7048853039741516\n",
      "Epoch 18, batch 90 D Loss: 1.377326488494873, G Loss: 0.7098895311355591\n",
      "Epoch 18, batch 91 D Loss: 1.397482991218567, G Loss: 0.7093233466148376\n",
      "Epoch 18, batch 92 D Loss: 1.3869590759277344, G Loss: 0.7093334794044495\n",
      "Epoch 18, batch 93 D Loss: 1.3909151554107666, G Loss: 0.7081575989723206\n",
      "Epoch 18, batch 94 D Loss: 1.3804662227630615, G Loss: 0.7118431329727173\n",
      "Epoch 18, batch 95 D Loss: 1.3892265558242798, G Loss: 0.7053501009941101\n",
      "Epoch 18, batch 96 D Loss: 1.3765592575073242, G Loss: 0.7150769233703613\n",
      "Epoch 18, batch 97 D Loss: 1.3913872241973877, G Loss: 0.7127761840820312\n",
      "Epoch 18, batch 98 D Loss: 1.3874714374542236, G Loss: 0.7114658355712891\n",
      "Epoch 18, batch 99 D Loss: 1.3860740661621094, G Loss: 0.7117394804954529\n",
      "Epoch 18, batch 100 D Loss: 1.3948228359222412, G Loss: 0.6984596848487854\n",
      "Epoch 18, batch 101 D Loss: 1.384117603302002, G Loss: 0.7111222147941589\n",
      "Epoch 18, batch 102 D Loss: 1.3779828548431396, G Loss: 0.7145113945007324\n",
      "Epoch 18, batch 103 D Loss: 1.3802342414855957, G Loss: 0.7078890800476074\n",
      "Epoch 18, batch 104 D Loss: 1.383188009262085, G Loss: 0.7101591229438782\n",
      "Epoch 18, batch 105 D Loss: 1.3814892768859863, G Loss: 0.7070726752281189\n",
      "Epoch 18, batch 106 D Loss: 1.3893253803253174, G Loss: 0.7034351825714111\n",
      "Epoch 18, batch 107 D Loss: 1.3855781555175781, G Loss: 0.696950376033783\n",
      "Epoch 18, batch 108 D Loss: 1.383274793624878, G Loss: 0.7021761536598206\n",
      "Epoch 18, batch 109 D Loss: 1.3791913986206055, G Loss: 0.7113252878189087\n",
      "Epoch 18, batch 110 D Loss: 1.3812386989593506, G Loss: 0.7092337012290955\n",
      "Epoch 18, batch 111 D Loss: 1.3854458332061768, G Loss: 0.7084059119224548\n",
      "Epoch 18, batch 112 D Loss: 1.3776755332946777, G Loss: 0.7089391350746155\n",
      "Epoch 18, batch 113 D Loss: 1.3979413509368896, G Loss: 0.6901321411132812\n",
      "Epoch 18, batch 114 D Loss: 1.3822805881500244, G Loss: 0.7087209820747375\n",
      "Epoch 18, batch 115 D Loss: 1.4077417850494385, G Loss: 0.6933784484863281\n",
      "Epoch 18, batch 116 D Loss: 1.3912353515625, G Loss: 0.6930536031723022\n",
      "Epoch 18, batch 117 D Loss: 1.3843281269073486, G Loss: 0.7009479999542236\n",
      "Epoch 18, batch 118 D Loss: 1.3985881805419922, G Loss: 0.6948202252388\n",
      "Epoch 18, batch 119 D Loss: 1.3717012405395508, G Loss: 0.7024399042129517\n",
      "Epoch 18, batch 120 D Loss: 1.3762872219085693, G Loss: 0.6990293264389038\n",
      "Epoch 18, batch 121 D Loss: 1.382119059562683, G Loss: 0.7041940093040466\n",
      "Epoch 18, batch 122 D Loss: 1.397633671760559, G Loss: 0.695686399936676\n",
      "Epoch 18, batch 123 D Loss: 1.3642396926879883, G Loss: 0.7117988467216492\n",
      "Epoch 18, batch 124 D Loss: 1.3823812007904053, G Loss: 0.7023893594741821\n",
      "Epoch 18, batch 125 D Loss: 1.3742132186889648, G Loss: 0.7065266966819763\n",
      "Epoch 18, batch 126 D Loss: 1.3682825565338135, G Loss: 0.7131624817848206\n",
      "Epoch 18, batch 127 D Loss: 1.4057732820510864, G Loss: 0.6847683787345886\n",
      "Epoch 18, batch 128 D Loss: 1.384268879890442, G Loss: 0.6992536783218384\n",
      "Epoch 18, batch 129 D Loss: 1.3830772638320923, G Loss: 0.6968345642089844\n",
      "Epoch 18, batch 130 D Loss: 1.4011144638061523, G Loss: 0.703237771987915\n",
      "Epoch 18, batch 131 D Loss: 1.4060856103897095, G Loss: 0.6916943192481995\n",
      "Epoch 18, batch 132 D Loss: 1.4036250114440918, G Loss: 0.6855321526527405\n",
      "Epoch 18, batch 133 D Loss: 1.3973459005355835, G Loss: 0.6950642466545105\n",
      "Epoch 18, batch 134 D Loss: 1.402059555053711, G Loss: 0.6991358399391174\n",
      "Epoch 18, batch 135 D Loss: 1.3911032676696777, G Loss: 0.7094517350196838\n",
      "Epoch 18, batch 136 D Loss: 1.4018163681030273, G Loss: 0.6818483471870422\n",
      "Epoch 18, batch 137 D Loss: 1.4001498222351074, G Loss: 0.6905092597007751\n",
      "Epoch 18, batch 138 D Loss: 1.3863177299499512, G Loss: 0.7001920938491821\n",
      "Epoch 18, batch 139 D Loss: 1.3950687646865845, G Loss: 0.6840496063232422\n",
      "Epoch 18, batch 140 D Loss: 1.3962297439575195, G Loss: 0.696753740310669\n",
      "Epoch 18, batch 141 D Loss: 1.3999208211898804, G Loss: 0.6945422291755676\n",
      "Epoch 18, batch 142 D Loss: 1.382130742073059, G Loss: 0.6950211524963379\n",
      "Epoch 18, batch 143 D Loss: 1.3789069652557373, G Loss: 0.7000454664230347\n",
      "Epoch 18, batch 144 D Loss: 1.3928104639053345, G Loss: 0.6968792080879211\n",
      "Epoch 18, batch 145 D Loss: 1.383603811264038, G Loss: 0.7110359072685242\n",
      "Epoch 18, batch 146 D Loss: 1.3873622417449951, G Loss: 0.7068390846252441\n",
      "Epoch 18, batch 147 D Loss: 1.3797998428344727, G Loss: 0.7069917917251587\n",
      "Epoch 18, batch 148 D Loss: 1.3970739841461182, G Loss: 0.7048828601837158\n",
      "Epoch 18, batch 149 D Loss: 1.3998897075653076, G Loss: 0.7061495780944824\n",
      "Epoch 18, batch 150 D Loss: 1.3789299726486206, G Loss: 0.7154042720794678\n",
      "Epoch 18, batch 151 D Loss: 1.3874715566635132, G Loss: 0.7143236398696899\n",
      "Epoch 18, batch 152 D Loss: 1.3914904594421387, G Loss: 0.722563624382019\n",
      "Epoch 18, batch 153 D Loss: 1.3962401151657104, G Loss: 0.7113127708435059\n",
      "Epoch 18, batch 154 D Loss: 1.3882412910461426, G Loss: 0.7199755907058716\n",
      "Epoch 18, batch 155 D Loss: 1.3807239532470703, G Loss: 0.7210575342178345\n",
      "Epoch 18, batch 156 D Loss: 1.3907192945480347, G Loss: 0.7184425592422485\n",
      "Epoch 18, batch 157 D Loss: 1.3913525342941284, G Loss: 0.7158104777336121\n",
      "Epoch 18, batch 158 D Loss: 1.3772112131118774, G Loss: 0.7278556823730469\n",
      "Epoch 18, batch 159 D Loss: 1.376871109008789, G Loss: 0.7290496826171875\n",
      "Epoch 18, batch 160 D Loss: 1.3971593379974365, G Loss: 0.7186851501464844\n",
      "Epoch 18, batch 161 D Loss: 1.395720362663269, G Loss: 0.7235365509986877\n",
      "Epoch 18, batch 162 D Loss: 1.3953126668930054, G Loss: 0.7111423015594482\n",
      "Epoch 18, batch 163 D Loss: 1.3822027444839478, G Loss: 0.7201605439186096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, batch 164 D Loss: 1.3939075469970703, G Loss: 0.7109194993972778\n",
      "Epoch 18, batch 165 D Loss: 1.3827275037765503, G Loss: 0.7141678333282471\n",
      "Epoch 18, batch 166 D Loss: 1.3798496723175049, G Loss: 0.7236623167991638\n",
      "Epoch 18, batch 167 D Loss: 1.3848438262939453, G Loss: 0.7118170857429504\n",
      "Epoch 18, batch 168 D Loss: 1.3892159461975098, G Loss: 0.7139450311660767\n",
      "Epoch 18, batch 169 D Loss: 1.3873310089111328, G Loss: 0.6996501088142395\n",
      "Epoch 18, batch 170 D Loss: 1.379212498664856, G Loss: 0.7105209231376648\n",
      "Epoch 18, batch 171 D Loss: 1.3880305290222168, G Loss: 0.6963679790496826\n",
      "Epoch 18, batch 172 D Loss: 1.367170810699463, G Loss: 0.7050542235374451\n",
      "Epoch 18, batch 173 D Loss: 1.387573003768921, G Loss: 0.6925904154777527\n",
      "Epoch 18, batch 174 D Loss: 1.396543264389038, G Loss: 0.6849422454833984\n",
      "Epoch 18, batch 175 D Loss: 1.3901811838150024, G Loss: 0.6837782263755798\n",
      "Epoch 18, batch 176 D Loss: 1.3958452939987183, G Loss: 0.6858213543891907\n",
      "Epoch 18, batch 177 D Loss: 1.3977012634277344, G Loss: 0.6730606555938721\n",
      "Epoch 18, batch 178 D Loss: 1.3735179901123047, G Loss: 0.6893970370292664\n",
      "Epoch 18, batch 179 D Loss: 1.3822228908538818, G Loss: 0.683594286441803\n",
      "Epoch 18, batch 180 D Loss: 1.3802204132080078, G Loss: 0.6856037974357605\n",
      "Epoch 18, batch 181 D Loss: 1.3950490951538086, G Loss: 0.6711171865463257\n",
      "Epoch 18, batch 182 D Loss: 1.3883545398712158, G Loss: 0.6814572215080261\n",
      "Epoch 18, batch 183 D Loss: 1.3995847702026367, G Loss: 0.6776559352874756\n",
      "Epoch 18, batch 184 D Loss: 1.395214557647705, G Loss: 0.6735499501228333\n",
      "Epoch 18, batch 185 D Loss: 1.3923673629760742, G Loss: 0.6720336079597473\n",
      "Epoch 18, batch 186 D Loss: 1.3857579231262207, G Loss: 0.676846444606781\n",
      "Epoch 18, batch 187 D Loss: 1.3880341053009033, G Loss: 0.6709365248680115\n",
      "Epoch 18, batch 188 D Loss: 1.3901245594024658, G Loss: 0.6663177013397217\n",
      "Epoch 18, batch 189 D Loss: 1.3841114044189453, G Loss: 0.6724075078964233\n",
      "Epoch 18, batch 190 D Loss: 1.3844051361083984, G Loss: 0.6759092211723328\n",
      "Epoch 18, batch 191 D Loss: 1.3843369483947754, G Loss: 0.6774978637695312\n",
      "Epoch 18, batch 192 D Loss: 1.3937392234802246, G Loss: 0.6723812222480774\n",
      "Epoch 18, batch 193 D Loss: 1.3863493204116821, G Loss: 0.6760979294776917\n",
      "Epoch 18, batch 194 D Loss: 1.3800296783447266, G Loss: 0.6798275113105774\n",
      "Epoch 18, batch 195 D Loss: 1.386212944984436, G Loss: 0.6736549139022827\n",
      "Epoch 18, batch 196 D Loss: 1.3899857997894287, G Loss: 0.6723014712333679\n",
      "Epoch 18, batch 197 D Loss: 1.3827650547027588, G Loss: 0.6808577179908752\n",
      "Epoch 18, batch 198 D Loss: 1.3859862089157104, G Loss: 0.6836981177330017\n",
      "Epoch 18, batch 199 D Loss: 1.374877691268921, G Loss: 0.6880773901939392\n",
      "Epoch 18, batch 200 D Loss: 1.3983078002929688, G Loss: 0.6709161400794983\n",
      "Epoch 19, batch 1 D Loss: 1.3795299530029297, G Loss: 0.6852073073387146\n",
      "Epoch 19, batch 2 D Loss: 1.3795325756072998, G Loss: 0.6897730827331543\n",
      "Epoch 19, batch 3 D Loss: 1.3912384510040283, G Loss: 0.6757610440254211\n",
      "Epoch 19, batch 4 D Loss: 1.3863513469696045, G Loss: 0.6846024394035339\n",
      "Epoch 19, batch 5 D Loss: 1.38565194606781, G Loss: 0.6843519806861877\n",
      "Epoch 19, batch 6 D Loss: 1.391519546508789, G Loss: 0.688332200050354\n",
      "Epoch 19, batch 7 D Loss: 1.387422800064087, G Loss: 0.6873666644096375\n",
      "Epoch 19, batch 8 D Loss: 1.3906781673431396, G Loss: 0.6911270618438721\n",
      "Epoch 19, batch 9 D Loss: 1.381291389465332, G Loss: 0.6933514475822449\n",
      "Epoch 19, batch 10 D Loss: 1.3948370218276978, G Loss: 0.6978746056556702\n",
      "Epoch 19, batch 11 D Loss: 1.390940546989441, G Loss: 0.6927940249443054\n",
      "Epoch 19, batch 12 D Loss: 1.3962290287017822, G Loss: 0.6863918900489807\n",
      "Epoch 19, batch 13 D Loss: 1.3902740478515625, G Loss: 0.6978376507759094\n",
      "Epoch 19, batch 14 D Loss: 1.384903907775879, G Loss: 0.6937574148178101\n",
      "Epoch 19, batch 15 D Loss: 1.3873103857040405, G Loss: 0.6848346590995789\n",
      "Epoch 19, batch 16 D Loss: 1.3855290412902832, G Loss: 0.6918154954910278\n",
      "Epoch 19, batch 17 D Loss: 1.385572910308838, G Loss: 0.6932084560394287\n",
      "Epoch 19, batch 18 D Loss: 1.389662504196167, G Loss: 0.6882795691490173\n",
      "Epoch 19, batch 19 D Loss: 1.3828670978546143, G Loss: 0.6900932788848877\n",
      "Epoch 19, batch 20 D Loss: 1.3785924911499023, G Loss: 0.6898455023765564\n",
      "Epoch 19, batch 21 D Loss: 1.3914024829864502, G Loss: 0.6853906512260437\n",
      "Epoch 19, batch 22 D Loss: 1.3851858377456665, G Loss: 0.6871983408927917\n",
      "Epoch 19, batch 23 D Loss: 1.3828428983688354, G Loss: 0.6888921856880188\n",
      "Epoch 19, batch 24 D Loss: 1.3864766359329224, G Loss: 0.6830549836158752\n",
      "Epoch 19, batch 25 D Loss: 1.3885200023651123, G Loss: 0.6794201731681824\n",
      "Epoch 19, batch 26 D Loss: 1.3811148405075073, G Loss: 0.6823751330375671\n",
      "Epoch 19, batch 27 D Loss: 1.378231406211853, G Loss: 0.6867044568061829\n",
      "Epoch 19, batch 28 D Loss: 1.3912901878356934, G Loss: 0.6821362376213074\n",
      "Epoch 19, batch 29 D Loss: 1.3796969652175903, G Loss: 0.6857351064682007\n",
      "Epoch 19, batch 30 D Loss: 1.3860758543014526, G Loss: 0.6841095089912415\n",
      "Epoch 19, batch 31 D Loss: 1.3768160343170166, G Loss: 0.6883405447006226\n",
      "Epoch 19, batch 32 D Loss: 1.3850226402282715, G Loss: 0.6826509237289429\n",
      "Epoch 19, batch 33 D Loss: 1.3874802589416504, G Loss: 0.6861824989318848\n",
      "Epoch 19, batch 34 D Loss: 1.3895602226257324, G Loss: 0.6831814050674438\n",
      "Epoch 19, batch 35 D Loss: 1.3790525197982788, G Loss: 0.6869388818740845\n",
      "Epoch 19, batch 36 D Loss: 1.383165717124939, G Loss: 0.6886740326881409\n",
      "Epoch 19, batch 37 D Loss: 1.382469892501831, G Loss: 0.6877484917640686\n",
      "Epoch 19, batch 38 D Loss: 1.3913154602050781, G Loss: 0.6893961429595947\n",
      "Epoch 19, batch 39 D Loss: 1.3884952068328857, G Loss: 0.6871967315673828\n",
      "Epoch 19, batch 40 D Loss: 1.3870294094085693, G Loss: 0.6858248114585876\n",
      "Epoch 19, batch 41 D Loss: 1.381819725036621, G Loss: 0.6904232501983643\n",
      "Epoch 19, batch 42 D Loss: 1.3852479457855225, G Loss: 0.6888481378555298\n",
      "Epoch 19, batch 43 D Loss: 1.380399465560913, G Loss: 0.6885786652565002\n",
      "Epoch 19, batch 44 D Loss: 1.388597846031189, G Loss: 0.692904531955719\n",
      "Epoch 19, batch 45 D Loss: 1.3873332738876343, G Loss: 0.6890789866447449\n",
      "Epoch 19, batch 46 D Loss: 1.3796172142028809, G Loss: 0.6923372149467468\n",
      "Epoch 19, batch 47 D Loss: 1.3891147375106812, G Loss: 0.6895163059234619\n",
      "Epoch 19, batch 48 D Loss: 1.3889544010162354, G Loss: 0.6861618161201477\n",
      "Epoch 19, batch 49 D Loss: 1.3650110960006714, G Loss: 0.7074407339096069\n",
      "Epoch 19, batch 50 D Loss: 1.3866307735443115, G Loss: 0.6902326345443726\n",
      "Epoch 19, batch 51 D Loss: 1.3768095970153809, G Loss: 0.7003309726715088\n",
      "Epoch 19, batch 52 D Loss: 1.3678644895553589, G Loss: 0.7031081914901733\n",
      "Epoch 19, batch 53 D Loss: 1.3900647163391113, G Loss: 0.6932070851325989\n",
      "Epoch 19, batch 54 D Loss: 1.3884161710739136, G Loss: 0.6974064707756042\n",
      "Epoch 19, batch 55 D Loss: 1.3921500444412231, G Loss: 0.6934890747070312\n",
      "Epoch 19, batch 56 D Loss: 1.366982340812683, G Loss: 0.7052519917488098\n",
      "Epoch 19, batch 57 D Loss: 1.3771138191223145, G Loss: 0.69976806640625\n",
      "Epoch 19, batch 58 D Loss: 1.3778170347213745, G Loss: 0.6953862905502319\n",
      "Epoch 19, batch 59 D Loss: 1.3747397661209106, G Loss: 0.6914279460906982\n",
      "Epoch 19, batch 60 D Loss: 1.3819268941879272, G Loss: 0.7012417316436768\n",
      "Epoch 19, batch 61 D Loss: 1.4073926210403442, G Loss: 0.6819431185722351\n",
      "Epoch 19, batch 62 D Loss: 1.3703588247299194, G Loss: 0.6932304501533508\n",
      "Epoch 19, batch 63 D Loss: 1.4047189950942993, G Loss: 0.678240180015564\n",
      "Epoch 19, batch 64 D Loss: 1.3764736652374268, G Loss: 0.7003231048583984\n",
      "Epoch 19, batch 65 D Loss: 1.3689980506896973, G Loss: 0.7068836688995361\n",
      "Epoch 19, batch 66 D Loss: 1.3943088054656982, G Loss: 0.6823503971099854\n",
      "Epoch 19, batch 67 D Loss: 1.389571189880371, G Loss: 0.6803203821182251\n",
      "Epoch 19, batch 68 D Loss: 1.3835769891738892, G Loss: 0.6848185658454895\n",
      "Epoch 19, batch 69 D Loss: 1.4081932306289673, G Loss: 0.6896740794181824\n",
      "Epoch 19, batch 70 D Loss: 1.3856638669967651, G Loss: 0.7063218951225281\n",
      "Epoch 19, batch 71 D Loss: 1.4124560356140137, G Loss: 0.6930132508277893\n",
      "Epoch 19, batch 72 D Loss: 1.414977788925171, G Loss: 0.6906667947769165\n",
      "Epoch 19, batch 73 D Loss: 1.3880864381790161, G Loss: 0.7008519172668457\n",
      "Epoch 19, batch 74 D Loss: 1.377863883972168, G Loss: 0.716559886932373\n",
      "Epoch 19, batch 75 D Loss: 1.3973026275634766, G Loss: 0.7061792612075806\n",
      "Epoch 19, batch 76 D Loss: 1.3792953491210938, G Loss: 0.7085790038108826\n",
      "Epoch 19, batch 77 D Loss: 1.3912827968597412, G Loss: 0.7065080404281616\n",
      "Epoch 19, batch 78 D Loss: 1.3889265060424805, G Loss: 0.710712194442749\n",
      "Epoch 19, batch 79 D Loss: 1.39640474319458, G Loss: 0.705594003200531\n",
      "Epoch 19, batch 80 D Loss: 1.3888001441955566, G Loss: 0.7093803882598877\n",
      "Epoch 19, batch 81 D Loss: 1.4025578498840332, G Loss: 0.6994471549987793\n",
      "Epoch 19, batch 82 D Loss: 1.3846163749694824, G Loss: 0.7074059844017029\n",
      "Epoch 19, batch 83 D Loss: 1.3878593444824219, G Loss: 0.702080488204956\n",
      "Epoch 19, batch 84 D Loss: 1.3833587169647217, G Loss: 0.7052969336509705\n",
      "Epoch 19, batch 85 D Loss: 1.394749641418457, G Loss: 0.697844922542572\n",
      "Epoch 19, batch 86 D Loss: 1.39413583278656, G Loss: 0.702074408531189\n",
      "Epoch 19, batch 87 D Loss: 1.3865206241607666, G Loss: 0.7033495903015137\n",
      "Epoch 19, batch 88 D Loss: 1.3855235576629639, G Loss: 0.7024083137512207\n",
      "Epoch 19, batch 89 D Loss: 1.3843977451324463, G Loss: 0.7054494619369507\n",
      "Epoch 19, batch 90 D Loss: 1.3886516094207764, G Loss: 0.7101892232894897\n",
      "Epoch 19, batch 91 D Loss: 1.3927853107452393, G Loss: 0.7012875080108643\n",
      "Epoch 19, batch 92 D Loss: 1.3860397338867188, G Loss: 0.7068738341331482\n",
      "Epoch 19, batch 93 D Loss: 1.3739938735961914, G Loss: 0.7137001752853394\n",
      "Epoch 19, batch 94 D Loss: 1.3841924667358398, G Loss: 0.7016561031341553\n",
      "Epoch 19, batch 95 D Loss: 1.3816412687301636, G Loss: 0.7001216411590576\n",
      "Epoch 19, batch 96 D Loss: 1.3859354257583618, G Loss: 0.706693708896637\n",
      "Epoch 19, batch 97 D Loss: 1.3803327083587646, G Loss: 0.7100926041603088\n",
      "Epoch 19, batch 98 D Loss: 1.381089687347412, G Loss: 0.7018821835517883\n",
      "Epoch 19, batch 99 D Loss: 1.370485544204712, G Loss: 0.7148086428642273\n",
      "Epoch 19, batch 100 D Loss: 1.3932929039001465, G Loss: 0.6912529468536377\n",
      "Epoch 19, batch 101 D Loss: 1.364977240562439, G Loss: 0.70296311378479\n",
      "Epoch 19, batch 102 D Loss: 1.3830804824829102, G Loss: 0.6947175860404968\n",
      "Epoch 19, batch 103 D Loss: 1.3884954452514648, G Loss: 0.6891329288482666\n",
      "Epoch 19, batch 104 D Loss: 1.3896067142486572, G Loss: 0.6975997090339661\n",
      "Epoch 19, batch 105 D Loss: 1.3947430849075317, G Loss: 0.6891272068023682\n",
      "Epoch 19, batch 106 D Loss: 1.38289475440979, G Loss: 0.6876986026763916\n",
      "Epoch 19, batch 107 D Loss: 1.3970907926559448, G Loss: 0.6927686929702759\n",
      "Epoch 19, batch 108 D Loss: 1.3721163272857666, G Loss: 0.7050336599349976\n",
      "Epoch 19, batch 109 D Loss: 1.4119116067886353, G Loss: 0.6822266578674316\n",
      "Epoch 19, batch 110 D Loss: 1.3730874061584473, G Loss: 0.6955673098564148\n",
      "Epoch 19, batch 111 D Loss: 1.3957362174987793, G Loss: 0.6942487955093384\n",
      "Epoch 19, batch 112 D Loss: 1.3730061054229736, G Loss: 0.703520655632019\n",
      "Epoch 19, batch 113 D Loss: 1.3764967918395996, G Loss: 0.7053586840629578\n",
      "Epoch 19, batch 114 D Loss: 1.3820786476135254, G Loss: 0.7003771066665649\n",
      "Epoch 19, batch 115 D Loss: 1.3965704441070557, G Loss: 0.6969661116600037\n",
      "Epoch 19, batch 116 D Loss: 1.3803973197937012, G Loss: 0.6957586407661438\n",
      "Epoch 19, batch 117 D Loss: 1.36891508102417, G Loss: 0.7121248841285706\n",
      "Epoch 19, batch 118 D Loss: 1.3746994733810425, G Loss: 0.7161991596221924\n",
      "Epoch 19, batch 119 D Loss: 1.3868234157562256, G Loss: 0.7097285389900208\n",
      "Epoch 19, batch 120 D Loss: 1.385258674621582, G Loss: 0.7167124152183533\n",
      "Epoch 19, batch 121 D Loss: 1.3886709213256836, G Loss: 0.7069915533065796\n",
      "Epoch 19, batch 122 D Loss: 1.3855223655700684, G Loss: 0.7045889496803284\n",
      "Epoch 19, batch 123 D Loss: 1.3657793998718262, G Loss: 0.7264928221702576\n",
      "Epoch 19, batch 124 D Loss: 1.4031869173049927, G Loss: 0.6973375678062439\n",
      "Epoch 19, batch 125 D Loss: 1.39053475856781, G Loss: 0.7007187604904175\n",
      "Epoch 19, batch 126 D Loss: 1.3953832387924194, G Loss: 0.6965574622154236\n",
      "Epoch 19, batch 127 D Loss: 1.3960754871368408, G Loss: 0.6973308324813843\n",
      "Epoch 19, batch 128 D Loss: 1.3881137371063232, G Loss: 0.7033501267433167\n",
      "Epoch 19, batch 129 D Loss: 1.3991329669952393, G Loss: 0.6909706592559814\n",
      "Epoch 19, batch 130 D Loss: 1.3974100351333618, G Loss: 0.6922032237052917\n",
      "Epoch 19, batch 131 D Loss: 1.3911545276641846, G Loss: 0.6982674598693848\n",
      "Epoch 19, batch 132 D Loss: 1.3948602676391602, G Loss: 0.6912751197814941\n",
      "Epoch 19, batch 133 D Loss: 1.3794114589691162, G Loss: 0.698560893535614\n",
      "Epoch 19, batch 134 D Loss: 1.3831136226654053, G Loss: 0.697422206401825\n",
      "Epoch 19, batch 135 D Loss: 1.3895612955093384, G Loss: 0.6919903755187988\n",
      "Epoch 19, batch 136 D Loss: 1.3855270147323608, G Loss: 0.6959319114685059\n",
      "Epoch 19, batch 137 D Loss: 1.3933305740356445, G Loss: 0.7002467513084412\n",
      "Epoch 19, batch 138 D Loss: 1.388081431388855, G Loss: 0.7012261748313904\n",
      "Epoch 19, batch 139 D Loss: 1.3876264095306396, G Loss: 0.6932215094566345\n",
      "Epoch 19, batch 140 D Loss: 1.393230676651001, G Loss: 0.6954532861709595\n",
      "Epoch 19, batch 141 D Loss: 1.3832714557647705, G Loss: 0.6974536180496216\n",
      "Epoch 19, batch 142 D Loss: 1.393669843673706, G Loss: 0.6895399689674377\n",
      "Epoch 19, batch 143 D Loss: 1.3840478658676147, G Loss: 0.6942087411880493\n",
      "Epoch 19, batch 144 D Loss: 1.3765366077423096, G Loss: 0.6980928778648376\n",
      "Epoch 19, batch 145 D Loss: 1.3859467506408691, G Loss: 0.6973473429679871\n",
      "Epoch 19, batch 146 D Loss: 1.3882827758789062, G Loss: 0.6902139782905579\n",
      "Epoch 19, batch 147 D Loss: 1.3912830352783203, G Loss: 0.6901372671127319\n",
      "Epoch 19, batch 148 D Loss: 1.3875317573547363, G Loss: 0.6925297379493713\n",
      "Epoch 19, batch 149 D Loss: 1.3878943920135498, G Loss: 0.6877607107162476\n",
      "Epoch 19, batch 150 D Loss: 1.387629508972168, G Loss: 0.6835000514984131\n",
      "Epoch 19, batch 151 D Loss: 1.3850088119506836, G Loss: 0.6918098330497742\n",
      "Epoch 19, batch 152 D Loss: 1.3858942985534668, G Loss: 0.6869157552719116\n",
      "Epoch 19, batch 153 D Loss: 1.3800816535949707, G Loss: 0.687404453754425\n",
      "Epoch 19, batch 154 D Loss: 1.378950595855713, G Loss: 0.6903753876686096\n",
      "Epoch 19, batch 155 D Loss: 1.399376392364502, G Loss: 0.6775929927825928\n",
      "Epoch 19, batch 156 D Loss: 1.3867697715759277, G Loss: 0.6835484504699707\n",
      "Epoch 19, batch 157 D Loss: 1.384822964668274, G Loss: 0.6891264915466309\n",
      "Epoch 19, batch 158 D Loss: 1.3807742595672607, G Loss: 0.6814624071121216\n",
      "Epoch 19, batch 159 D Loss: 1.377248764038086, G Loss: 0.6913799047470093\n",
      "Epoch 19, batch 160 D Loss: 1.380152940750122, G Loss: 0.6883236765861511\n",
      "Epoch 19, batch 161 D Loss: 1.3764008283615112, G Loss: 0.6972714066505432\n",
      "Epoch 19, batch 162 D Loss: 1.380733609199524, G Loss: 0.6854986548423767\n",
      "Epoch 19, batch 163 D Loss: 1.3774080276489258, G Loss: 0.6897773146629333\n",
      "Epoch 19, batch 164 D Loss: 1.3913377523422241, G Loss: 0.6826173663139343\n",
      "Epoch 19, batch 165 D Loss: 1.3915117979049683, G Loss: 0.6863981485366821\n",
      "Epoch 19, batch 166 D Loss: 1.3939772844314575, G Loss: 0.6830140948295593\n",
      "Epoch 19, batch 167 D Loss: 1.3857018947601318, G Loss: 0.6869458556175232\n",
      "Epoch 19, batch 168 D Loss: 1.3860793113708496, G Loss: 0.6870432496070862\n",
      "Epoch 19, batch 169 D Loss: 1.4068338871002197, G Loss: 0.6814542412757874\n",
      "Epoch 19, batch 170 D Loss: 1.384782314300537, G Loss: 0.6867079734802246\n",
      "Epoch 19, batch 171 D Loss: 1.3782415390014648, G Loss: 0.6943531632423401\n",
      "Epoch 19, batch 172 D Loss: 1.380663275718689, G Loss: 0.6902096271514893\n",
      "Epoch 19, batch 173 D Loss: 1.3867734670639038, G Loss: 0.6926449537277222\n",
      "Epoch 19, batch 174 D Loss: 1.37044358253479, G Loss: 0.7038676738739014\n",
      "Epoch 19, batch 175 D Loss: 1.388681411743164, G Loss: 0.6946187019348145\n",
      "Epoch 19, batch 176 D Loss: 1.3765121698379517, G Loss: 0.6964780688285828\n",
      "Epoch 19, batch 177 D Loss: 1.388113021850586, G Loss: 0.6909413933753967\n",
      "Epoch 19, batch 178 D Loss: 1.3786969184875488, G Loss: 0.6994321346282959\n",
      "Epoch 19, batch 179 D Loss: 1.3795733451843262, G Loss: 0.692291259765625\n",
      "Epoch 19, batch 180 D Loss: 1.385171890258789, G Loss: 0.6919146180152893\n",
      "Epoch 19, batch 181 D Loss: 1.3677568435668945, G Loss: 0.7048376202583313\n",
      "Epoch 19, batch 182 D Loss: 1.3846821784973145, G Loss: 0.6986129283905029\n",
      "Epoch 19, batch 183 D Loss: 1.404807209968567, G Loss: 0.6835371255874634\n",
      "Epoch 19, batch 184 D Loss: 1.3886082172393799, G Loss: 0.7027941346168518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, batch 185 D Loss: 1.4006853103637695, G Loss: 0.6896684765815735\n",
      "Epoch 19, batch 186 D Loss: 1.3877758979797363, G Loss: 0.6906479597091675\n",
      "Epoch 19, batch 187 D Loss: 1.3990626335144043, G Loss: 0.6973207592964172\n",
      "Epoch 19, batch 188 D Loss: 1.3837904930114746, G Loss: 0.7001205682754517\n",
      "Epoch 19, batch 189 D Loss: 1.3940123319625854, G Loss: 0.6931362152099609\n",
      "Epoch 19, batch 190 D Loss: 1.3719768524169922, G Loss: 0.7058193683624268\n",
      "Epoch 19, batch 191 D Loss: 1.3842426538467407, G Loss: 0.7007827162742615\n",
      "Epoch 19, batch 192 D Loss: 1.3960976600646973, G Loss: 0.6966503858566284\n",
      "Epoch 19, batch 193 D Loss: 1.3779385089874268, G Loss: 0.7109968066215515\n",
      "Epoch 19, batch 194 D Loss: 1.3977493047714233, G Loss: 0.6932820081710815\n",
      "Epoch 19, batch 195 D Loss: 1.3985164165496826, G Loss: 0.6978277564048767\n",
      "Epoch 19, batch 196 D Loss: 1.3854317665100098, G Loss: 0.7021507024765015\n",
      "Epoch 19, batch 197 D Loss: 1.3811984062194824, G Loss: 0.6951602101325989\n",
      "Epoch 19, batch 198 D Loss: 1.381941795349121, G Loss: 0.703783392906189\n",
      "Epoch 19, batch 199 D Loss: 1.3940891027450562, G Loss: 0.6961669921875\n",
      "Epoch 19, batch 200 D Loss: 1.3882864713668823, G Loss: 0.6971991062164307\n",
      "Epoch 20, batch 1 D Loss: 1.3725056648254395, G Loss: 0.7040303945541382\n",
      "Epoch 20, batch 2 D Loss: 1.3848087787628174, G Loss: 0.7039648294448853\n",
      "Epoch 20, batch 3 D Loss: 1.3771421909332275, G Loss: 0.70684814453125\n",
      "Epoch 20, batch 4 D Loss: 1.3855023384094238, G Loss: 0.7006253004074097\n",
      "Epoch 20, batch 5 D Loss: 1.3930144309997559, G Loss: 0.6902632117271423\n",
      "Epoch 20, batch 6 D Loss: 1.3797029256820679, G Loss: 0.7063248157501221\n",
      "Epoch 20, batch 7 D Loss: 1.3826435804367065, G Loss: 0.7014434933662415\n",
      "Epoch 20, batch 8 D Loss: 1.39250910282135, G Loss: 0.7021847367286682\n",
      "Epoch 20, batch 9 D Loss: 1.3795621395111084, G Loss: 0.7021706104278564\n",
      "Epoch 20, batch 10 D Loss: 1.3912091255187988, G Loss: 0.7077909111976624\n",
      "Epoch 20, batch 11 D Loss: 1.3897162675857544, G Loss: 0.6949414014816284\n",
      "Epoch 20, batch 12 D Loss: 1.3818658590316772, G Loss: 0.7085304856300354\n",
      "Epoch 20, batch 13 D Loss: 1.375945806503296, G Loss: 0.7130797505378723\n",
      "Epoch 20, batch 14 D Loss: 1.3937922716140747, G Loss: 0.7044310569763184\n",
      "Epoch 20, batch 15 D Loss: 1.3971021175384521, G Loss: 0.695639967918396\n",
      "Epoch 20, batch 16 D Loss: 1.3932383060455322, G Loss: 0.705159604549408\n",
      "Epoch 20, batch 17 D Loss: 1.3877731561660767, G Loss: 0.711957573890686\n",
      "Epoch 20, batch 18 D Loss: 1.390954852104187, G Loss: 0.7069973945617676\n",
      "Epoch 20, batch 19 D Loss: 1.396233081817627, G Loss: 0.7021753191947937\n",
      "Epoch 20, batch 20 D Loss: 1.3921746015548706, G Loss: 0.6963092088699341\n",
      "Epoch 20, batch 21 D Loss: 1.3929548263549805, G Loss: 0.6967836022377014\n",
      "Epoch 20, batch 22 D Loss: 1.398327112197876, G Loss: 0.6933643221855164\n",
      "Epoch 20, batch 23 D Loss: 1.387199878692627, G Loss: 0.6985020637512207\n",
      "Epoch 20, batch 24 D Loss: 1.3930447101593018, G Loss: 0.6881365776062012\n",
      "Epoch 20, batch 25 D Loss: 1.3802673816680908, G Loss: 0.7012179493904114\n",
      "Epoch 20, batch 26 D Loss: 1.3905168771743774, G Loss: 0.6960868835449219\n",
      "Epoch 20, batch 27 D Loss: 1.393507719039917, G Loss: 0.6874563097953796\n",
      "Epoch 20, batch 28 D Loss: 1.3871996402740479, G Loss: 0.6926899552345276\n",
      "Epoch 20, batch 29 D Loss: 1.384511947631836, G Loss: 0.6946367025375366\n",
      "Epoch 20, batch 30 D Loss: 1.3933427333831787, G Loss: 0.688183069229126\n",
      "Epoch 20, batch 31 D Loss: 1.3738951683044434, G Loss: 0.6986430883407593\n",
      "Epoch 20, batch 32 D Loss: 1.3833742141723633, G Loss: 0.6977566480636597\n",
      "Epoch 20, batch 33 D Loss: 1.3910713195800781, G Loss: 0.6836369037628174\n",
      "Epoch 20, batch 34 D Loss: 1.3789193630218506, G Loss: 0.6932342648506165\n",
      "Epoch 20, batch 35 D Loss: 1.3911106586456299, G Loss: 0.687483549118042\n",
      "Epoch 20, batch 36 D Loss: 1.3871746063232422, G Loss: 0.6908401250839233\n",
      "Epoch 20, batch 37 D Loss: 1.3866727352142334, G Loss: 0.6885236501693726\n",
      "Epoch 20, batch 38 D Loss: 1.370420217514038, G Loss: 0.7072904706001282\n",
      "Epoch 20, batch 39 D Loss: 1.392564296722412, G Loss: 0.7007076740264893\n",
      "Epoch 20, batch 40 D Loss: 1.39906907081604, G Loss: 0.6944168210029602\n",
      "Epoch 20, batch 41 D Loss: 1.3884243965148926, G Loss: 0.6971995830535889\n",
      "Epoch 20, batch 42 D Loss: 1.3908315896987915, G Loss: 0.6933654546737671\n",
      "Epoch 20, batch 43 D Loss: 1.379495620727539, G Loss: 0.708452045917511\n",
      "Epoch 20, batch 44 D Loss: 1.3927502632141113, G Loss: 0.6943383812904358\n",
      "Epoch 20, batch 45 D Loss: 1.3915380239486694, G Loss: 0.7049747705459595\n",
      "Epoch 20, batch 46 D Loss: 1.3950574398040771, G Loss: 0.6956632137298584\n",
      "Epoch 20, batch 47 D Loss: 1.4058294296264648, G Loss: 0.6924623250961304\n",
      "Epoch 20, batch 48 D Loss: 1.3919339179992676, G Loss: 0.7053365111351013\n",
      "Epoch 20, batch 49 D Loss: 1.3982560634613037, G Loss: 0.6989438533782959\n",
      "Epoch 20, batch 50 D Loss: 1.3927080631256104, G Loss: 0.6989914178848267\n",
      "Epoch 20, batch 51 D Loss: 1.3889530897140503, G Loss: 0.6990007162094116\n",
      "Epoch 20, batch 52 D Loss: 1.3937870264053345, G Loss: 0.6976528763771057\n",
      "Epoch 20, batch 53 D Loss: 1.3881280422210693, G Loss: 0.703988790512085\n",
      "Epoch 20, batch 54 D Loss: 1.3832776546478271, G Loss: 0.7011628746986389\n",
      "Epoch 20, batch 55 D Loss: 1.3852388858795166, G Loss: 0.6976562738418579\n",
      "Epoch 20, batch 56 D Loss: 1.3860636949539185, G Loss: 0.6963049173355103\n",
      "Epoch 20, batch 57 D Loss: 1.3913309574127197, G Loss: 0.6946094036102295\n",
      "Epoch 20, batch 58 D Loss: 1.3824541568756104, G Loss: 0.6978099346160889\n",
      "Epoch 20, batch 59 D Loss: 1.3843975067138672, G Loss: 0.7015087604522705\n",
      "Epoch 20, batch 60 D Loss: 1.382979154586792, G Loss: 0.7015817761421204\n",
      "Epoch 20, batch 61 D Loss: 1.3808245658874512, G Loss: 0.699800968170166\n",
      "Epoch 20, batch 62 D Loss: 1.3952523469924927, G Loss: 0.6941964626312256\n",
      "Epoch 20, batch 63 D Loss: 1.3767173290252686, G Loss: 0.704425036907196\n",
      "Epoch 20, batch 64 D Loss: 1.3847582340240479, G Loss: 0.6971120238304138\n",
      "Epoch 20, batch 65 D Loss: 1.381957769393921, G Loss: 0.7026910185813904\n",
      "Epoch 20, batch 66 D Loss: 1.3870415687561035, G Loss: 0.7020462155342102\n",
      "Epoch 20, batch 67 D Loss: 1.3933091163635254, G Loss: 0.6996275186538696\n",
      "Epoch 20, batch 68 D Loss: 1.388637661933899, G Loss: 0.6942188143730164\n",
      "Epoch 20, batch 69 D Loss: 1.388139009475708, G Loss: 0.7027862668037415\n",
      "Epoch 20, batch 70 D Loss: 1.395117998123169, G Loss: 0.6860332489013672\n",
      "Epoch 20, batch 71 D Loss: 1.3784433603286743, G Loss: 0.7028209567070007\n",
      "Epoch 20, batch 72 D Loss: 1.4016985893249512, G Loss: 0.6887792348861694\n",
      "Epoch 20, batch 73 D Loss: 1.3742725849151611, G Loss: 0.7116503119468689\n",
      "Epoch 20, batch 74 D Loss: 1.393608570098877, G Loss: 0.6872140765190125\n",
      "Epoch 20, batch 75 D Loss: 1.3677866458892822, G Loss: 0.7013246417045593\n",
      "Epoch 20, batch 76 D Loss: 1.3889405727386475, G Loss: 0.7135017514228821\n",
      "Epoch 20, batch 77 D Loss: 1.3772673606872559, G Loss: 0.7026311755180359\n",
      "Epoch 20, batch 78 D Loss: 1.3934149742126465, G Loss: 0.68684983253479\n",
      "Epoch 20, batch 79 D Loss: 1.3718199729919434, G Loss: 0.706150233745575\n",
      "Epoch 20, batch 80 D Loss: 1.380876064300537, G Loss: 0.7075080871582031\n",
      "Epoch 20, batch 81 D Loss: 1.3737599849700928, G Loss: 0.7021432518959045\n",
      "Epoch 20, batch 82 D Loss: 1.3818562030792236, G Loss: 0.7126280069351196\n",
      "Epoch 20, batch 83 D Loss: 1.3843282461166382, G Loss: 0.6933093070983887\n",
      "Epoch 20, batch 84 D Loss: 1.3654977083206177, G Loss: 0.6990150213241577\n",
      "Epoch 20, batch 85 D Loss: 1.373854637145996, G Loss: 0.710120677947998\n",
      "Epoch 20, batch 86 D Loss: 1.36317777633667, G Loss: 0.714205801486969\n",
      "Epoch 20, batch 87 D Loss: 1.3797461986541748, G Loss: 0.6915867328643799\n",
      "Epoch 20, batch 88 D Loss: 1.38346529006958, G Loss: 0.6926348209381104\n",
      "Epoch 20, batch 89 D Loss: 1.3999649286270142, G Loss: 0.6935880184173584\n",
      "Epoch 20, batch 90 D Loss: 1.388498306274414, G Loss: 0.6892786622047424\n",
      "Epoch 20, batch 91 D Loss: 1.3933517932891846, G Loss: 0.6976903676986694\n",
      "Epoch 20, batch 92 D Loss: 1.3980085849761963, G Loss: 0.7109898328781128\n",
      "Epoch 20, batch 93 D Loss: 1.3724346160888672, G Loss: 0.6951454281806946\n",
      "Epoch 20, batch 94 D Loss: 1.3969191312789917, G Loss: 0.6893452405929565\n",
      "Epoch 20, batch 95 D Loss: 1.4057178497314453, G Loss: 0.6854214668273926\n",
      "Epoch 20, batch 96 D Loss: 1.391733169555664, G Loss: 0.688391923904419\n",
      "Epoch 20, batch 97 D Loss: 1.4102144241333008, G Loss: 0.6707578897476196\n",
      "Epoch 20, batch 98 D Loss: 1.3736084699630737, G Loss: 0.689257025718689\n",
      "Epoch 20, batch 99 D Loss: 1.4253952503204346, G Loss: 0.6829091906547546\n",
      "Epoch 20, batch 100 D Loss: 1.3855702877044678, G Loss: 0.6879271864891052\n",
      "Epoch 20, batch 101 D Loss: 1.3959197998046875, G Loss: 0.6835604310035706\n",
      "Epoch 20, batch 102 D Loss: 1.3892223834991455, G Loss: 0.6823086738586426\n",
      "Epoch 20, batch 103 D Loss: 1.3939182758331299, G Loss: 0.6868395209312439\n",
      "Epoch 20, batch 104 D Loss: 1.390518069267273, G Loss: 0.6756458878517151\n",
      "Epoch 20, batch 105 D Loss: 1.3979507684707642, G Loss: 0.6756374835968018\n",
      "Epoch 20, batch 106 D Loss: 1.3888030052185059, G Loss: 0.6795107126235962\n",
      "Epoch 20, batch 107 D Loss: 1.40256667137146, G Loss: 0.6809948682785034\n",
      "Epoch 20, batch 108 D Loss: 1.3882414102554321, G Loss: 0.6803019046783447\n",
      "Epoch 20, batch 109 D Loss: 1.3784887790679932, G Loss: 0.6872360110282898\n",
      "Epoch 20, batch 110 D Loss: 1.3862855434417725, G Loss: 0.6838579773902893\n",
      "Epoch 20, batch 111 D Loss: 1.3941491842269897, G Loss: 0.690473198890686\n",
      "Epoch 20, batch 112 D Loss: 1.3951201438903809, G Loss: 0.6863906383514404\n",
      "Epoch 20, batch 113 D Loss: 1.394890546798706, G Loss: 0.6855981349945068\n",
      "Epoch 20, batch 114 D Loss: 1.401595950126648, G Loss: 0.6900861263275146\n",
      "Epoch 20, batch 115 D Loss: 1.3897709846496582, G Loss: 0.6953058838844299\n",
      "Epoch 20, batch 116 D Loss: 1.387778639793396, G Loss: 0.6895058155059814\n",
      "Epoch 20, batch 117 D Loss: 1.3876157999038696, G Loss: 0.6983016729354858\n",
      "Epoch 20, batch 118 D Loss: 1.3921992778778076, G Loss: 0.6790111660957336\n",
      "Epoch 20, batch 119 D Loss: 1.380232334136963, G Loss: 0.6927310228347778\n",
      "Epoch 20, batch 120 D Loss: 1.3817070722579956, G Loss: 0.6805275082588196\n",
      "Epoch 20, batch 121 D Loss: 1.3912289142608643, G Loss: 0.6823728680610657\n",
      "Epoch 20, batch 122 D Loss: 1.406389832496643, G Loss: 0.6900443434715271\n",
      "Epoch 20, batch 123 D Loss: 1.382920742034912, G Loss: 0.693463921546936\n",
      "Epoch 20, batch 124 D Loss: 1.3960752487182617, G Loss: 0.6910107135772705\n",
      "Epoch 20, batch 125 D Loss: 1.3705759048461914, G Loss: 0.6907299757003784\n",
      "Epoch 20, batch 126 D Loss: 1.380608081817627, G Loss: 0.6907930970191956\n",
      "Epoch 20, batch 127 D Loss: 1.3833577632904053, G Loss: 0.6940473318099976\n",
      "Epoch 20, batch 128 D Loss: 1.3711025714874268, G Loss: 0.6789719462394714\n",
      "Epoch 20, batch 129 D Loss: 1.3920657634735107, G Loss: 0.6868792772293091\n",
      "Epoch 20, batch 130 D Loss: 1.3906705379486084, G Loss: 0.6726580858230591\n",
      "Epoch 20, batch 131 D Loss: 1.3893183469772339, G Loss: 0.6868877410888672\n",
      "Epoch 20, batch 132 D Loss: 1.3796017169952393, G Loss: 0.6842545866966248\n",
      "Epoch 20, batch 133 D Loss: 1.3860180377960205, G Loss: 0.6854940056800842\n",
      "Epoch 20, batch 134 D Loss: 1.3785018920898438, G Loss: 0.6897118091583252\n",
      "Epoch 20, batch 135 D Loss: 1.3656346797943115, G Loss: 0.6811299920082092\n",
      "Epoch 20, batch 136 D Loss: 1.3769201040267944, G Loss: 0.6833299398422241\n",
      "Epoch 20, batch 137 D Loss: 1.3888497352600098, G Loss: 0.6698761582374573\n",
      "Epoch 20, batch 138 D Loss: 1.399082899093628, G Loss: 0.6804491281509399\n",
      "Epoch 20, batch 139 D Loss: 1.3991734981536865, G Loss: 0.6843259334564209\n",
      "Epoch 20, batch 140 D Loss: 1.409681797027588, G Loss: 0.6808691620826721\n",
      "Epoch 20, batch 141 D Loss: 1.3957774639129639, G Loss: 0.6866798400878906\n",
      "Epoch 20, batch 142 D Loss: 1.3780194520950317, G Loss: 0.6831690073013306\n",
      "Epoch 20, batch 143 D Loss: 1.3659998178482056, G Loss: 0.6905177235603333\n",
      "Epoch 20, batch 144 D Loss: 1.417136311531067, G Loss: 0.6641358733177185\n",
      "Epoch 20, batch 145 D Loss: 1.3863041400909424, G Loss: 0.6780042052268982\n",
      "Epoch 20, batch 146 D Loss: 1.403020977973938, G Loss: 0.6814097762107849\n",
      "Epoch 20, batch 147 D Loss: 1.3498570919036865, G Loss: 0.694770872592926\n",
      "Epoch 20, batch 148 D Loss: 1.3815114498138428, G Loss: 0.697479784488678\n",
      "Epoch 20, batch 149 D Loss: 1.3753507137298584, G Loss: 0.6982915997505188\n",
      "Epoch 20, batch 150 D Loss: 1.3882380723953247, G Loss: 0.6921429634094238\n",
      "Epoch 20, batch 151 D Loss: 1.3736486434936523, G Loss: 0.6837035417556763\n",
      "Epoch 20, batch 152 D Loss: 1.404505968093872, G Loss: 0.6836673021316528\n",
      "Epoch 20, batch 153 D Loss: 1.3809607028961182, G Loss: 0.6933320760726929\n",
      "Epoch 20, batch 154 D Loss: 1.3829219341278076, G Loss: 0.6972156763076782\n",
      "Epoch 20, batch 155 D Loss: 1.3820387125015259, G Loss: 0.6882501244544983\n",
      "Epoch 20, batch 156 D Loss: 1.383399486541748, G Loss: 0.6872149705886841\n",
      "Epoch 20, batch 157 D Loss: 1.3667535781860352, G Loss: 0.6857890486717224\n",
      "Epoch 20, batch 158 D Loss: 1.3845891952514648, G Loss: 0.6900001764297485\n",
      "Epoch 20, batch 159 D Loss: 1.3798402547836304, G Loss: 0.6855332851409912\n",
      "Epoch 20, batch 160 D Loss: 1.404679536819458, G Loss: 0.6658080220222473\n",
      "Epoch 20, batch 161 D Loss: 1.3889535665512085, G Loss: 0.6746485233306885\n",
      "Epoch 20, batch 162 D Loss: 1.4096851348876953, G Loss: 0.6730678081512451\n",
      "Epoch 20, batch 163 D Loss: 1.3775289058685303, G Loss: 0.6867320537567139\n",
      "Epoch 20, batch 164 D Loss: 1.3844807147979736, G Loss: 0.6809024810791016\n",
      "Epoch 20, batch 165 D Loss: 1.3931152820587158, G Loss: 0.687497615814209\n",
      "Epoch 20, batch 166 D Loss: 1.4004476070404053, G Loss: 0.6819837093353271\n",
      "Epoch 20, batch 167 D Loss: 1.4042696952819824, G Loss: 0.6802104115486145\n",
      "Epoch 20, batch 168 D Loss: 1.3898375034332275, G Loss: 0.6889994740486145\n",
      "Epoch 20, batch 169 D Loss: 1.3713077306747437, G Loss: 0.696872353553772\n",
      "Epoch 20, batch 170 D Loss: 1.362877368927002, G Loss: 0.7043242454528809\n",
      "Epoch 20, batch 171 D Loss: 1.3765015602111816, G Loss: 0.6996048092842102\n",
      "Epoch 20, batch 172 D Loss: 1.3868800401687622, G Loss: 0.6922947764396667\n",
      "Epoch 20, batch 173 D Loss: 1.3871033191680908, G Loss: 0.6989697813987732\n",
      "Epoch 20, batch 174 D Loss: 1.375457525253296, G Loss: 0.6954986453056335\n",
      "Epoch 20, batch 175 D Loss: 1.3882628679275513, G Loss: 0.7018903493881226\n",
      "Epoch 20, batch 176 D Loss: 1.3810596466064453, G Loss: 0.7058247327804565\n",
      "Epoch 20, batch 177 D Loss: 1.3840875625610352, G Loss: 0.6981441974639893\n",
      "Epoch 20, batch 178 D Loss: 1.3662927150726318, G Loss: 0.7038460373878479\n",
      "Epoch 20, batch 179 D Loss: 1.3672075271606445, G Loss: 0.6980084776878357\n",
      "Epoch 20, batch 180 D Loss: 1.3820428848266602, G Loss: 0.6913169622421265\n",
      "Epoch 20, batch 181 D Loss: 1.3640764951705933, G Loss: 0.693483829498291\n",
      "Epoch 20, batch 182 D Loss: 1.3824217319488525, G Loss: 0.6956650018692017\n",
      "Epoch 20, batch 183 D Loss: 1.381460428237915, G Loss: 0.6897485256195068\n",
      "Epoch 20, batch 184 D Loss: 1.3993139266967773, G Loss: 0.6769880056381226\n",
      "Epoch 20, batch 185 D Loss: 1.3861825466156006, G Loss: 0.6947852969169617\n",
      "Epoch 20, batch 186 D Loss: 1.3808684349060059, G Loss: 0.6910467743873596\n",
      "Epoch 20, batch 187 D Loss: 1.376249074935913, G Loss: 0.6937620043754578\n",
      "Epoch 20, batch 188 D Loss: 1.3670580387115479, G Loss: 0.6900931000709534\n",
      "Epoch 20, batch 189 D Loss: 1.389426589012146, G Loss: 0.686675488948822\n",
      "Epoch 20, batch 190 D Loss: 1.387988567352295, G Loss: 0.6941984295845032\n",
      "Epoch 20, batch 191 D Loss: 1.3816139698028564, G Loss: 0.694770872592926\n",
      "Epoch 20, batch 192 D Loss: 1.3621516227722168, G Loss: 0.6970007419586182\n",
      "Epoch 20, batch 193 D Loss: 1.3802474737167358, G Loss: 0.6971616148948669\n",
      "Epoch 20, batch 194 D Loss: 1.3784496784210205, G Loss: 0.6895947456359863\n",
      "Epoch 20, batch 195 D Loss: 1.3946787118911743, G Loss: 0.6869981288909912\n",
      "Epoch 20, batch 196 D Loss: 1.3962458372116089, G Loss: 0.6782025694847107\n",
      "Epoch 20, batch 197 D Loss: 1.3926210403442383, G Loss: 0.6909675598144531\n",
      "Epoch 20, batch 198 D Loss: 1.3889520168304443, G Loss: 0.6982565522193909\n",
      "Epoch 20, batch 199 D Loss: 1.35408353805542, G Loss: 0.7193771600723267\n",
      "Epoch 20, batch 200 D Loss: 1.405153512954712, G Loss: 0.6868523359298706\n",
      "Epoch 21, batch 1 D Loss: 1.371633768081665, G Loss: 0.7201335430145264\n",
      "Epoch 21, batch 2 D Loss: 1.3559150695800781, G Loss: 0.7291790843009949\n",
      "Epoch 21, batch 3 D Loss: 1.3947045803070068, G Loss: 0.7011955380439758\n",
      "Epoch 21, batch 4 D Loss: 1.3949413299560547, G Loss: 0.704907238483429\n",
      "Epoch 21, batch 5 D Loss: 1.3866485357284546, G Loss: 0.7009555697441101\n",
      "Epoch 21, batch 6 D Loss: 1.4218292236328125, G Loss: 0.6874850392341614\n",
      "Epoch 21, batch 7 D Loss: 1.3685996532440186, G Loss: 0.7155880928039551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, batch 8 D Loss: 1.370948076248169, G Loss: 0.7225251793861389\n",
      "Epoch 21, batch 9 D Loss: 1.3914594650268555, G Loss: 0.6988261938095093\n",
      "Epoch 21, batch 10 D Loss: 1.3671907186508179, G Loss: 0.7126110196113586\n",
      "Epoch 21, batch 11 D Loss: 1.3727176189422607, G Loss: 0.7272986769676208\n",
      "Epoch 21, batch 12 D Loss: 1.3928706645965576, G Loss: 0.7002027034759521\n",
      "Epoch 21, batch 13 D Loss: 1.3975307941436768, G Loss: 0.7037094831466675\n",
      "Epoch 21, batch 14 D Loss: 1.3836637735366821, G Loss: 0.7225643992424011\n",
      "Epoch 21, batch 15 D Loss: 1.3936787843704224, G Loss: 0.69631028175354\n",
      "Epoch 21, batch 16 D Loss: 1.3830065727233887, G Loss: 0.7216741442680359\n",
      "Epoch 21, batch 17 D Loss: 1.4218534231185913, G Loss: 0.6951402425765991\n",
      "Epoch 21, batch 18 D Loss: 1.3714478015899658, G Loss: 0.7285867929458618\n",
      "Epoch 21, batch 19 D Loss: 1.413832664489746, G Loss: 0.6947146058082581\n",
      "Epoch 21, batch 20 D Loss: 1.4051787853240967, G Loss: 0.7012129426002502\n",
      "Epoch 21, batch 21 D Loss: 1.4105441570281982, G Loss: 0.7037211060523987\n",
      "Epoch 21, batch 22 D Loss: 1.405388593673706, G Loss: 0.6999498009681702\n",
      "Epoch 21, batch 23 D Loss: 1.390205979347229, G Loss: 0.7213863134384155\n",
      "Epoch 21, batch 24 D Loss: 1.3720495700836182, G Loss: 0.7211184501647949\n",
      "Epoch 21, batch 25 D Loss: 1.375420093536377, G Loss: 0.7221775054931641\n",
      "Epoch 21, batch 26 D Loss: 1.4288251399993896, G Loss: 0.7005544900894165\n",
      "Epoch 21, batch 27 D Loss: 1.4047352075576782, G Loss: 0.7199764251708984\n",
      "Epoch 21, batch 28 D Loss: 1.4030864238739014, G Loss: 0.7045289874076843\n",
      "Epoch 21, batch 29 D Loss: 1.3906798362731934, G Loss: 0.7137733697891235\n",
      "Epoch 21, batch 30 D Loss: 1.3817787170410156, G Loss: 0.7266979217529297\n",
      "Epoch 21, batch 31 D Loss: 1.4002593755722046, G Loss: 0.7305176258087158\n",
      "Epoch 21, batch 32 D Loss: 1.3942887783050537, G Loss: 0.7105368971824646\n",
      "Epoch 21, batch 33 D Loss: 1.4125306606292725, G Loss: 0.7154461741447449\n",
      "Epoch 21, batch 34 D Loss: 1.3919464349746704, G Loss: 0.7409175038337708\n",
      "Epoch 21, batch 35 D Loss: 1.3931941986083984, G Loss: 0.7352899312973022\n",
      "Epoch 21, batch 36 D Loss: 1.4108586311340332, G Loss: 0.7190616726875305\n",
      "Epoch 21, batch 37 D Loss: 1.3970201015472412, G Loss: 0.7398537397384644\n",
      "Epoch 21, batch 38 D Loss: 1.3565564155578613, G Loss: 0.7417798042297363\n",
      "Epoch 21, batch 39 D Loss: 1.4056615829467773, G Loss: 0.7184228301048279\n",
      "Epoch 21, batch 40 D Loss: 1.388303518295288, G Loss: 0.7329110503196716\n",
      "Epoch 21, batch 41 D Loss: 1.3886641263961792, G Loss: 0.7347911596298218\n",
      "Epoch 21, batch 42 D Loss: 1.417352318763733, G Loss: 0.7078497409820557\n",
      "Epoch 21, batch 43 D Loss: 1.391467571258545, G Loss: 0.7130682468414307\n",
      "Epoch 21, batch 44 D Loss: 1.3791658878326416, G Loss: 0.7233514189720154\n",
      "Epoch 21, batch 45 D Loss: 1.4135156869888306, G Loss: 0.7064995765686035\n",
      "Epoch 21, batch 46 D Loss: 1.4004801511764526, G Loss: 0.7198177576065063\n",
      "Epoch 21, batch 47 D Loss: 1.4042860269546509, G Loss: 0.7254906296730042\n",
      "Epoch 21, batch 48 D Loss: 1.3810768127441406, G Loss: 0.7139205932617188\n",
      "Epoch 21, batch 49 D Loss: 1.3917454481124878, G Loss: 0.7198028564453125\n",
      "Epoch 21, batch 50 D Loss: 1.4047434329986572, G Loss: 0.719773530960083\n",
      "Epoch 21, batch 51 D Loss: 1.389268398284912, G Loss: 0.7194003462791443\n",
      "Epoch 21, batch 52 D Loss: 1.398046612739563, G Loss: 0.7150807976722717\n",
      "Epoch 21, batch 53 D Loss: 1.382915735244751, G Loss: 0.7243962287902832\n",
      "Epoch 21, batch 54 D Loss: 1.3779182434082031, G Loss: 0.7342625260353088\n",
      "Epoch 21, batch 55 D Loss: 1.381556749343872, G Loss: 0.7231819033622742\n",
      "Epoch 21, batch 56 D Loss: 1.3811156749725342, G Loss: 0.7219165563583374\n",
      "Epoch 21, batch 57 D Loss: 1.3831183910369873, G Loss: 0.7317147850990295\n",
      "Epoch 21, batch 58 D Loss: 1.383703589439392, G Loss: 0.7298354506492615\n",
      "Epoch 21, batch 59 D Loss: 1.3785067796707153, G Loss: 0.7298488616943359\n",
      "Epoch 21, batch 60 D Loss: 1.3738312721252441, G Loss: 0.7405868768692017\n",
      "Epoch 21, batch 61 D Loss: 1.3849942684173584, G Loss: 0.7329038977622986\n",
      "Epoch 21, batch 62 D Loss: 1.3827898502349854, G Loss: 0.7236437797546387\n",
      "Epoch 21, batch 63 D Loss: 1.3743828535079956, G Loss: 0.7158262133598328\n",
      "Epoch 21, batch 64 D Loss: 1.3853058815002441, G Loss: 0.7203781008720398\n",
      "Epoch 21, batch 65 D Loss: 1.3629405498504639, G Loss: 0.7327404022216797\n",
      "Epoch 21, batch 66 D Loss: 1.3858880996704102, G Loss: 0.726056694984436\n",
      "Epoch 21, batch 67 D Loss: 1.346496820449829, G Loss: 0.7499191761016846\n",
      "Epoch 21, batch 68 D Loss: 1.3750762939453125, G Loss: 0.7160954475402832\n",
      "Epoch 21, batch 69 D Loss: 1.3911550045013428, G Loss: 0.7068840265274048\n",
      "Epoch 21, batch 70 D Loss: 1.3815478086471558, G Loss: 0.7095837593078613\n",
      "Epoch 21, batch 71 D Loss: 1.3605899810791016, G Loss: 0.7233759164810181\n",
      "Epoch 21, batch 72 D Loss: 1.4028944969177246, G Loss: 0.6982629895210266\n",
      "Epoch 21, batch 73 D Loss: 1.3843591213226318, G Loss: 0.7145717740058899\n",
      "Epoch 21, batch 74 D Loss: 1.344819188117981, G Loss: 0.7169009447097778\n",
      "Epoch 21, batch 75 D Loss: 1.3952995538711548, G Loss: 0.702235996723175\n",
      "Epoch 21, batch 76 D Loss: 1.3642971515655518, G Loss: 0.7269361019134521\n",
      "Epoch 21, batch 77 D Loss: 1.3987035751342773, G Loss: 0.6872807145118713\n",
      "Epoch 21, batch 78 D Loss: 1.3569080829620361, G Loss: 0.713173508644104\n",
      "Epoch 21, batch 79 D Loss: 1.3982133865356445, G Loss: 0.7059373259544373\n",
      "Epoch 21, batch 80 D Loss: 1.386437177658081, G Loss: 0.6836975812911987\n",
      "Epoch 21, batch 81 D Loss: 1.3783544301986694, G Loss: 0.6919331550598145\n",
      "Epoch 21, batch 82 D Loss: 1.3954205513000488, G Loss: 0.6848093271255493\n",
      "Epoch 21, batch 83 D Loss: 1.3554842472076416, G Loss: 0.7103311419487\n",
      "Epoch 21, batch 84 D Loss: 1.3546029329299927, G Loss: 0.7001075148582458\n",
      "Epoch 21, batch 85 D Loss: 1.414116621017456, G Loss: 0.6811487674713135\n",
      "Epoch 21, batch 86 D Loss: 1.3742785453796387, G Loss: 0.6902996301651001\n",
      "Epoch 21, batch 87 D Loss: 1.3696308135986328, G Loss: 0.6933046579360962\n",
      "Epoch 21, batch 88 D Loss: 1.3756580352783203, G Loss: 0.6884706020355225\n",
      "Epoch 21, batch 89 D Loss: 1.3549680709838867, G Loss: 0.7006399631500244\n",
      "Epoch 21, batch 90 D Loss: 1.385990858078003, G Loss: 0.6805423498153687\n",
      "Epoch 21, batch 91 D Loss: 1.3514015674591064, G Loss: 0.6847033500671387\n",
      "Epoch 21, batch 92 D Loss: 1.3812047243118286, G Loss: 0.6697959303855896\n",
      "Epoch 21, batch 93 D Loss: 1.4061129093170166, G Loss: 0.6547296047210693\n",
      "Epoch 21, batch 94 D Loss: 1.3816646337509155, G Loss: 0.7051713466644287\n",
      "Epoch 21, batch 95 D Loss: 1.395037293434143, G Loss: 0.666909396648407\n",
      "Epoch 21, batch 96 D Loss: 1.4143121242523193, G Loss: 0.6535331606864929\n",
      "Epoch 21, batch 97 D Loss: 1.4228310585021973, G Loss: 0.6526552438735962\n",
      "Epoch 21, batch 98 D Loss: 1.3892872333526611, G Loss: 0.662577748298645\n",
      "Epoch 21, batch 99 D Loss: 1.4235180616378784, G Loss: 0.6508055925369263\n",
      "Epoch 21, batch 100 D Loss: 1.375108003616333, G Loss: 0.6703332662582397\n",
      "Epoch 21, batch 101 D Loss: 1.3880687952041626, G Loss: 0.6662216186523438\n",
      "Epoch 21, batch 102 D Loss: 1.3874492645263672, G Loss: 0.6695961952209473\n",
      "Epoch 21, batch 103 D Loss: 1.3805618286132812, G Loss: 0.6735278964042664\n",
      "Epoch 21, batch 104 D Loss: 1.376619577407837, G Loss: 0.6709744334220886\n",
      "Epoch 21, batch 105 D Loss: 1.388800024986267, G Loss: 0.6636796593666077\n",
      "Epoch 21, batch 106 D Loss: 1.385162115097046, G Loss: 0.6719499230384827\n",
      "Epoch 21, batch 107 D Loss: 1.3926236629486084, G Loss: 0.6577567458152771\n",
      "Epoch 21, batch 108 D Loss: 1.3974502086639404, G Loss: 0.6641321778297424\n",
      "Epoch 21, batch 109 D Loss: 1.3793978691101074, G Loss: 0.6619532108306885\n",
      "Epoch 21, batch 110 D Loss: 1.3942131996154785, G Loss: 0.6690732836723328\n",
      "Epoch 21, batch 111 D Loss: 1.3915034532546997, G Loss: 0.6706900000572205\n",
      "Epoch 21, batch 112 D Loss: 1.388445258140564, G Loss: 0.6723219156265259\n",
      "Epoch 21, batch 113 D Loss: 1.394322156906128, G Loss: 0.6628153920173645\n",
      "Epoch 21, batch 114 D Loss: 1.3921045064926147, G Loss: 0.6712809205055237\n",
      "Epoch 21, batch 115 D Loss: 1.392526388168335, G Loss: 0.6691723465919495\n",
      "Epoch 21, batch 116 D Loss: 1.382651925086975, G Loss: 0.6713654398918152\n",
      "Epoch 21, batch 117 D Loss: 1.3909387588500977, G Loss: 0.6762092709541321\n",
      "Epoch 21, batch 118 D Loss: 1.3796225786209106, G Loss: 0.6764145493507385\n",
      "Epoch 21, batch 119 D Loss: 1.3762860298156738, G Loss: 0.6886654496192932\n",
      "Epoch 21, batch 120 D Loss: 1.4037935733795166, G Loss: 0.6620914936065674\n",
      "Epoch 21, batch 121 D Loss: 1.382098913192749, G Loss: 0.6789125800132751\n",
      "Epoch 21, batch 122 D Loss: 1.4109313488006592, G Loss: 0.6714859008789062\n",
      "Epoch 21, batch 123 D Loss: 1.397230625152588, G Loss: 0.677208662033081\n",
      "Epoch 21, batch 124 D Loss: 1.4147744178771973, G Loss: 0.6605570316314697\n",
      "Epoch 21, batch 125 D Loss: 1.4017776250839233, G Loss: 0.6746577620506287\n",
      "Epoch 21, batch 126 D Loss: 1.3757991790771484, G Loss: 0.6910689473152161\n",
      "Epoch 21, batch 127 D Loss: 1.37353515625, G Loss: 0.6849997639656067\n",
      "Epoch 21, batch 128 D Loss: 1.4014146327972412, G Loss: 0.676388680934906\n",
      "Epoch 21, batch 129 D Loss: 1.4175750017166138, G Loss: 0.6805408000946045\n",
      "Epoch 21, batch 130 D Loss: 1.3942887783050537, G Loss: 0.6874868869781494\n",
      "Epoch 21, batch 131 D Loss: 1.4054811000823975, G Loss: 0.6773034930229187\n",
      "Epoch 21, batch 132 D Loss: 1.409102201461792, G Loss: 0.6756688952445984\n",
      "Epoch 21, batch 133 D Loss: 1.3865386247634888, G Loss: 0.6804935336112976\n",
      "Epoch 21, batch 134 D Loss: 1.4096050262451172, G Loss: 0.6753540635108948\n",
      "Epoch 21, batch 135 D Loss: 1.3827378749847412, G Loss: 0.6926853656768799\n",
      "Epoch 21, batch 136 D Loss: 1.380875825881958, G Loss: 0.6941786408424377\n",
      "Epoch 21, batch 137 D Loss: 1.396984577178955, G Loss: 0.6794680953025818\n",
      "Epoch 21, batch 138 D Loss: 1.4092235565185547, G Loss: 0.6761507987976074\n",
      "Epoch 21, batch 139 D Loss: 1.3848645687103271, G Loss: 0.6898359656333923\n",
      "Epoch 21, batch 140 D Loss: 1.3948944807052612, G Loss: 0.6794484853744507\n",
      "Epoch 21, batch 141 D Loss: 1.3871848583221436, G Loss: 0.6831464171409607\n",
      "Epoch 21, batch 142 D Loss: 1.4056801795959473, G Loss: 0.6849796175956726\n",
      "Epoch 21, batch 143 D Loss: 1.4117932319641113, G Loss: 0.6765779256820679\n",
      "Epoch 21, batch 144 D Loss: 1.3958287239074707, G Loss: 0.6785091161727905\n",
      "Epoch 21, batch 145 D Loss: 1.4095916748046875, G Loss: 0.6755303740501404\n",
      "Epoch 21, batch 146 D Loss: 1.375798225402832, G Loss: 0.689382791519165\n",
      "Epoch 21, batch 147 D Loss: 1.3814648389816284, G Loss: 0.6888185739517212\n",
      "Epoch 21, batch 148 D Loss: 1.3967665433883667, G Loss: 0.67699134349823\n",
      "Epoch 21, batch 149 D Loss: 1.3885793685913086, G Loss: 0.6883038282394409\n",
      "Epoch 21, batch 150 D Loss: 1.388535499572754, G Loss: 0.6889101266860962\n",
      "Epoch 21, batch 151 D Loss: 1.3860794305801392, G Loss: 0.6858735084533691\n",
      "Epoch 21, batch 152 D Loss: 1.3820652961730957, G Loss: 0.6871629953384399\n",
      "Epoch 21, batch 153 D Loss: 1.3813544511795044, G Loss: 0.6948959231376648\n",
      "Epoch 21, batch 154 D Loss: 1.3812954425811768, G Loss: 0.6828321218490601\n",
      "Epoch 21, batch 155 D Loss: 1.3868863582611084, G Loss: 0.6824814081192017\n",
      "Epoch 21, batch 156 D Loss: 1.3700873851776123, G Loss: 0.6970459222793579\n",
      "Epoch 21, batch 157 D Loss: 1.3718242645263672, G Loss: 0.6931993365287781\n",
      "Epoch 21, batch 158 D Loss: 1.3828680515289307, G Loss: 0.6931731700897217\n",
      "Epoch 21, batch 159 D Loss: 1.3794140815734863, G Loss: 0.6830548644065857\n",
      "Epoch 21, batch 160 D Loss: 1.3851418495178223, G Loss: 0.6833742260932922\n",
      "Epoch 21, batch 161 D Loss: 1.3935344219207764, G Loss: 0.6915977597236633\n",
      "Epoch 21, batch 162 D Loss: 1.397189736366272, G Loss: 0.6826773285865784\n",
      "Epoch 21, batch 163 D Loss: 1.4085649251937866, G Loss: 0.6729859113693237\n",
      "Epoch 21, batch 164 D Loss: 1.389138102531433, G Loss: 0.6866475939750671\n",
      "Epoch 21, batch 165 D Loss: 1.3837859630584717, G Loss: 0.687671959400177\n",
      "Epoch 21, batch 166 D Loss: 1.370856761932373, G Loss: 0.6970849633216858\n",
      "Epoch 21, batch 167 D Loss: 1.376060962677002, G Loss: 0.6910626292228699\n",
      "Epoch 21, batch 168 D Loss: 1.3567883968353271, G Loss: 0.7053642868995667\n",
      "Epoch 21, batch 169 D Loss: 1.3924102783203125, G Loss: 0.6876455545425415\n",
      "Epoch 21, batch 170 D Loss: 1.3731073141098022, G Loss: 0.6955346465110779\n",
      "Epoch 21, batch 171 D Loss: 1.373656153678894, G Loss: 0.6952337622642517\n",
      "Epoch 21, batch 172 D Loss: 1.3833502531051636, G Loss: 0.6932771801948547\n",
      "Epoch 21, batch 173 D Loss: 1.3915950059890747, G Loss: 0.6849755644798279\n",
      "Epoch 21, batch 174 D Loss: 1.381986141204834, G Loss: 0.6993023753166199\n",
      "Epoch 21, batch 175 D Loss: 1.3889389038085938, G Loss: 0.6954503059387207\n",
      "Epoch 21, batch 176 D Loss: 1.3841955661773682, G Loss: 0.7078635692596436\n",
      "Epoch 21, batch 177 D Loss: 1.3696374893188477, G Loss: 0.7108105421066284\n",
      "Epoch 21, batch 178 D Loss: 1.4093964099884033, G Loss: 0.6937103867530823\n",
      "Epoch 21, batch 179 D Loss: 1.387925386428833, G Loss: 0.6980522274971008\n",
      "Epoch 21, batch 180 D Loss: 1.3646655082702637, G Loss: 0.7054982781410217\n",
      "Epoch 21, batch 181 D Loss: 1.3715858459472656, G Loss: 0.7094352841377258\n",
      "Epoch 21, batch 182 D Loss: 1.4234344959259033, G Loss: 0.6874231696128845\n",
      "Epoch 21, batch 183 D Loss: 1.3923777341842651, G Loss: 0.7034148573875427\n",
      "Epoch 21, batch 184 D Loss: 1.3715920448303223, G Loss: 0.711225688457489\n",
      "Epoch 21, batch 185 D Loss: 1.3756709098815918, G Loss: 0.7062826752662659\n",
      "Epoch 21, batch 186 D Loss: 1.3850699663162231, G Loss: 0.7040002346038818\n",
      "Epoch 21, batch 187 D Loss: 1.370715618133545, G Loss: 0.7147033214569092\n",
      "Epoch 21, batch 188 D Loss: 1.3804478645324707, G Loss: 0.7150058746337891\n",
      "Epoch 21, batch 189 D Loss: 1.3652218580245972, G Loss: 0.7303507924079895\n",
      "Epoch 21, batch 190 D Loss: 1.3837804794311523, G Loss: 0.7193742990493774\n",
      "Epoch 21, batch 191 D Loss: 1.376037359237671, G Loss: 0.7152946591377258\n",
      "Epoch 21, batch 192 D Loss: 1.3720735311508179, G Loss: 0.714646577835083\n",
      "Epoch 21, batch 193 D Loss: 1.3832334280014038, G Loss: 0.7257084846496582\n",
      "Epoch 21, batch 194 D Loss: 1.374597191810608, G Loss: 0.7271992564201355\n",
      "Epoch 21, batch 195 D Loss: 1.3832969665527344, G Loss: 0.7214757800102234\n",
      "Epoch 21, batch 196 D Loss: 1.3794939517974854, G Loss: 0.7274161577224731\n",
      "Epoch 21, batch 197 D Loss: 1.391796350479126, G Loss: 0.7083475589752197\n",
      "Epoch 21, batch 198 D Loss: 1.3847146034240723, G Loss: 0.7303984761238098\n",
      "Epoch 21, batch 199 D Loss: 1.3721327781677246, G Loss: 0.7327580451965332\n",
      "Epoch 21, batch 200 D Loss: 1.3611297607421875, G Loss: 0.7338609099388123\n",
      "Epoch 22, batch 1 D Loss: 1.3789770603179932, G Loss: 0.7226722836494446\n",
      "Epoch 22, batch 2 D Loss: 1.3913073539733887, G Loss: 0.7129514813423157\n",
      "Epoch 22, batch 3 D Loss: 1.3768646717071533, G Loss: 0.721122682094574\n",
      "Epoch 22, batch 4 D Loss: 1.376169204711914, G Loss: 0.7307685017585754\n",
      "Epoch 22, batch 5 D Loss: 1.387481689453125, G Loss: 0.7170026302337646\n",
      "Epoch 22, batch 6 D Loss: 1.3630657196044922, G Loss: 0.7307465076446533\n",
      "Epoch 22, batch 7 D Loss: 1.3812720775604248, G Loss: 0.7286163568496704\n",
      "Epoch 22, batch 8 D Loss: 1.364353060722351, G Loss: 0.7175093293190002\n",
      "Epoch 22, batch 9 D Loss: 1.3731392621994019, G Loss: 0.7292441725730896\n",
      "Epoch 22, batch 10 D Loss: 1.3813618421554565, G Loss: 0.7353877425193787\n",
      "Epoch 22, batch 11 D Loss: 1.33818781375885, G Loss: 0.7373970150947571\n",
      "Epoch 22, batch 12 D Loss: 1.3638968467712402, G Loss: 0.7344722151756287\n",
      "Epoch 22, batch 13 D Loss: 1.388721227645874, G Loss: 0.7111265063285828\n",
      "Epoch 22, batch 14 D Loss: 1.3686060905456543, G Loss: 0.7158942222595215\n",
      "Epoch 22, batch 15 D Loss: 1.4005508422851562, G Loss: 0.7090113759040833\n",
      "Epoch 22, batch 16 D Loss: 1.3851323127746582, G Loss: 0.692335844039917\n",
      "Epoch 22, batch 17 D Loss: 1.3598854541778564, G Loss: 0.7154529690742493\n",
      "Epoch 22, batch 18 D Loss: 1.3587968349456787, G Loss: 0.7208142280578613\n",
      "Epoch 22, batch 19 D Loss: 1.3933663368225098, G Loss: 0.6872912645339966\n",
      "Epoch 22, batch 20 D Loss: 1.366959810256958, G Loss: 0.7274602651596069\n",
      "Epoch 22, batch 21 D Loss: 1.3769031763076782, G Loss: 0.7103707790374756\n",
      "Epoch 22, batch 22 D Loss: 1.3977333307266235, G Loss: 0.6972026228904724\n",
      "Epoch 22, batch 23 D Loss: 1.3542824983596802, G Loss: 0.6980937719345093\n",
      "Epoch 22, batch 24 D Loss: 1.3816616535186768, G Loss: 0.7102636098861694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, batch 25 D Loss: 1.3981003761291504, G Loss: 0.7008825540542603\n",
      "Epoch 22, batch 26 D Loss: 1.3634600639343262, G Loss: 0.7144461870193481\n",
      "Epoch 22, batch 27 D Loss: 1.3807637691497803, G Loss: 0.6870588064193726\n",
      "Epoch 22, batch 28 D Loss: 1.4097120761871338, G Loss: 0.6903108358383179\n",
      "Epoch 22, batch 29 D Loss: 1.3668148517608643, G Loss: 0.705241858959198\n",
      "Epoch 22, batch 30 D Loss: 1.389866590499878, G Loss: 0.6922444105148315\n",
      "Epoch 22, batch 31 D Loss: 1.3904370069503784, G Loss: 0.6907076835632324\n",
      "Epoch 22, batch 32 D Loss: 1.389008641242981, G Loss: 0.6994274854660034\n",
      "Epoch 22, batch 33 D Loss: 1.411341905593872, G Loss: 0.686635434627533\n",
      "Epoch 22, batch 34 D Loss: 1.3722152709960938, G Loss: 0.703748881816864\n",
      "Epoch 22, batch 35 D Loss: 1.3967164754867554, G Loss: 0.7031659483909607\n",
      "Epoch 22, batch 36 D Loss: 1.3928781747817993, G Loss: 0.7018795013427734\n",
      "Epoch 22, batch 37 D Loss: 1.3894723653793335, G Loss: 0.6989151239395142\n",
      "Epoch 22, batch 38 D Loss: 1.3650000095367432, G Loss: 0.7183699011802673\n",
      "Epoch 22, batch 39 D Loss: 1.3989487886428833, G Loss: 0.7098888158798218\n",
      "Epoch 22, batch 40 D Loss: 1.434418797492981, G Loss: 0.6690571308135986\n",
      "Epoch 22, batch 41 D Loss: 1.4146414995193481, G Loss: 0.6955020427703857\n",
      "Epoch 22, batch 42 D Loss: 1.363999605178833, G Loss: 0.7008883953094482\n",
      "Epoch 22, batch 43 D Loss: 1.3722851276397705, G Loss: 0.7035794258117676\n",
      "Epoch 22, batch 44 D Loss: 1.3966652154922485, G Loss: 0.6899707317352295\n",
      "Epoch 22, batch 45 D Loss: 1.4058836698532104, G Loss: 0.7002404928207397\n",
      "Epoch 22, batch 46 D Loss: 1.443124771118164, G Loss: 0.6649805307388306\n",
      "Epoch 22, batch 47 D Loss: 1.4230666160583496, G Loss: 0.6888558268547058\n",
      "Epoch 22, batch 48 D Loss: 1.4525303840637207, G Loss: 0.699322521686554\n",
      "Epoch 22, batch 49 D Loss: 1.4118585586547852, G Loss: 0.7065234184265137\n",
      "Epoch 22, batch 50 D Loss: 1.3951733112335205, G Loss: 0.6946726441383362\n",
      "Epoch 22, batch 51 D Loss: 1.4202675819396973, G Loss: 0.6852201819419861\n",
      "Epoch 22, batch 52 D Loss: 1.4084248542785645, G Loss: 0.6923481822013855\n",
      "Epoch 22, batch 53 D Loss: 1.3935680389404297, G Loss: 0.701609194278717\n",
      "Epoch 22, batch 54 D Loss: 1.4024958610534668, G Loss: 0.6937354803085327\n",
      "Epoch 22, batch 55 D Loss: 1.4161741733551025, G Loss: 0.7002500891685486\n",
      "Epoch 22, batch 56 D Loss: 1.4072117805480957, G Loss: 0.6954111456871033\n",
      "Epoch 22, batch 57 D Loss: 1.3952813148498535, G Loss: 0.6970946788787842\n",
      "Epoch 22, batch 58 D Loss: 1.4183167219161987, G Loss: 0.6892892718315125\n",
      "Epoch 22, batch 59 D Loss: 1.4024301767349243, G Loss: 0.7014647126197815\n",
      "Epoch 22, batch 60 D Loss: 1.397544503211975, G Loss: 0.7059513330459595\n",
      "Epoch 22, batch 61 D Loss: 1.3951572179794312, G Loss: 0.7088159322738647\n",
      "Epoch 22, batch 62 D Loss: 1.3963649272918701, G Loss: 0.7048845887184143\n",
      "Epoch 22, batch 63 D Loss: 1.3695757389068604, G Loss: 0.7197281122207642\n",
      "Epoch 22, batch 64 D Loss: 1.365673542022705, G Loss: 0.7279279232025146\n",
      "Epoch 22, batch 65 D Loss: 1.382922887802124, G Loss: 0.7237741351127625\n",
      "Epoch 22, batch 66 D Loss: 1.3895655870437622, G Loss: 0.7214668393135071\n",
      "Epoch 22, batch 67 D Loss: 1.3735625743865967, G Loss: 0.7222702503204346\n",
      "Epoch 22, batch 68 D Loss: 1.404491901397705, G Loss: 0.7085088491439819\n",
      "Epoch 22, batch 69 D Loss: 1.3754280805587769, G Loss: 0.7263529300689697\n",
      "Epoch 22, batch 70 D Loss: 1.3775575160980225, G Loss: 0.7514581084251404\n",
      "Epoch 22, batch 71 D Loss: 1.3987877368927002, G Loss: 0.7100379467010498\n",
      "Epoch 22, batch 72 D Loss: 1.4071764945983887, G Loss: 0.7180069088935852\n",
      "Epoch 22, batch 73 D Loss: 1.3963768482208252, G Loss: 0.7124871611595154\n",
      "Epoch 22, batch 74 D Loss: 1.4037704467773438, G Loss: 0.6971896290779114\n",
      "Epoch 22, batch 75 D Loss: 1.3788392543792725, G Loss: 0.7068672180175781\n",
      "Epoch 22, batch 76 D Loss: 1.3691415786743164, G Loss: 0.7269428968429565\n",
      "Epoch 22, batch 77 D Loss: 1.377169132232666, G Loss: 0.7000839114189148\n",
      "Epoch 22, batch 78 D Loss: 1.3654251098632812, G Loss: 0.7167041897773743\n",
      "Epoch 22, batch 79 D Loss: 1.4099979400634766, G Loss: 0.6788875460624695\n",
      "Epoch 22, batch 80 D Loss: 1.3766294717788696, G Loss: 0.6976157426834106\n",
      "Epoch 22, batch 81 D Loss: 1.357231616973877, G Loss: 0.698056697845459\n",
      "Epoch 22, batch 82 D Loss: 1.4295408725738525, G Loss: 0.6837136149406433\n",
      "Epoch 22, batch 83 D Loss: 1.3727312088012695, G Loss: 0.7063920497894287\n",
      "Epoch 22, batch 84 D Loss: 1.4152240753173828, G Loss: 0.6819818615913391\n",
      "Epoch 22, batch 85 D Loss: 1.4004955291748047, G Loss: 0.6997787952423096\n",
      "Epoch 22, batch 86 D Loss: 1.394205093383789, G Loss: 0.6814184784889221\n",
      "Epoch 22, batch 87 D Loss: 1.3828825950622559, G Loss: 0.7061434984207153\n",
      "Epoch 22, batch 88 D Loss: 1.377300500869751, G Loss: 0.7116302251815796\n",
      "Epoch 22, batch 89 D Loss: 1.3970239162445068, G Loss: 0.7110595107078552\n",
      "Epoch 22, batch 90 D Loss: 1.3822174072265625, G Loss: 0.695817232131958\n",
      "Epoch 22, batch 91 D Loss: 1.3747105598449707, G Loss: 0.7058413624763489\n",
      "Epoch 22, batch 92 D Loss: 1.3752185106277466, G Loss: 0.7027217745780945\n",
      "Epoch 22, batch 93 D Loss: 1.382523775100708, G Loss: 0.6969658732414246\n",
      "Epoch 22, batch 94 D Loss: 1.3920292854309082, G Loss: 0.7020373344421387\n",
      "Epoch 22, batch 95 D Loss: 1.390333890914917, G Loss: 0.6917091608047485\n",
      "Epoch 22, batch 96 D Loss: 1.3849905729293823, G Loss: 0.6895912289619446\n",
      "Epoch 22, batch 97 D Loss: 1.3738086223602295, G Loss: 0.6941160559654236\n",
      "Epoch 22, batch 98 D Loss: 1.3691881895065308, G Loss: 0.7033787369728088\n",
      "Epoch 22, batch 99 D Loss: 1.3723969459533691, G Loss: 0.7023520469665527\n",
      "Epoch 22, batch 100 D Loss: 1.3838053941726685, G Loss: 0.6801420450210571\n",
      "Epoch 22, batch 101 D Loss: 1.3685317039489746, G Loss: 0.6933632493019104\n",
      "Epoch 22, batch 102 D Loss: 1.3648144006729126, G Loss: 0.6931506395339966\n",
      "Epoch 22, batch 103 D Loss: 1.3615951538085938, G Loss: 0.707263171672821\n",
      "Epoch 22, batch 104 D Loss: 1.3811123371124268, G Loss: 0.6869038343429565\n",
      "Epoch 22, batch 105 D Loss: 1.3641722202301025, G Loss: 0.6936097741127014\n",
      "Epoch 22, batch 106 D Loss: 1.376633882522583, G Loss: 0.6814523935317993\n",
      "Epoch 22, batch 107 D Loss: 1.3923523426055908, G Loss: 0.6817117929458618\n",
      "Epoch 22, batch 108 D Loss: 1.3963358402252197, G Loss: 0.6829400658607483\n",
      "Epoch 22, batch 109 D Loss: 1.390535831451416, G Loss: 0.6912544369697571\n",
      "Epoch 22, batch 110 D Loss: 1.3544814586639404, G Loss: 0.7119818925857544\n",
      "Epoch 22, batch 111 D Loss: 1.3697874546051025, G Loss: 0.7070559859275818\n",
      "Epoch 22, batch 112 D Loss: 1.3880341053009033, G Loss: 0.702907919883728\n",
      "Epoch 22, batch 113 D Loss: 1.379345178604126, G Loss: 0.7114689350128174\n",
      "Epoch 22, batch 114 D Loss: 1.3821923732757568, G Loss: 0.691091775894165\n",
      "Epoch 22, batch 115 D Loss: 1.3973653316497803, G Loss: 0.6848634481430054\n",
      "Epoch 22, batch 116 D Loss: 1.3767683506011963, G Loss: 0.6973032355308533\n",
      "Epoch 22, batch 117 D Loss: 1.3832008838653564, G Loss: 0.6951857209205627\n",
      "Epoch 22, batch 118 D Loss: 1.3929972648620605, G Loss: 0.6893957257270813\n",
      "Epoch 22, batch 119 D Loss: 1.375596523284912, G Loss: 0.7004687786102295\n",
      "Epoch 22, batch 120 D Loss: 1.3660048246383667, G Loss: 0.6890422105789185\n",
      "Epoch 22, batch 121 D Loss: 1.4110698699951172, G Loss: 0.67087721824646\n",
      "Epoch 22, batch 122 D Loss: 1.4028537273406982, G Loss: 0.6818000674247742\n",
      "Epoch 22, batch 123 D Loss: 1.3954722881317139, G Loss: 0.6765661835670471\n",
      "Epoch 22, batch 124 D Loss: 1.3976367712020874, G Loss: 0.6831453442573547\n",
      "Epoch 22, batch 125 D Loss: 1.4052886962890625, G Loss: 0.6858898997306824\n",
      "Epoch 22, batch 126 D Loss: 1.3919062614440918, G Loss: 0.681815505027771\n",
      "Epoch 22, batch 127 D Loss: 1.4199984073638916, G Loss: 0.6773778796195984\n",
      "Epoch 22, batch 128 D Loss: 1.3916767835617065, G Loss: 0.6907903552055359\n",
      "Epoch 22, batch 129 D Loss: 1.3891398906707764, G Loss: 0.6939178705215454\n",
      "Epoch 22, batch 130 D Loss: 1.382961630821228, G Loss: 0.6808574199676514\n",
      "Epoch 22, batch 131 D Loss: 1.3919541835784912, G Loss: 0.6805992126464844\n",
      "Epoch 22, batch 132 D Loss: 1.376193642616272, G Loss: 0.6944809556007385\n",
      "Epoch 22, batch 133 D Loss: 1.381551742553711, G Loss: 0.6770222187042236\n",
      "Epoch 22, batch 134 D Loss: 1.3753143548965454, G Loss: 0.6814402937889099\n",
      "Epoch 22, batch 135 D Loss: 1.3744453191757202, G Loss: 0.6818322539329529\n",
      "Epoch 22, batch 136 D Loss: 1.409949779510498, G Loss: 0.6506693363189697\n",
      "Epoch 22, batch 137 D Loss: 1.380235195159912, G Loss: 0.6847063302993774\n",
      "Epoch 22, batch 138 D Loss: 1.3898952007293701, G Loss: 0.6861157417297363\n",
      "Epoch 22, batch 139 D Loss: 1.38836669921875, G Loss: 0.6788307428359985\n",
      "Epoch 22, batch 140 D Loss: 1.446887493133545, G Loss: 0.6421819925308228\n",
      "Epoch 22, batch 141 D Loss: 1.413620948791504, G Loss: 0.672330379486084\n",
      "Epoch 22, batch 142 D Loss: 1.3866143226623535, G Loss: 0.6805704236030579\n",
      "Epoch 22, batch 143 D Loss: 1.4077308177947998, G Loss: 0.6845659017562866\n",
      "Epoch 22, batch 144 D Loss: 1.3813340663909912, G Loss: 0.6613917350769043\n",
      "Epoch 22, batch 145 D Loss: 1.4079923629760742, G Loss: 0.6756134033203125\n",
      "Epoch 22, batch 146 D Loss: 1.3981952667236328, G Loss: 0.6876376271247864\n",
      "Epoch 22, batch 147 D Loss: 1.3916292190551758, G Loss: 0.6961315274238586\n",
      "Epoch 22, batch 148 D Loss: 1.3901288509368896, G Loss: 0.6936378479003906\n",
      "Epoch 22, batch 149 D Loss: 1.3986049890518188, G Loss: 0.6871926188468933\n",
      "Epoch 22, batch 150 D Loss: 1.415693998336792, G Loss: 0.6824526786804199\n",
      "Epoch 22, batch 151 D Loss: 1.3772099018096924, G Loss: 0.697326123714447\n",
      "Epoch 22, batch 152 D Loss: 1.402016282081604, G Loss: 0.6918144226074219\n",
      "Epoch 22, batch 153 D Loss: 1.3926626443862915, G Loss: 0.6815078854560852\n",
      "Epoch 22, batch 154 D Loss: 1.410649061203003, G Loss: 0.6690212488174438\n",
      "Epoch 22, batch 155 D Loss: 1.3907195329666138, G Loss: 0.6912593841552734\n",
      "Epoch 22, batch 156 D Loss: 1.3982758522033691, G Loss: 0.6947811245918274\n",
      "Epoch 22, batch 157 D Loss: 1.3701707124710083, G Loss: 0.7079475522041321\n",
      "Epoch 22, batch 158 D Loss: 1.3771140575408936, G Loss: 0.7093620896339417\n",
      "Epoch 22, batch 159 D Loss: 1.3862872123718262, G Loss: 0.6930050849914551\n",
      "Epoch 22, batch 160 D Loss: 1.3808932304382324, G Loss: 0.7072001695632935\n",
      "Epoch 22, batch 161 D Loss: 1.3954646587371826, G Loss: 0.6950536966323853\n",
      "Epoch 22, batch 162 D Loss: 1.3950425386428833, G Loss: 0.6975732445716858\n",
      "Epoch 22, batch 163 D Loss: 1.3764629364013672, G Loss: 0.7059834003448486\n",
      "Epoch 22, batch 164 D Loss: 1.384268045425415, G Loss: 0.7024437189102173\n",
      "Epoch 22, batch 165 D Loss: 1.3700200319290161, G Loss: 0.713137686252594\n",
      "Epoch 22, batch 166 D Loss: 1.38493013381958, G Loss: 0.7018519043922424\n",
      "Epoch 22, batch 167 D Loss: 1.3831920623779297, G Loss: 0.6942905187606812\n",
      "Epoch 22, batch 168 D Loss: 1.384467363357544, G Loss: 0.7064656019210815\n",
      "Epoch 22, batch 169 D Loss: 1.3696424961090088, G Loss: 0.7047606706619263\n",
      "Epoch 22, batch 170 D Loss: 1.3998303413391113, G Loss: 0.6875569224357605\n",
      "Epoch 22, batch 171 D Loss: 1.3691784143447876, G Loss: 0.6949166059494019\n",
      "Epoch 22, batch 172 D Loss: 1.3707778453826904, G Loss: 0.7046334147453308\n",
      "Epoch 22, batch 173 D Loss: 1.4027717113494873, G Loss: 0.6830750107765198\n",
      "Epoch 22, batch 174 D Loss: 1.3820627927780151, G Loss: 0.6972787380218506\n",
      "Epoch 22, batch 175 D Loss: 1.3948917388916016, G Loss: 0.6928348541259766\n",
      "Epoch 22, batch 176 D Loss: 1.395391821861267, G Loss: 0.6897361874580383\n",
      "Epoch 22, batch 177 D Loss: 1.3919663429260254, G Loss: 0.6911811828613281\n",
      "Epoch 22, batch 178 D Loss: 1.380156397819519, G Loss: 0.7005802392959595\n",
      "Epoch 22, batch 179 D Loss: 1.3780925273895264, G Loss: 0.7020378112792969\n",
      "Epoch 22, batch 180 D Loss: 1.3649117946624756, G Loss: 0.7164625525474548\n",
      "Epoch 22, batch 181 D Loss: 1.4006901979446411, G Loss: 0.6894490122795105\n",
      "Epoch 22, batch 182 D Loss: 1.3729097843170166, G Loss: 0.699247419834137\n",
      "Epoch 22, batch 183 D Loss: 1.3910696506500244, G Loss: 0.6891028881072998\n",
      "Epoch 22, batch 184 D Loss: 1.382427453994751, G Loss: 0.697418749332428\n",
      "Epoch 22, batch 185 D Loss: 1.390494704246521, G Loss: 0.6856319308280945\n",
      "Epoch 22, batch 186 D Loss: 1.3547773361206055, G Loss: 0.7209451198577881\n",
      "Epoch 22, batch 187 D Loss: 1.3660166263580322, G Loss: 0.7136147022247314\n",
      "Epoch 22, batch 188 D Loss: 1.3460783958435059, G Loss: 0.7232961058616638\n",
      "Epoch 22, batch 189 D Loss: 1.3769633769989014, G Loss: 0.6958715915679932\n",
      "Epoch 22, batch 190 D Loss: 1.3815126419067383, G Loss: 0.699714720249176\n",
      "Epoch 22, batch 191 D Loss: 1.3915786743164062, G Loss: 0.7124873399734497\n",
      "Epoch 22, batch 192 D Loss: 1.3806116580963135, G Loss: 0.7127240896224976\n",
      "Epoch 22, batch 193 D Loss: 1.372843861579895, G Loss: 0.6981410980224609\n",
      "Epoch 22, batch 194 D Loss: 1.399154543876648, G Loss: 0.6962481737136841\n",
      "Epoch 22, batch 195 D Loss: 1.3784000873565674, G Loss: 0.7136785984039307\n",
      "Epoch 22, batch 196 D Loss: 1.381159782409668, G Loss: 0.7134503126144409\n",
      "Epoch 22, batch 197 D Loss: 1.3867391347885132, G Loss: 0.7257042527198792\n",
      "Epoch 22, batch 198 D Loss: 1.3548715114593506, G Loss: 0.722888171672821\n",
      "Epoch 22, batch 199 D Loss: 1.384031057357788, G Loss: 0.6897133588790894\n",
      "Epoch 22, batch 200 D Loss: 1.370307207107544, G Loss: 0.7086664438247681\n",
      "Epoch 23, batch 1 D Loss: 1.3972891569137573, G Loss: 0.6930854916572571\n",
      "Epoch 23, batch 2 D Loss: 1.3830591440200806, G Loss: 0.7133837938308716\n",
      "Epoch 23, batch 3 D Loss: 1.3891091346740723, G Loss: 0.7085590362548828\n",
      "Epoch 23, batch 4 D Loss: 1.3999807834625244, G Loss: 0.6877660155296326\n",
      "Epoch 23, batch 5 D Loss: 1.428485631942749, G Loss: 0.6936774253845215\n",
      "Epoch 23, batch 6 D Loss: 1.4027678966522217, G Loss: 0.7174919843673706\n",
      "Epoch 23, batch 7 D Loss: 1.3799350261688232, G Loss: 0.721510648727417\n",
      "Epoch 23, batch 8 D Loss: 1.3927278518676758, G Loss: 0.7310535311698914\n",
      "Epoch 23, batch 9 D Loss: 1.372520923614502, G Loss: 0.7258322834968567\n",
      "Epoch 23, batch 10 D Loss: 1.387986183166504, G Loss: 0.7108516097068787\n",
      "Epoch 23, batch 11 D Loss: 1.3999898433685303, G Loss: 0.7045612335205078\n",
      "Epoch 23, batch 12 D Loss: 1.3973506689071655, G Loss: 0.7159280180931091\n",
      "Epoch 23, batch 13 D Loss: 1.3599088191986084, G Loss: 0.7583626508712769\n",
      "Epoch 23, batch 14 D Loss: 1.3883132934570312, G Loss: 0.7326268553733826\n",
      "Epoch 23, batch 15 D Loss: 1.4061427116394043, G Loss: 0.7222589254379272\n",
      "Epoch 23, batch 16 D Loss: 1.4113471508026123, G Loss: 0.7280027270317078\n",
      "Epoch 23, batch 17 D Loss: 1.3937644958496094, G Loss: 0.728757917881012\n",
      "Epoch 23, batch 18 D Loss: 1.4101229906082153, G Loss: 0.7061406970024109\n",
      "Epoch 23, batch 19 D Loss: 1.3787970542907715, G Loss: 0.7234247326850891\n",
      "Epoch 23, batch 20 D Loss: 1.4002023935317993, G Loss: 0.7214325666427612\n",
      "Epoch 23, batch 21 D Loss: 1.402752161026001, G Loss: 0.7171775698661804\n",
      "Epoch 23, batch 22 D Loss: 1.3863168954849243, G Loss: 0.7243853807449341\n",
      "Epoch 23, batch 23 D Loss: 1.399633526802063, G Loss: 0.7249442934989929\n",
      "Epoch 23, batch 24 D Loss: 1.405238151550293, G Loss: 0.713589608669281\n",
      "Epoch 23, batch 25 D Loss: 1.385674238204956, G Loss: 0.7226234674453735\n",
      "Epoch 23, batch 26 D Loss: 1.3984978199005127, G Loss: 0.7043396830558777\n",
      "Epoch 23, batch 27 D Loss: 1.4277210235595703, G Loss: 0.6845686435699463\n",
      "Epoch 23, batch 28 D Loss: 1.4100346565246582, G Loss: 0.7033914923667908\n",
      "Epoch 23, batch 29 D Loss: 1.3895628452301025, G Loss: 0.6911875009536743\n",
      "Epoch 23, batch 30 D Loss: 1.412510633468628, G Loss: 0.6794894933700562\n",
      "Epoch 23, batch 31 D Loss: 1.390141487121582, G Loss: 0.6845893859863281\n",
      "Epoch 23, batch 32 D Loss: 1.3796732425689697, G Loss: 0.6759616136550903\n",
      "Epoch 23, batch 33 D Loss: 1.4100446701049805, G Loss: 0.6545122265815735\n",
      "Epoch 23, batch 34 D Loss: 1.3892743587493896, G Loss: 0.6839033365249634\n",
      "Epoch 23, batch 35 D Loss: 1.3981276750564575, G Loss: 0.6720869541168213\n",
      "Epoch 23, batch 36 D Loss: 1.3881394863128662, G Loss: 0.6719577312469482\n",
      "Epoch 23, batch 37 D Loss: 1.3615713119506836, G Loss: 0.681739866733551\n",
      "Epoch 23, batch 38 D Loss: 1.379652500152588, G Loss: 0.6753513813018799\n",
      "Epoch 23, batch 39 D Loss: 1.3900443315505981, G Loss: 0.6619726419448853\n",
      "Epoch 23, batch 40 D Loss: 1.4041197299957275, G Loss: 0.6617804765701294\n",
      "Epoch 23, batch 41 D Loss: 1.3874988555908203, G Loss: 0.6774101853370667\n",
      "Epoch 23, batch 42 D Loss: 1.3764913082122803, G Loss: 0.6682570576667786\n",
      "Epoch 23, batch 43 D Loss: 1.3974425792694092, G Loss: 0.6743733286857605\n",
      "Epoch 23, batch 44 D Loss: 1.3807551860809326, G Loss: 0.6763390898704529\n",
      "Epoch 23, batch 45 D Loss: 1.3907086849212646, G Loss: 0.6698991656303406\n",
      "Epoch 23, batch 46 D Loss: 1.3760480880737305, G Loss: 0.6756423711776733\n",
      "Epoch 23, batch 47 D Loss: 1.3786921501159668, G Loss: 0.6859394907951355\n",
      "Epoch 23, batch 48 D Loss: 1.3885421752929688, G Loss: 0.6804539561271667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, batch 49 D Loss: 1.3847174644470215, G Loss: 0.6851991415023804\n",
      "Epoch 23, batch 50 D Loss: 1.4194128513336182, G Loss: 0.6631423830986023\n",
      "Epoch 23, batch 51 D Loss: 1.3987820148468018, G Loss: 0.6808828711509705\n",
      "Epoch 23, batch 52 D Loss: 1.3814594745635986, G Loss: 0.6822310090065002\n",
      "Epoch 23, batch 53 D Loss: 1.3931472301483154, G Loss: 0.7015833854675293\n",
      "Epoch 23, batch 54 D Loss: 1.3750962018966675, G Loss: 0.7035576701164246\n",
      "Epoch 23, batch 55 D Loss: 1.3793268203735352, G Loss: 0.7135298848152161\n",
      "Epoch 23, batch 56 D Loss: 1.3670953512191772, G Loss: 0.7201257348060608\n",
      "Epoch 23, batch 57 D Loss: 1.38155198097229, G Loss: 0.7249944806098938\n",
      "Epoch 23, batch 58 D Loss: 1.3721894025802612, G Loss: 0.7366292476654053\n",
      "Epoch 23, batch 59 D Loss: 1.3741605281829834, G Loss: 0.728668749332428\n",
      "Epoch 23, batch 60 D Loss: 1.3816683292388916, G Loss: 0.7315877676010132\n",
      "Epoch 23, batch 61 D Loss: 1.3907191753387451, G Loss: 0.7244865298271179\n",
      "Epoch 23, batch 62 D Loss: 1.377331256866455, G Loss: 0.732781171798706\n",
      "Epoch 23, batch 63 D Loss: 1.375114917755127, G Loss: 0.7468792796134949\n",
      "Epoch 23, batch 64 D Loss: 1.3728382587432861, G Loss: 0.7455795407295227\n",
      "Epoch 23, batch 65 D Loss: 1.353588342666626, G Loss: 0.7251836657524109\n",
      "Epoch 23, batch 66 D Loss: 1.38166344165802, G Loss: 0.7262755632400513\n",
      "Epoch 23, batch 67 D Loss: 1.3561760187149048, G Loss: 0.7535369992256165\n",
      "Epoch 23, batch 68 D Loss: 1.3749279975891113, G Loss: 0.7427701354026794\n",
      "Epoch 23, batch 69 D Loss: 1.3564386367797852, G Loss: 0.749540388584137\n",
      "Epoch 23, batch 70 D Loss: 1.3881224393844604, G Loss: 0.7381736040115356\n",
      "Epoch 23, batch 71 D Loss: 1.4012733697891235, G Loss: 0.715431809425354\n",
      "Epoch 23, batch 72 D Loss: 1.3536534309387207, G Loss: 0.7472970485687256\n",
      "Epoch 23, batch 73 D Loss: 1.4101046323776245, G Loss: 0.7197555303573608\n",
      "Epoch 23, batch 74 D Loss: 1.3895868062973022, G Loss: 0.7305233478546143\n",
      "Epoch 23, batch 75 D Loss: 1.3713936805725098, G Loss: 0.7397903203964233\n",
      "Epoch 23, batch 76 D Loss: 1.3962738513946533, G Loss: 0.7107328772544861\n",
      "Epoch 23, batch 77 D Loss: 1.3954126834869385, G Loss: 0.7093552350997925\n",
      "Epoch 23, batch 78 D Loss: 1.3609737157821655, G Loss: 0.7188979983329773\n",
      "Epoch 23, batch 79 D Loss: 1.3771986961364746, G Loss: 0.723982036113739\n",
      "Epoch 23, batch 80 D Loss: 1.3934762477874756, G Loss: 0.6991525888442993\n",
      "Epoch 23, batch 81 D Loss: 1.3687392473220825, G Loss: 0.6990438103675842\n",
      "Epoch 23, batch 82 D Loss: 1.3728591203689575, G Loss: 0.7086074948310852\n",
      "Epoch 23, batch 83 D Loss: 1.3639576435089111, G Loss: 0.7038971781730652\n",
      "Epoch 23, batch 84 D Loss: 1.395903468132019, G Loss: 0.6931941509246826\n",
      "Epoch 23, batch 85 D Loss: 1.4015395641326904, G Loss: 0.6852747201919556\n",
      "Epoch 23, batch 86 D Loss: 1.4425361156463623, G Loss: 0.6723137497901917\n",
      "Epoch 23, batch 87 D Loss: 1.3913440704345703, G Loss: 0.6913847327232361\n",
      "Epoch 23, batch 88 D Loss: 1.4152357578277588, G Loss: 0.6575935482978821\n",
      "Epoch 23, batch 89 D Loss: 1.373502254486084, G Loss: 0.6887446641921997\n",
      "Epoch 23, batch 90 D Loss: 1.386366844177246, G Loss: 0.6784051656723022\n",
      "Epoch 23, batch 91 D Loss: 1.3832405805587769, G Loss: 0.671040415763855\n",
      "Epoch 23, batch 92 D Loss: 1.3983190059661865, G Loss: 0.669009268283844\n",
      "Epoch 23, batch 93 D Loss: 1.3953793048858643, G Loss: 0.6819504499435425\n",
      "Epoch 23, batch 94 D Loss: 1.3619823455810547, G Loss: 0.6950538754463196\n",
      "Epoch 23, batch 95 D Loss: 1.3875652551651, G Loss: 0.6805024743080139\n",
      "Epoch 23, batch 96 D Loss: 1.3644450902938843, G Loss: 0.6795966625213623\n",
      "Epoch 23, batch 97 D Loss: 1.372394323348999, G Loss: 0.6774519085884094\n",
      "Epoch 23, batch 98 D Loss: 1.4158508777618408, G Loss: 0.6655172109603882\n",
      "Epoch 23, batch 99 D Loss: 1.4029908180236816, G Loss: 0.6756049990653992\n",
      "Epoch 23, batch 100 D Loss: 1.393471121788025, G Loss: 0.6854671239852905\n",
      "Epoch 23, batch 101 D Loss: 1.4154212474822998, G Loss: 0.6735829710960388\n",
      "Epoch 23, batch 102 D Loss: 1.3762974739074707, G Loss: 0.6856706142425537\n",
      "Epoch 23, batch 103 D Loss: 1.412982702255249, G Loss: 0.6682659983634949\n",
      "Epoch 23, batch 104 D Loss: 1.3973798751831055, G Loss: 0.6932623982429504\n",
      "Epoch 23, batch 105 D Loss: 1.410203456878662, G Loss: 0.6830015778541565\n",
      "Epoch 23, batch 106 D Loss: 1.3878417015075684, G Loss: 0.6950986981391907\n",
      "Epoch 23, batch 107 D Loss: 1.3799350261688232, G Loss: 0.6950369477272034\n",
      "Epoch 23, batch 108 D Loss: 1.4023998975753784, G Loss: 0.6862327456474304\n",
      "Epoch 23, batch 109 D Loss: 1.410776972770691, G Loss: 0.693758487701416\n",
      "Epoch 23, batch 110 D Loss: 1.4343838691711426, G Loss: 0.6801387071609497\n",
      "Epoch 23, batch 111 D Loss: 1.3921217918395996, G Loss: 0.6862760782241821\n",
      "Epoch 23, batch 112 D Loss: 1.4390418529510498, G Loss: 0.6769542694091797\n",
      "Epoch 23, batch 113 D Loss: 1.3860080242156982, G Loss: 0.6971402764320374\n",
      "Epoch 23, batch 114 D Loss: 1.4196875095367432, G Loss: 0.6771780252456665\n",
      "Epoch 23, batch 115 D Loss: 1.3982994556427002, G Loss: 0.7017546892166138\n",
      "Epoch 23, batch 116 D Loss: 1.402762532234192, G Loss: 0.704373836517334\n",
      "Epoch 23, batch 117 D Loss: 1.4047518968582153, G Loss: 0.6948220133781433\n",
      "Epoch 23, batch 118 D Loss: 1.4106813669204712, G Loss: 0.6993252038955688\n",
      "Epoch 23, batch 119 D Loss: 1.383962631225586, G Loss: 0.7189881205558777\n",
      "Epoch 23, batch 120 D Loss: 1.3836296796798706, G Loss: 0.7142580151557922\n",
      "Epoch 23, batch 121 D Loss: 1.4122107028961182, G Loss: 0.6958116292953491\n",
      "Epoch 23, batch 122 D Loss: 1.3805768489837646, G Loss: 0.7281625866889954\n",
      "Epoch 23, batch 123 D Loss: 1.4164600372314453, G Loss: 0.7174715995788574\n",
      "Epoch 23, batch 124 D Loss: 1.399843692779541, G Loss: 0.7220399379730225\n",
      "Epoch 23, batch 125 D Loss: 1.4179707765579224, G Loss: 0.7004507184028625\n",
      "Epoch 23, batch 126 D Loss: 1.3847777843475342, G Loss: 0.7253020405769348\n",
      "Epoch 23, batch 127 D Loss: 1.406721591949463, G Loss: 0.717970609664917\n",
      "Epoch 23, batch 128 D Loss: 1.4171488285064697, G Loss: 0.7110257148742676\n",
      "Epoch 23, batch 129 D Loss: 1.3860869407653809, G Loss: 0.7268896698951721\n",
      "Epoch 23, batch 130 D Loss: 1.3987478017807007, G Loss: 0.7079775929450989\n",
      "Epoch 23, batch 131 D Loss: 1.4008651971817017, G Loss: 0.721920907497406\n",
      "Epoch 23, batch 132 D Loss: 1.37480890750885, G Loss: 0.7310949563980103\n",
      "Epoch 23, batch 133 D Loss: 1.3863415718078613, G Loss: 0.7151647806167603\n",
      "Epoch 23, batch 134 D Loss: 1.377326488494873, G Loss: 0.734442949295044\n",
      "Epoch 23, batch 135 D Loss: 1.4058207273483276, G Loss: 0.7067999839782715\n",
      "Epoch 23, batch 136 D Loss: 1.4020872116088867, G Loss: 0.7141845226287842\n",
      "Epoch 23, batch 137 D Loss: 1.3576265573501587, G Loss: 0.7299638986587524\n",
      "Epoch 23, batch 138 D Loss: 1.3830244541168213, G Loss: 0.7372139096260071\n",
      "Epoch 23, batch 139 D Loss: 1.3822931051254272, G Loss: 0.7261111736297607\n",
      "Epoch 23, batch 140 D Loss: 1.368338942527771, G Loss: 0.7361708879470825\n",
      "Epoch 23, batch 141 D Loss: 1.4051464796066284, G Loss: 0.7082170844078064\n",
      "Epoch 23, batch 142 D Loss: 1.3926279544830322, G Loss: 0.7155045866966248\n",
      "Epoch 23, batch 143 D Loss: 1.431380271911621, G Loss: 0.69318026304245\n",
      "Epoch 23, batch 144 D Loss: 1.355004906654358, G Loss: 0.7267650365829468\n",
      "Epoch 23, batch 145 D Loss: 1.4101834297180176, G Loss: 0.7139686346054077\n",
      "Epoch 23, batch 146 D Loss: 1.3910906314849854, G Loss: 0.7113337516784668\n",
      "Epoch 23, batch 147 D Loss: 1.3991328477859497, G Loss: 0.7009958028793335\n",
      "Epoch 23, batch 148 D Loss: 1.3916178941726685, G Loss: 0.7174659967422485\n",
      "Epoch 23, batch 149 D Loss: 1.4053046703338623, G Loss: 0.7022525072097778\n",
      "Epoch 23, batch 150 D Loss: 1.371891975402832, G Loss: 0.7176336050033569\n",
      "Epoch 23, batch 151 D Loss: 1.3715083599090576, G Loss: 0.7007055878639221\n",
      "Epoch 23, batch 152 D Loss: 1.3818011283874512, G Loss: 0.7003034353256226\n",
      "Epoch 23, batch 153 D Loss: 1.3931797742843628, G Loss: 0.6879476308822632\n",
      "Epoch 23, batch 154 D Loss: 1.3639872074127197, G Loss: 0.7014740109443665\n",
      "Epoch 23, batch 155 D Loss: 1.3897910118103027, G Loss: 0.6931806206703186\n",
      "Epoch 23, batch 156 D Loss: 1.3543009757995605, G Loss: 0.7052462100982666\n",
      "Epoch 23, batch 157 D Loss: 1.3698432445526123, G Loss: 0.6950514912605286\n",
      "Epoch 23, batch 158 D Loss: 1.3758488893508911, G Loss: 0.6873103976249695\n",
      "Epoch 23, batch 159 D Loss: 1.3585827350616455, G Loss: 0.7045592665672302\n",
      "Epoch 23, batch 160 D Loss: 1.353866457939148, G Loss: 0.677518904209137\n",
      "Epoch 23, batch 161 D Loss: 1.3810336589813232, G Loss: 0.6751837134361267\n",
      "Epoch 23, batch 162 D Loss: 1.3741657733917236, G Loss: 0.681114137172699\n",
      "Epoch 23, batch 163 D Loss: 1.3714141845703125, G Loss: 0.6873250007629395\n",
      "Epoch 23, batch 164 D Loss: 1.3718267679214478, G Loss: 0.6891484260559082\n",
      "Epoch 23, batch 165 D Loss: 1.3623170852661133, G Loss: 0.6965975761413574\n",
      "Epoch 23, batch 166 D Loss: 1.339402675628662, G Loss: 0.6937249898910522\n",
      "Epoch 23, batch 167 D Loss: 1.3850221633911133, G Loss: 0.6943474411964417\n",
      "Epoch 23, batch 168 D Loss: 1.3468658924102783, G Loss: 0.7035914659500122\n",
      "Epoch 23, batch 169 D Loss: 1.3557922840118408, G Loss: 0.7030107975006104\n",
      "Epoch 23, batch 170 D Loss: 1.344764232635498, G Loss: 0.6998579502105713\n",
      "Epoch 23, batch 171 D Loss: 1.337645173072815, G Loss: 0.7068541646003723\n",
      "Epoch 23, batch 172 D Loss: 1.3912522792816162, G Loss: 0.6782079935073853\n",
      "Epoch 23, batch 173 D Loss: 1.370060920715332, G Loss: 0.7033073902130127\n",
      "Epoch 23, batch 174 D Loss: 1.334474802017212, G Loss: 0.7033356428146362\n",
      "Epoch 23, batch 175 D Loss: 1.3606247901916504, G Loss: 0.7002894878387451\n",
      "Epoch 23, batch 176 D Loss: 1.4029302597045898, G Loss: 0.6857866048812866\n",
      "Epoch 23, batch 177 D Loss: 1.3706777095794678, G Loss: 0.6835812330245972\n",
      "Epoch 23, batch 178 D Loss: 1.353672742843628, G Loss: 0.7048124074935913\n",
      "Epoch 23, batch 179 D Loss: 1.3920679092407227, G Loss: 0.6828303337097168\n",
      "Epoch 23, batch 180 D Loss: 1.3584282398223877, G Loss: 0.7018424272537231\n",
      "Epoch 23, batch 181 D Loss: 1.3838191032409668, G Loss: 0.6702384352684021\n",
      "Epoch 23, batch 182 D Loss: 1.3320386409759521, G Loss: 0.7167976498603821\n",
      "Epoch 23, batch 183 D Loss: 1.3999375104904175, G Loss: 0.6938337683677673\n",
      "Epoch 23, batch 184 D Loss: 1.3589844703674316, G Loss: 0.719971776008606\n",
      "Epoch 23, batch 185 D Loss: 1.3640525341033936, G Loss: 0.716115415096283\n",
      "Epoch 23, batch 186 D Loss: 1.3704588413238525, G Loss: 0.6968898177146912\n",
      "Epoch 23, batch 187 D Loss: 1.4124906063079834, G Loss: 0.6773486137390137\n",
      "Epoch 23, batch 188 D Loss: 1.3913757801055908, G Loss: 0.714633047580719\n",
      "Epoch 23, batch 189 D Loss: 1.3747482299804688, G Loss: 0.707145094871521\n",
      "Epoch 23, batch 190 D Loss: 1.3846395015716553, G Loss: 0.7014886736869812\n",
      "Epoch 23, batch 191 D Loss: 1.3799830675125122, G Loss: 0.6905784010887146\n",
      "Epoch 23, batch 192 D Loss: 1.423058032989502, G Loss: 0.7000268697738647\n",
      "Epoch 23, batch 193 D Loss: 1.3932898044586182, G Loss: 0.678729236125946\n",
      "Epoch 23, batch 194 D Loss: 1.4033865928649902, G Loss: 0.6909409165382385\n",
      "Epoch 23, batch 195 D Loss: 1.4243459701538086, G Loss: 0.6849008202552795\n",
      "Epoch 23, batch 196 D Loss: 1.4368646144866943, G Loss: 0.6804531812667847\n",
      "Epoch 23, batch 197 D Loss: 1.3905110359191895, G Loss: 0.6957405209541321\n",
      "Epoch 23, batch 198 D Loss: 1.426748514175415, G Loss: 0.6729307770729065\n",
      "Epoch 23, batch 199 D Loss: 1.3816726207733154, G Loss: 0.7006656527519226\n",
      "Epoch 23, batch 200 D Loss: 1.409958839416504, G Loss: 0.6951985359191895\n",
      "Epoch 24, batch 1 D Loss: 1.3854994773864746, G Loss: 0.6823028326034546\n",
      "Epoch 24, batch 2 D Loss: 1.3961429595947266, G Loss: 0.6739215850830078\n",
      "Epoch 24, batch 3 D Loss: 1.3682234287261963, G Loss: 0.6830768585205078\n",
      "Epoch 24, batch 4 D Loss: 1.386535882949829, G Loss: 0.6877349019050598\n",
      "Epoch 24, batch 5 D Loss: 1.3992184400558472, G Loss: 0.6721979379653931\n",
      "Epoch 24, batch 6 D Loss: 1.3716309070587158, G Loss: 0.6765198707580566\n",
      "Epoch 24, batch 7 D Loss: 1.4203808307647705, G Loss: 0.6728628277778625\n",
      "Epoch 24, batch 8 D Loss: 1.4010372161865234, G Loss: 0.6749381422996521\n",
      "Epoch 24, batch 9 D Loss: 1.3814611434936523, G Loss: 0.6788250207901001\n",
      "Epoch 24, batch 10 D Loss: 1.4023032188415527, G Loss: 0.6767266988754272\n",
      "Epoch 24, batch 11 D Loss: 1.4199336767196655, G Loss: 0.6668144464492798\n",
      "Epoch 24, batch 12 D Loss: 1.425529956817627, G Loss: 0.6707270741462708\n",
      "Epoch 24, batch 13 D Loss: 1.3773458003997803, G Loss: 0.6881235241889954\n",
      "Epoch 24, batch 14 D Loss: 1.4080545902252197, G Loss: 0.6802376508712769\n",
      "Epoch 24, batch 15 D Loss: 1.42710542678833, G Loss: 0.6562309265136719\n",
      "Epoch 24, batch 16 D Loss: 1.3746702671051025, G Loss: 0.6865146160125732\n",
      "Epoch 24, batch 17 D Loss: 1.3964861631393433, G Loss: 0.6825904250144958\n",
      "Epoch 24, batch 18 D Loss: 1.4237703084945679, G Loss: 0.6728581786155701\n",
      "Epoch 24, batch 19 D Loss: 1.3950374126434326, G Loss: 0.6808667778968811\n",
      "Epoch 24, batch 20 D Loss: 1.3912274837493896, G Loss: 0.6829054355621338\n",
      "Epoch 24, batch 21 D Loss: 1.4030306339263916, G Loss: 0.6692320108413696\n",
      "Epoch 24, batch 22 D Loss: 1.435607671737671, G Loss: 0.6515316963195801\n",
      "Epoch 24, batch 23 D Loss: 1.4351284503936768, G Loss: 0.6530326008796692\n",
      "Epoch 24, batch 24 D Loss: 1.3915281295776367, G Loss: 0.6669589877128601\n",
      "Epoch 24, batch 25 D Loss: 1.443373203277588, G Loss: 0.6625151038169861\n",
      "Epoch 24, batch 26 D Loss: 1.4086382389068604, G Loss: 0.6654953956604004\n",
      "Epoch 24, batch 27 D Loss: 1.419600486755371, G Loss: 0.6749769449234009\n",
      "Epoch 24, batch 28 D Loss: 1.4011415243148804, G Loss: 0.6989642381668091\n",
      "Epoch 24, batch 29 D Loss: 1.456040859222412, G Loss: 0.6629485487937927\n",
      "Epoch 24, batch 30 D Loss: 1.3914986848831177, G Loss: 0.6897673010826111\n",
      "Epoch 24, batch 31 D Loss: 1.3822444677352905, G Loss: 0.7001236081123352\n",
      "Epoch 24, batch 32 D Loss: 1.409632682800293, G Loss: 0.6900694370269775\n",
      "Epoch 24, batch 33 D Loss: 1.4282636642456055, G Loss: 0.684588611125946\n",
      "Epoch 24, batch 34 D Loss: 1.401627540588379, G Loss: 0.6889514327049255\n",
      "Epoch 24, batch 35 D Loss: 1.4208085536956787, G Loss: 0.6838653087615967\n",
      "Epoch 24, batch 36 D Loss: 1.401602029800415, G Loss: 0.7028224468231201\n",
      "Epoch 24, batch 37 D Loss: 1.4075355529785156, G Loss: 0.687814474105835\n",
      "Epoch 24, batch 38 D Loss: 1.4193955659866333, G Loss: 0.701500654220581\n",
      "Epoch 24, batch 39 D Loss: 1.4088001251220703, G Loss: 0.7046180963516235\n",
      "Epoch 24, batch 40 D Loss: 1.3895679712295532, G Loss: 0.7159785628318787\n",
      "Epoch 24, batch 41 D Loss: 1.3967571258544922, G Loss: 0.7171134948730469\n",
      "Epoch 24, batch 42 D Loss: 1.4099783897399902, G Loss: 0.7066949605941772\n",
      "Epoch 24, batch 43 D Loss: 1.403578519821167, G Loss: 0.7290846109390259\n",
      "Epoch 24, batch 44 D Loss: 1.383392333984375, G Loss: 0.7239488363265991\n",
      "Epoch 24, batch 45 D Loss: 1.404576301574707, G Loss: 0.7226335406303406\n",
      "Epoch 24, batch 46 D Loss: 1.369197130203247, G Loss: 0.733283519744873\n",
      "Epoch 24, batch 47 D Loss: 1.3884755373001099, G Loss: 0.7306227684020996\n",
      "Epoch 24, batch 48 D Loss: 1.412529706954956, G Loss: 0.714982271194458\n",
      "Epoch 24, batch 49 D Loss: 1.3968307971954346, G Loss: 0.7293291687965393\n",
      "Epoch 24, batch 50 D Loss: 1.4037572145462036, G Loss: 0.7234342098236084\n",
      "Epoch 24, batch 51 D Loss: 1.385108470916748, G Loss: 0.7285811901092529\n",
      "Epoch 24, batch 52 D Loss: 1.3807212114334106, G Loss: 0.7282589077949524\n",
      "Epoch 24, batch 53 D Loss: 1.3792918920516968, G Loss: 0.732042670249939\n",
      "Epoch 24, batch 54 D Loss: 1.3824145793914795, G Loss: 0.7220109701156616\n",
      "Epoch 24, batch 55 D Loss: 1.4013350009918213, G Loss: 0.7230406999588013\n",
      "Epoch 24, batch 56 D Loss: 1.3983194828033447, G Loss: 0.735511839389801\n",
      "Epoch 24, batch 57 D Loss: 1.344738483428955, G Loss: 0.730807900428772\n",
      "Epoch 24, batch 58 D Loss: 1.3758777379989624, G Loss: 0.7338019013404846\n",
      "Epoch 24, batch 59 D Loss: 1.3880839347839355, G Loss: 0.7268297076225281\n",
      "Epoch 24, batch 60 D Loss: 1.3818708658218384, G Loss: 0.7272317409515381\n",
      "Epoch 24, batch 61 D Loss: 1.3764545917510986, G Loss: 0.7362427711486816\n",
      "Epoch 24, batch 62 D Loss: 1.3764994144439697, G Loss: 0.7320185303688049\n",
      "Epoch 24, batch 63 D Loss: 1.3828693628311157, G Loss: 0.7276214361190796\n",
      "Epoch 24, batch 64 D Loss: 1.3746302127838135, G Loss: 0.7388215661048889\n",
      "Epoch 24, batch 65 D Loss: 1.371567726135254, G Loss: 0.7389408349990845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, batch 66 D Loss: 1.354954481124878, G Loss: 0.7431755065917969\n",
      "Epoch 24, batch 67 D Loss: 1.357404351234436, G Loss: 0.7281873226165771\n",
      "Epoch 24, batch 68 D Loss: 1.3599357604980469, G Loss: 0.7338122725486755\n",
      "Epoch 24, batch 69 D Loss: 1.3695073127746582, G Loss: 0.739107072353363\n",
      "Epoch 24, batch 70 D Loss: 1.3517721891403198, G Loss: 0.7393361926078796\n",
      "Epoch 24, batch 71 D Loss: 1.3690755367279053, G Loss: 0.7354543209075928\n",
      "Epoch 24, batch 72 D Loss: 1.3574308156967163, G Loss: 0.7286399602890015\n",
      "Epoch 24, batch 73 D Loss: 1.352164626121521, G Loss: 0.7591400146484375\n",
      "Epoch 24, batch 74 D Loss: 1.341245412826538, G Loss: 0.7520656585693359\n",
      "Epoch 24, batch 75 D Loss: 1.362973690032959, G Loss: 0.70911705493927\n",
      "Epoch 24, batch 76 D Loss: 1.350006103515625, G Loss: 0.7295694947242737\n",
      "Epoch 24, batch 77 D Loss: 1.381463646888733, G Loss: 0.7307199239730835\n",
      "Epoch 24, batch 78 D Loss: 1.311138391494751, G Loss: 0.7540907859802246\n",
      "Epoch 24, batch 79 D Loss: 1.3560540676116943, G Loss: 0.738201916217804\n",
      "Epoch 24, batch 80 D Loss: 1.3817306756973267, G Loss: 0.7119249701499939\n",
      "Epoch 24, batch 81 D Loss: 1.3737013339996338, G Loss: 0.7573325634002686\n",
      "Epoch 24, batch 82 D Loss: 1.3616158962249756, G Loss: 0.7236552238464355\n",
      "Epoch 24, batch 83 D Loss: 1.3530205488204956, G Loss: 0.695269763469696\n",
      "Epoch 24, batch 84 D Loss: 1.3962194919586182, G Loss: 0.6869940161705017\n",
      "Epoch 24, batch 85 D Loss: 1.3325906991958618, G Loss: 0.738192081451416\n",
      "Epoch 24, batch 86 D Loss: 1.3740448951721191, G Loss: 0.7239024639129639\n",
      "Epoch 24, batch 87 D Loss: 1.324509620666504, G Loss: 0.7128762602806091\n",
      "Epoch 24, batch 88 D Loss: 1.3583873510360718, G Loss: 0.7203658223152161\n",
      "Epoch 24, batch 89 D Loss: 1.344467043876648, G Loss: 0.7154783606529236\n",
      "Epoch 24, batch 90 D Loss: 1.3130985498428345, G Loss: 0.7313899397850037\n",
      "Epoch 24, batch 91 D Loss: 1.3708746433258057, G Loss: 0.7033061981201172\n",
      "Epoch 24, batch 92 D Loss: 1.3552688360214233, G Loss: 0.722413182258606\n",
      "Epoch 24, batch 93 D Loss: 1.3741945028305054, G Loss: 0.7032355666160583\n",
      "Epoch 24, batch 94 D Loss: 1.3712818622589111, G Loss: 0.699694037437439\n",
      "Epoch 24, batch 95 D Loss: 1.3277881145477295, G Loss: 0.7201559543609619\n",
      "Epoch 24, batch 96 D Loss: 1.3531339168548584, G Loss: 0.732590913772583\n",
      "Epoch 24, batch 97 D Loss: 1.3979554176330566, G Loss: 0.7126874327659607\n",
      "Epoch 24, batch 98 D Loss: 1.4006152153015137, G Loss: 0.7001980543136597\n",
      "Epoch 24, batch 99 D Loss: 1.2652854919433594, G Loss: 0.7691017985343933\n",
      "Epoch 24, batch 100 D Loss: 1.3773415088653564, G Loss: 0.7200295925140381\n",
      "Epoch 24, batch 101 D Loss: 1.33243989944458, G Loss: 0.7458547949790955\n",
      "Epoch 24, batch 102 D Loss: 1.3673155307769775, G Loss: 0.7135093808174133\n",
      "Epoch 24, batch 103 D Loss: 1.3773235082626343, G Loss: 0.6885455250740051\n",
      "Epoch 24, batch 104 D Loss: 1.4232817888259888, G Loss: 0.6814419031143188\n",
      "Epoch 24, batch 105 D Loss: 1.370872974395752, G Loss: 0.7213830351829529\n",
      "Epoch 24, batch 106 D Loss: 1.447568416595459, G Loss: 0.6858245730400085\n",
      "Epoch 24, batch 107 D Loss: 1.3931385278701782, G Loss: 0.6917693614959717\n",
      "Epoch 24, batch 108 D Loss: 1.3695182800292969, G Loss: 0.7226335406303406\n",
      "Epoch 24, batch 109 D Loss: 1.3709619045257568, G Loss: 0.6676496863365173\n",
      "Epoch 24, batch 110 D Loss: 1.3340940475463867, G Loss: 0.749306321144104\n",
      "Epoch 24, batch 111 D Loss: 1.3714344501495361, G Loss: 0.71596360206604\n",
      "Epoch 24, batch 112 D Loss: 1.413327932357788, G Loss: 0.682941198348999\n",
      "Epoch 24, batch 113 D Loss: 1.4217402935028076, G Loss: 0.6887996792793274\n",
      "Epoch 24, batch 114 D Loss: 1.4000883102416992, G Loss: 0.6797387599945068\n",
      "Epoch 24, batch 115 D Loss: 1.3936100006103516, G Loss: 0.677344799041748\n",
      "Epoch 24, batch 116 D Loss: 1.3637748956680298, G Loss: 0.6967782378196716\n",
      "Epoch 24, batch 117 D Loss: 1.4168872833251953, G Loss: 0.682696521282196\n",
      "Epoch 24, batch 118 D Loss: 1.4099003076553345, G Loss: 0.6723095178604126\n",
      "Epoch 24, batch 119 D Loss: 1.438232660293579, G Loss: 0.6633131504058838\n",
      "Epoch 24, batch 120 D Loss: 1.3723559379577637, G Loss: 0.6926825046539307\n",
      "Epoch 24, batch 121 D Loss: 1.431944727897644, G Loss: 0.6646909117698669\n",
      "Epoch 24, batch 122 D Loss: 1.3981719017028809, G Loss: 0.6894667148590088\n",
      "Epoch 24, batch 123 D Loss: 1.4084930419921875, G Loss: 0.6798505187034607\n",
      "Epoch 24, batch 124 D Loss: 1.4151813983917236, G Loss: 0.6679084897041321\n",
      "Epoch 24, batch 125 D Loss: 1.429750919342041, G Loss: 0.6817648410797119\n",
      "Epoch 24, batch 126 D Loss: 1.3758726119995117, G Loss: 0.7090672254562378\n",
      "Epoch 24, batch 127 D Loss: 1.4484870433807373, G Loss: 0.6721978187561035\n",
      "Epoch 24, batch 128 D Loss: 1.4145596027374268, G Loss: 0.6826213002204895\n",
      "Epoch 24, batch 129 D Loss: 1.3942008018493652, G Loss: 0.6861581206321716\n",
      "Epoch 24, batch 130 D Loss: 1.4618074893951416, G Loss: 0.6395145654678345\n",
      "Epoch 24, batch 131 D Loss: 1.4089030027389526, G Loss: 0.6668777465820312\n",
      "Epoch 24, batch 132 D Loss: 1.4223805665969849, G Loss: 0.6634872555732727\n",
      "Epoch 24, batch 133 D Loss: 1.4357826709747314, G Loss: 0.6686741709709167\n",
      "Epoch 24, batch 134 D Loss: 1.4675407409667969, G Loss: 0.6478644609451294\n",
      "Epoch 24, batch 135 D Loss: 1.4655985832214355, G Loss: 0.6666949391365051\n",
      "Epoch 24, batch 136 D Loss: 1.4362276792526245, G Loss: 0.6698920726776123\n",
      "Epoch 24, batch 137 D Loss: 1.4092540740966797, G Loss: 0.6826480627059937\n",
      "Epoch 24, batch 138 D Loss: 1.4193475246429443, G Loss: 0.6558830738067627\n",
      "Epoch 24, batch 139 D Loss: 1.4260001182556152, G Loss: 0.6734787225723267\n",
      "Epoch 24, batch 140 D Loss: 1.4545540809631348, G Loss: 0.687869668006897\n",
      "Epoch 24, batch 141 D Loss: 1.4369395971298218, G Loss: 0.6856735944747925\n",
      "Epoch 24, batch 142 D Loss: 1.4566558599472046, G Loss: 0.6809578537940979\n",
      "Epoch 24, batch 143 D Loss: 1.4483070373535156, G Loss: 0.6873660087585449\n",
      "Epoch 24, batch 144 D Loss: 1.4315217733383179, G Loss: 0.7073830962181091\n",
      "Epoch 24, batch 145 D Loss: 1.4253828525543213, G Loss: 0.6991193294525146\n",
      "Epoch 24, batch 146 D Loss: 1.430420160293579, G Loss: 0.6836211681365967\n",
      "Epoch 24, batch 147 D Loss: 1.441476821899414, G Loss: 0.693630576133728\n",
      "Epoch 24, batch 148 D Loss: 1.4082387685775757, G Loss: 0.6993928551673889\n",
      "Epoch 24, batch 149 D Loss: 1.417729377746582, G Loss: 0.7101148962974548\n",
      "Epoch 24, batch 150 D Loss: 1.424990177154541, G Loss: 0.7027971744537354\n",
      "Epoch 24, batch 151 D Loss: 1.3994183540344238, G Loss: 0.7273321747779846\n",
      "Epoch 24, batch 152 D Loss: 1.4006807804107666, G Loss: 0.7281205654144287\n",
      "Epoch 24, batch 153 D Loss: 1.3829333782196045, G Loss: 0.7441315650939941\n",
      "Epoch 24, batch 154 D Loss: 1.4043214321136475, G Loss: 0.732268750667572\n",
      "Epoch 24, batch 155 D Loss: 1.3832250833511353, G Loss: 0.7508273124694824\n",
      "Epoch 24, batch 156 D Loss: 1.3691301345825195, G Loss: 0.7342625260353088\n",
      "Epoch 24, batch 157 D Loss: 1.420184850692749, G Loss: 0.7233067154884338\n",
      "Epoch 24, batch 158 D Loss: 1.4084932804107666, G Loss: 0.7322009205818176\n",
      "Epoch 24, batch 159 D Loss: 1.3935706615447998, G Loss: 0.7587949633598328\n",
      "Epoch 24, batch 160 D Loss: 1.4248651266098022, G Loss: 0.7267056107521057\n",
      "Epoch 24, batch 161 D Loss: 1.4231128692626953, G Loss: 0.7289814949035645\n",
      "Epoch 24, batch 162 D Loss: 1.4186067581176758, G Loss: 0.7325568199157715\n",
      "Epoch 24, batch 163 D Loss: 1.4040145874023438, G Loss: 0.7309516668319702\n",
      "Epoch 24, batch 164 D Loss: 1.3895974159240723, G Loss: 0.7469372749328613\n",
      "Epoch 24, batch 165 D Loss: 1.3475791215896606, G Loss: 0.7548882365226746\n",
      "Epoch 24, batch 166 D Loss: 1.3888838291168213, G Loss: 0.7392343878746033\n",
      "Epoch 24, batch 167 D Loss: 1.3531339168548584, G Loss: 0.757607102394104\n",
      "Epoch 24, batch 168 D Loss: 1.384225606918335, G Loss: 0.7294706702232361\n",
      "Epoch 24, batch 169 D Loss: 1.3588321208953857, G Loss: 0.7494909763336182\n",
      "Epoch 24, batch 170 D Loss: 1.3859931230545044, G Loss: 0.7241339683532715\n",
      "Epoch 24, batch 171 D Loss: 1.3779215812683105, G Loss: 0.7456086874008179\n",
      "Epoch 24, batch 172 D Loss: 1.345024585723877, G Loss: 0.7636852860450745\n",
      "Epoch 24, batch 173 D Loss: 1.3642899990081787, G Loss: 0.7453659176826477\n",
      "Epoch 24, batch 174 D Loss: 1.3766850233078003, G Loss: 0.7172865867614746\n",
      "Epoch 24, batch 175 D Loss: 1.3567590713500977, G Loss: 0.7380955219268799\n",
      "Epoch 24, batch 176 D Loss: 1.370973825454712, G Loss: 0.7275927066802979\n",
      "Epoch 24, batch 177 D Loss: 1.3595010042190552, G Loss: 0.7329396605491638\n",
      "Epoch 24, batch 178 D Loss: 1.3640027046203613, G Loss: 0.7277681827545166\n",
      "Epoch 24, batch 179 D Loss: 1.3699524402618408, G Loss: 0.7210920453071594\n",
      "Epoch 24, batch 180 D Loss: 1.360142469406128, G Loss: 0.7320869565010071\n",
      "Epoch 24, batch 181 D Loss: 1.3566282987594604, G Loss: 0.7425231337547302\n",
      "Epoch 24, batch 182 D Loss: 1.3493356704711914, G Loss: 0.7363038659095764\n",
      "Epoch 24, batch 183 D Loss: 1.3325130939483643, G Loss: 0.740612804889679\n",
      "Epoch 24, batch 184 D Loss: 1.3640638589859009, G Loss: 0.7401209473609924\n",
      "Epoch 24, batch 185 D Loss: 1.3499197959899902, G Loss: 0.7259806990623474\n",
      "Epoch 24, batch 186 D Loss: 1.3353227376937866, G Loss: 0.7582850456237793\n",
      "Epoch 24, batch 187 D Loss: 1.3528878688812256, G Loss: 0.71437668800354\n",
      "Epoch 24, batch 188 D Loss: 1.3452568054199219, G Loss: 0.7130584716796875\n",
      "Epoch 24, batch 189 D Loss: 1.364574909210205, G Loss: 0.6981447339057922\n",
      "Epoch 24, batch 190 D Loss: 1.333702564239502, G Loss: 0.7273520827293396\n",
      "Epoch 24, batch 191 D Loss: 1.3394830226898193, G Loss: 0.7302870154380798\n",
      "Epoch 24, batch 192 D Loss: 1.3313850164413452, G Loss: 0.7373067736625671\n",
      "Epoch 24, batch 193 D Loss: 1.368897795677185, G Loss: 0.7109127044677734\n",
      "Epoch 24, batch 194 D Loss: 1.3573670387268066, G Loss: 0.6928904056549072\n",
      "Epoch 24, batch 195 D Loss: 1.363800287246704, G Loss: 0.7036284804344177\n",
      "Epoch 24, batch 196 D Loss: 1.3497605323791504, G Loss: 0.739556074142456\n",
      "Epoch 24, batch 197 D Loss: 1.3590363264083862, G Loss: 0.7016381621360779\n",
      "Epoch 24, batch 198 D Loss: 1.294621229171753, G Loss: 0.7827726602554321\n",
      "Epoch 24, batch 199 D Loss: 1.3342711925506592, G Loss: 0.7317670583724976\n",
      "Epoch 24, batch 200 D Loss: 1.3721308708190918, G Loss: 0.6872566342353821\n",
      "Epoch 25, batch 1 D Loss: 1.3381567001342773, G Loss: 0.7111996412277222\n",
      "Epoch 25, batch 2 D Loss: 1.3785789012908936, G Loss: 0.6914623975753784\n",
      "Epoch 25, batch 3 D Loss: 1.3092937469482422, G Loss: 0.7339370846748352\n",
      "Epoch 25, batch 4 D Loss: 1.3337428569793701, G Loss: 0.7131420373916626\n",
      "Epoch 25, batch 5 D Loss: 1.3115684986114502, G Loss: 0.7306607961654663\n",
      "Epoch 25, batch 6 D Loss: 1.334454894065857, G Loss: 0.7070181369781494\n",
      "Epoch 25, batch 7 D Loss: 1.3271276950836182, G Loss: 0.7181649208068848\n",
      "Epoch 25, batch 8 D Loss: 1.3533741235733032, G Loss: 0.6686211228370667\n",
      "Epoch 25, batch 9 D Loss: 1.360736608505249, G Loss: 0.6834040284156799\n",
      "Epoch 25, batch 10 D Loss: 1.3446524143218994, G Loss: 0.6820077300071716\n",
      "Epoch 25, batch 11 D Loss: 1.3589065074920654, G Loss: 0.7064053416252136\n",
      "Epoch 25, batch 12 D Loss: 1.326979160308838, G Loss: 0.7300025820732117\n",
      "Epoch 25, batch 13 D Loss: 1.358191967010498, G Loss: 0.6988214254379272\n",
      "Epoch 25, batch 14 D Loss: 1.3741041421890259, G Loss: 0.6841413974761963\n",
      "Epoch 25, batch 15 D Loss: 1.4197351932525635, G Loss: 0.6621153950691223\n",
      "Epoch 25, batch 16 D Loss: 1.4127449989318848, G Loss: 0.6639812588691711\n",
      "Epoch 25, batch 17 D Loss: 1.338822841644287, G Loss: 0.7041740417480469\n",
      "Epoch 25, batch 18 D Loss: 1.359886646270752, G Loss: 0.6886571645736694\n",
      "Epoch 25, batch 19 D Loss: 1.4088484048843384, G Loss: 0.6634842157363892\n",
      "Epoch 25, batch 20 D Loss: 1.3715523481369019, G Loss: 0.6855388879776001\n",
      "Epoch 25, batch 21 D Loss: 1.3605842590332031, G Loss: 0.7105247378349304\n",
      "Epoch 25, batch 22 D Loss: 1.3527112007141113, G Loss: 0.7112507820129395\n",
      "Epoch 25, batch 23 D Loss: 1.477203130722046, G Loss: 0.6269875168800354\n",
      "Epoch 25, batch 24 D Loss: 1.358616590499878, G Loss: 0.705435037612915\n",
      "Epoch 25, batch 25 D Loss: 1.3012769222259521, G Loss: 0.7463690042495728\n",
      "Epoch 25, batch 26 D Loss: 1.47028648853302, G Loss: 0.6376811265945435\n",
      "Epoch 25, batch 27 D Loss: 1.4108211994171143, G Loss: 0.673735499382019\n",
      "Epoch 25, batch 28 D Loss: 1.3814111948013306, G Loss: 0.6880365610122681\n",
      "Epoch 25, batch 29 D Loss: 1.4778244495391846, G Loss: 0.642915666103363\n",
      "Epoch 25, batch 30 D Loss: 1.465592622756958, G Loss: 0.6462039351463318\n",
      "Epoch 25, batch 31 D Loss: 1.3912652730941772, G Loss: 0.6814826130867004\n",
      "Epoch 25, batch 32 D Loss: 1.4316127300262451, G Loss: 0.6615054607391357\n",
      "Epoch 25, batch 33 D Loss: 1.3949942588806152, G Loss: 0.6590172052383423\n",
      "Epoch 25, batch 34 D Loss: 1.4434140920639038, G Loss: 0.6435478925704956\n",
      "Epoch 25, batch 35 D Loss: 1.465238094329834, G Loss: 0.6385179758071899\n",
      "Epoch 25, batch 36 D Loss: 1.418400526046753, G Loss: 0.6828916668891907\n",
      "Epoch 25, batch 37 D Loss: 1.4410982131958008, G Loss: 0.6430140733718872\n",
      "Epoch 25, batch 38 D Loss: 1.387974500656128, G Loss: 0.6823259592056274\n",
      "Epoch 25, batch 39 D Loss: 1.4511430263519287, G Loss: 0.6479198932647705\n",
      "Epoch 25, batch 40 D Loss: 1.3951321840286255, G Loss: 0.6888366937637329\n",
      "Epoch 25, batch 41 D Loss: 1.4328367710113525, G Loss: 0.6824373602867126\n",
      "Epoch 25, batch 42 D Loss: 1.4003090858459473, G Loss: 0.6839644908905029\n",
      "Epoch 25, batch 43 D Loss: 1.435692310333252, G Loss: 0.6492538452148438\n",
      "Epoch 25, batch 44 D Loss: 1.4543781280517578, G Loss: 0.6626129150390625\n",
      "Epoch 25, batch 45 D Loss: 1.4285345077514648, G Loss: 0.6801120042800903\n",
      "Epoch 25, batch 46 D Loss: 1.4439783096313477, G Loss: 0.670945942401886\n",
      "Epoch 25, batch 47 D Loss: 1.448038101196289, G Loss: 0.6789065599441528\n",
      "Epoch 25, batch 48 D Loss: 1.417325735092163, G Loss: 0.6846708655357361\n",
      "Epoch 25, batch 49 D Loss: 1.4235796928405762, G Loss: 0.6743853092193604\n",
      "Epoch 25, batch 50 D Loss: 1.4550296068191528, G Loss: 0.6817374229431152\n",
      "Epoch 25, batch 51 D Loss: 1.450442910194397, G Loss: 0.6752206683158875\n",
      "Epoch 25, batch 52 D Loss: 1.4389197826385498, G Loss: 0.6898350715637207\n",
      "Epoch 25, batch 53 D Loss: 1.460620641708374, G Loss: 0.6606919765472412\n",
      "Epoch 25, batch 54 D Loss: 1.4339618682861328, G Loss: 0.6720597743988037\n",
      "Epoch 25, batch 55 D Loss: 1.4045913219451904, G Loss: 0.6889750957489014\n",
      "Epoch 25, batch 56 D Loss: 1.4223123788833618, G Loss: 0.7011048197746277\n",
      "Epoch 25, batch 57 D Loss: 1.4049150943756104, G Loss: 0.7035010457038879\n",
      "Epoch 25, batch 58 D Loss: 1.4009547233581543, G Loss: 0.7085450887680054\n",
      "Epoch 25, batch 59 D Loss: 1.4123330116271973, G Loss: 0.6857542395591736\n",
      "Epoch 25, batch 60 D Loss: 1.4676241874694824, G Loss: 0.6734393239021301\n",
      "Epoch 25, batch 61 D Loss: 1.420145034790039, G Loss: 0.7041749358177185\n",
      "Epoch 25, batch 62 D Loss: 1.4232016801834106, G Loss: 0.6919248104095459\n",
      "Epoch 25, batch 63 D Loss: 1.4276224374771118, G Loss: 0.670311689376831\n",
      "Epoch 25, batch 64 D Loss: 1.4280245304107666, G Loss: 0.682240903377533\n",
      "Epoch 25, batch 65 D Loss: 1.3951690196990967, G Loss: 0.6980621218681335\n",
      "Epoch 25, batch 66 D Loss: 1.4107165336608887, G Loss: 0.6772892475128174\n",
      "Epoch 25, batch 67 D Loss: 1.4293138980865479, G Loss: 0.6822466850280762\n",
      "Epoch 25, batch 68 D Loss: 1.405472755432129, G Loss: 0.7038002014160156\n",
      "Epoch 25, batch 69 D Loss: 1.3898707628250122, G Loss: 0.7120778560638428\n",
      "Epoch 25, batch 70 D Loss: 1.3949447870254517, G Loss: 0.7199943661689758\n",
      "Epoch 25, batch 71 D Loss: 1.4115359783172607, G Loss: 0.7052867412567139\n",
      "Epoch 25, batch 72 D Loss: 1.417128086090088, G Loss: 0.7110708355903625\n",
      "Epoch 25, batch 73 D Loss: 1.3972852230072021, G Loss: 0.7090322375297546\n",
      "Epoch 25, batch 74 D Loss: 1.4450654983520508, G Loss: 0.6972503662109375\n",
      "Epoch 25, batch 75 D Loss: 1.4095189571380615, G Loss: 0.7116020321846008\n",
      "Epoch 25, batch 76 D Loss: 1.4236652851104736, G Loss: 0.7085879445075989\n",
      "Epoch 25, batch 77 D Loss: 1.386803388595581, G Loss: 0.7430111169815063\n",
      "Epoch 25, batch 78 D Loss: 1.4145232439041138, G Loss: 0.7286695837974548\n",
      "Epoch 25, batch 79 D Loss: 1.3851206302642822, G Loss: 0.7580650448799133\n",
      "Epoch 25, batch 80 D Loss: 1.378686785697937, G Loss: 0.7434784173965454\n",
      "Epoch 25, batch 81 D Loss: 1.3877103328704834, G Loss: 0.7541876435279846\n",
      "Epoch 25, batch 82 D Loss: 1.403120756149292, G Loss: 0.7217829823493958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, batch 83 D Loss: 1.3925737142562866, G Loss: 0.7382438778877258\n",
      "Epoch 25, batch 84 D Loss: 1.3983261585235596, G Loss: 0.7374697923660278\n",
      "Epoch 25, batch 85 D Loss: 1.3755836486816406, G Loss: 0.7400043606758118\n",
      "Epoch 25, batch 86 D Loss: 1.3819403648376465, G Loss: 0.7367367744445801\n",
      "Epoch 25, batch 87 D Loss: 1.3706883192062378, G Loss: 0.7588114738464355\n",
      "Epoch 25, batch 88 D Loss: 1.3665823936462402, G Loss: 0.7446591854095459\n",
      "Epoch 25, batch 89 D Loss: 1.3678306341171265, G Loss: 0.747163712978363\n",
      "Epoch 25, batch 90 D Loss: 1.3725028038024902, G Loss: 0.7439869046211243\n",
      "Epoch 25, batch 91 D Loss: 1.347355604171753, G Loss: 0.758188784122467\n",
      "Epoch 25, batch 92 D Loss: 1.3666393756866455, G Loss: 0.7460123300552368\n",
      "Epoch 25, batch 93 D Loss: 1.353956937789917, G Loss: 0.7533068656921387\n",
      "Epoch 25, batch 94 D Loss: 1.3395695686340332, G Loss: 0.7615939974784851\n",
      "Epoch 25, batch 95 D Loss: 1.346341609954834, G Loss: 0.7547622919082642\n",
      "Epoch 25, batch 96 D Loss: 1.3606774806976318, G Loss: 0.7508819699287415\n",
      "Epoch 25, batch 97 D Loss: 1.3655576705932617, G Loss: 0.7347156405448914\n",
      "Epoch 25, batch 98 D Loss: 1.3647030591964722, G Loss: 0.733971118927002\n",
      "Epoch 25, batch 99 D Loss: 1.3491183519363403, G Loss: 0.7490414977073669\n",
      "Epoch 25, batch 100 D Loss: 1.3221734762191772, G Loss: 0.7738012671470642\n",
      "Epoch 25, batch 101 D Loss: 1.3594157695770264, G Loss: 0.7530204057693481\n",
      "Epoch 25, batch 102 D Loss: 1.344740390777588, G Loss: 0.7420526146888733\n",
      "Epoch 25, batch 103 D Loss: 1.3510069847106934, G Loss: 0.7407292127609253\n",
      "Epoch 25, batch 104 D Loss: 1.347949743270874, G Loss: 0.7500771880149841\n",
      "Epoch 25, batch 105 D Loss: 1.3395602703094482, G Loss: 0.7286835312843323\n",
      "Epoch 25, batch 106 D Loss: 1.32929265499115, G Loss: 0.7315384149551392\n",
      "Epoch 25, batch 107 D Loss: 1.3375945091247559, G Loss: 0.7300207614898682\n",
      "Epoch 25, batch 108 D Loss: 1.3501489162445068, G Loss: 0.7390052676200867\n",
      "Epoch 25, batch 109 D Loss: 1.3240902423858643, G Loss: 0.7310752868652344\n",
      "Epoch 25, batch 110 D Loss: 1.3582544326782227, G Loss: 0.7227450013160706\n",
      "Epoch 25, batch 111 D Loss: 1.3252668380737305, G Loss: 0.7289581894874573\n",
      "Epoch 25, batch 112 D Loss: 1.3151271343231201, G Loss: 0.730715811252594\n",
      "Epoch 25, batch 113 D Loss: 1.3222334384918213, G Loss: 0.729250967502594\n",
      "Epoch 25, batch 114 D Loss: 1.2609827518463135, G Loss: 0.7753430008888245\n",
      "Epoch 25, batch 115 D Loss: 1.3392112255096436, G Loss: 0.7030807733535767\n",
      "Epoch 25, batch 116 D Loss: 1.3079783916473389, G Loss: 0.7459878325462341\n",
      "Epoch 25, batch 117 D Loss: 1.3477082252502441, G Loss: 0.7105774879455566\n",
      "Epoch 25, batch 118 D Loss: 1.3397499322891235, G Loss: 0.7480581402778625\n",
      "Epoch 25, batch 119 D Loss: 1.3969991207122803, G Loss: 0.7044634222984314\n",
      "Epoch 25, batch 120 D Loss: 1.3083531856536865, G Loss: 0.7161221504211426\n",
      "Epoch 25, batch 121 D Loss: 1.3862357139587402, G Loss: 0.7328402996063232\n",
      "Epoch 25, batch 122 D Loss: 1.284684419631958, G Loss: 0.7778843641281128\n",
      "Epoch 25, batch 123 D Loss: 1.2735258340835571, G Loss: 0.7738339900970459\n",
      "Epoch 25, batch 124 D Loss: 1.3932363986968994, G Loss: 0.7271106243133545\n",
      "Epoch 25, batch 125 D Loss: 1.3877322673797607, G Loss: 0.7187742590904236\n",
      "Epoch 25, batch 126 D Loss: 1.3312275409698486, G Loss: 0.757625937461853\n",
      "Epoch 25, batch 127 D Loss: 1.3562147617340088, G Loss: 0.7292006611824036\n",
      "Epoch 25, batch 128 D Loss: 1.3498278856277466, G Loss: 0.7092626690864563\n",
      "Epoch 25, batch 129 D Loss: 1.3717693090438843, G Loss: 0.7079551815986633\n",
      "Epoch 25, batch 130 D Loss: 1.3585045337677002, G Loss: 0.7215806841850281\n",
      "Epoch 25, batch 131 D Loss: 1.3834455013275146, G Loss: 0.7168096303939819\n",
      "Epoch 25, batch 132 D Loss: 1.3422577381134033, G Loss: 0.6906933784484863\n",
      "Epoch 25, batch 133 D Loss: 1.3905723094940186, G Loss: 0.6694549322128296\n",
      "Epoch 25, batch 134 D Loss: 1.3830087184906006, G Loss: 0.6734786033630371\n",
      "Epoch 25, batch 135 D Loss: 1.3836910724639893, G Loss: 0.7098802924156189\n",
      "Epoch 25, batch 136 D Loss: 1.3823643922805786, G Loss: 0.7145568132400513\n",
      "Epoch 25, batch 137 D Loss: 1.377631425857544, G Loss: 0.6901331543922424\n",
      "Epoch 25, batch 138 D Loss: 1.3270010948181152, G Loss: 0.6954569220542908\n",
      "Epoch 25, batch 139 D Loss: 1.3470532894134521, G Loss: 0.6744548678398132\n",
      "Epoch 25, batch 140 D Loss: 1.4619355201721191, G Loss: 0.6434576511383057\n",
      "Epoch 25, batch 141 D Loss: 1.40836763381958, G Loss: 0.6909862756729126\n",
      "Epoch 25, batch 142 D Loss: 1.3499433994293213, G Loss: 0.7159092426300049\n",
      "Epoch 25, batch 143 D Loss: 1.383986473083496, G Loss: 0.7160500288009644\n",
      "Epoch 25, batch 144 D Loss: 1.3974299430847168, G Loss: 0.690686047077179\n",
      "Epoch 25, batch 145 D Loss: 1.3984705209732056, G Loss: 0.685616672039032\n",
      "Epoch 25, batch 146 D Loss: 1.4106967449188232, G Loss: 0.6631688475608826\n",
      "Epoch 25, batch 147 D Loss: 1.4306994676589966, G Loss: 0.6489859223365784\n",
      "Epoch 25, batch 148 D Loss: 1.430401086807251, G Loss: 0.636182963848114\n",
      "Epoch 25, batch 149 D Loss: 1.5339341163635254, G Loss: 0.6127005219459534\n",
      "Epoch 25, batch 150 D Loss: 1.435518503189087, G Loss: 0.6405296921730042\n",
      "Epoch 25, batch 151 D Loss: 1.4051262140274048, G Loss: 0.6737220883369446\n",
      "Epoch 25, batch 152 D Loss: 1.405336856842041, G Loss: 0.6693013906478882\n",
      "Epoch 25, batch 153 D Loss: 1.4701416492462158, G Loss: 0.6321053504943848\n",
      "Epoch 25, batch 154 D Loss: 1.47072172164917, G Loss: 0.6415572166442871\n",
      "Epoch 25, batch 155 D Loss: 1.4100456237792969, G Loss: 0.6668327450752258\n",
      "Epoch 25, batch 156 D Loss: 1.3696967363357544, G Loss: 0.6694151163101196\n",
      "Epoch 25, batch 157 D Loss: 1.4594995975494385, G Loss: 0.6435829401016235\n",
      "Epoch 25, batch 158 D Loss: 1.4207106828689575, G Loss: 0.6892507076263428\n",
      "Epoch 25, batch 159 D Loss: 1.3869210481643677, G Loss: 0.680491030216217\n",
      "Epoch 25, batch 160 D Loss: 1.4155782461166382, G Loss: 0.6640477180480957\n",
      "Epoch 25, batch 161 D Loss: 1.4229331016540527, G Loss: 0.6658535599708557\n",
      "Epoch 25, batch 162 D Loss: 1.46525239944458, G Loss: 0.6577973365783691\n",
      "Epoch 25, batch 163 D Loss: 1.4642307758331299, G Loss: 0.6611691117286682\n",
      "Epoch 25, batch 164 D Loss: 1.4429984092712402, G Loss: 0.6652915477752686\n",
      "Epoch 25, batch 165 D Loss: 1.422356367111206, G Loss: 0.6763935685157776\n",
      "Epoch 25, batch 166 D Loss: 1.4113999605178833, G Loss: 0.6878269910812378\n",
      "Epoch 25, batch 167 D Loss: 1.4510388374328613, G Loss: 0.6686004400253296\n",
      "Epoch 25, batch 168 D Loss: 1.4410887956619263, G Loss: 0.6734400987625122\n",
      "Epoch 25, batch 169 D Loss: 1.450850009918213, G Loss: 0.6514078378677368\n",
      "Epoch 25, batch 170 D Loss: 1.4667818546295166, G Loss: 0.6601573824882507\n",
      "Epoch 25, batch 171 D Loss: 1.4513788223266602, G Loss: 0.6527184844017029\n",
      "Epoch 25, batch 172 D Loss: 1.457897663116455, G Loss: 0.6416623592376709\n",
      "Epoch 25, batch 173 D Loss: 1.4275798797607422, G Loss: 0.664675772190094\n",
      "Epoch 25, batch 174 D Loss: 1.4064841270446777, G Loss: 0.6872536540031433\n",
      "Epoch 25, batch 175 D Loss: 1.4414087533950806, G Loss: 0.6825652122497559\n",
      "Epoch 25, batch 176 D Loss: 1.4482887983322144, G Loss: 0.6687004566192627\n",
      "Epoch 25, batch 177 D Loss: 1.4320132732391357, G Loss: 0.6936546564102173\n",
      "Epoch 25, batch 178 D Loss: 1.4456714391708374, G Loss: 0.6825510263442993\n",
      "Epoch 25, batch 179 D Loss: 1.4384914636611938, G Loss: 0.6751487255096436\n",
      "Epoch 25, batch 180 D Loss: 1.4406218528747559, G Loss: 0.6825320720672607\n",
      "Epoch 25, batch 181 D Loss: 1.4302560091018677, G Loss: 0.7096216678619385\n",
      "Epoch 25, batch 182 D Loss: 1.4294040203094482, G Loss: 0.6938002109527588\n",
      "Epoch 25, batch 183 D Loss: 1.3974192142486572, G Loss: 0.7162497639656067\n",
      "Epoch 25, batch 184 D Loss: 1.3950273990631104, G Loss: 0.7131598591804504\n",
      "Epoch 25, batch 185 D Loss: 1.4163618087768555, G Loss: 0.71991366147995\n",
      "Epoch 25, batch 186 D Loss: 1.4042161703109741, G Loss: 0.7153602838516235\n",
      "Epoch 25, batch 187 D Loss: 1.4165270328521729, G Loss: 0.7100145816802979\n",
      "Epoch 25, batch 188 D Loss: 1.4352879524230957, G Loss: 0.7217342853546143\n",
      "Epoch 25, batch 189 D Loss: 1.4366538524627686, G Loss: 0.7232455611228943\n",
      "Epoch 25, batch 190 D Loss: 1.398772120475769, G Loss: 0.7326980829238892\n",
      "Epoch 25, batch 191 D Loss: 1.3882744312286377, G Loss: 0.7299526929855347\n",
      "Epoch 25, batch 192 D Loss: 1.4063661098480225, G Loss: 0.7300125360488892\n",
      "Epoch 25, batch 193 D Loss: 1.3919589519500732, G Loss: 0.7483207583427429\n",
      "Epoch 25, batch 194 D Loss: 1.3988722562789917, G Loss: 0.7352741956710815\n",
      "Epoch 25, batch 195 D Loss: 1.397012710571289, G Loss: 0.7687206864356995\n",
      "Epoch 25, batch 196 D Loss: 1.4071240425109863, G Loss: 0.742698609828949\n",
      "Epoch 25, batch 197 D Loss: 1.4148378372192383, G Loss: 0.7321564555168152\n",
      "Epoch 25, batch 198 D Loss: 1.3885681629180908, G Loss: 0.7411477565765381\n",
      "Epoch 25, batch 199 D Loss: 1.381284236907959, G Loss: 0.7543345093727112\n",
      "Epoch 25, batch 200 D Loss: 1.3997459411621094, G Loss: 0.743815541267395\n",
      "Epoch 26, batch 1 D Loss: 1.413264274597168, G Loss: 0.7324714660644531\n",
      "Epoch 26, batch 2 D Loss: 1.3957443237304688, G Loss: 0.7498385906219482\n",
      "Epoch 26, batch 3 D Loss: 1.3821334838867188, G Loss: 0.7524950504302979\n",
      "Epoch 26, batch 4 D Loss: 1.3801565170288086, G Loss: 0.7404883503913879\n",
      "Epoch 26, batch 5 D Loss: 1.3585669994354248, G Loss: 0.7478305697441101\n",
      "Epoch 26, batch 6 D Loss: 1.3838423490524292, G Loss: 0.7460383772850037\n",
      "Epoch 26, batch 7 D Loss: 1.3525851964950562, G Loss: 0.7662796974182129\n",
      "Epoch 26, batch 8 D Loss: 1.3559014797210693, G Loss: 0.7546329498291016\n",
      "Epoch 26, batch 9 D Loss: 1.3740098476409912, G Loss: 0.7423964142799377\n",
      "Epoch 26, batch 10 D Loss: 1.381946325302124, G Loss: 0.728882908821106\n",
      "Epoch 26, batch 11 D Loss: 1.3782358169555664, G Loss: 0.7351720929145813\n",
      "Epoch 26, batch 12 D Loss: 1.3524731397628784, G Loss: 0.746366024017334\n",
      "Epoch 26, batch 13 D Loss: 1.3759562969207764, G Loss: 0.7305915951728821\n",
      "Epoch 26, batch 14 D Loss: 1.3543028831481934, G Loss: 0.7471802234649658\n",
      "Epoch 26, batch 15 D Loss: 1.3675559759140015, G Loss: 0.739205002784729\n",
      "Epoch 26, batch 16 D Loss: 1.3534959554672241, G Loss: 0.7460768818855286\n",
      "Epoch 26, batch 17 D Loss: 1.3399834632873535, G Loss: 0.7452728748321533\n",
      "Epoch 26, batch 18 D Loss: 1.354515552520752, G Loss: 0.746874213218689\n",
      "Epoch 26, batch 19 D Loss: 1.352696418762207, G Loss: 0.7305889129638672\n",
      "Epoch 26, batch 20 D Loss: 1.372016191482544, G Loss: 0.7250698804855347\n",
      "Epoch 26, batch 21 D Loss: 1.35762357711792, G Loss: 0.7348625659942627\n",
      "Epoch 26, batch 22 D Loss: 1.3218985795974731, G Loss: 0.7481128573417664\n",
      "Epoch 26, batch 23 D Loss: 1.3462128639221191, G Loss: 0.749582827091217\n",
      "Epoch 26, batch 24 D Loss: 1.3183047771453857, G Loss: 0.7616560459136963\n",
      "Epoch 26, batch 25 D Loss: 1.3500239849090576, G Loss: 0.7368373870849609\n",
      "Epoch 26, batch 26 D Loss: 1.366539478302002, G Loss: 0.7099451422691345\n",
      "Epoch 26, batch 27 D Loss: 1.3366334438323975, G Loss: 0.7433381676673889\n",
      "Epoch 26, batch 28 D Loss: 1.343592643737793, G Loss: 0.7174382209777832\n",
      "Epoch 26, batch 29 D Loss: 1.3111141920089722, G Loss: 0.7362120747566223\n",
      "Epoch 26, batch 30 D Loss: 1.3539698123931885, G Loss: 0.7194639444351196\n",
      "Epoch 26, batch 31 D Loss: 1.3309242725372314, G Loss: 0.7528263330459595\n",
      "Epoch 26, batch 32 D Loss: 1.3331552743911743, G Loss: 0.7339666485786438\n",
      "Epoch 26, batch 33 D Loss: 1.3391878604888916, G Loss: 0.780251145362854\n",
      "Epoch 26, batch 34 D Loss: 1.3524494171142578, G Loss: 0.7246244549751282\n",
      "Epoch 26, batch 35 D Loss: 1.3604940176010132, G Loss: 0.7146533131599426\n",
      "Epoch 26, batch 36 D Loss: 1.3441617488861084, G Loss: 0.7335682511329651\n",
      "Epoch 26, batch 37 D Loss: 1.323542833328247, G Loss: 0.7460406422615051\n",
      "Epoch 26, batch 38 D Loss: 1.3884919881820679, G Loss: 0.7010260820388794\n",
      "Epoch 26, batch 39 D Loss: 1.3848810195922852, G Loss: 0.7139602899551392\n",
      "Epoch 26, batch 40 D Loss: 1.3637990951538086, G Loss: 0.7218847870826721\n",
      "Epoch 26, batch 41 D Loss: 1.327184796333313, G Loss: 0.734712541103363\n",
      "Epoch 26, batch 42 D Loss: 1.396275520324707, G Loss: 0.7245897054672241\n",
      "Epoch 26, batch 43 D Loss: 1.3605401515960693, G Loss: 0.7195382118225098\n",
      "Epoch 26, batch 44 D Loss: 1.378669261932373, G Loss: 0.6926484107971191\n",
      "Epoch 26, batch 45 D Loss: 1.3529505729675293, G Loss: 0.6879922747612\n",
      "Epoch 26, batch 46 D Loss: 1.3770304918289185, G Loss: 0.6658774018287659\n",
      "Epoch 26, batch 47 D Loss: 1.3441007137298584, G Loss: 0.7139541506767273\n",
      "Epoch 26, batch 48 D Loss: 1.3665876388549805, G Loss: 0.7050462961196899\n",
      "Epoch 26, batch 49 D Loss: 1.3922028541564941, G Loss: 0.6746026873588562\n",
      "Epoch 26, batch 50 D Loss: 1.385047435760498, G Loss: 0.667457103729248\n",
      "Epoch 26, batch 51 D Loss: 1.3954719305038452, G Loss: 0.6737072467803955\n",
      "Epoch 26, batch 52 D Loss: 1.3742759227752686, G Loss: 0.6686403155326843\n",
      "Epoch 26, batch 53 D Loss: 1.4337496757507324, G Loss: 0.6478331685066223\n",
      "Epoch 26, batch 54 D Loss: 1.3549202680587769, G Loss: 0.6748904585838318\n",
      "Epoch 26, batch 55 D Loss: 1.3667027950286865, G Loss: 0.6761430501937866\n",
      "Epoch 26, batch 56 D Loss: 1.4012898206710815, G Loss: 0.6629025936126709\n",
      "Epoch 26, batch 57 D Loss: 1.3839507102966309, G Loss: 0.6880882382392883\n",
      "Epoch 26, batch 58 D Loss: 1.4362763166427612, G Loss: 0.6355538964271545\n",
      "Epoch 26, batch 59 D Loss: 1.4050040245056152, G Loss: 0.6724610328674316\n",
      "Epoch 26, batch 60 D Loss: 1.3740613460540771, G Loss: 0.7006164789199829\n",
      "Epoch 26, batch 61 D Loss: 1.4126958847045898, G Loss: 0.669663667678833\n",
      "Epoch 26, batch 62 D Loss: 1.3832738399505615, G Loss: 0.6893740296363831\n",
      "Epoch 26, batch 63 D Loss: 1.3724318742752075, G Loss: 0.6926392912864685\n",
      "Epoch 26, batch 64 D Loss: 1.363786220550537, G Loss: 0.6991361379623413\n",
      "Epoch 26, batch 65 D Loss: 1.445988416671753, G Loss: 0.6460941433906555\n",
      "Epoch 26, batch 66 D Loss: 1.363016128540039, G Loss: 0.6890629529953003\n",
      "Epoch 26, batch 67 D Loss: 1.4068291187286377, G Loss: 0.6647749543190002\n",
      "Epoch 26, batch 68 D Loss: 1.4084033966064453, G Loss: 0.6708974242210388\n",
      "Epoch 26, batch 69 D Loss: 1.4017632007598877, G Loss: 0.6827611327171326\n",
      "Epoch 26, batch 70 D Loss: 1.3958282470703125, G Loss: 0.7002617120742798\n",
      "Epoch 26, batch 71 D Loss: 1.413486361503601, G Loss: 0.6885471940040588\n",
      "Epoch 26, batch 72 D Loss: 1.403106927871704, G Loss: 0.6897010803222656\n",
      "Epoch 26, batch 73 D Loss: 1.3474583625793457, G Loss: 0.7086865901947021\n",
      "Epoch 26, batch 74 D Loss: 1.4188079833984375, G Loss: 0.6734632253646851\n",
      "Epoch 26, batch 75 D Loss: 1.42958402633667, G Loss: 0.6716269850730896\n",
      "Epoch 26, batch 76 D Loss: 1.4189047813415527, G Loss: 0.6532720327377319\n",
      "Epoch 26, batch 77 D Loss: 1.373950481414795, G Loss: 0.7028021216392517\n",
      "Epoch 26, batch 78 D Loss: 1.4278157949447632, G Loss: 0.6821744441986084\n",
      "Epoch 26, batch 79 D Loss: 1.4127919673919678, G Loss: 0.666745662689209\n",
      "Epoch 26, batch 80 D Loss: 1.4487268924713135, G Loss: 0.65749192237854\n",
      "Epoch 26, batch 81 D Loss: 1.4201209545135498, G Loss: 0.6736523509025574\n",
      "Epoch 26, batch 82 D Loss: 1.435853123664856, G Loss: 0.6591702103614807\n",
      "Epoch 26, batch 83 D Loss: 1.4074413776397705, G Loss: 0.696804940700531\n",
      "Epoch 26, batch 84 D Loss: 1.4300041198730469, G Loss: 0.6907041072845459\n",
      "Epoch 26, batch 85 D Loss: 1.416473627090454, G Loss: 0.6924802660942078\n",
      "Epoch 26, batch 86 D Loss: 1.436437726020813, G Loss: 0.6722572445869446\n",
      "Epoch 26, batch 87 D Loss: 1.4074442386627197, G Loss: 0.6914119124412537\n",
      "Epoch 26, batch 88 D Loss: 1.4139883518218994, G Loss: 0.6969141364097595\n",
      "Epoch 26, batch 89 D Loss: 1.4219396114349365, G Loss: 0.7018200159072876\n",
      "Epoch 26, batch 90 D Loss: 1.4122686386108398, G Loss: 0.7009434700012207\n",
      "Epoch 26, batch 91 D Loss: 1.4222592115402222, G Loss: 0.6917778849601746\n",
      "Epoch 26, batch 92 D Loss: 1.407322883605957, G Loss: 0.6898807287216187\n",
      "Epoch 26, batch 93 D Loss: 1.4591736793518066, G Loss: 0.6715493202209473\n",
      "Epoch 26, batch 94 D Loss: 1.4027376174926758, G Loss: 0.6878525018692017\n",
      "Epoch 26, batch 95 D Loss: 1.4252846240997314, G Loss: 0.6882005333900452\n",
      "Epoch 26, batch 96 D Loss: 1.4206867218017578, G Loss: 0.6805670261383057\n",
      "Epoch 26, batch 97 D Loss: 1.4242908954620361, G Loss: 0.6748717427253723\n",
      "Epoch 26, batch 98 D Loss: 1.4187068939208984, G Loss: 0.6933433413505554\n",
      "Epoch 26, batch 99 D Loss: 1.4144837856292725, G Loss: 0.6891725063323975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, batch 100 D Loss: 1.4033219814300537, G Loss: 0.7016038298606873\n",
      "Epoch 26, batch 101 D Loss: 1.4116630554199219, G Loss: 0.6909725666046143\n",
      "Epoch 26, batch 102 D Loss: 1.392859697341919, G Loss: 0.7066993117332458\n",
      "Epoch 26, batch 103 D Loss: 1.3863675594329834, G Loss: 0.6917694807052612\n",
      "Epoch 26, batch 104 D Loss: 1.4167044162750244, G Loss: 0.6932836771011353\n",
      "Epoch 26, batch 105 D Loss: 1.4572060108184814, G Loss: 0.6529728174209595\n",
      "Epoch 26, batch 106 D Loss: 1.3875665664672852, G Loss: 0.6923269033432007\n",
      "Epoch 26, batch 107 D Loss: 1.4256017208099365, G Loss: 0.6670995354652405\n",
      "Epoch 26, batch 108 D Loss: 1.4491305351257324, G Loss: 0.6586428880691528\n",
      "Epoch 26, batch 109 D Loss: 1.4236043691635132, G Loss: 0.6887817978858948\n",
      "Epoch 26, batch 110 D Loss: 1.4091136455535889, G Loss: 0.6812981963157654\n",
      "Epoch 26, batch 111 D Loss: 1.4295845031738281, G Loss: 0.6867011189460754\n",
      "Epoch 26, batch 112 D Loss: 1.3847861289978027, G Loss: 0.7097219228744507\n",
      "Epoch 26, batch 113 D Loss: 1.4058618545532227, G Loss: 0.6984977126121521\n",
      "Epoch 26, batch 114 D Loss: 1.3968830108642578, G Loss: 0.7043396830558777\n",
      "Epoch 26, batch 115 D Loss: 1.3782827854156494, G Loss: 0.7116735577583313\n",
      "Epoch 26, batch 116 D Loss: 1.3998955488204956, G Loss: 0.7052835822105408\n",
      "Epoch 26, batch 117 D Loss: 1.4058442115783691, G Loss: 0.7014302015304565\n",
      "Epoch 26, batch 118 D Loss: 1.3883317708969116, G Loss: 0.7131636738777161\n",
      "Epoch 26, batch 119 D Loss: 1.385375738143921, G Loss: 0.7138465046882629\n",
      "Epoch 26, batch 120 D Loss: 1.4124667644500732, G Loss: 0.6974639892578125\n",
      "Epoch 26, batch 121 D Loss: 1.4021122455596924, G Loss: 0.7041326761245728\n",
      "Epoch 26, batch 122 D Loss: 1.4114325046539307, G Loss: 0.7009209990501404\n",
      "Epoch 26, batch 123 D Loss: 1.3999621868133545, G Loss: 0.7107753157615662\n",
      "Epoch 26, batch 124 D Loss: 1.3918161392211914, G Loss: 0.7090291380882263\n",
      "Epoch 26, batch 125 D Loss: 1.3963271379470825, G Loss: 0.7102691531181335\n",
      "Epoch 26, batch 126 D Loss: 1.3890328407287598, G Loss: 0.708052396774292\n",
      "Epoch 26, batch 127 D Loss: 1.3859286308288574, G Loss: 0.7048584222793579\n",
      "Epoch 26, batch 128 D Loss: 1.3788514137268066, G Loss: 0.7081012725830078\n",
      "Epoch 26, batch 129 D Loss: 1.3912864923477173, G Loss: 0.708472728729248\n",
      "Epoch 26, batch 130 D Loss: 1.3767650127410889, G Loss: 0.7118473052978516\n",
      "Epoch 26, batch 131 D Loss: 1.3928606510162354, G Loss: 0.6983975172042847\n",
      "Epoch 26, batch 132 D Loss: 1.3833582401275635, G Loss: 0.7095800042152405\n",
      "Epoch 26, batch 133 D Loss: 1.373636245727539, G Loss: 0.7187466621398926\n",
      "Epoch 26, batch 134 D Loss: 1.3858935832977295, G Loss: 0.6999474167823792\n",
      "Epoch 26, batch 135 D Loss: 1.371313452720642, G Loss: 0.7082223296165466\n",
      "Epoch 26, batch 136 D Loss: 1.3788437843322754, G Loss: 0.7068910002708435\n",
      "Epoch 26, batch 137 D Loss: 1.3696174621582031, G Loss: 0.706336498260498\n",
      "Epoch 26, batch 138 D Loss: 1.3742034435272217, G Loss: 0.7082026600837708\n",
      "Epoch 26, batch 139 D Loss: 1.3774676322937012, G Loss: 0.7035725116729736\n",
      "Epoch 26, batch 140 D Loss: 1.381330966949463, G Loss: 0.7031112909317017\n",
      "Epoch 26, batch 141 D Loss: 1.3692262172698975, G Loss: 0.6983601450920105\n",
      "Epoch 26, batch 142 D Loss: 1.3687584400177002, G Loss: 0.7135435342788696\n",
      "Epoch 26, batch 143 D Loss: 1.3742479085922241, G Loss: 0.7050485014915466\n",
      "Epoch 26, batch 144 D Loss: 1.3742470741271973, G Loss: 0.7041129469871521\n",
      "Epoch 26, batch 145 D Loss: 1.3739206790924072, G Loss: 0.7069995999336243\n",
      "Epoch 26, batch 146 D Loss: 1.369284749031067, G Loss: 0.7021562457084656\n",
      "Epoch 26, batch 147 D Loss: 1.367154598236084, G Loss: 0.7072863578796387\n",
      "Epoch 26, batch 148 D Loss: 1.368340015411377, G Loss: 0.7013751268386841\n",
      "Epoch 26, batch 149 D Loss: 1.3658536672592163, G Loss: 0.7046499848365784\n",
      "Epoch 26, batch 150 D Loss: 1.364724040031433, G Loss: 0.7028876543045044\n",
      "Epoch 26, batch 151 D Loss: 1.382840633392334, G Loss: 0.6932560801506042\n",
      "Epoch 26, batch 152 D Loss: 1.3690812587738037, G Loss: 0.7034208178520203\n",
      "Epoch 26, batch 153 D Loss: 1.3440988063812256, G Loss: 0.7118916511535645\n",
      "Epoch 26, batch 154 D Loss: 1.351280689239502, G Loss: 0.7097436785697937\n",
      "Epoch 26, batch 155 D Loss: 1.3448126316070557, G Loss: 0.7183791995048523\n",
      "Epoch 26, batch 156 D Loss: 1.3449287414550781, G Loss: 0.7073198556900024\n",
      "Epoch 26, batch 157 D Loss: 1.3487849235534668, G Loss: 0.6984891295433044\n",
      "Epoch 26, batch 158 D Loss: 1.3555610179901123, G Loss: 0.7089900374412537\n",
      "Epoch 26, batch 159 D Loss: 1.365451455116272, G Loss: 0.6963299512863159\n",
      "Epoch 26, batch 160 D Loss: 1.3373117446899414, G Loss: 0.7295200824737549\n",
      "Epoch 26, batch 161 D Loss: 1.353832721710205, G Loss: 0.7317226529121399\n",
      "Epoch 26, batch 162 D Loss: 1.388534426689148, G Loss: 0.6803351640701294\n",
      "Epoch 26, batch 163 D Loss: 1.388453483581543, G Loss: 0.7059328556060791\n",
      "Epoch 26, batch 164 D Loss: 1.3722927570343018, G Loss: 0.7086532711982727\n",
      "Epoch 26, batch 165 D Loss: 1.3640228509902954, G Loss: 0.7132853865623474\n",
      "Epoch 26, batch 166 D Loss: 1.3487858772277832, G Loss: 0.7288908958435059\n",
      "Epoch 26, batch 167 D Loss: 1.3306901454925537, G Loss: 0.7315854430198669\n",
      "Epoch 26, batch 168 D Loss: 1.3656792640686035, G Loss: 0.7300496101379395\n",
      "Epoch 26, batch 169 D Loss: 1.348582148551941, G Loss: 0.7142385244369507\n",
      "Epoch 26, batch 170 D Loss: 1.3878421783447266, G Loss: 0.7001727223396301\n",
      "Epoch 26, batch 171 D Loss: 1.3771439790725708, G Loss: 0.7053096294403076\n",
      "Epoch 26, batch 172 D Loss: 1.3551828861236572, G Loss: 0.7148815989494324\n",
      "Epoch 26, batch 173 D Loss: 1.3475639820098877, G Loss: 0.7263518571853638\n",
      "Epoch 26, batch 174 D Loss: 1.3907647132873535, G Loss: 0.7045544385910034\n",
      "Epoch 26, batch 175 D Loss: 1.352593183517456, G Loss: 0.733109712600708\n",
      "Epoch 26, batch 176 D Loss: 1.3845934867858887, G Loss: 0.6935299634933472\n",
      "Epoch 26, batch 177 D Loss: 1.3800444602966309, G Loss: 0.7150469422340393\n",
      "Epoch 26, batch 178 D Loss: 1.3684568405151367, G Loss: 0.7166944742202759\n",
      "Epoch 26, batch 179 D Loss: 1.371452808380127, G Loss: 0.6980166435241699\n",
      "Epoch 26, batch 180 D Loss: 1.3586492538452148, G Loss: 0.7097657918930054\n",
      "Epoch 26, batch 181 D Loss: 1.3762415647506714, G Loss: 0.7052783966064453\n",
      "Epoch 26, batch 182 D Loss: 1.3626753091812134, G Loss: 0.7012667059898376\n",
      "Epoch 26, batch 183 D Loss: 1.3604583740234375, G Loss: 0.7132908701896667\n",
      "Epoch 26, batch 184 D Loss: 1.362269401550293, G Loss: 0.7157784104347229\n",
      "Epoch 26, batch 185 D Loss: 1.3578441143035889, G Loss: 0.7029176950454712\n",
      "Epoch 26, batch 186 D Loss: 1.3729965686798096, G Loss: 0.712928831577301\n",
      "Epoch 26, batch 187 D Loss: 1.3558349609375, G Loss: 0.7356991767883301\n",
      "Epoch 26, batch 188 D Loss: 1.4057590961456299, G Loss: 0.6965945363044739\n",
      "Epoch 26, batch 189 D Loss: 1.3878800868988037, G Loss: 0.6932637691497803\n",
      "Epoch 26, batch 190 D Loss: 1.3869688510894775, G Loss: 0.703433096408844\n",
      "Epoch 26, batch 191 D Loss: 1.3620041608810425, G Loss: 0.7059648036956787\n",
      "Epoch 26, batch 192 D Loss: 1.390472173690796, G Loss: 0.6954217553138733\n",
      "Epoch 26, batch 193 D Loss: 1.3458137512207031, G Loss: 0.713452160358429\n",
      "Epoch 26, batch 194 D Loss: 1.3709845542907715, G Loss: 0.7167214751243591\n",
      "Epoch 26, batch 195 D Loss: 1.3739051818847656, G Loss: 0.7012946605682373\n",
      "Epoch 26, batch 196 D Loss: 1.39546799659729, G Loss: 0.69161057472229\n",
      "Epoch 26, batch 197 D Loss: 1.4098286628723145, G Loss: 0.7092012166976929\n",
      "Epoch 26, batch 198 D Loss: 1.403278112411499, G Loss: 0.6935441493988037\n",
      "Epoch 26, batch 199 D Loss: 1.3666201829910278, G Loss: 0.6947087645530701\n",
      "Epoch 26, batch 200 D Loss: 1.406894326210022, G Loss: 0.6861533522605896\n",
      "Epoch 27, batch 1 D Loss: 1.3887276649475098, G Loss: 0.7026836276054382\n",
      "Epoch 27, batch 2 D Loss: 1.3984355926513672, G Loss: 0.6977031230926514\n",
      "Epoch 27, batch 3 D Loss: 1.3685754537582397, G Loss: 0.7019198536872864\n",
      "Epoch 27, batch 4 D Loss: 1.4287408590316772, G Loss: 0.6749204397201538\n",
      "Epoch 27, batch 5 D Loss: 1.3942477703094482, G Loss: 0.7021373510360718\n",
      "Epoch 27, batch 6 D Loss: 1.3709499835968018, G Loss: 0.7024538516998291\n",
      "Epoch 27, batch 7 D Loss: 1.4002323150634766, G Loss: 0.6848030686378479\n",
      "Epoch 27, batch 8 D Loss: 1.4042060375213623, G Loss: 0.6834559440612793\n",
      "Epoch 27, batch 9 D Loss: 1.3911850452423096, G Loss: 0.6862278580665588\n",
      "Epoch 27, batch 10 D Loss: 1.4117507934570312, G Loss: 0.6774959564208984\n",
      "Epoch 27, batch 11 D Loss: 1.3946876525878906, G Loss: 0.6865170001983643\n",
      "Epoch 27, batch 12 D Loss: 1.3528761863708496, G Loss: 0.7133747935295105\n",
      "Epoch 27, batch 13 D Loss: 1.403132438659668, G Loss: 0.6979732513427734\n",
      "Epoch 27, batch 14 D Loss: 1.4122629165649414, G Loss: 0.6787504553794861\n",
      "Epoch 27, batch 15 D Loss: 1.449007272720337, G Loss: 0.6717605590820312\n",
      "Epoch 27, batch 16 D Loss: 1.3840991258621216, G Loss: 0.6989343762397766\n",
      "Epoch 27, batch 17 D Loss: 1.408727765083313, G Loss: 0.6887290477752686\n",
      "Epoch 27, batch 18 D Loss: 1.4085743427276611, G Loss: 0.6809406876564026\n",
      "Epoch 27, batch 19 D Loss: 1.409460186958313, G Loss: 0.6753743886947632\n",
      "Epoch 27, batch 20 D Loss: 1.4254825115203857, G Loss: 0.6563308835029602\n",
      "Epoch 27, batch 21 D Loss: 1.4279088973999023, G Loss: 0.6586330533027649\n",
      "Epoch 27, batch 22 D Loss: 1.3988701105117798, G Loss: 0.6673339009284973\n",
      "Epoch 27, batch 23 D Loss: 1.4190359115600586, G Loss: 0.6726745367050171\n",
      "Epoch 27, batch 24 D Loss: 1.39894437789917, G Loss: 0.6793981790542603\n",
      "Epoch 27, batch 25 D Loss: 1.429667592048645, G Loss: 0.6672300696372986\n",
      "Epoch 27, batch 26 D Loss: 1.397613525390625, G Loss: 0.692332923412323\n",
      "Epoch 27, batch 27 D Loss: 1.4427906274795532, G Loss: 0.6572754383087158\n",
      "Epoch 27, batch 28 D Loss: 1.40773344039917, G Loss: 0.6671651601791382\n",
      "Epoch 27, batch 29 D Loss: 1.4465084075927734, G Loss: 0.6554233431816101\n",
      "Epoch 27, batch 30 D Loss: 1.396531581878662, G Loss: 0.6684632301330566\n",
      "Epoch 27, batch 31 D Loss: 1.4087276458740234, G Loss: 0.6874874830245972\n",
      "Epoch 27, batch 32 D Loss: 1.4249871969223022, G Loss: 0.674760639667511\n",
      "Epoch 27, batch 33 D Loss: 1.4181572198867798, G Loss: 0.6817039251327515\n",
      "Epoch 27, batch 34 D Loss: 1.4190490245819092, G Loss: 0.6746981143951416\n",
      "Epoch 27, batch 35 D Loss: 1.4300174713134766, G Loss: 0.6814509630203247\n",
      "Epoch 27, batch 36 D Loss: 1.4018449783325195, G Loss: 0.6913606524467468\n",
      "Epoch 27, batch 37 D Loss: 1.4185175895690918, G Loss: 0.6758646368980408\n",
      "Epoch 27, batch 38 D Loss: 1.4111008644104004, G Loss: 0.6836613416671753\n",
      "Epoch 27, batch 39 D Loss: 1.422062635421753, G Loss: 0.6823129057884216\n",
      "Epoch 27, batch 40 D Loss: 1.4179692268371582, G Loss: 0.6947811245918274\n",
      "Epoch 27, batch 41 D Loss: 1.424626111984253, G Loss: 0.6990951299667358\n",
      "Epoch 27, batch 42 D Loss: 1.387101411819458, G Loss: 0.717132031917572\n",
      "Epoch 27, batch 43 D Loss: 1.3947789669036865, G Loss: 0.7091764807701111\n",
      "Epoch 27, batch 44 D Loss: 1.4111021757125854, G Loss: 0.7049843072891235\n",
      "Epoch 27, batch 45 D Loss: 1.4150155782699585, G Loss: 0.7115857005119324\n",
      "Epoch 27, batch 46 D Loss: 1.410916805267334, G Loss: 0.7170887589454651\n",
      "Epoch 27, batch 47 D Loss: 1.391613245010376, G Loss: 0.7237827777862549\n",
      "Epoch 27, batch 48 D Loss: 1.3948471546173096, G Loss: 0.7187916040420532\n",
      "Epoch 27, batch 49 D Loss: 1.4097118377685547, G Loss: 0.7297736406326294\n",
      "Epoch 27, batch 50 D Loss: 1.3983213901519775, G Loss: 0.7061713933944702\n",
      "Epoch 27, batch 51 D Loss: 1.3820600509643555, G Loss: 0.7319674491882324\n",
      "Epoch 27, batch 52 D Loss: 1.3948363065719604, G Loss: 0.7237679362297058\n",
      "Epoch 27, batch 53 D Loss: 1.3676798343658447, G Loss: 0.7213475108146667\n",
      "Epoch 27, batch 54 D Loss: 1.4094507694244385, G Loss: 0.7218905687332153\n",
      "Epoch 27, batch 55 D Loss: 1.4091765880584717, G Loss: 0.7333706021308899\n",
      "Epoch 27, batch 56 D Loss: 1.3886094093322754, G Loss: 0.7381579875946045\n",
      "Epoch 27, batch 57 D Loss: 1.396728515625, G Loss: 0.729499101638794\n",
      "Epoch 27, batch 58 D Loss: 1.397721290588379, G Loss: 0.7442927360534668\n",
      "Epoch 27, batch 59 D Loss: 1.393601894378662, G Loss: 0.7462844252586365\n",
      "Epoch 27, batch 60 D Loss: 1.377532720565796, G Loss: 0.7391825318336487\n",
      "Epoch 27, batch 61 D Loss: 1.3766028881072998, G Loss: 0.753391444683075\n",
      "Epoch 27, batch 62 D Loss: 1.4174152612686157, G Loss: 0.7359102368354797\n",
      "Epoch 27, batch 63 D Loss: 1.3686103820800781, G Loss: 0.731224536895752\n",
      "Epoch 27, batch 64 D Loss: 1.4176076650619507, G Loss: 0.7366848587989807\n",
      "Epoch 27, batch 65 D Loss: 1.36893892288208, G Loss: 0.7224041819572449\n",
      "Epoch 27, batch 66 D Loss: 1.3892523050308228, G Loss: 0.7229101657867432\n",
      "Epoch 27, batch 67 D Loss: 1.384211540222168, G Loss: 0.7191333174705505\n",
      "Epoch 27, batch 68 D Loss: 1.4003255367279053, G Loss: 0.7051341533660889\n",
      "Epoch 27, batch 69 D Loss: 1.4046630859375, G Loss: 0.6987418532371521\n",
      "Epoch 27, batch 70 D Loss: 1.3853281736373901, G Loss: 0.7155063152313232\n",
      "Epoch 27, batch 71 D Loss: 1.3798282146453857, G Loss: 0.713611900806427\n",
      "Epoch 27, batch 72 D Loss: 1.4015529155731201, G Loss: 0.6999601125717163\n",
      "Epoch 27, batch 73 D Loss: 1.39395272731781, G Loss: 0.7004822492599487\n",
      "Epoch 27, batch 74 D Loss: 1.3711297512054443, G Loss: 0.7067387104034424\n",
      "Epoch 27, batch 75 D Loss: 1.3960912227630615, G Loss: 0.7073845863342285\n",
      "Epoch 27, batch 76 D Loss: 1.3895068168640137, G Loss: 0.6968064904212952\n",
      "Epoch 27, batch 77 D Loss: 1.3793504238128662, G Loss: 0.7178633213043213\n",
      "Epoch 27, batch 78 D Loss: 1.3562180995941162, G Loss: 0.7206562757492065\n",
      "Epoch 27, batch 79 D Loss: 1.3663729429244995, G Loss: 0.7125430107116699\n",
      "Epoch 27, batch 80 D Loss: 1.3949038982391357, G Loss: 0.6989185810089111\n",
      "Epoch 27, batch 81 D Loss: 1.376190185546875, G Loss: 0.7224624156951904\n",
      "Epoch 27, batch 82 D Loss: 1.3537311553955078, G Loss: 0.7200733423233032\n",
      "Epoch 27, batch 83 D Loss: 1.378624439239502, G Loss: 0.7096402049064636\n",
      "Epoch 27, batch 84 D Loss: 1.3957653045654297, G Loss: 0.6863151788711548\n",
      "Epoch 27, batch 85 D Loss: 1.3686586618423462, G Loss: 0.7103821039199829\n",
      "Epoch 27, batch 86 D Loss: 1.3742594718933105, G Loss: 0.7087016105651855\n",
      "Epoch 27, batch 87 D Loss: 1.356668472290039, G Loss: 0.703582763671875\n",
      "Epoch 27, batch 88 D Loss: 1.3593262434005737, G Loss: 0.7208399772644043\n",
      "Epoch 27, batch 89 D Loss: 1.3735909461975098, G Loss: 0.7008350491523743\n",
      "Epoch 27, batch 90 D Loss: 1.363956093788147, G Loss: 0.709261953830719\n",
      "Epoch 27, batch 91 D Loss: 1.3492283821105957, G Loss: 0.7222599983215332\n",
      "Epoch 27, batch 92 D Loss: 1.3539193868637085, G Loss: 0.7396283149719238\n",
      "Epoch 27, batch 93 D Loss: 1.3485620021820068, G Loss: 0.7305055856704712\n",
      "Epoch 27, batch 94 D Loss: 1.355222225189209, G Loss: 0.7292829751968384\n",
      "Epoch 27, batch 95 D Loss: 1.3719056844711304, G Loss: 0.732882022857666\n",
      "Epoch 27, batch 96 D Loss: 1.3758898973464966, G Loss: 0.70488440990448\n",
      "Epoch 27, batch 97 D Loss: 1.3993339538574219, G Loss: 0.6951649188995361\n",
      "Epoch 27, batch 98 D Loss: 1.351463794708252, G Loss: 0.7254342436790466\n",
      "Epoch 27, batch 99 D Loss: 1.3790456056594849, G Loss: 0.745292603969574\n",
      "Epoch 27, batch 100 D Loss: 1.4068870544433594, G Loss: 0.6908798217773438\n",
      "Epoch 27, batch 101 D Loss: 1.3578190803527832, G Loss: 0.7349008321762085\n",
      "Epoch 27, batch 102 D Loss: 1.342142939567566, G Loss: 0.7242360711097717\n",
      "Epoch 27, batch 103 D Loss: 1.3889967203140259, G Loss: 0.7138080596923828\n",
      "Epoch 27, batch 104 D Loss: 1.342801809310913, G Loss: 0.7141954898834229\n",
      "Epoch 27, batch 105 D Loss: 1.3746148347854614, G Loss: 0.7096026539802551\n",
      "Epoch 27, batch 106 D Loss: 1.4027235507965088, G Loss: 0.691572904586792\n",
      "Epoch 27, batch 107 D Loss: 1.3845576047897339, G Loss: 0.7061968445777893\n",
      "Epoch 27, batch 108 D Loss: 1.3602607250213623, G Loss: 0.7420692443847656\n",
      "Epoch 27, batch 109 D Loss: 1.3766047954559326, G Loss: 0.7159055471420288\n",
      "Epoch 27, batch 110 D Loss: 1.3899223804473877, G Loss: 0.7006643414497375\n",
      "Epoch 27, batch 111 D Loss: 1.3942348957061768, G Loss: 0.6910961270332336\n",
      "Epoch 27, batch 112 D Loss: 1.3855915069580078, G Loss: 0.7153192162513733\n",
      "Epoch 27, batch 113 D Loss: 1.3786652088165283, G Loss: 0.6861025094985962\n",
      "Epoch 27, batch 114 D Loss: 1.3724265098571777, G Loss: 0.6993566155433655\n",
      "Epoch 27, batch 115 D Loss: 1.368560552597046, G Loss: 0.7032137513160706\n",
      "Epoch 27, batch 116 D Loss: 1.3919835090637207, G Loss: 0.7002056241035461\n",
      "Epoch 27, batch 117 D Loss: 1.366056203842163, G Loss: 0.6918597221374512\n",
      "Epoch 27, batch 118 D Loss: 1.365105152130127, G Loss: 0.6973389983177185\n",
      "Epoch 27, batch 119 D Loss: 1.3678719997406006, G Loss: 0.6954663991928101\n",
      "Epoch 27, batch 120 D Loss: 1.3939204216003418, G Loss: 0.6908763647079468\n",
      "Epoch 27, batch 121 D Loss: 1.344632625579834, G Loss: 0.7102121114730835\n",
      "Epoch 27, batch 122 D Loss: 1.379040002822876, G Loss: 0.6932060718536377\n",
      "Epoch 27, batch 123 D Loss: 1.3611154556274414, G Loss: 0.6869300007820129\n",
      "Epoch 27, batch 124 D Loss: 1.399354100227356, G Loss: 0.6811590790748596\n",
      "Epoch 27, batch 125 D Loss: 1.4029748439788818, G Loss: 0.6957520842552185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, batch 126 D Loss: 1.357229232788086, G Loss: 0.7058441638946533\n",
      "Epoch 27, batch 127 D Loss: 1.3607728481292725, G Loss: 0.6910920143127441\n",
      "Epoch 27, batch 128 D Loss: 1.3623275756835938, G Loss: 0.7045254707336426\n",
      "Epoch 27, batch 129 D Loss: 1.3771836757659912, G Loss: 0.6834561228752136\n",
      "Epoch 27, batch 130 D Loss: 1.3988193273544312, G Loss: 0.6881062388420105\n",
      "Epoch 27, batch 131 D Loss: 1.351125955581665, G Loss: 0.7097386121749878\n",
      "Epoch 27, batch 132 D Loss: 1.3561705350875854, G Loss: 0.6987876892089844\n",
      "Epoch 27, batch 133 D Loss: 1.354321002960205, G Loss: 0.7051486372947693\n",
      "Epoch 27, batch 134 D Loss: 1.3938219547271729, G Loss: 0.7106506824493408\n",
      "Epoch 27, batch 135 D Loss: 1.4000133275985718, G Loss: 0.6796510815620422\n",
      "Epoch 27, batch 136 D Loss: 1.363555908203125, G Loss: 0.7103630900382996\n",
      "Epoch 27, batch 137 D Loss: 1.3851492404937744, G Loss: 0.6974656581878662\n",
      "Epoch 27, batch 138 D Loss: 1.3849786520004272, G Loss: 0.701534628868103\n",
      "Epoch 27, batch 139 D Loss: 1.3786958456039429, G Loss: 0.6847068071365356\n",
      "Epoch 27, batch 140 D Loss: 1.3829267024993896, G Loss: 0.7014679908752441\n",
      "Epoch 27, batch 141 D Loss: 1.367762804031372, G Loss: 0.6907808184623718\n",
      "Epoch 27, batch 142 D Loss: 1.386979579925537, G Loss: 0.6754573583602905\n",
      "Epoch 27, batch 143 D Loss: 1.3707120418548584, G Loss: 0.6924329400062561\n",
      "Epoch 27, batch 144 D Loss: 1.3475227355957031, G Loss: 0.7052512168884277\n",
      "Epoch 27, batch 145 D Loss: 1.3747632503509521, G Loss: 0.677041232585907\n",
      "Epoch 27, batch 146 D Loss: 1.3604540824890137, G Loss: 0.7011832594871521\n",
      "Epoch 27, batch 147 D Loss: 1.3865444660186768, G Loss: 0.6669542789459229\n",
      "Epoch 27, batch 148 D Loss: 1.375201940536499, G Loss: 0.6661331653594971\n",
      "Epoch 27, batch 149 D Loss: 1.3782672882080078, G Loss: 0.672671377658844\n",
      "Epoch 27, batch 150 D Loss: 1.3978060483932495, G Loss: 0.6814928650856018\n",
      "Epoch 27, batch 151 D Loss: 1.3934537172317505, G Loss: 0.6792146563529968\n",
      "Epoch 27, batch 152 D Loss: 1.392024040222168, G Loss: 0.6875730752944946\n",
      "Epoch 27, batch 153 D Loss: 1.385256290435791, G Loss: 0.6677982807159424\n",
      "Epoch 27, batch 154 D Loss: 1.3427832126617432, G Loss: 0.7006615400314331\n",
      "Epoch 27, batch 155 D Loss: 1.3868780136108398, G Loss: 0.6761952042579651\n",
      "Epoch 27, batch 156 D Loss: 1.36653733253479, G Loss: 0.6932364106178284\n",
      "Epoch 27, batch 157 D Loss: 1.3758556842803955, G Loss: 0.7030093669891357\n",
      "Epoch 27, batch 158 D Loss: 1.3835790157318115, G Loss: 0.6876917481422424\n",
      "Epoch 27, batch 159 D Loss: 1.368505835533142, G Loss: 0.6955585479736328\n",
      "Epoch 27, batch 160 D Loss: 1.441333293914795, G Loss: 0.6718167066574097\n",
      "Epoch 27, batch 161 D Loss: 1.4394117593765259, G Loss: 0.6768991351127625\n",
      "Epoch 27, batch 162 D Loss: 1.3822834491729736, G Loss: 0.7038916945457458\n",
      "Epoch 27, batch 163 D Loss: 1.4233747720718384, G Loss: 0.6875717043876648\n",
      "Epoch 27, batch 164 D Loss: 1.417837142944336, G Loss: 0.6823146343231201\n",
      "Epoch 27, batch 165 D Loss: 1.3829092979431152, G Loss: 0.698934018611908\n",
      "Epoch 27, batch 166 D Loss: 1.3726835250854492, G Loss: 0.6958699226379395\n",
      "Epoch 27, batch 167 D Loss: 1.394425630569458, G Loss: 0.7187883257865906\n",
      "Epoch 27, batch 168 D Loss: 1.4397501945495605, G Loss: 0.6894147992134094\n",
      "Epoch 27, batch 169 D Loss: 1.429648756980896, G Loss: 0.6852254271507263\n",
      "Epoch 27, batch 170 D Loss: 1.4771239757537842, G Loss: 0.6589924097061157\n",
      "Epoch 27, batch 171 D Loss: 1.3502243757247925, G Loss: 0.7353944182395935\n",
      "Epoch 27, batch 172 D Loss: 1.4117090702056885, G Loss: 0.7001751661300659\n",
      "Epoch 27, batch 173 D Loss: 1.4230499267578125, G Loss: 0.6840534806251526\n",
      "Epoch 27, batch 174 D Loss: 1.4067933559417725, G Loss: 0.7078275084495544\n",
      "Epoch 27, batch 175 D Loss: 1.4084657430648804, G Loss: 0.6906095147132874\n",
      "Epoch 27, batch 176 D Loss: 1.381068229675293, G Loss: 0.7145724296569824\n",
      "Epoch 27, batch 177 D Loss: 1.4158920049667358, G Loss: 0.678191602230072\n",
      "Epoch 27, batch 178 D Loss: 1.3693759441375732, G Loss: 0.7059295177459717\n",
      "Epoch 27, batch 179 D Loss: 1.360542893409729, G Loss: 0.6997831463813782\n",
      "Epoch 27, batch 180 D Loss: 1.3957654237747192, G Loss: 0.6916587352752686\n",
      "Epoch 27, batch 181 D Loss: 1.4078257083892822, G Loss: 0.6856765747070312\n",
      "Epoch 27, batch 182 D Loss: 1.426910638809204, G Loss: 0.6632938385009766\n",
      "Epoch 27, batch 183 D Loss: 1.3702316284179688, G Loss: 0.709591269493103\n",
      "Epoch 27, batch 184 D Loss: 1.4272544384002686, G Loss: 0.6763498783111572\n",
      "Epoch 27, batch 185 D Loss: 1.3826122283935547, G Loss: 0.6895646452903748\n",
      "Epoch 27, batch 186 D Loss: 1.417614459991455, G Loss: 0.6811604499816895\n",
      "Epoch 27, batch 187 D Loss: 1.4097102880477905, G Loss: 0.6938760876655579\n",
      "Epoch 27, batch 188 D Loss: 1.3983056545257568, G Loss: 0.6773867607116699\n",
      "Epoch 27, batch 189 D Loss: 1.4229390621185303, G Loss: 0.6832607388496399\n",
      "Epoch 27, batch 190 D Loss: 1.394820213317871, G Loss: 0.6754507422447205\n",
      "Epoch 27, batch 191 D Loss: 1.430661678314209, G Loss: 0.6677619814872742\n",
      "Epoch 27, batch 192 D Loss: 1.3906246423721313, G Loss: 0.6758338212966919\n",
      "Epoch 27, batch 193 D Loss: 1.4023088216781616, G Loss: 0.6613748669624329\n",
      "Epoch 27, batch 194 D Loss: 1.376490831375122, G Loss: 0.6964964270591736\n",
      "Epoch 27, batch 195 D Loss: 1.3631999492645264, G Loss: 0.7020242810249329\n",
      "Epoch 27, batch 196 D Loss: 1.403956413269043, G Loss: 0.6825736165046692\n",
      "Epoch 27, batch 197 D Loss: 1.4093031883239746, G Loss: 0.6942058801651001\n",
      "Epoch 27, batch 198 D Loss: 1.3680169582366943, G Loss: 0.6967647671699524\n",
      "Epoch 27, batch 199 D Loss: 1.3897178173065186, G Loss: 0.6935034394264221\n",
      "Epoch 27, batch 200 D Loss: 1.3876129388809204, G Loss: 0.7035946846008301\n",
      "Epoch 28, batch 1 D Loss: 1.427384853363037, G Loss: 0.667859673500061\n",
      "Epoch 28, batch 2 D Loss: 1.4262694120407104, G Loss: 0.6816479563713074\n",
      "Epoch 28, batch 3 D Loss: 1.4025120735168457, G Loss: 0.6856536269187927\n",
      "Epoch 28, batch 4 D Loss: 1.4024696350097656, G Loss: 0.7020806074142456\n",
      "Epoch 28, batch 5 D Loss: 1.3748135566711426, G Loss: 0.7176527976989746\n",
      "Epoch 28, batch 6 D Loss: 1.3945726156234741, G Loss: 0.6896955966949463\n",
      "Epoch 28, batch 7 D Loss: 1.3969311714172363, G Loss: 0.6896719336509705\n",
      "Epoch 28, batch 8 D Loss: 1.4035954475402832, G Loss: 0.6936165690422058\n",
      "Epoch 28, batch 9 D Loss: 1.4016578197479248, G Loss: 0.6707122325897217\n",
      "Epoch 28, batch 10 D Loss: 1.383080005645752, G Loss: 0.6988621354103088\n",
      "Epoch 28, batch 11 D Loss: 1.4049813747406006, G Loss: 0.6882469058036804\n",
      "Epoch 28, batch 12 D Loss: 1.4059092998504639, G Loss: 0.6990628242492676\n",
      "Epoch 28, batch 13 D Loss: 1.4372289180755615, G Loss: 0.6930473446846008\n",
      "Epoch 28, batch 14 D Loss: 1.3892091512680054, G Loss: 0.700822651386261\n",
      "Epoch 28, batch 15 D Loss: 1.365616798400879, G Loss: 0.7409013509750366\n",
      "Epoch 28, batch 16 D Loss: 1.419093370437622, G Loss: 0.7031984925270081\n",
      "Epoch 28, batch 17 D Loss: 1.3740681409835815, G Loss: 0.7115753293037415\n",
      "Epoch 28, batch 18 D Loss: 1.388182282447815, G Loss: 0.7075198888778687\n",
      "Epoch 28, batch 19 D Loss: 1.394065022468567, G Loss: 0.7392362952232361\n",
      "Epoch 28, batch 20 D Loss: 1.3861242532730103, G Loss: 0.7231050133705139\n",
      "Epoch 28, batch 21 D Loss: 1.3682076930999756, G Loss: 0.7470172047615051\n",
      "Epoch 28, batch 22 D Loss: 1.4248214960098267, G Loss: 0.7281821370124817\n",
      "Epoch 28, batch 23 D Loss: 1.4018291234970093, G Loss: 0.7141940593719482\n",
      "Epoch 28, batch 24 D Loss: 1.3910880088806152, G Loss: 0.7343877553939819\n",
      "Epoch 28, batch 25 D Loss: 1.3799583911895752, G Loss: 0.7507597208023071\n",
      "Epoch 28, batch 26 D Loss: 1.3958561420440674, G Loss: 0.741702139377594\n",
      "Epoch 28, batch 27 D Loss: 1.3745735883712769, G Loss: 0.7426278591156006\n",
      "Epoch 28, batch 28 D Loss: 1.3612618446350098, G Loss: 0.7456943392753601\n",
      "Epoch 28, batch 29 D Loss: 1.3761740922927856, G Loss: 0.7333430647850037\n",
      "Epoch 28, batch 30 D Loss: 1.383880615234375, G Loss: 0.7481744289398193\n",
      "Epoch 28, batch 31 D Loss: 1.3474825620651245, G Loss: 0.7645080089569092\n",
      "Epoch 28, batch 32 D Loss: 1.3705861568450928, G Loss: 0.7618211507797241\n",
      "Epoch 28, batch 33 D Loss: 1.3853962421417236, G Loss: 0.7361254096031189\n",
      "Epoch 28, batch 34 D Loss: 1.366072177886963, G Loss: 0.7657836675643921\n",
      "Epoch 28, batch 35 D Loss: 1.419102668762207, G Loss: 0.7355014681816101\n",
      "Epoch 28, batch 36 D Loss: 1.358737587928772, G Loss: 0.7517710328102112\n",
      "Epoch 28, batch 37 D Loss: 1.3429718017578125, G Loss: 0.747362494468689\n",
      "Epoch 28, batch 38 D Loss: 1.3755468130111694, G Loss: 0.7396209239959717\n",
      "Epoch 28, batch 39 D Loss: 1.362579345703125, G Loss: 0.73793625831604\n",
      "Epoch 28, batch 40 D Loss: 1.3561550378799438, G Loss: 0.7422763705253601\n",
      "Epoch 28, batch 41 D Loss: 1.3471918106079102, G Loss: 0.7502424120903015\n",
      "Epoch 28, batch 42 D Loss: 1.4020264148712158, G Loss: 0.7117182016372681\n",
      "Epoch 28, batch 43 D Loss: 1.3672208786010742, G Loss: 0.7481737732887268\n",
      "Epoch 28, batch 44 D Loss: 1.3993474245071411, G Loss: 0.7277817726135254\n",
      "Epoch 28, batch 45 D Loss: 1.4117375612258911, G Loss: 0.7460787892341614\n",
      "Epoch 28, batch 46 D Loss: 1.3404614925384521, G Loss: 0.7520648837089539\n",
      "Epoch 28, batch 47 D Loss: 1.3515689373016357, G Loss: 0.7458907961845398\n",
      "Epoch 28, batch 48 D Loss: 1.357816219329834, G Loss: 0.7331055402755737\n",
      "Epoch 28, batch 49 D Loss: 1.3534706830978394, G Loss: 0.732109010219574\n",
      "Epoch 28, batch 50 D Loss: 1.3617267608642578, G Loss: 0.744265079498291\n",
      "Epoch 28, batch 51 D Loss: 1.3827643394470215, G Loss: 0.7137560844421387\n",
      "Epoch 28, batch 52 D Loss: 1.375304937362671, G Loss: 0.7335859537124634\n",
      "Epoch 28, batch 53 D Loss: 1.4078315496444702, G Loss: 0.7392749190330505\n",
      "Epoch 28, batch 54 D Loss: 1.343103289604187, G Loss: 0.7389670014381409\n",
      "Epoch 28, batch 55 D Loss: 1.381134271621704, G Loss: 0.7256397008895874\n",
      "Epoch 28, batch 56 D Loss: 1.3571430444717407, G Loss: 0.7484686970710754\n",
      "Epoch 28, batch 57 D Loss: 1.3675477504730225, G Loss: 0.7382706999778748\n",
      "Epoch 28, batch 58 D Loss: 1.323675274848938, G Loss: 0.7635716199874878\n",
      "Epoch 28, batch 59 D Loss: 1.3296165466308594, G Loss: 0.7544885277748108\n",
      "Epoch 28, batch 60 D Loss: 1.3802554607391357, G Loss: 0.7494341135025024\n",
      "Epoch 28, batch 61 D Loss: 1.346049427986145, G Loss: 0.7572935223579407\n",
      "Epoch 28, batch 62 D Loss: 1.3290424346923828, G Loss: 0.7411983609199524\n",
      "Epoch 28, batch 63 D Loss: 1.3610756397247314, G Loss: 0.7446539998054504\n",
      "Epoch 28, batch 64 D Loss: 1.3432152271270752, G Loss: 0.7554122805595398\n",
      "Epoch 28, batch 65 D Loss: 1.3242601156234741, G Loss: 0.7736955285072327\n",
      "Epoch 28, batch 66 D Loss: 1.350935459136963, G Loss: 0.744324266910553\n",
      "Epoch 28, batch 67 D Loss: 1.3551299571990967, G Loss: 0.764089822769165\n",
      "Epoch 28, batch 68 D Loss: 1.3639394044876099, G Loss: 0.7334102392196655\n",
      "Epoch 28, batch 69 D Loss: 1.400288701057434, G Loss: 0.6985253691673279\n",
      "Epoch 28, batch 70 D Loss: 1.373254656791687, G Loss: 0.7179432511329651\n",
      "Epoch 28, batch 71 D Loss: 1.36251962184906, G Loss: 0.7532198429107666\n",
      "Epoch 28, batch 72 D Loss: 1.338099479675293, G Loss: 0.7621884346008301\n",
      "Epoch 28, batch 73 D Loss: 1.3653799295425415, G Loss: 0.7450607419013977\n",
      "Epoch 28, batch 74 D Loss: 1.394704818725586, G Loss: 0.7257066369056702\n",
      "Epoch 28, batch 75 D Loss: 1.40199875831604, G Loss: 0.7121137380599976\n",
      "Epoch 28, batch 76 D Loss: 1.386337399482727, G Loss: 0.7184301018714905\n",
      "Epoch 28, batch 77 D Loss: 1.3650152683258057, G Loss: 0.726195216178894\n",
      "Epoch 28, batch 78 D Loss: 1.3325812816619873, G Loss: 0.7631778717041016\n",
      "Epoch 28, batch 79 D Loss: 1.3465654850006104, G Loss: 0.737123429775238\n",
      "Epoch 28, batch 80 D Loss: 1.3626787662506104, G Loss: 0.7357426285743713\n",
      "Epoch 28, batch 81 D Loss: 1.4226446151733398, G Loss: 0.7296717166900635\n",
      "Epoch 28, batch 82 D Loss: 1.3853363990783691, G Loss: 0.704867959022522\n",
      "Epoch 28, batch 83 D Loss: 1.4218697547912598, G Loss: 0.7099881172180176\n",
      "Epoch 28, batch 84 D Loss: 1.3734548091888428, G Loss: 0.7214301228523254\n",
      "Epoch 28, batch 85 D Loss: 1.3464007377624512, G Loss: 0.7174625992774963\n",
      "Epoch 28, batch 86 D Loss: 1.3723766803741455, G Loss: 0.7106128931045532\n",
      "Epoch 28, batch 87 D Loss: 1.3460512161254883, G Loss: 0.7070374488830566\n",
      "Epoch 28, batch 88 D Loss: 1.3542627096176147, G Loss: 0.7132327556610107\n",
      "Epoch 28, batch 89 D Loss: 1.3352001905441284, G Loss: 0.7176657319068909\n",
      "Epoch 28, batch 90 D Loss: 1.368091344833374, G Loss: 0.7021809220314026\n",
      "Epoch 28, batch 91 D Loss: 1.3492860794067383, G Loss: 0.6947884559631348\n",
      "Epoch 28, batch 92 D Loss: 1.4000768661499023, G Loss: 0.7084629535675049\n",
      "Epoch 28, batch 93 D Loss: 1.3972458839416504, G Loss: 0.6724932193756104\n",
      "Epoch 28, batch 94 D Loss: 1.341282844543457, G Loss: 0.7074611186981201\n",
      "Epoch 28, batch 95 D Loss: 1.350083351135254, G Loss: 0.7165126204490662\n",
      "Epoch 28, batch 96 D Loss: 1.2962253093719482, G Loss: 0.7422292828559875\n",
      "Epoch 28, batch 97 D Loss: 1.3624022006988525, G Loss: 0.7474150657653809\n",
      "Epoch 28, batch 98 D Loss: 1.370781660079956, G Loss: 0.7206102013587952\n",
      "Epoch 28, batch 99 D Loss: 1.3115456104278564, G Loss: 0.7617300152778625\n",
      "Epoch 28, batch 100 D Loss: 1.2772974967956543, G Loss: 0.7282614707946777\n",
      "Epoch 28, batch 101 D Loss: 1.405155897140503, G Loss: 0.7265253663063049\n",
      "Epoch 28, batch 102 D Loss: 1.3517577648162842, G Loss: 0.7373564839363098\n",
      "Epoch 28, batch 103 D Loss: 1.3464516401290894, G Loss: 0.7038195133209229\n",
      "Epoch 28, batch 104 D Loss: 1.406217098236084, G Loss: 0.6863505840301514\n",
      "Epoch 28, batch 105 D Loss: 1.3815008401870728, G Loss: 0.7071725726127625\n",
      "Epoch 28, batch 106 D Loss: 1.3193520307540894, G Loss: 0.7099977135658264\n",
      "Epoch 28, batch 107 D Loss: 1.3286150693893433, G Loss: 0.716509222984314\n",
      "Epoch 28, batch 108 D Loss: 1.3792928457260132, G Loss: 0.7162879705429077\n",
      "Epoch 28, batch 109 D Loss: 1.3304967880249023, G Loss: 0.7448863983154297\n",
      "Epoch 28, batch 110 D Loss: 1.3538994789123535, G Loss: 0.7691975235939026\n",
      "Epoch 28, batch 111 D Loss: 1.4357283115386963, G Loss: 0.6880602240562439\n",
      "Epoch 28, batch 112 D Loss: 1.3711979389190674, G Loss: 0.7183607220649719\n",
      "Epoch 28, batch 113 D Loss: 1.4224833250045776, G Loss: 0.6834096312522888\n",
      "Epoch 28, batch 114 D Loss: 1.3746850490570068, G Loss: 0.7475522756576538\n",
      "Epoch 28, batch 115 D Loss: 1.3376784324645996, G Loss: 0.6963400840759277\n",
      "Epoch 28, batch 116 D Loss: 1.4203569889068604, G Loss: 0.6889784932136536\n",
      "Epoch 28, batch 117 D Loss: 1.4543386697769165, G Loss: 0.672314465045929\n",
      "Epoch 28, batch 118 D Loss: 1.4064433574676514, G Loss: 0.6808850169181824\n",
      "Epoch 28, batch 119 D Loss: 1.3961912393569946, G Loss: 0.7038853168487549\n",
      "Epoch 28, batch 120 D Loss: 1.3897380828857422, G Loss: 0.6823485493659973\n",
      "Epoch 28, batch 121 D Loss: 1.3900659084320068, G Loss: 0.6782961487770081\n",
      "Epoch 28, batch 122 D Loss: 1.4818803071975708, G Loss: 0.6297881603240967\n",
      "Epoch 28, batch 123 D Loss: 1.5475594997406006, G Loss: 0.6241459846496582\n",
      "Epoch 28, batch 124 D Loss: 1.363438606262207, G Loss: 0.6756839752197266\n",
      "Epoch 28, batch 125 D Loss: 1.425074815750122, G Loss: 0.6905288100242615\n",
      "Epoch 28, batch 126 D Loss: 1.479209065437317, G Loss: 0.6656273007392883\n",
      "Epoch 28, batch 127 D Loss: 1.4167006015777588, G Loss: 0.6768717765808105\n",
      "Epoch 28, batch 128 D Loss: 1.4556469917297363, G Loss: 0.6537050604820251\n",
      "Epoch 28, batch 129 D Loss: 1.3925483226776123, G Loss: 0.7118663787841797\n",
      "Epoch 28, batch 130 D Loss: 1.401949405670166, G Loss: 0.6685810089111328\n",
      "Epoch 28, batch 131 D Loss: 1.4640858173370361, G Loss: 0.6369683742523193\n",
      "Epoch 28, batch 132 D Loss: 1.4225497245788574, G Loss: 0.6531143188476562\n",
      "Epoch 28, batch 133 D Loss: 1.4292521476745605, G Loss: 0.6406653523445129\n",
      "Epoch 28, batch 134 D Loss: 1.487729787826538, G Loss: 0.6556172966957092\n",
      "Epoch 28, batch 135 D Loss: 1.3975017070770264, G Loss: 0.6624388098716736\n",
      "Epoch 28, batch 136 D Loss: 1.3808574676513672, G Loss: 0.6752716898918152\n",
      "Epoch 28, batch 137 D Loss: 1.4113322496414185, G Loss: 0.6647014021873474\n",
      "Epoch 28, batch 138 D Loss: 1.4727675914764404, G Loss: 0.6400486826896667\n",
      "Epoch 28, batch 139 D Loss: 1.47817063331604, G Loss: 0.6526814103126526\n",
      "Epoch 28, batch 140 D Loss: 1.3994333744049072, G Loss: 0.6716327667236328\n",
      "Epoch 28, batch 141 D Loss: 1.4857759475708008, G Loss: 0.6608309745788574\n",
      "Epoch 28, batch 142 D Loss: 1.3320472240447998, G Loss: 0.7020789384841919\n",
      "Epoch 28, batch 143 D Loss: 1.4589588642120361, G Loss: 0.6654395461082458\n",
      "Epoch 28, batch 144 D Loss: 1.4037021398544312, G Loss: 0.6850870251655579\n",
      "Epoch 28, batch 145 D Loss: 1.4303653240203857, G Loss: 0.678773820400238\n",
      "Epoch 28, batch 146 D Loss: 1.46163010597229, G Loss: 0.6659733057022095\n",
      "Epoch 28, batch 147 D Loss: 1.4498944282531738, G Loss: 0.6786729693412781\n",
      "Epoch 28, batch 148 D Loss: 1.4465131759643555, G Loss: 0.6758078932762146\n",
      "Epoch 28, batch 149 D Loss: 1.4686310291290283, G Loss: 0.6809460520744324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, batch 150 D Loss: 1.5192646980285645, G Loss: 0.6567144989967346\n",
      "Epoch 28, batch 151 D Loss: 1.4203039407730103, G Loss: 0.7116013169288635\n",
      "Epoch 28, batch 152 D Loss: 1.4414770603179932, G Loss: 0.7020301222801208\n",
      "Epoch 28, batch 153 D Loss: 1.4444096088409424, G Loss: 0.7115322947502136\n",
      "Epoch 28, batch 154 D Loss: 1.4308674335479736, G Loss: 0.7331880331039429\n",
      "Epoch 28, batch 155 D Loss: 1.4158388376235962, G Loss: 0.7273871898651123\n",
      "Epoch 28, batch 156 D Loss: 1.4043407440185547, G Loss: 0.7449262738227844\n",
      "Epoch 28, batch 157 D Loss: 1.4143844842910767, G Loss: 0.7435257434844971\n",
      "Epoch 28, batch 158 D Loss: 1.3882180452346802, G Loss: 0.7625239491462708\n",
      "Epoch 28, batch 159 D Loss: 1.3922841548919678, G Loss: 0.752342700958252\n",
      "Epoch 28, batch 160 D Loss: 1.3931668996810913, G Loss: 0.7714205980300903\n",
      "Epoch 28, batch 161 D Loss: 1.4046608209609985, G Loss: 0.7617230415344238\n",
      "Epoch 28, batch 162 D Loss: 1.3820306062698364, G Loss: 0.8122462630271912\n",
      "Epoch 28, batch 163 D Loss: 1.407857894897461, G Loss: 0.779701828956604\n",
      "Epoch 28, batch 164 D Loss: 1.3662338256835938, G Loss: 0.752610445022583\n",
      "Epoch 28, batch 165 D Loss: 1.4051337242126465, G Loss: 0.780028760433197\n",
      "Epoch 28, batch 166 D Loss: 1.4066076278686523, G Loss: 0.7658859491348267\n",
      "Epoch 28, batch 167 D Loss: 1.4086766242980957, G Loss: 0.7529487609863281\n",
      "Epoch 28, batch 168 D Loss: 1.3762587308883667, G Loss: 0.7835480570793152\n",
      "Epoch 28, batch 169 D Loss: 1.4133392572402954, G Loss: 0.731392502784729\n",
      "Epoch 28, batch 170 D Loss: 1.3769242763519287, G Loss: 0.7817153930664062\n",
      "Epoch 28, batch 171 D Loss: 1.380569577217102, G Loss: 0.7484014630317688\n",
      "Epoch 28, batch 172 D Loss: 1.3606852293014526, G Loss: 0.801134467124939\n",
      "Epoch 28, batch 173 D Loss: 1.405226230621338, G Loss: 0.733597993850708\n",
      "Epoch 28, batch 174 D Loss: 1.3701684474945068, G Loss: 0.7955909967422485\n",
      "Epoch 28, batch 175 D Loss: 1.3990154266357422, G Loss: 0.7752273678779602\n",
      "Epoch 28, batch 176 D Loss: 1.3907068967819214, G Loss: 0.7545257806777954\n",
      "Epoch 28, batch 177 D Loss: 1.4375314712524414, G Loss: 0.7323455214500427\n",
      "Epoch 28, batch 178 D Loss: 1.358626365661621, G Loss: 0.7932355999946594\n",
      "Epoch 28, batch 179 D Loss: 1.3453850746154785, G Loss: 0.7516782879829407\n",
      "Epoch 28, batch 180 D Loss: 1.377217173576355, G Loss: 0.7637682557106018\n",
      "Epoch 28, batch 181 D Loss: 1.3925793170928955, G Loss: 0.745521605014801\n",
      "Epoch 28, batch 182 D Loss: 1.331369400024414, G Loss: 0.7856606245040894\n",
      "Epoch 28, batch 183 D Loss: 1.3937621116638184, G Loss: 0.737648606300354\n",
      "Epoch 28, batch 184 D Loss: 1.3624701499938965, G Loss: 0.7457472085952759\n",
      "Epoch 28, batch 185 D Loss: 1.4072964191436768, G Loss: 0.7187178134918213\n",
      "Epoch 28, batch 186 D Loss: 1.3524837493896484, G Loss: 0.750399649143219\n",
      "Epoch 28, batch 187 D Loss: 1.403139591217041, G Loss: 0.7263889908790588\n",
      "Epoch 28, batch 188 D Loss: 1.3916362524032593, G Loss: 0.7294477820396423\n",
      "Epoch 28, batch 189 D Loss: 1.3744323253631592, G Loss: 0.7523862719535828\n",
      "Epoch 28, batch 190 D Loss: 1.3921194076538086, G Loss: 0.7202128767967224\n",
      "Epoch 28, batch 191 D Loss: 1.3519752025604248, G Loss: 0.7462477684020996\n",
      "Epoch 28, batch 192 D Loss: 1.3537209033966064, G Loss: 0.7615035772323608\n",
      "Epoch 28, batch 193 D Loss: 1.399399995803833, G Loss: 0.7351698279380798\n",
      "Epoch 28, batch 194 D Loss: 1.400658369064331, G Loss: 0.7118988633155823\n",
      "Epoch 28, batch 195 D Loss: 1.3825154304504395, G Loss: 0.7099323868751526\n",
      "Epoch 28, batch 196 D Loss: 1.3495289087295532, G Loss: 0.7514746785163879\n",
      "Epoch 28, batch 197 D Loss: 1.3229446411132812, G Loss: 0.7457071542739868\n",
      "Epoch 28, batch 198 D Loss: 1.3627246618270874, G Loss: 0.738538384437561\n",
      "Epoch 28, batch 199 D Loss: 1.3361026048660278, G Loss: 0.7571588754653931\n",
      "Epoch 28, batch 200 D Loss: 1.3670659065246582, G Loss: 0.7182936072349548\n",
      "Epoch 29, batch 1 D Loss: 1.3460626602172852, G Loss: 0.7274985313415527\n",
      "Epoch 29, batch 2 D Loss: 1.3573176860809326, G Loss: 0.7246134877204895\n",
      "Epoch 29, batch 3 D Loss: 1.3870577812194824, G Loss: 0.7234371900558472\n",
      "Epoch 29, batch 4 D Loss: 1.30989670753479, G Loss: 0.7497428059577942\n",
      "Epoch 29, batch 5 D Loss: 1.3860044479370117, G Loss: 0.711839497089386\n",
      "Epoch 29, batch 6 D Loss: 1.340548038482666, G Loss: 0.7487032413482666\n",
      "Epoch 29, batch 7 D Loss: 1.3232333660125732, G Loss: 0.7615417242050171\n",
      "Epoch 29, batch 8 D Loss: 1.350231409072876, G Loss: 0.7465808987617493\n",
      "Epoch 29, batch 9 D Loss: 1.3660869598388672, G Loss: 0.7347701787948608\n",
      "Epoch 29, batch 10 D Loss: 1.339752435684204, G Loss: 0.7558441758155823\n",
      "Epoch 29, batch 11 D Loss: 1.317529559135437, G Loss: 0.7383384704589844\n",
      "Epoch 29, batch 12 D Loss: 1.372387409210205, G Loss: 0.7299938201904297\n",
      "Epoch 29, batch 13 D Loss: 1.3103169202804565, G Loss: 0.7646276950836182\n",
      "Epoch 29, batch 14 D Loss: 1.3416426181793213, G Loss: 0.7683637738227844\n",
      "Epoch 29, batch 15 D Loss: 1.3546371459960938, G Loss: 0.7629888653755188\n",
      "Epoch 29, batch 16 D Loss: 1.3582959175109863, G Loss: 0.7597708106040955\n",
      "Epoch 29, batch 17 D Loss: 1.3324997425079346, G Loss: 0.7457094788551331\n",
      "Epoch 29, batch 18 D Loss: 1.2862392663955688, G Loss: 0.76811683177948\n",
      "Epoch 29, batch 19 D Loss: 1.3243441581726074, G Loss: 0.7491147518157959\n",
      "Epoch 29, batch 20 D Loss: 1.327380895614624, G Loss: 0.7589225769042969\n",
      "Epoch 29, batch 21 D Loss: 1.2853500843048096, G Loss: 0.7789260149002075\n",
      "Epoch 29, batch 22 D Loss: 1.391599178314209, G Loss: 0.7172375321388245\n",
      "Epoch 29, batch 23 D Loss: 1.3248084783554077, G Loss: 0.765422523021698\n",
      "Epoch 29, batch 24 D Loss: 1.3709588050842285, G Loss: 0.6995658278465271\n",
      "Epoch 29, batch 25 D Loss: 1.3719780445098877, G Loss: 0.7450175285339355\n",
      "Epoch 29, batch 26 D Loss: 1.3989391326904297, G Loss: 0.7055124044418335\n",
      "Epoch 29, batch 27 D Loss: 1.3258836269378662, G Loss: 0.7578306794166565\n",
      "Epoch 29, batch 28 D Loss: 1.408441185951233, G Loss: 0.7067026495933533\n",
      "Epoch 29, batch 29 D Loss: 1.331554889678955, G Loss: 0.7207152843475342\n",
      "Epoch 29, batch 30 D Loss: 1.2715132236480713, G Loss: 0.7653279304504395\n",
      "Epoch 29, batch 31 D Loss: 1.3298954963684082, G Loss: 0.7198794484138489\n",
      "Epoch 29, batch 32 D Loss: 1.3767890930175781, G Loss: 0.6740177869796753\n",
      "Epoch 29, batch 33 D Loss: 1.3619705438613892, G Loss: 0.738926112651825\n",
      "Epoch 29, batch 34 D Loss: 1.3392844200134277, G Loss: 0.7015398144721985\n",
      "Epoch 29, batch 35 D Loss: 1.3481073379516602, G Loss: 0.6986063122749329\n",
      "Epoch 29, batch 36 D Loss: 1.3286244869232178, G Loss: 0.7080333232879639\n",
      "Epoch 29, batch 37 D Loss: 1.3838346004486084, G Loss: 0.6767610311508179\n",
      "Epoch 29, batch 38 D Loss: 1.3556464910507202, G Loss: 0.7269642353057861\n",
      "Epoch 29, batch 39 D Loss: 1.336950421333313, G Loss: 0.6673357486724854\n",
      "Epoch 29, batch 40 D Loss: 1.4373241662979126, G Loss: 0.6598863005638123\n",
      "Epoch 29, batch 41 D Loss: 1.306443691253662, G Loss: 0.6930936574935913\n",
      "Epoch 29, batch 42 D Loss: 1.4036893844604492, G Loss: 0.6877080798149109\n",
      "Epoch 29, batch 43 D Loss: 1.3259713649749756, G Loss: 0.6777427792549133\n",
      "Epoch 29, batch 44 D Loss: 1.3445179462432861, G Loss: 0.6886252760887146\n",
      "Epoch 29, batch 45 D Loss: 1.3371021747589111, G Loss: 0.6713190674781799\n",
      "Epoch 29, batch 46 D Loss: 1.3917473554611206, G Loss: 0.6554192304611206\n",
      "Epoch 29, batch 47 D Loss: 1.3688697814941406, G Loss: 0.6604331731796265\n",
      "Epoch 29, batch 48 D Loss: 1.3672927618026733, G Loss: 0.6946233510971069\n",
      "Epoch 29, batch 49 D Loss: 1.411562204360962, G Loss: 0.6453534960746765\n",
      "Epoch 29, batch 50 D Loss: 1.3871889114379883, G Loss: 0.6547045707702637\n",
      "Epoch 29, batch 51 D Loss: 1.338477611541748, G Loss: 0.6735120415687561\n",
      "Epoch 29, batch 52 D Loss: 1.3370673656463623, G Loss: 0.689079999923706\n",
      "Epoch 29, batch 53 D Loss: 1.3915200233459473, G Loss: 0.6539162993431091\n",
      "Epoch 29, batch 54 D Loss: 1.3845396041870117, G Loss: 0.6484530568122864\n",
      "Epoch 29, batch 55 D Loss: 1.356405258178711, G Loss: 0.6723096370697021\n",
      "Epoch 29, batch 56 D Loss: 1.3589023351669312, G Loss: 0.677842915058136\n",
      "Epoch 29, batch 57 D Loss: 1.3662344217300415, G Loss: 0.662192702293396\n",
      "Epoch 29, batch 58 D Loss: 1.344871163368225, G Loss: 0.6818191409111023\n",
      "Epoch 29, batch 59 D Loss: 1.3535411357879639, G Loss: 0.7037168145179749\n",
      "Epoch 29, batch 60 D Loss: 1.3903772830963135, G Loss: 0.6615096926689148\n",
      "Epoch 29, batch 61 D Loss: 1.3938014507293701, G Loss: 0.6834457516670227\n",
      "Epoch 29, batch 62 D Loss: 1.3842241764068604, G Loss: 0.6851385235786438\n",
      "Epoch 29, batch 63 D Loss: 1.3758419752120972, G Loss: 0.688620388507843\n",
      "Epoch 29, batch 64 D Loss: 1.378090262413025, G Loss: 0.6867617964744568\n",
      "Epoch 29, batch 65 D Loss: 1.348987102508545, G Loss: 0.7044486403465271\n",
      "Epoch 29, batch 66 D Loss: 1.348125696182251, G Loss: 0.6678071618080139\n",
      "Epoch 29, batch 67 D Loss: 1.3982157707214355, G Loss: 0.6698768138885498\n",
      "Epoch 29, batch 68 D Loss: 1.3795762062072754, G Loss: 0.7010252475738525\n",
      "Epoch 29, batch 69 D Loss: 1.3318617343902588, G Loss: 0.720832884311676\n",
      "Epoch 29, batch 70 D Loss: 1.3399887084960938, G Loss: 0.7001228332519531\n",
      "Epoch 29, batch 71 D Loss: 1.325402021408081, G Loss: 0.730243444442749\n",
      "Epoch 29, batch 72 D Loss: 1.3563997745513916, G Loss: 0.7244201898574829\n",
      "Epoch 29, batch 73 D Loss: 1.347564935684204, G Loss: 0.7058370113372803\n",
      "Epoch 29, batch 74 D Loss: 1.3629131317138672, G Loss: 0.7179436683654785\n",
      "Epoch 29, batch 75 D Loss: 1.3204271793365479, G Loss: 0.7062508463859558\n",
      "Epoch 29, batch 76 D Loss: 1.3378911018371582, G Loss: 0.6935933828353882\n",
      "Epoch 29, batch 77 D Loss: 1.3393669128417969, G Loss: 0.7105349898338318\n",
      "Epoch 29, batch 78 D Loss: 1.411155104637146, G Loss: 0.6551738977432251\n",
      "Epoch 29, batch 79 D Loss: 1.3615291118621826, G Loss: 0.7217137217521667\n",
      "Epoch 29, batch 80 D Loss: 1.3818007707595825, G Loss: 0.7014959454536438\n",
      "Epoch 29, batch 81 D Loss: 1.3932116031646729, G Loss: 0.680022656917572\n",
      "Epoch 29, batch 82 D Loss: 1.3486042022705078, G Loss: 0.7138893008232117\n",
      "Epoch 29, batch 83 D Loss: 1.374469518661499, G Loss: 0.6943256855010986\n",
      "Epoch 29, batch 84 D Loss: 1.3991053104400635, G Loss: 0.691059947013855\n",
      "Epoch 29, batch 85 D Loss: 1.5134637355804443, G Loss: 0.6323410868644714\n",
      "Epoch 29, batch 86 D Loss: 1.4508135318756104, G Loss: 0.6618470549583435\n",
      "Epoch 29, batch 87 D Loss: 1.406667947769165, G Loss: 0.6679717898368835\n",
      "Epoch 29, batch 88 D Loss: 1.483689308166504, G Loss: 0.6252570152282715\n",
      "Epoch 29, batch 89 D Loss: 1.3681615591049194, G Loss: 0.6929469108581543\n",
      "Epoch 29, batch 90 D Loss: 1.328317642211914, G Loss: 0.7170568704605103\n",
      "Epoch 29, batch 91 D Loss: 1.4648243188858032, G Loss: 0.6600929498672485\n",
      "Epoch 29, batch 92 D Loss: 1.487168550491333, G Loss: 0.675061047077179\n",
      "Epoch 29, batch 93 D Loss: 1.4845662117004395, G Loss: 0.6790413856506348\n",
      "Epoch 29, batch 94 D Loss: 1.3383903503417969, G Loss: 0.719621479511261\n",
      "Epoch 29, batch 95 D Loss: 1.3757655620574951, G Loss: 0.6650639176368713\n",
      "Epoch 29, batch 96 D Loss: 1.430593490600586, G Loss: 0.6897583603858948\n",
      "Epoch 29, batch 97 D Loss: 1.4205851554870605, G Loss: 0.6843226552009583\n",
      "Epoch 29, batch 98 D Loss: 1.406879186630249, G Loss: 0.6986417174339294\n",
      "Epoch 29, batch 99 D Loss: 1.4015085697174072, G Loss: 0.6820819973945618\n",
      "Epoch 29, batch 100 D Loss: 1.5128881931304932, G Loss: 0.6675575375556946\n",
      "Epoch 29, batch 101 D Loss: 1.5339207649230957, G Loss: 0.6320521831512451\n",
      "Epoch 29, batch 102 D Loss: 1.4586563110351562, G Loss: 0.6693933010101318\n",
      "Epoch 29, batch 103 D Loss: 1.4552007913589478, G Loss: 0.6772720217704773\n",
      "Epoch 29, batch 104 D Loss: 1.4691746234893799, G Loss: 0.6852642893791199\n",
      "Epoch 29, batch 105 D Loss: 1.4487509727478027, G Loss: 0.6711542010307312\n",
      "Epoch 29, batch 106 D Loss: 1.482149362564087, G Loss: 0.6651638150215149\n",
      "Epoch 29, batch 107 D Loss: 1.401557445526123, G Loss: 0.7182042002677917\n",
      "Epoch 29, batch 108 D Loss: 1.4065256118774414, G Loss: 0.6895086169242859\n",
      "Epoch 29, batch 109 D Loss: 1.4783908128738403, G Loss: 0.7076252698898315\n",
      "Epoch 29, batch 110 D Loss: 1.394197940826416, G Loss: 0.721882700920105\n",
      "Epoch 29, batch 111 D Loss: 1.4475003480911255, G Loss: 0.6901678442955017\n",
      "Epoch 29, batch 112 D Loss: 1.5034053325653076, G Loss: 0.6613459587097168\n",
      "Epoch 29, batch 113 D Loss: 1.4655784368515015, G Loss: 0.6996084451675415\n",
      "Epoch 29, batch 114 D Loss: 1.4580258131027222, G Loss: 0.6949931383132935\n",
      "Epoch 29, batch 115 D Loss: 1.4351568222045898, G Loss: 0.7129992842674255\n",
      "Epoch 29, batch 116 D Loss: 1.418341040611267, G Loss: 0.721498966217041\n",
      "Epoch 29, batch 117 D Loss: 1.4339971542358398, G Loss: 0.7117730975151062\n",
      "Epoch 29, batch 118 D Loss: 1.4035780429840088, G Loss: 0.7285208702087402\n",
      "Epoch 29, batch 119 D Loss: 1.4189727306365967, G Loss: 0.7327911853790283\n",
      "Epoch 29, batch 120 D Loss: 1.4471616744995117, G Loss: 0.7232365608215332\n",
      "Epoch 29, batch 121 D Loss: 1.405426263809204, G Loss: 0.754686176776886\n",
      "Epoch 29, batch 122 D Loss: 1.421797275543213, G Loss: 0.7572852373123169\n",
      "Epoch 29, batch 123 D Loss: 1.3972902297973633, G Loss: 0.7803264856338501\n",
      "Epoch 29, batch 124 D Loss: 1.4257116317749023, G Loss: 0.7422128319740295\n",
      "Epoch 29, batch 125 D Loss: 1.4206687211990356, G Loss: 0.7474653720855713\n",
      "Epoch 29, batch 126 D Loss: 1.4430397748947144, G Loss: 0.7561267018318176\n",
      "Epoch 29, batch 127 D Loss: 1.41511070728302, G Loss: 0.7615745067596436\n",
      "Epoch 29, batch 128 D Loss: 1.4096591472625732, G Loss: 0.7850829362869263\n",
      "Epoch 29, batch 129 D Loss: 1.3952298164367676, G Loss: 0.7884535789489746\n",
      "Epoch 29, batch 130 D Loss: 1.4280688762664795, G Loss: 0.7819024920463562\n",
      "Epoch 29, batch 131 D Loss: 1.4169764518737793, G Loss: 0.7539511919021606\n",
      "Epoch 29, batch 132 D Loss: 1.3858171701431274, G Loss: 0.7898174524307251\n",
      "Epoch 29, batch 133 D Loss: 1.4620389938354492, G Loss: 0.7355894446372986\n",
      "Epoch 29, batch 134 D Loss: 1.3717541694641113, G Loss: 0.7909039855003357\n",
      "Epoch 29, batch 135 D Loss: 1.3975555896759033, G Loss: 0.7801605463027954\n",
      "Epoch 29, batch 136 D Loss: 1.3920845985412598, G Loss: 0.7693992853164673\n",
      "Epoch 29, batch 137 D Loss: 1.391799807548523, G Loss: 0.7744705080986023\n",
      "Epoch 29, batch 138 D Loss: 1.437345027923584, G Loss: 0.7228338718414307\n",
      "Epoch 29, batch 139 D Loss: 1.3865426778793335, G Loss: 0.771338939666748\n",
      "Epoch 29, batch 140 D Loss: 1.4017462730407715, G Loss: 0.7339804172515869\n",
      "Epoch 29, batch 141 D Loss: 1.3735451698303223, G Loss: 0.7897415161132812\n",
      "Epoch 29, batch 142 D Loss: 1.3681608438491821, G Loss: 0.793292224407196\n",
      "Epoch 29, batch 143 D Loss: 1.349729299545288, G Loss: 0.7903230786323547\n",
      "Epoch 29, batch 144 D Loss: 1.4198578596115112, G Loss: 0.741395890712738\n",
      "Epoch 29, batch 145 D Loss: 1.4300944805145264, G Loss: 0.7316344380378723\n",
      "Epoch 29, batch 146 D Loss: 1.3895139694213867, G Loss: 0.7666786909103394\n",
      "Epoch 29, batch 147 D Loss: 1.3612537384033203, G Loss: 0.7728710174560547\n",
      "Epoch 29, batch 148 D Loss: 1.394040584564209, G Loss: 0.7622169256210327\n",
      "Epoch 29, batch 149 D Loss: 1.4259495735168457, G Loss: 0.7452163100242615\n",
      "Epoch 29, batch 150 D Loss: 1.3878920078277588, G Loss: 0.7393823266029358\n",
      "Epoch 29, batch 151 D Loss: 1.3857778310775757, G Loss: 0.7068279981613159\n",
      "Epoch 29, batch 152 D Loss: 1.4021148681640625, G Loss: 0.7338900566101074\n",
      "Epoch 29, batch 153 D Loss: 1.4207910299301147, G Loss: 0.73044753074646\n",
      "Epoch 29, batch 154 D Loss: 1.3594276905059814, G Loss: 0.7338111400604248\n",
      "Epoch 29, batch 155 D Loss: 1.3738921880722046, G Loss: 0.7331525683403015\n",
      "Epoch 29, batch 156 D Loss: 1.3905837535858154, G Loss: 0.7262973785400391\n",
      "Epoch 29, batch 157 D Loss: 1.3964170217514038, G Loss: 0.7153840065002441\n",
      "Epoch 29, batch 158 D Loss: 1.3797414302825928, G Loss: 0.7366659641265869\n",
      "Epoch 29, batch 159 D Loss: 1.36824369430542, G Loss: 0.73264479637146\n",
      "Epoch 29, batch 160 D Loss: 1.3652968406677246, G Loss: 0.7341810464859009\n",
      "Epoch 29, batch 161 D Loss: 1.3644969463348389, G Loss: 0.7443050146102905\n",
      "Epoch 29, batch 162 D Loss: 1.4018807411193848, G Loss: 0.7154504656791687\n",
      "Epoch 29, batch 163 D Loss: 1.4085872173309326, G Loss: 0.7104712128639221\n",
      "Epoch 29, batch 164 D Loss: 1.3780380487442017, G Loss: 0.7106135487556458\n",
      "Epoch 29, batch 165 D Loss: 1.3622266054153442, G Loss: 0.7185478806495667\n",
      "Epoch 29, batch 166 D Loss: 1.3691730499267578, G Loss: 0.7358646392822266\n",
      "Epoch 29, batch 167 D Loss: 1.368632197380066, G Loss: 0.7027135491371155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, batch 168 D Loss: 1.3251802921295166, G Loss: 0.7308558821678162\n",
      "Epoch 29, batch 169 D Loss: 1.3617206811904907, G Loss: 0.7316222190856934\n",
      "Epoch 29, batch 170 D Loss: 1.375091791152954, G Loss: 0.7176907062530518\n",
      "Epoch 29, batch 171 D Loss: 1.3434216976165771, G Loss: 0.7359188199043274\n",
      "Epoch 29, batch 172 D Loss: 1.3474098443984985, G Loss: 0.7580899596214294\n",
      "Epoch 29, batch 173 D Loss: 1.3564358949661255, G Loss: 0.745464026927948\n",
      "Epoch 29, batch 174 D Loss: 1.371282935142517, G Loss: 0.7097559571266174\n",
      "Epoch 29, batch 175 D Loss: 1.3717485666275024, G Loss: 0.7149448990821838\n",
      "Epoch 29, batch 176 D Loss: 1.3282742500305176, G Loss: 0.7642418146133423\n",
      "Epoch 29, batch 177 D Loss: 1.34942626953125, G Loss: 0.7436718940734863\n",
      "Epoch 29, batch 178 D Loss: 1.3696284294128418, G Loss: 0.7188148498535156\n",
      "Epoch 29, batch 179 D Loss: 1.3565782308578491, G Loss: 0.7298155426979065\n",
      "Epoch 29, batch 180 D Loss: 1.3562246561050415, G Loss: 0.7353705763816833\n",
      "Epoch 29, batch 181 D Loss: 1.3471075296401978, G Loss: 0.7506394386291504\n",
      "Epoch 29, batch 182 D Loss: 1.3573771715164185, G Loss: 0.7422115206718445\n",
      "Epoch 29, batch 183 D Loss: 1.3600964546203613, G Loss: 0.7374679446220398\n",
      "Epoch 29, batch 184 D Loss: 1.3312512636184692, G Loss: 0.7462149262428284\n",
      "Epoch 29, batch 185 D Loss: 1.3482258319854736, G Loss: 0.7221537232398987\n",
      "Epoch 29, batch 186 D Loss: 1.373725414276123, G Loss: 0.7378384470939636\n",
      "Epoch 29, batch 187 D Loss: 1.3546781539916992, G Loss: 0.7184266448020935\n",
      "Epoch 29, batch 188 D Loss: 1.3222978115081787, G Loss: 0.7743818759918213\n",
      "Epoch 29, batch 189 D Loss: 1.3211925029754639, G Loss: 0.7568304538726807\n",
      "Epoch 29, batch 190 D Loss: 1.3315409421920776, G Loss: 0.7679152488708496\n",
      "Epoch 29, batch 191 D Loss: 1.3562239408493042, G Loss: 0.7492349147796631\n",
      "Epoch 29, batch 192 D Loss: 1.3892767429351807, G Loss: 0.7168208956718445\n",
      "Epoch 29, batch 193 D Loss: 1.3905010223388672, G Loss: 0.6949636936187744\n",
      "Epoch 29, batch 194 D Loss: 1.3781849145889282, G Loss: 0.7326503992080688\n",
      "Epoch 29, batch 195 D Loss: 1.3670659065246582, G Loss: 0.7692432403564453\n",
      "Epoch 29, batch 196 D Loss: 1.3334636688232422, G Loss: 0.7817758321762085\n",
      "Epoch 29, batch 197 D Loss: 1.4038403034210205, G Loss: 0.7166602611541748\n",
      "Epoch 29, batch 198 D Loss: 1.376671314239502, G Loss: 0.7436518669128418\n",
      "Epoch 29, batch 199 D Loss: 1.35542631149292, G Loss: 0.7351554036140442\n",
      "Epoch 29, batch 200 D Loss: 1.3865668773651123, G Loss: 0.7145469784736633\n",
      "Epoch 30, batch 1 D Loss: 1.390245795249939, G Loss: 0.7360595464706421\n",
      "Epoch 30, batch 2 D Loss: 1.3840956687927246, G Loss: 0.705112099647522\n",
      "Epoch 30, batch 3 D Loss: 1.375006914138794, G Loss: 0.7016295790672302\n",
      "Epoch 30, batch 4 D Loss: 1.4117474555969238, G Loss: 0.7027217149734497\n",
      "Epoch 30, batch 5 D Loss: 1.402052879333496, G Loss: 0.7020242214202881\n",
      "Epoch 30, batch 6 D Loss: 1.3640705347061157, G Loss: 0.7296777963638306\n",
      "Epoch 30, batch 7 D Loss: 1.3605213165283203, G Loss: 0.7273286581039429\n",
      "Epoch 30, batch 8 D Loss: 1.3484458923339844, G Loss: 0.7263057827949524\n",
      "Epoch 30, batch 9 D Loss: 1.3847904205322266, G Loss: 0.6835094690322876\n",
      "Epoch 30, batch 10 D Loss: 1.3998531103134155, G Loss: 0.6844112873077393\n",
      "Epoch 30, batch 11 D Loss: 1.4261870384216309, G Loss: 0.7320427894592285\n",
      "Epoch 30, batch 12 D Loss: 1.3912575244903564, G Loss: 0.6952725052833557\n",
      "Epoch 30, batch 13 D Loss: 1.4150149822235107, G Loss: 0.6925101280212402\n",
      "Epoch 30, batch 14 D Loss: 1.3988046646118164, G Loss: 0.7023449540138245\n",
      "Epoch 30, batch 15 D Loss: 1.3752425909042358, G Loss: 0.7436655163764954\n",
      "Epoch 30, batch 16 D Loss: 1.4026591777801514, G Loss: 0.693473219871521\n",
      "Epoch 30, batch 17 D Loss: 1.372915506362915, G Loss: 0.6907204985618591\n",
      "Epoch 30, batch 18 D Loss: 1.3776837587356567, G Loss: 0.7035723924636841\n",
      "Epoch 30, batch 19 D Loss: 1.4036650657653809, G Loss: 0.7110083103179932\n",
      "Epoch 30, batch 20 D Loss: 1.3779656887054443, G Loss: 0.7034633159637451\n",
      "Epoch 30, batch 21 D Loss: 1.4015076160430908, G Loss: 0.7043207287788391\n",
      "Epoch 30, batch 22 D Loss: 1.4083302021026611, G Loss: 0.666748046875\n",
      "Epoch 30, batch 23 D Loss: 1.3732993602752686, G Loss: 0.7029302716255188\n",
      "Epoch 30, batch 24 D Loss: 1.4143245220184326, G Loss: 0.6893203258514404\n",
      "Epoch 30, batch 25 D Loss: 1.4016640186309814, G Loss: 0.6838880777359009\n",
      "Epoch 30, batch 26 D Loss: 1.397562861442566, G Loss: 0.7040088772773743\n",
      "Epoch 30, batch 27 D Loss: 1.3677500486373901, G Loss: 0.7128795385360718\n",
      "Epoch 30, batch 28 D Loss: 1.3385019302368164, G Loss: 0.7226966619491577\n",
      "Epoch 30, batch 29 D Loss: 1.3789663314819336, G Loss: 0.7019677758216858\n",
      "Epoch 30, batch 30 D Loss: 1.3787037134170532, G Loss: 0.7026286125183105\n",
      "Epoch 30, batch 31 D Loss: 1.40355384349823, G Loss: 0.6915853023529053\n",
      "Epoch 30, batch 32 D Loss: 1.387020230293274, G Loss: 0.6805541515350342\n",
      "Epoch 30, batch 33 D Loss: 1.385477900505066, G Loss: 0.7044458985328674\n",
      "Epoch 30, batch 34 D Loss: 1.3685154914855957, G Loss: 0.6770692467689514\n",
      "Epoch 30, batch 35 D Loss: 1.4125007390975952, G Loss: 0.6620624661445618\n",
      "Epoch 30, batch 36 D Loss: 1.3593577146530151, G Loss: 0.7118903398513794\n",
      "Epoch 30, batch 37 D Loss: 1.3928768634796143, G Loss: 0.7124658226966858\n",
      "Epoch 30, batch 38 D Loss: 1.4005579948425293, G Loss: 0.6879869103431702\n",
      "Epoch 30, batch 39 D Loss: 1.414931058883667, G Loss: 0.6614484190940857\n",
      "Epoch 30, batch 40 D Loss: 1.3947169780731201, G Loss: 0.6697008013725281\n",
      "Epoch 30, batch 41 D Loss: 1.3708595037460327, G Loss: 0.6928811073303223\n",
      "Epoch 30, batch 42 D Loss: 1.4163930416107178, G Loss: 0.6483336091041565\n",
      "Epoch 30, batch 43 D Loss: 1.3808982372283936, G Loss: 0.6684868335723877\n",
      "Epoch 30, batch 44 D Loss: 1.4325714111328125, G Loss: 0.6499208211898804\n",
      "Epoch 30, batch 45 D Loss: 1.397653341293335, G Loss: 0.6636101007461548\n",
      "Epoch 30, batch 46 D Loss: 1.3528506755828857, G Loss: 0.6837801337242126\n",
      "Epoch 30, batch 47 D Loss: 1.3571217060089111, G Loss: 0.6912828683853149\n",
      "Epoch 30, batch 48 D Loss: 1.3991843461990356, G Loss: 0.6609680652618408\n",
      "Epoch 30, batch 49 D Loss: 1.3991491794586182, G Loss: 0.6575461030006409\n",
      "Epoch 30, batch 50 D Loss: 1.3649396896362305, G Loss: 0.680122971534729\n",
      "Epoch 30, batch 51 D Loss: 1.3936755657196045, G Loss: 0.6662687063217163\n",
      "Epoch 30, batch 52 D Loss: 1.4278305768966675, G Loss: 0.6586821675300598\n",
      "Epoch 30, batch 53 D Loss: 1.3950029611587524, G Loss: 0.6576282382011414\n",
      "Epoch 30, batch 54 D Loss: 1.4110243320465088, G Loss: 0.6701613664627075\n",
      "Epoch 30, batch 55 D Loss: 1.392225980758667, G Loss: 0.669013500213623\n",
      "Epoch 30, batch 56 D Loss: 1.4159390926361084, G Loss: 0.6397899389266968\n",
      "Epoch 30, batch 57 D Loss: 1.3910107612609863, G Loss: 0.6638303995132446\n",
      "Epoch 30, batch 58 D Loss: 1.4419825077056885, G Loss: 0.6343935132026672\n",
      "Epoch 30, batch 59 D Loss: 1.384136438369751, G Loss: 0.6679059863090515\n",
      "Epoch 30, batch 60 D Loss: 1.3864282369613647, G Loss: 0.6623881459236145\n",
      "Epoch 30, batch 61 D Loss: 1.4150519371032715, G Loss: 0.6503495573997498\n",
      "Epoch 30, batch 62 D Loss: 1.3851834535598755, G Loss: 0.6711061596870422\n",
      "Epoch 30, batch 63 D Loss: 1.4291744232177734, G Loss: 0.6528882384300232\n",
      "Epoch 30, batch 64 D Loss: 1.4254626035690308, G Loss: 0.6600873470306396\n",
      "Epoch 30, batch 65 D Loss: 1.3596642017364502, G Loss: 0.6667770147323608\n",
      "Epoch 30, batch 66 D Loss: 1.3965680599212646, G Loss: 0.6686985492706299\n",
      "Epoch 30, batch 67 D Loss: 1.4088318347930908, G Loss: 0.6656097173690796\n",
      "Epoch 30, batch 68 D Loss: 1.4166982173919678, G Loss: 0.6490421295166016\n",
      "Epoch 30, batch 69 D Loss: 1.3968377113342285, G Loss: 0.6722200512886047\n",
      "Epoch 30, batch 70 D Loss: 1.3803491592407227, G Loss: 0.6759980916976929\n",
      "Epoch 30, batch 71 D Loss: 1.3707175254821777, G Loss: 0.667815089225769\n",
      "Epoch 30, batch 72 D Loss: 1.4027886390686035, G Loss: 0.6673150658607483\n",
      "Epoch 30, batch 73 D Loss: 1.4353713989257812, G Loss: 0.653445303440094\n",
      "Epoch 30, batch 74 D Loss: 1.3995513916015625, G Loss: 0.6639336943626404\n",
      "Epoch 30, batch 75 D Loss: 1.4174418449401855, G Loss: 0.6544552445411682\n",
      "Epoch 30, batch 76 D Loss: 1.414678692817688, G Loss: 0.6721636056900024\n",
      "Epoch 30, batch 77 D Loss: 1.429030418395996, G Loss: 0.6567550897598267\n",
      "Epoch 30, batch 78 D Loss: 1.419198989868164, G Loss: 0.6626508831977844\n",
      "Epoch 30, batch 79 D Loss: 1.406142234802246, G Loss: 0.662458598613739\n",
      "Epoch 30, batch 80 D Loss: 1.4091898202896118, G Loss: 0.6702907085418701\n",
      "Epoch 30, batch 81 D Loss: 1.4169249534606934, G Loss: 0.6718982458114624\n",
      "Epoch 30, batch 82 D Loss: 1.3928296566009521, G Loss: 0.6805081963539124\n",
      "Epoch 30, batch 83 D Loss: 1.3901026248931885, G Loss: 0.686573326587677\n",
      "Epoch 30, batch 84 D Loss: 1.3783857822418213, G Loss: 0.6814215779304504\n",
      "Epoch 30, batch 85 D Loss: 1.3858528137207031, G Loss: 0.6836901903152466\n",
      "Epoch 30, batch 86 D Loss: 1.3877252340316772, G Loss: 0.6857503652572632\n",
      "Epoch 30, batch 87 D Loss: 1.4108247756958008, G Loss: 0.6785290241241455\n",
      "Epoch 30, batch 88 D Loss: 1.423640489578247, G Loss: 0.6599030494689941\n",
      "Epoch 30, batch 89 D Loss: 1.4275953769683838, G Loss: 0.6718160510063171\n",
      "Epoch 30, batch 90 D Loss: 1.4021379947662354, G Loss: 0.6776787638664246\n",
      "Epoch 30, batch 91 D Loss: 1.4174652099609375, G Loss: 0.6736696362495422\n",
      "Epoch 30, batch 92 D Loss: 1.4237316846847534, G Loss: 0.6765287518501282\n",
      "Epoch 30, batch 93 D Loss: 1.413694143295288, G Loss: 0.67771315574646\n",
      "Epoch 30, batch 94 D Loss: 1.3946517705917358, G Loss: 0.6920726299285889\n",
      "Epoch 30, batch 95 D Loss: 1.4001154899597168, G Loss: 0.6933460235595703\n",
      "Epoch 30, batch 96 D Loss: 1.3832671642303467, G Loss: 0.7046554684638977\n",
      "Epoch 30, batch 97 D Loss: 1.4084298610687256, G Loss: 0.6964461803436279\n",
      "Epoch 30, batch 98 D Loss: 1.4023548364639282, G Loss: 0.7033882141113281\n",
      "Epoch 30, batch 99 D Loss: 1.4092586040496826, G Loss: 0.7152429819107056\n",
      "Epoch 30, batch 100 D Loss: 1.3886957168579102, G Loss: 0.7224988341331482\n",
      "Epoch 30, batch 101 D Loss: 1.4188661575317383, G Loss: 0.6929962038993835\n",
      "Epoch 30, batch 102 D Loss: 1.3917524814605713, G Loss: 0.7241388559341431\n",
      "Epoch 30, batch 103 D Loss: 1.4458706378936768, G Loss: 0.6880847215652466\n",
      "Epoch 30, batch 104 D Loss: 1.3706085681915283, G Loss: 0.7314260601997375\n",
      "Epoch 30, batch 105 D Loss: 1.384358286857605, G Loss: 0.7226053476333618\n",
      "Epoch 30, batch 106 D Loss: 1.3859825134277344, G Loss: 0.726784348487854\n",
      "Epoch 30, batch 107 D Loss: 1.389864444732666, G Loss: 0.7348211407661438\n",
      "Epoch 30, batch 108 D Loss: 1.3752377033233643, G Loss: 0.7324117422103882\n",
      "Epoch 30, batch 109 D Loss: 1.4086296558380127, G Loss: 0.6918724775314331\n",
      "Epoch 30, batch 110 D Loss: 1.4096505641937256, G Loss: 0.7072806358337402\n",
      "Epoch 30, batch 111 D Loss: 1.3868203163146973, G Loss: 0.7302125692367554\n",
      "Epoch 30, batch 112 D Loss: 1.4316401481628418, G Loss: 0.6962384581565857\n",
      "Epoch 30, batch 113 D Loss: 1.403578519821167, G Loss: 0.7107309103012085\n",
      "Epoch 30, batch 114 D Loss: 1.4129353761672974, G Loss: 0.7166903614997864\n",
      "Epoch 30, batch 115 D Loss: 1.39666748046875, G Loss: 0.7287524342536926\n",
      "Epoch 30, batch 116 D Loss: 1.3730615377426147, G Loss: 0.7174835205078125\n",
      "Epoch 30, batch 117 D Loss: 1.3702079057693481, G Loss: 0.7401261329650879\n",
      "Epoch 30, batch 118 D Loss: 1.4101500511169434, G Loss: 0.7085983157157898\n",
      "Epoch 30, batch 119 D Loss: 1.3883228302001953, G Loss: 0.7236784100532532\n",
      "Epoch 30, batch 120 D Loss: 1.3815703392028809, G Loss: 0.7220342755317688\n",
      "Epoch 30, batch 121 D Loss: 1.382548451423645, G Loss: 0.7140855193138123\n",
      "Epoch 30, batch 122 D Loss: 1.3927879333496094, G Loss: 0.7210824489593506\n",
      "Epoch 30, batch 123 D Loss: 1.4075847864151, G Loss: 0.7046694755554199\n",
      "Epoch 30, batch 124 D Loss: 1.3717589378356934, G Loss: 0.7211510539054871\n",
      "Epoch 30, batch 125 D Loss: 1.391800880432129, G Loss: 0.721376359462738\n",
      "Epoch 30, batch 126 D Loss: 1.3861405849456787, G Loss: 0.7119548916816711\n",
      "Epoch 30, batch 127 D Loss: 1.385709524154663, G Loss: 0.7156413793563843\n",
      "Epoch 30, batch 128 D Loss: 1.393496036529541, G Loss: 0.7065004706382751\n",
      "Epoch 30, batch 129 D Loss: 1.3846467733383179, G Loss: 0.7119482159614563\n",
      "Epoch 30, batch 130 D Loss: 1.3752975463867188, G Loss: 0.7241856455802917\n",
      "Epoch 30, batch 131 D Loss: 1.3884339332580566, G Loss: 0.7062936425209045\n",
      "Epoch 30, batch 132 D Loss: 1.379974365234375, G Loss: 0.7164794206619263\n",
      "Epoch 30, batch 133 D Loss: 1.3894076347351074, G Loss: 0.7081316113471985\n",
      "Epoch 30, batch 134 D Loss: 1.388001799583435, G Loss: 0.7112838625907898\n",
      "Epoch 30, batch 135 D Loss: 1.381464958190918, G Loss: 0.7174397110939026\n",
      "Epoch 30, batch 136 D Loss: 1.391127586364746, G Loss: 0.7102994322776794\n",
      "Epoch 30, batch 137 D Loss: 1.3915607929229736, G Loss: 0.7119736671447754\n",
      "Epoch 30, batch 138 D Loss: 1.374601125717163, G Loss: 0.7176623344421387\n",
      "Epoch 30, batch 139 D Loss: 1.3796238899230957, G Loss: 0.7177536487579346\n",
      "Epoch 30, batch 140 D Loss: 1.3792781829833984, G Loss: 0.7203866839408875\n",
      "Epoch 30, batch 141 D Loss: 1.382311224937439, G Loss: 0.718304455280304\n",
      "Epoch 30, batch 142 D Loss: 1.3850023746490479, G Loss: 0.7208461165428162\n",
      "Epoch 30, batch 143 D Loss: 1.3792533874511719, G Loss: 0.7257417440414429\n",
      "Epoch 30, batch 144 D Loss: 1.368640661239624, G Loss: 0.7254831194877625\n",
      "Epoch 30, batch 145 D Loss: 1.3852547407150269, G Loss: 0.7277466654777527\n",
      "Epoch 30, batch 146 D Loss: 1.3735498189926147, G Loss: 0.7280524373054504\n",
      "Epoch 30, batch 147 D Loss: 1.3601393699645996, G Loss: 0.7284943461418152\n",
      "Epoch 30, batch 148 D Loss: 1.3763988018035889, G Loss: 0.7337924242019653\n",
      "Epoch 30, batch 149 D Loss: 1.375335931777954, G Loss: 0.7308650016784668\n",
      "Epoch 30, batch 150 D Loss: 1.367680311203003, G Loss: 0.7373856902122498\n",
      "Epoch 30, batch 151 D Loss: 1.3727444410324097, G Loss: 0.730562686920166\n",
      "Epoch 30, batch 152 D Loss: 1.3634898662567139, G Loss: 0.7398303747177124\n",
      "Epoch 30, batch 153 D Loss: 1.360107660293579, G Loss: 0.755821704864502\n",
      "Epoch 30, batch 154 D Loss: 1.3616833686828613, G Loss: 0.7497163414955139\n",
      "Epoch 30, batch 155 D Loss: 1.3741583824157715, G Loss: 0.7434142827987671\n",
      "Epoch 30, batch 156 D Loss: 1.3616018295288086, G Loss: 0.7419366240501404\n",
      "Epoch 30, batch 157 D Loss: 1.3514785766601562, G Loss: 0.7572733163833618\n",
      "Epoch 30, batch 158 D Loss: 1.3562564849853516, G Loss: 0.7507199048995972\n",
      "Epoch 30, batch 159 D Loss: 1.3636572360992432, G Loss: 0.7642985582351685\n",
      "Epoch 30, batch 160 D Loss: 1.3673067092895508, G Loss: 0.7456875443458557\n",
      "Epoch 30, batch 161 D Loss: 1.3699606657028198, G Loss: 0.7415632605552673\n",
      "Epoch 30, batch 162 D Loss: 1.3370380401611328, G Loss: 0.762119472026825\n",
      "Epoch 30, batch 163 D Loss: 1.3412779569625854, G Loss: 0.7714347839355469\n",
      "Epoch 30, batch 164 D Loss: 1.3474570512771606, G Loss: 0.7559245228767395\n",
      "Epoch 30, batch 165 D Loss: 1.376950740814209, G Loss: 0.7501605749130249\n",
      "Epoch 30, batch 166 D Loss: 1.3672640323638916, G Loss: 0.7446922063827515\n",
      "Epoch 30, batch 167 D Loss: 1.3528026342391968, G Loss: 0.7638013362884521\n",
      "Epoch 30, batch 168 D Loss: 1.359451413154602, G Loss: 0.7526682019233704\n",
      "Epoch 30, batch 169 D Loss: 1.3219804763793945, G Loss: 0.7790295481681824\n",
      "Epoch 30, batch 170 D Loss: 1.336145281791687, G Loss: 0.7589833736419678\n",
      "Epoch 30, batch 171 D Loss: 1.3680351972579956, G Loss: 0.7491135597229004\n",
      "Epoch 30, batch 172 D Loss: 1.3777494430541992, G Loss: 0.7456879615783691\n",
      "Epoch 30, batch 173 D Loss: 1.380164623260498, G Loss: 0.7209429740905762\n",
      "Epoch 30, batch 174 D Loss: 1.3793747425079346, G Loss: 0.7356554269790649\n",
      "Epoch 30, batch 175 D Loss: 1.3948559761047363, G Loss: 0.7220984101295471\n",
      "Epoch 30, batch 176 D Loss: 1.347043514251709, G Loss: 0.7378464341163635\n",
      "Epoch 30, batch 177 D Loss: 1.3542721271514893, G Loss: 0.7462277412414551\n",
      "Epoch 30, batch 178 D Loss: 1.3712199926376343, G Loss: 0.720278799533844\n",
      "Epoch 30, batch 179 D Loss: 1.35377836227417, G Loss: 0.7368435859680176\n",
      "Epoch 30, batch 180 D Loss: 1.3647792339324951, G Loss: 0.7172671556472778\n",
      "Epoch 30, batch 181 D Loss: 1.3647780418395996, G Loss: 0.7223769426345825\n",
      "Epoch 30, batch 182 D Loss: 1.3729532957077026, G Loss: 0.7383450865745544\n",
      "Epoch 30, batch 183 D Loss: 1.3269736766815186, G Loss: 0.7181462049484253\n",
      "Epoch 30, batch 184 D Loss: 1.374671220779419, G Loss: 0.7093861103057861\n",
      "Epoch 30, batch 185 D Loss: 1.3682438135147095, G Loss: 0.6935017108917236\n",
      "Epoch 30, batch 186 D Loss: 1.4014911651611328, G Loss: 0.6950030326843262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, batch 187 D Loss: 1.341491937637329, G Loss: 0.7213913202285767\n",
      "Epoch 30, batch 188 D Loss: 1.3488383293151855, G Loss: 0.721376359462738\n",
      "Epoch 30, batch 189 D Loss: 1.3465120792388916, G Loss: 0.7040497660636902\n",
      "Epoch 30, batch 190 D Loss: 1.4291132688522339, G Loss: 0.6825835704803467\n",
      "Epoch 30, batch 191 D Loss: 1.36391282081604, G Loss: 0.7221243381500244\n",
      "Epoch 30, batch 192 D Loss: 1.354623794555664, G Loss: 0.7040138840675354\n",
      "Epoch 30, batch 193 D Loss: 1.3708208799362183, G Loss: 0.7003875970840454\n",
      "Epoch 30, batch 194 D Loss: 1.426849126815796, G Loss: 0.6768457889556885\n",
      "Epoch 30, batch 195 D Loss: 1.3949958086013794, G Loss: 0.6881692409515381\n",
      "Epoch 30, batch 196 D Loss: 1.3682661056518555, G Loss: 0.6843467950820923\n",
      "Epoch 30, batch 197 D Loss: 1.3681504726409912, G Loss: 0.6897717118263245\n",
      "Epoch 30, batch 198 D Loss: 1.3867590427398682, G Loss: 0.6842774748802185\n",
      "Epoch 30, batch 199 D Loss: 1.395920991897583, G Loss: 0.6881294846534729\n",
      "Epoch 30, batch 200 D Loss: 1.355093002319336, G Loss: 0.6898924112319946\n",
      "Epoch 31, batch 1 D Loss: 1.3850586414337158, G Loss: 0.6973803639411926\n",
      "Epoch 31, batch 2 D Loss: 1.3690242767333984, G Loss: 0.6970719695091248\n",
      "Epoch 31, batch 3 D Loss: 1.402913212776184, G Loss: 0.6772859692573547\n",
      "Epoch 31, batch 4 D Loss: 1.376206636428833, G Loss: 0.6684377789497375\n",
      "Epoch 31, batch 5 D Loss: 1.3688620328903198, G Loss: 0.6922823190689087\n",
      "Epoch 31, batch 6 D Loss: 1.3713736534118652, G Loss: 0.6927594542503357\n",
      "Epoch 31, batch 7 D Loss: 1.3764371871948242, G Loss: 0.688668966293335\n",
      "Epoch 31, batch 8 D Loss: 1.3833197355270386, G Loss: 0.6671082973480225\n",
      "Epoch 31, batch 9 D Loss: 1.3844504356384277, G Loss: 0.680097758769989\n",
      "Epoch 31, batch 10 D Loss: 1.393216848373413, G Loss: 0.6921635270118713\n",
      "Epoch 31, batch 11 D Loss: 1.4170856475830078, G Loss: 0.6743502020835876\n",
      "Epoch 31, batch 12 D Loss: 1.395264744758606, G Loss: 0.6815332174301147\n",
      "Epoch 31, batch 13 D Loss: 1.3982799053192139, G Loss: 0.6671614646911621\n",
      "Epoch 31, batch 14 D Loss: 1.347370982170105, G Loss: 0.6845694780349731\n",
      "Epoch 31, batch 15 D Loss: 1.41159987449646, G Loss: 0.6536027789115906\n",
      "Epoch 31, batch 16 D Loss: 1.3538411855697632, G Loss: 0.6886402368545532\n",
      "Epoch 31, batch 17 D Loss: 1.3954334259033203, G Loss: 0.6737470030784607\n",
      "Epoch 31, batch 18 D Loss: 1.3830103874206543, G Loss: 0.6685962080955505\n",
      "Epoch 31, batch 19 D Loss: 1.3867874145507812, G Loss: 0.6630542874336243\n",
      "Epoch 31, batch 20 D Loss: 1.3797967433929443, G Loss: 0.6691582202911377\n",
      "Epoch 31, batch 21 D Loss: 1.385751724243164, G Loss: 0.6664800047874451\n",
      "Epoch 31, batch 22 D Loss: 1.3650736808776855, G Loss: 0.6607995629310608\n",
      "Epoch 31, batch 23 D Loss: 1.3789372444152832, G Loss: 0.6729880571365356\n",
      "Epoch 31, batch 24 D Loss: 1.379738211631775, G Loss: 0.6623363494873047\n",
      "Epoch 31, batch 25 D Loss: 1.3852742910385132, G Loss: 0.6633296012878418\n",
      "Epoch 31, batch 26 D Loss: 1.3714109659194946, G Loss: 0.6720790863037109\n",
      "Epoch 31, batch 27 D Loss: 1.3943049907684326, G Loss: 0.6624948382377625\n",
      "Epoch 31, batch 28 D Loss: 1.377338171005249, G Loss: 0.6640834212303162\n",
      "Epoch 31, batch 29 D Loss: 1.3846218585968018, G Loss: 0.6441064476966858\n",
      "Epoch 31, batch 30 D Loss: 1.3772310018539429, G Loss: 0.6511685848236084\n",
      "Epoch 31, batch 31 D Loss: 1.3776862621307373, G Loss: 0.6588079333305359\n",
      "Epoch 31, batch 32 D Loss: 1.3963186740875244, G Loss: 0.6608455181121826\n",
      "Epoch 31, batch 33 D Loss: 1.3595762252807617, G Loss: 0.6798461079597473\n",
      "Epoch 31, batch 34 D Loss: 1.4183744192123413, G Loss: 0.6607571244239807\n",
      "Epoch 31, batch 35 D Loss: 1.3804035186767578, G Loss: 0.6529420614242554\n",
      "Epoch 31, batch 36 D Loss: 1.3860636949539185, G Loss: 0.6582362651824951\n",
      "Epoch 31, batch 37 D Loss: 1.3633272647857666, G Loss: 0.6958253383636475\n",
      "Epoch 31, batch 38 D Loss: 1.3707396984100342, G Loss: 0.6768298149108887\n",
      "Epoch 31, batch 39 D Loss: 1.3773140907287598, G Loss: 0.6875667572021484\n",
      "Epoch 31, batch 40 D Loss: 1.3905836343765259, G Loss: 0.6733395457267761\n",
      "Epoch 31, batch 41 D Loss: 1.4141385555267334, G Loss: 0.6620252728462219\n",
      "Epoch 31, batch 42 D Loss: 1.3739025592803955, G Loss: 0.6690412759780884\n",
      "Epoch 31, batch 43 D Loss: 1.4351325035095215, G Loss: 0.6386294364929199\n",
      "Epoch 31, batch 44 D Loss: 1.4213941097259521, G Loss: 0.6736445426940918\n",
      "Epoch 31, batch 45 D Loss: 1.3627592325210571, G Loss: 0.6807783246040344\n",
      "Epoch 31, batch 46 D Loss: 1.393932580947876, G Loss: 0.66884446144104\n",
      "Epoch 31, batch 47 D Loss: 1.3318028450012207, G Loss: 0.7064906358718872\n",
      "Epoch 31, batch 48 D Loss: 1.4061098098754883, G Loss: 0.6636778116226196\n",
      "Epoch 31, batch 49 D Loss: 1.3897478580474854, G Loss: 0.6726831197738647\n",
      "Epoch 31, batch 50 D Loss: 1.381446361541748, G Loss: 0.6776819825172424\n",
      "Epoch 31, batch 51 D Loss: 1.4736926555633545, G Loss: 0.6298006176948547\n",
      "Epoch 31, batch 52 D Loss: 1.3983983993530273, G Loss: 0.6823969483375549\n",
      "Epoch 31, batch 53 D Loss: 1.4086694717407227, G Loss: 0.662689208984375\n",
      "Epoch 31, batch 54 D Loss: 1.383033275604248, G Loss: 0.6988656520843506\n",
      "Epoch 31, batch 55 D Loss: 1.4155327081680298, G Loss: 0.652099072933197\n",
      "Epoch 31, batch 56 D Loss: 1.4192047119140625, G Loss: 0.6674821972846985\n",
      "Epoch 31, batch 57 D Loss: 1.404118299484253, G Loss: 0.6656008362770081\n",
      "Epoch 31, batch 58 D Loss: 1.4406344890594482, G Loss: 0.6594056487083435\n",
      "Epoch 31, batch 59 D Loss: 1.4043073654174805, G Loss: 0.6798915266990662\n",
      "Epoch 31, batch 60 D Loss: 1.453808307647705, G Loss: 0.6692140102386475\n",
      "Epoch 31, batch 61 D Loss: 1.454437494277954, G Loss: 0.658226490020752\n",
      "Epoch 31, batch 62 D Loss: 1.4551279544830322, G Loss: 0.6658011078834534\n",
      "Epoch 31, batch 63 D Loss: 1.4230964183807373, G Loss: 0.6765207648277283\n",
      "Epoch 31, batch 64 D Loss: 1.4186530113220215, G Loss: 0.6793321371078491\n",
      "Epoch 31, batch 65 D Loss: 1.4201736450195312, G Loss: 0.6907936930656433\n",
      "Epoch 31, batch 66 D Loss: 1.4261068105697632, G Loss: 0.6660527586936951\n",
      "Epoch 31, batch 67 D Loss: 1.4286030530929565, G Loss: 0.6769368052482605\n",
      "Epoch 31, batch 68 D Loss: 1.4240796566009521, G Loss: 0.6792559623718262\n",
      "Epoch 31, batch 69 D Loss: 1.4136552810668945, G Loss: 0.7220489978790283\n",
      "Epoch 31, batch 70 D Loss: 1.3965730667114258, G Loss: 0.7113955616950989\n",
      "Epoch 31, batch 71 D Loss: 1.4190607070922852, G Loss: 0.7014314532279968\n",
      "Epoch 31, batch 72 D Loss: 1.4217305183410645, G Loss: 0.706305980682373\n",
      "Epoch 31, batch 73 D Loss: 1.4461199045181274, G Loss: 0.6889166235923767\n",
      "Epoch 31, batch 74 D Loss: 1.4227421283721924, G Loss: 0.6921799182891846\n",
      "Epoch 31, batch 75 D Loss: 1.431377649307251, G Loss: 0.6893395185470581\n",
      "Epoch 31, batch 76 D Loss: 1.4181783199310303, G Loss: 0.7128399610519409\n",
      "Epoch 31, batch 77 D Loss: 1.413649559020996, G Loss: 0.704872727394104\n",
      "Epoch 31, batch 78 D Loss: 1.4351704120635986, G Loss: 0.7029985785484314\n",
      "Epoch 31, batch 79 D Loss: 1.390796422958374, G Loss: 0.7296217083930969\n",
      "Epoch 31, batch 80 D Loss: 1.4131581783294678, G Loss: 0.7181500792503357\n",
      "Epoch 31, batch 81 D Loss: 1.4150428771972656, G Loss: 0.7241616249084473\n",
      "Epoch 31, batch 82 D Loss: 1.4037652015686035, G Loss: 0.7359532117843628\n",
      "Epoch 31, batch 83 D Loss: 1.4026267528533936, G Loss: 0.7242426872253418\n",
      "Epoch 31, batch 84 D Loss: 1.4151382446289062, G Loss: 0.7294820547103882\n",
      "Epoch 31, batch 85 D Loss: 1.3970428705215454, G Loss: 0.7269515991210938\n",
      "Epoch 31, batch 86 D Loss: 1.4002652168273926, G Loss: 0.7363533973693848\n",
      "Epoch 31, batch 87 D Loss: 1.3966166973114014, G Loss: 0.7405693531036377\n",
      "Epoch 31, batch 88 D Loss: 1.3815455436706543, G Loss: 0.7463737726211548\n",
      "Epoch 31, batch 89 D Loss: 1.3877646923065186, G Loss: 0.7459383606910706\n",
      "Epoch 31, batch 90 D Loss: 1.388363242149353, G Loss: 0.7449000477790833\n",
      "Epoch 31, batch 91 D Loss: 1.3691020011901855, G Loss: 0.760469913482666\n",
      "Epoch 31, batch 92 D Loss: 1.3685739040374756, G Loss: 0.7653903961181641\n",
      "Epoch 31, batch 93 D Loss: 1.392716884613037, G Loss: 0.7481441497802734\n",
      "Epoch 31, batch 94 D Loss: 1.394187331199646, G Loss: 0.7430504560470581\n",
      "Epoch 31, batch 95 D Loss: 1.383784532546997, G Loss: 0.7437877058982849\n",
      "Epoch 31, batch 96 D Loss: 1.406813621520996, G Loss: 0.7361196279525757\n",
      "Epoch 31, batch 97 D Loss: 1.3442168235778809, G Loss: 0.7896205186843872\n",
      "Epoch 31, batch 98 D Loss: 1.3604637384414673, G Loss: 0.7637816071510315\n",
      "Epoch 31, batch 99 D Loss: 1.4090080261230469, G Loss: 0.723857045173645\n",
      "Epoch 31, batch 100 D Loss: 1.3765064477920532, G Loss: 0.7459681034088135\n",
      "Epoch 31, batch 101 D Loss: 1.3737990856170654, G Loss: 0.7554041147232056\n",
      "Epoch 31, batch 102 D Loss: 1.404477596282959, G Loss: 0.7294092774391174\n",
      "Epoch 31, batch 103 D Loss: 1.374342441558838, G Loss: 0.7604910135269165\n",
      "Epoch 31, batch 104 D Loss: 1.38715660572052, G Loss: 0.7497551441192627\n",
      "Epoch 31, batch 105 D Loss: 1.3761250972747803, G Loss: 0.747397243976593\n",
      "Epoch 31, batch 106 D Loss: 1.3802411556243896, G Loss: 0.76524817943573\n",
      "Epoch 31, batch 107 D Loss: 1.3731603622436523, G Loss: 0.7279644012451172\n",
      "Epoch 31, batch 108 D Loss: 1.4197273254394531, G Loss: 0.7058694958686829\n",
      "Epoch 31, batch 109 D Loss: 1.3732688426971436, G Loss: 0.7350479960441589\n",
      "Epoch 31, batch 110 D Loss: 1.3825056552886963, G Loss: 0.754152238368988\n",
      "Epoch 31, batch 111 D Loss: 1.3722858428955078, G Loss: 0.7261072397232056\n",
      "Epoch 31, batch 112 D Loss: 1.408782958984375, G Loss: 0.7089652419090271\n",
      "Epoch 31, batch 113 D Loss: 1.367082118988037, G Loss: 0.7306733727455139\n",
      "Epoch 31, batch 114 D Loss: 1.3643016815185547, G Loss: 0.7473673820495605\n",
      "Epoch 31, batch 115 D Loss: 1.402522087097168, G Loss: 0.6980894207954407\n",
      "Epoch 31, batch 116 D Loss: 1.37856125831604, G Loss: 0.7250869870185852\n",
      "Epoch 31, batch 117 D Loss: 1.3765827417373657, G Loss: 0.7367346286773682\n",
      "Epoch 31, batch 118 D Loss: 1.366869568824768, G Loss: 0.711264967918396\n",
      "Epoch 31, batch 119 D Loss: 1.3715035915374756, G Loss: 0.7054920792579651\n",
      "Epoch 31, batch 120 D Loss: 1.365344762802124, G Loss: 0.7050796747207642\n",
      "Epoch 31, batch 121 D Loss: 1.3649221658706665, G Loss: 0.7252255082130432\n",
      "Epoch 31, batch 122 D Loss: 1.388108491897583, G Loss: 0.7105657458305359\n",
      "Epoch 31, batch 123 D Loss: 1.404511570930481, G Loss: 0.6944078207015991\n",
      "Epoch 31, batch 124 D Loss: 1.3948012590408325, G Loss: 0.6984729170799255\n",
      "Epoch 31, batch 125 D Loss: 1.3866468667984009, G Loss: 0.7050287127494812\n",
      "Epoch 31, batch 126 D Loss: 1.4022812843322754, G Loss: 0.6680035591125488\n",
      "Epoch 31, batch 127 D Loss: 1.4195492267608643, G Loss: 0.682058572769165\n",
      "Epoch 31, batch 128 D Loss: 1.3806543350219727, G Loss: 0.6942211389541626\n",
      "Epoch 31, batch 129 D Loss: 1.368700623512268, G Loss: 0.6905010938644409\n",
      "Epoch 31, batch 130 D Loss: 1.3952761888504028, G Loss: 0.6758676171302795\n",
      "Epoch 31, batch 131 D Loss: 1.3849682807922363, G Loss: 0.6874932050704956\n",
      "Epoch 31, batch 132 D Loss: 1.3967077732086182, G Loss: 0.6828606128692627\n",
      "Epoch 31, batch 133 D Loss: 1.3921010494232178, G Loss: 0.685410737991333\n",
      "Epoch 31, batch 134 D Loss: 1.3858156204223633, G Loss: 0.6850875616073608\n",
      "Epoch 31, batch 135 D Loss: 1.3917104005813599, G Loss: 0.6894451975822449\n",
      "Epoch 31, batch 136 D Loss: 1.4152584075927734, G Loss: 0.6767668128013611\n",
      "Epoch 31, batch 137 D Loss: 1.3912265300750732, G Loss: 0.6881759166717529\n",
      "Epoch 31, batch 138 D Loss: 1.3629891872406006, G Loss: 0.6987630724906921\n",
      "Epoch 31, batch 139 D Loss: 1.3914828300476074, G Loss: 0.679802417755127\n",
      "Epoch 31, batch 140 D Loss: 1.3729186058044434, G Loss: 0.6795743703842163\n",
      "Epoch 31, batch 141 D Loss: 1.362381935119629, G Loss: 0.6965725421905518\n",
      "Epoch 31, batch 142 D Loss: 1.3821828365325928, G Loss: 0.6819506883621216\n",
      "Epoch 31, batch 143 D Loss: 1.3910205364227295, G Loss: 0.6841726899147034\n",
      "Epoch 31, batch 144 D Loss: 1.376732587814331, G Loss: 0.6905122995376587\n",
      "Epoch 31, batch 145 D Loss: 1.371734857559204, G Loss: 0.696616530418396\n",
      "Epoch 31, batch 146 D Loss: 1.3828136920928955, G Loss: 0.683647096157074\n",
      "Epoch 31, batch 147 D Loss: 1.382847547531128, G Loss: 0.683732807636261\n",
      "Epoch 31, batch 148 D Loss: 1.370468258857727, G Loss: 0.6840336322784424\n",
      "Epoch 31, batch 149 D Loss: 1.3568284511566162, G Loss: 0.701423168182373\n",
      "Epoch 31, batch 150 D Loss: 1.3680534362792969, G Loss: 0.6853597164154053\n",
      "Epoch 31, batch 151 D Loss: 1.371567726135254, G Loss: 0.6960911750793457\n",
      "Epoch 31, batch 152 D Loss: 1.3586993217468262, G Loss: 0.6842584013938904\n",
      "Epoch 31, batch 153 D Loss: 1.3684451580047607, G Loss: 0.6869639754295349\n",
      "Epoch 31, batch 154 D Loss: 1.3578872680664062, G Loss: 0.7037903070449829\n",
      "Epoch 31, batch 155 D Loss: 1.375054121017456, G Loss: 0.7017397880554199\n",
      "Epoch 31, batch 156 D Loss: 1.3922417163848877, G Loss: 0.6679343581199646\n",
      "Epoch 31, batch 157 D Loss: 1.3516218662261963, G Loss: 0.6835732460021973\n",
      "Epoch 31, batch 158 D Loss: 1.3346176147460938, G Loss: 0.7214025855064392\n",
      "Epoch 31, batch 159 D Loss: 1.3822940587997437, G Loss: 0.6783893704414368\n",
      "Epoch 31, batch 160 D Loss: 1.3533689975738525, G Loss: 0.7073606848716736\n",
      "Epoch 31, batch 161 D Loss: 1.3814531564712524, G Loss: 0.6914212107658386\n",
      "Epoch 31, batch 162 D Loss: 1.3628885746002197, G Loss: 0.699256420135498\n",
      "Epoch 31, batch 163 D Loss: 1.350993037223816, G Loss: 0.7092255353927612\n",
      "Epoch 31, batch 164 D Loss: 1.3754284381866455, G Loss: 0.6999901533126831\n",
      "Epoch 31, batch 165 D Loss: 1.3601969480514526, G Loss: 0.7067091464996338\n",
      "Epoch 31, batch 166 D Loss: 1.3787769079208374, G Loss: 0.7053791284561157\n",
      "Epoch 31, batch 167 D Loss: 1.3942859172821045, G Loss: 0.6906446814537048\n",
      "Epoch 31, batch 168 D Loss: 1.3965669870376587, G Loss: 0.676713764667511\n",
      "Epoch 31, batch 169 D Loss: 1.3721295595169067, G Loss: 0.7024937272071838\n",
      "Epoch 31, batch 170 D Loss: 1.3685710430145264, G Loss: 0.6906251311302185\n",
      "Epoch 31, batch 171 D Loss: 1.4002690315246582, G Loss: 0.6711540222167969\n",
      "Epoch 31, batch 172 D Loss: 1.3137974739074707, G Loss: 0.7246758341789246\n",
      "Epoch 31, batch 173 D Loss: 1.357515811920166, G Loss: 0.7150266766548157\n",
      "Epoch 31, batch 174 D Loss: 1.356302261352539, G Loss: 0.6716583967208862\n",
      "Epoch 31, batch 175 D Loss: 1.3352839946746826, G Loss: 0.7021062970161438\n",
      "Epoch 31, batch 176 D Loss: 1.387904167175293, G Loss: 0.6794697642326355\n",
      "Epoch 31, batch 177 D Loss: 1.3542022705078125, G Loss: 0.7062414288520813\n",
      "Epoch 31, batch 178 D Loss: 1.4016337394714355, G Loss: 0.6792199611663818\n",
      "Epoch 31, batch 179 D Loss: 1.3395205736160278, G Loss: 0.6872591376304626\n",
      "Epoch 31, batch 180 D Loss: 1.3372948169708252, G Loss: 0.6866365671157837\n",
      "Epoch 31, batch 181 D Loss: 1.369523525238037, G Loss: 0.686989426612854\n",
      "Epoch 31, batch 182 D Loss: 1.3489282131195068, G Loss: 0.7108984589576721\n",
      "Epoch 31, batch 183 D Loss: 1.3505483865737915, G Loss: 0.6543058156967163\n",
      "Epoch 31, batch 184 D Loss: 1.4183549880981445, G Loss: 0.645051896572113\n",
      "Epoch 31, batch 185 D Loss: 1.3987348079681396, G Loss: 0.6608132123947144\n",
      "Epoch 31, batch 186 D Loss: 1.37699294090271, G Loss: 0.6742763519287109\n",
      "Epoch 31, batch 187 D Loss: 1.4118642807006836, G Loss: 0.6522824168205261\n",
      "Epoch 31, batch 188 D Loss: 1.392993450164795, G Loss: 0.6774659752845764\n",
      "Epoch 31, batch 189 D Loss: 1.429509162902832, G Loss: 0.675667405128479\n",
      "Epoch 31, batch 190 D Loss: 1.4093157052993774, G Loss: 0.6728755831718445\n",
      "Epoch 31, batch 191 D Loss: 1.362612247467041, G Loss: 0.683790922164917\n",
      "Epoch 31, batch 192 D Loss: 1.410944938659668, G Loss: 0.6720976829528809\n",
      "Epoch 31, batch 193 D Loss: 1.3586171865463257, G Loss: 0.6834561228752136\n",
      "Epoch 31, batch 194 D Loss: 1.4056923389434814, G Loss: 0.6625509858131409\n",
      "Epoch 31, batch 195 D Loss: 1.376828908920288, G Loss: 0.700600266456604\n",
      "Epoch 31, batch 196 D Loss: 1.3763030767440796, G Loss: 0.6881646513938904\n",
      "Epoch 31, batch 197 D Loss: 1.3824710845947266, G Loss: 0.6864053606987\n",
      "Epoch 31, batch 198 D Loss: 1.3833180665969849, G Loss: 0.6839689612388611\n",
      "Epoch 31, batch 199 D Loss: 1.3980590105056763, G Loss: 0.6838783025741577\n",
      "Epoch 31, batch 200 D Loss: 1.4154990911483765, G Loss: 0.6824803948402405\n",
      "Epoch 32, batch 1 D Loss: 1.390144944190979, G Loss: 0.7101197838783264\n",
      "Epoch 32, batch 2 D Loss: 1.4067506790161133, G Loss: 0.6852865815162659\n",
      "Epoch 32, batch 3 D Loss: 1.4330437183380127, G Loss: 0.6848389506340027\n",
      "Epoch 32, batch 4 D Loss: 1.3764523267745972, G Loss: 0.7165541648864746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, batch 5 D Loss: 1.3896509408950806, G Loss: 0.705270528793335\n",
      "Epoch 32, batch 6 D Loss: 1.3903298377990723, G Loss: 0.7150810360908508\n",
      "Epoch 32, batch 7 D Loss: 1.4081354141235352, G Loss: 0.7023767828941345\n",
      "Epoch 32, batch 8 D Loss: 1.3856992721557617, G Loss: 0.7196933031082153\n",
      "Epoch 32, batch 9 D Loss: 1.38627290725708, G Loss: 0.7101118564605713\n",
      "Epoch 32, batch 10 D Loss: 1.3974010944366455, G Loss: 0.7189908027648926\n",
      "Epoch 32, batch 11 D Loss: 1.366300344467163, G Loss: 0.7282008528709412\n",
      "Epoch 32, batch 12 D Loss: 1.3870916366577148, G Loss: 0.7357000708580017\n",
      "Epoch 32, batch 13 D Loss: 1.4023146629333496, G Loss: 0.7147114276885986\n",
      "Epoch 32, batch 14 D Loss: 1.386986255645752, G Loss: 0.7344802021980286\n",
      "Epoch 32, batch 15 D Loss: 1.3685343265533447, G Loss: 0.7557653784751892\n",
      "Epoch 32, batch 16 D Loss: 1.3781731128692627, G Loss: 0.736318051815033\n",
      "Epoch 32, batch 17 D Loss: 1.3925039768218994, G Loss: 0.7289077639579773\n",
      "Epoch 32, batch 18 D Loss: 1.3842220306396484, G Loss: 0.7360458374023438\n",
      "Epoch 32, batch 19 D Loss: 1.3678407669067383, G Loss: 0.7477771639823914\n",
      "Epoch 32, batch 20 D Loss: 1.3665771484375, G Loss: 0.7471386194229126\n",
      "Epoch 32, batch 21 D Loss: 1.381422758102417, G Loss: 0.7340983748435974\n",
      "Epoch 32, batch 22 D Loss: 1.3671300411224365, G Loss: 0.7699500918388367\n",
      "Epoch 32, batch 23 D Loss: 1.3652076721191406, G Loss: 0.7312752604484558\n",
      "Epoch 32, batch 24 D Loss: 1.3547656536102295, G Loss: 0.7630801200866699\n",
      "Epoch 32, batch 25 D Loss: 1.4129903316497803, G Loss: 0.7020168900489807\n",
      "Epoch 32, batch 26 D Loss: 1.373046636581421, G Loss: 0.7471376061439514\n",
      "Epoch 32, batch 27 D Loss: 1.3844506740570068, G Loss: 0.7205908298492432\n",
      "Epoch 32, batch 28 D Loss: 1.3754589557647705, G Loss: 0.7338794469833374\n",
      "Epoch 32, batch 29 D Loss: 1.4099012613296509, G Loss: 0.7103202939033508\n",
      "Epoch 32, batch 30 D Loss: 1.3931629657745361, G Loss: 0.7593586444854736\n",
      "Epoch 32, batch 31 D Loss: 1.3378396034240723, G Loss: 0.7460818290710449\n",
      "Epoch 32, batch 32 D Loss: 1.38374662399292, G Loss: 0.7077435851097107\n",
      "Epoch 32, batch 33 D Loss: 1.370997428894043, G Loss: 0.7367973327636719\n",
      "Epoch 32, batch 34 D Loss: 1.4294461011886597, G Loss: 0.6759436130523682\n",
      "Epoch 32, batch 35 D Loss: 1.4031634330749512, G Loss: 0.7039193511009216\n",
      "Epoch 32, batch 36 D Loss: 1.4020676612854004, G Loss: 0.720285177230835\n",
      "Epoch 32, batch 37 D Loss: 1.3843867778778076, G Loss: 0.7349608540534973\n",
      "Epoch 32, batch 38 D Loss: 1.4128365516662598, G Loss: 0.7202914357185364\n",
      "Epoch 32, batch 39 D Loss: 1.4045379161834717, G Loss: 0.7150785326957703\n",
      "Epoch 32, batch 40 D Loss: 1.3943276405334473, G Loss: 0.6965394616127014\n",
      "Epoch 32, batch 41 D Loss: 1.367905616760254, G Loss: 0.7280943989753723\n",
      "Epoch 32, batch 42 D Loss: 1.3635157346725464, G Loss: 0.732841432094574\n",
      "Epoch 32, batch 43 D Loss: 1.3875142335891724, G Loss: 0.7102708220481873\n",
      "Epoch 32, batch 44 D Loss: 1.4165527820587158, G Loss: 0.6934005618095398\n",
      "Epoch 32, batch 45 D Loss: 1.4614601135253906, G Loss: 0.6686556935310364\n",
      "Epoch 32, batch 46 D Loss: 1.3959388732910156, G Loss: 0.71736741065979\n",
      "Epoch 32, batch 47 D Loss: 1.3979188203811646, G Loss: 0.7042217254638672\n",
      "Epoch 32, batch 48 D Loss: 1.38448965549469, G Loss: 0.7372273206710815\n",
      "Epoch 32, batch 49 D Loss: 1.425004243850708, G Loss: 0.7044253349304199\n",
      "Epoch 32, batch 50 D Loss: 1.374761700630188, G Loss: 0.7157136797904968\n",
      "Epoch 32, batch 51 D Loss: 1.4053497314453125, G Loss: 0.7191758751869202\n",
      "Epoch 32, batch 52 D Loss: 1.4386444091796875, G Loss: 0.6798638701438904\n",
      "Epoch 32, batch 53 D Loss: 1.4418184757232666, G Loss: 0.7032756209373474\n",
      "Epoch 32, batch 54 D Loss: 1.4339783191680908, G Loss: 0.6995880007743835\n",
      "Epoch 32, batch 55 D Loss: 1.4211565256118774, G Loss: 0.6914929747581482\n",
      "Epoch 32, batch 56 D Loss: 1.423985481262207, G Loss: 0.696208655834198\n",
      "Epoch 32, batch 57 D Loss: 1.416457176208496, G Loss: 0.6984301805496216\n",
      "Epoch 32, batch 58 D Loss: 1.3766831159591675, G Loss: 0.7285298109054565\n",
      "Epoch 32, batch 59 D Loss: 1.4361088275909424, G Loss: 0.6943888664245605\n",
      "Epoch 32, batch 60 D Loss: 1.4293036460876465, G Loss: 0.6821281313896179\n",
      "Epoch 32, batch 61 D Loss: 1.4526221752166748, G Loss: 0.6832749843597412\n",
      "Epoch 32, batch 62 D Loss: 1.446260690689087, G Loss: 0.6955364942550659\n",
      "Epoch 32, batch 63 D Loss: 1.385819673538208, G Loss: 0.6979389786720276\n",
      "Epoch 32, batch 64 D Loss: 1.4176595211029053, G Loss: 0.6966952681541443\n",
      "Epoch 32, batch 65 D Loss: 1.430228590965271, G Loss: 0.6862967014312744\n",
      "Epoch 32, batch 66 D Loss: 1.3862159252166748, G Loss: 0.7153452038764954\n",
      "Epoch 32, batch 67 D Loss: 1.4012415409088135, G Loss: 0.7077701091766357\n",
      "Epoch 32, batch 68 D Loss: 1.3999207019805908, G Loss: 0.7158111333847046\n",
      "Epoch 32, batch 69 D Loss: 1.4079689979553223, G Loss: 0.7081728577613831\n",
      "Epoch 32, batch 70 D Loss: 1.4280354976654053, G Loss: 0.7033224701881409\n",
      "Epoch 32, batch 71 D Loss: 1.4117698669433594, G Loss: 0.7021642923355103\n",
      "Epoch 32, batch 72 D Loss: 1.4095906019210815, G Loss: 0.7129619121551514\n",
      "Epoch 32, batch 73 D Loss: 1.4205951690673828, G Loss: 0.7146726250648499\n",
      "Epoch 32, batch 74 D Loss: 1.4211052656173706, G Loss: 0.710429310798645\n",
      "Epoch 32, batch 75 D Loss: 1.4026631116867065, G Loss: 0.7184011936187744\n",
      "Epoch 32, batch 76 D Loss: 1.4122058153152466, G Loss: 0.7147353887557983\n",
      "Epoch 32, batch 77 D Loss: 1.396622657775879, G Loss: 0.7163737416267395\n",
      "Epoch 32, batch 78 D Loss: 1.3992302417755127, G Loss: 0.7119385600090027\n",
      "Epoch 32, batch 79 D Loss: 1.3918414115905762, G Loss: 0.7280628085136414\n",
      "Epoch 32, batch 80 D Loss: 1.397047758102417, G Loss: 0.7180717587471008\n",
      "Epoch 32, batch 81 D Loss: 1.392344355583191, G Loss: 0.7170361280441284\n",
      "Epoch 32, batch 82 D Loss: 1.3974018096923828, G Loss: 0.722399115562439\n",
      "Epoch 32, batch 83 D Loss: 1.397078514099121, G Loss: 0.7280031442642212\n",
      "Epoch 32, batch 84 D Loss: 1.4034440517425537, G Loss: 0.716386616230011\n",
      "Epoch 32, batch 85 D Loss: 1.3848576545715332, G Loss: 0.7362539768218994\n",
      "Epoch 32, batch 86 D Loss: 1.392770767211914, G Loss: 0.7266185283660889\n",
      "Epoch 32, batch 87 D Loss: 1.3866560459136963, G Loss: 0.736174464225769\n",
      "Epoch 32, batch 88 D Loss: 1.391394019126892, G Loss: 0.7145622372627258\n",
      "Epoch 32, batch 89 D Loss: 1.3950260877609253, G Loss: 0.7395918369293213\n",
      "Epoch 32, batch 90 D Loss: 1.3799866437911987, G Loss: 0.7384620904922485\n",
      "Epoch 32, batch 91 D Loss: 1.4051417112350464, G Loss: 0.7202520966529846\n",
      "Epoch 32, batch 92 D Loss: 1.3742644786834717, G Loss: 0.746101438999176\n",
      "Epoch 32, batch 93 D Loss: 1.3777556419372559, G Loss: 0.7382745146751404\n",
      "Epoch 32, batch 94 D Loss: 1.3672502040863037, G Loss: 0.741008460521698\n",
      "Epoch 32, batch 95 D Loss: 1.3530293703079224, G Loss: 0.7501605749130249\n",
      "Epoch 32, batch 96 D Loss: 1.3943443298339844, G Loss: 0.7389011979103088\n",
      "Epoch 32, batch 97 D Loss: 1.3802731037139893, G Loss: 0.7375869750976562\n",
      "Epoch 32, batch 98 D Loss: 1.4063924551010132, G Loss: 0.7135176658630371\n",
      "Epoch 32, batch 99 D Loss: 1.3648717403411865, G Loss: 0.741433322429657\n",
      "Epoch 32, batch 100 D Loss: 1.4031072854995728, G Loss: 0.700340747833252\n",
      "Epoch 32, batch 101 D Loss: 1.3810997009277344, G Loss: 0.725056529045105\n",
      "Epoch 32, batch 102 D Loss: 1.3666309118270874, G Loss: 0.7379041910171509\n",
      "Epoch 32, batch 103 D Loss: 1.3589394092559814, G Loss: 0.7321922779083252\n",
      "Epoch 32, batch 104 D Loss: 1.3837275505065918, G Loss: 0.7015405893325806\n",
      "Epoch 32, batch 105 D Loss: 1.4018700122833252, G Loss: 0.7059329748153687\n",
      "Epoch 32, batch 106 D Loss: 1.39616060256958, G Loss: 0.7039563655853271\n",
      "Epoch 32, batch 107 D Loss: 1.3958451747894287, G Loss: 0.7033451795578003\n",
      "Epoch 32, batch 108 D Loss: 1.4091649055480957, G Loss: 0.7028315663337708\n",
      "Epoch 32, batch 109 D Loss: 1.3933837413787842, G Loss: 0.708788275718689\n",
      "Epoch 32, batch 110 D Loss: 1.364297866821289, G Loss: 0.7238364219665527\n",
      "Epoch 32, batch 111 D Loss: 1.3755942583084106, G Loss: 0.7115388512611389\n",
      "Epoch 32, batch 112 D Loss: 1.4005417823791504, G Loss: 0.6885356307029724\n",
      "Epoch 32, batch 113 D Loss: 1.3872095346450806, G Loss: 0.7037684917449951\n",
      "Epoch 32, batch 114 D Loss: 1.3851003646850586, G Loss: 0.7117606997489929\n",
      "Epoch 32, batch 115 D Loss: 1.3794059753417969, G Loss: 0.7238145470619202\n",
      "Epoch 32, batch 116 D Loss: 1.39524507522583, G Loss: 0.7023348808288574\n",
      "Epoch 32, batch 117 D Loss: 1.3859200477600098, G Loss: 0.6976639628410339\n",
      "Epoch 32, batch 118 D Loss: 1.3572568893432617, G Loss: 0.7260881662368774\n",
      "Epoch 32, batch 119 D Loss: 1.3934801816940308, G Loss: 0.6996914744377136\n",
      "Epoch 32, batch 120 D Loss: 1.37449312210083, G Loss: 0.6902515888214111\n",
      "Epoch 32, batch 121 D Loss: 1.3771477937698364, G Loss: 0.7183192372322083\n",
      "Epoch 32, batch 122 D Loss: 1.3925902843475342, G Loss: 0.700694739818573\n",
      "Epoch 32, batch 123 D Loss: 1.3681316375732422, G Loss: 0.7079555988311768\n",
      "Epoch 32, batch 124 D Loss: 1.4084205627441406, G Loss: 0.6989012360572815\n",
      "Epoch 32, batch 125 D Loss: 1.4152957201004028, G Loss: 0.6922776103019714\n",
      "Epoch 32, batch 126 D Loss: 1.3698335886001587, G Loss: 0.7072075605392456\n",
      "Epoch 32, batch 127 D Loss: 1.3926843404769897, G Loss: 0.695824384689331\n",
      "Epoch 32, batch 128 D Loss: 1.3879812955856323, G Loss: 0.7058399319648743\n",
      "Epoch 32, batch 129 D Loss: 1.3685171604156494, G Loss: 0.708835244178772\n",
      "Epoch 32, batch 130 D Loss: 1.3787033557891846, G Loss: 0.7076900601387024\n",
      "Epoch 32, batch 131 D Loss: 1.3638105392456055, G Loss: 0.7142555117607117\n",
      "Epoch 32, batch 132 D Loss: 1.4008339643478394, G Loss: 0.7037650942802429\n",
      "Epoch 32, batch 133 D Loss: 1.3772697448730469, G Loss: 0.7025927901268005\n",
      "Epoch 32, batch 134 D Loss: 1.3797191381454468, G Loss: 0.6905816793441772\n",
      "Epoch 32, batch 135 D Loss: 1.37636399269104, G Loss: 0.7005607485771179\n",
      "Epoch 32, batch 136 D Loss: 1.3863399028778076, G Loss: 0.6917124390602112\n",
      "Epoch 32, batch 137 D Loss: 1.3973040580749512, G Loss: 0.7039520740509033\n",
      "Epoch 32, batch 138 D Loss: 1.3752399682998657, G Loss: 0.7134778499603271\n",
      "Epoch 32, batch 139 D Loss: 1.369692087173462, G Loss: 0.7063778638839722\n",
      "Epoch 32, batch 140 D Loss: 1.3571484088897705, G Loss: 0.7447068095207214\n",
      "Epoch 32, batch 141 D Loss: 1.3765463829040527, G Loss: 0.7147762179374695\n",
      "Epoch 32, batch 142 D Loss: 1.3623251914978027, G Loss: 0.727348804473877\n",
      "Epoch 32, batch 143 D Loss: 1.3693828582763672, G Loss: 0.700806200504303\n",
      "Epoch 32, batch 144 D Loss: 1.3804631233215332, G Loss: 0.7098473310470581\n",
      "Epoch 32, batch 145 D Loss: 1.3883109092712402, G Loss: 0.7057555913925171\n",
      "Epoch 32, batch 146 D Loss: 1.3632075786590576, G Loss: 0.731544554233551\n",
      "Epoch 32, batch 147 D Loss: 1.4032092094421387, G Loss: 0.7001067996025085\n",
      "Epoch 32, batch 148 D Loss: 1.3890278339385986, G Loss: 0.705532968044281\n",
      "Epoch 32, batch 149 D Loss: 1.3777704238891602, G Loss: 0.7172653079032898\n",
      "Epoch 32, batch 150 D Loss: 1.3862102031707764, G Loss: 0.7050884366035461\n",
      "Epoch 32, batch 151 D Loss: 1.3967456817626953, G Loss: 0.7068626880645752\n",
      "Epoch 32, batch 152 D Loss: 1.3706672191619873, G Loss: 0.702627420425415\n",
      "Epoch 32, batch 153 D Loss: 1.3810200691223145, G Loss: 0.7038272619247437\n",
      "Epoch 32, batch 154 D Loss: 1.3845373392105103, G Loss: 0.694624662399292\n",
      "Epoch 32, batch 155 D Loss: 1.3818309307098389, G Loss: 0.7063665986061096\n",
      "Epoch 32, batch 156 D Loss: 1.4016907215118408, G Loss: 0.7037621140480042\n",
      "Epoch 32, batch 157 D Loss: 1.3924951553344727, G Loss: 0.6948599219322205\n",
      "Epoch 32, batch 158 D Loss: 1.3537050485610962, G Loss: 0.7013911604881287\n",
      "Epoch 32, batch 159 D Loss: 1.3809337615966797, G Loss: 0.688039243221283\n",
      "Epoch 32, batch 160 D Loss: 1.3794962167739868, G Loss: 0.6913718581199646\n",
      "Epoch 32, batch 161 D Loss: 1.3897393941879272, G Loss: 0.6902937293052673\n",
      "Epoch 32, batch 162 D Loss: 1.3944171667099, G Loss: 0.6768618226051331\n",
      "Epoch 32, batch 163 D Loss: 1.3751599788665771, G Loss: 0.6827134490013123\n",
      "Epoch 32, batch 164 D Loss: 1.4028513431549072, G Loss: 0.673210084438324\n",
      "Epoch 32, batch 165 D Loss: 1.3712102174758911, G Loss: 0.6906111240386963\n",
      "Epoch 32, batch 166 D Loss: 1.3965733051300049, G Loss: 0.6805814504623413\n",
      "Epoch 32, batch 167 D Loss: 1.3793951272964478, G Loss: 0.6879351735115051\n",
      "Epoch 32, batch 168 D Loss: 1.365734338760376, G Loss: 0.691220223903656\n",
      "Epoch 32, batch 169 D Loss: 1.379302740097046, G Loss: 0.6803286671638489\n",
      "Epoch 32, batch 170 D Loss: 1.3717823028564453, G Loss: 0.6954323053359985\n",
      "Epoch 32, batch 171 D Loss: 1.387681484222412, G Loss: 0.6813450455665588\n",
      "Epoch 32, batch 172 D Loss: 1.3596570491790771, G Loss: 0.6945781707763672\n",
      "Epoch 32, batch 173 D Loss: 1.371891975402832, G Loss: 0.6850694417953491\n",
      "Epoch 32, batch 174 D Loss: 1.4011070728302002, G Loss: 0.6727461218833923\n",
      "Epoch 32, batch 175 D Loss: 1.3755592107772827, G Loss: 0.6970261335372925\n",
      "Epoch 32, batch 176 D Loss: 1.397425651550293, G Loss: 0.6732597351074219\n",
      "Epoch 32, batch 177 D Loss: 1.3676576614379883, G Loss: 0.6907795667648315\n",
      "Epoch 32, batch 178 D Loss: 1.3681672811508179, G Loss: 0.6887308359146118\n",
      "Epoch 32, batch 179 D Loss: 1.356886386871338, G Loss: 0.6951763033866882\n",
      "Epoch 32, batch 180 D Loss: 1.3501653671264648, G Loss: 0.6863455176353455\n",
      "Epoch 32, batch 181 D Loss: 1.387939453125, G Loss: 0.672192394733429\n",
      "Epoch 32, batch 182 D Loss: 1.3967790603637695, G Loss: 0.6778404116630554\n",
      "Epoch 32, batch 183 D Loss: 1.3491102457046509, G Loss: 0.6916007399559021\n",
      "Epoch 32, batch 184 D Loss: 1.379078984260559, G Loss: 0.6954547762870789\n",
      "Epoch 32, batch 185 D Loss: 1.3621822595596313, G Loss: 0.6841956973075867\n",
      "Epoch 32, batch 186 D Loss: 1.364516019821167, G Loss: 0.6872934103012085\n",
      "Epoch 32, batch 187 D Loss: 1.3882074356079102, G Loss: 0.6839960217475891\n",
      "Epoch 32, batch 188 D Loss: 1.3717316389083862, G Loss: 0.6821684837341309\n",
      "Epoch 32, batch 189 D Loss: 1.3508154153823853, G Loss: 0.6819767951965332\n",
      "Epoch 32, batch 190 D Loss: 1.3451478481292725, G Loss: 0.6989977955818176\n",
      "Epoch 32, batch 191 D Loss: 1.3600059747695923, G Loss: 0.6905503869056702\n",
      "Epoch 32, batch 192 D Loss: 1.3905175924301147, G Loss: 0.6905545592308044\n",
      "Epoch 32, batch 193 D Loss: 1.3296256065368652, G Loss: 0.7001843452453613\n",
      "Epoch 32, batch 194 D Loss: 1.4104914665222168, G Loss: 0.6763395071029663\n",
      "Epoch 32, batch 195 D Loss: 1.3465603590011597, G Loss: 0.711016058921814\n",
      "Epoch 32, batch 196 D Loss: 1.4276833534240723, G Loss: 0.6524507403373718\n",
      "Epoch 32, batch 197 D Loss: 1.360735297203064, G Loss: 0.7015554904937744\n",
      "Epoch 32, batch 198 D Loss: 1.3515024185180664, G Loss: 0.6852733492851257\n",
      "Epoch 32, batch 199 D Loss: 1.361642837524414, G Loss: 0.6907951831817627\n",
      "Epoch 32, batch 200 D Loss: 1.3785935640335083, G Loss: 0.6769893765449524\n",
      "Epoch 33, batch 1 D Loss: 1.364192008972168, G Loss: 0.6913095712661743\n",
      "Epoch 33, batch 2 D Loss: 1.3213653564453125, G Loss: 0.7178756594657898\n",
      "Epoch 33, batch 3 D Loss: 1.37746262550354, G Loss: 0.6943013668060303\n",
      "Epoch 33, batch 4 D Loss: 1.3857388496398926, G Loss: 0.6973901391029358\n",
      "Epoch 33, batch 5 D Loss: 1.3747766017913818, G Loss: 0.6867992877960205\n",
      "Epoch 33, batch 6 D Loss: 1.389999508857727, G Loss: 0.6879503130912781\n",
      "Epoch 33, batch 7 D Loss: 1.3753807544708252, G Loss: 0.7030950784683228\n",
      "Epoch 33, batch 8 D Loss: 1.3998703956604004, G Loss: 0.6845352649688721\n",
      "Epoch 33, batch 9 D Loss: 1.3578249216079712, G Loss: 0.7029637694358826\n",
      "Epoch 33, batch 10 D Loss: 1.3992271423339844, G Loss: 0.6792146563529968\n",
      "Epoch 33, batch 11 D Loss: 1.3829290866851807, G Loss: 0.7029359936714172\n",
      "Epoch 33, batch 12 D Loss: 1.3748691082000732, G Loss: 0.682558536529541\n",
      "Epoch 33, batch 13 D Loss: 1.4124869108200073, G Loss: 0.6673314571380615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, batch 14 D Loss: 1.4229531288146973, G Loss: 0.6764671206474304\n",
      "Epoch 33, batch 15 D Loss: 1.35402512550354, G Loss: 0.6861212253570557\n",
      "Epoch 33, batch 16 D Loss: 1.3994091749191284, G Loss: 0.6748863458633423\n",
      "Epoch 33, batch 17 D Loss: 1.3960745334625244, G Loss: 0.6793010830879211\n",
      "Epoch 33, batch 18 D Loss: 1.4081792831420898, G Loss: 0.657193660736084\n",
      "Epoch 33, batch 19 D Loss: 1.4284074306488037, G Loss: 0.65465247631073\n",
      "Epoch 33, batch 20 D Loss: 1.381730556488037, G Loss: 0.6744457483291626\n",
      "Epoch 33, batch 21 D Loss: 1.368166208267212, G Loss: 0.6859285831451416\n",
      "Epoch 33, batch 22 D Loss: 1.4298946857452393, G Loss: 0.6678692102432251\n",
      "Epoch 33, batch 23 D Loss: 1.4135768413543701, G Loss: 0.6375040411949158\n",
      "Epoch 33, batch 24 D Loss: 1.3746033906936646, G Loss: 0.6899166703224182\n",
      "Epoch 33, batch 25 D Loss: 1.409529685974121, G Loss: 0.6598656177520752\n",
      "Epoch 33, batch 26 D Loss: 1.4069023132324219, G Loss: 0.6645033359527588\n",
      "Epoch 33, batch 27 D Loss: 1.4049217700958252, G Loss: 0.6561464071273804\n",
      "Epoch 33, batch 28 D Loss: 1.3918285369873047, G Loss: 0.6635538339614868\n",
      "Epoch 33, batch 29 D Loss: 1.3887486457824707, G Loss: 0.6775560975074768\n",
      "Epoch 33, batch 30 D Loss: 1.3960283994674683, G Loss: 0.6772240400314331\n",
      "Epoch 33, batch 31 D Loss: 1.3767025470733643, G Loss: 0.6762782335281372\n",
      "Epoch 33, batch 32 D Loss: 1.3931690454483032, G Loss: 0.6700930595397949\n",
      "Epoch 33, batch 33 D Loss: 1.4201457500457764, G Loss: 0.650452196598053\n",
      "Epoch 33, batch 34 D Loss: 1.3952624797821045, G Loss: 0.6640157103538513\n",
      "Epoch 33, batch 35 D Loss: 1.3997628688812256, G Loss: 0.6578537821769714\n",
      "Epoch 33, batch 36 D Loss: 1.3893994092941284, G Loss: 0.6680185794830322\n",
      "Epoch 33, batch 37 D Loss: 1.392235517501831, G Loss: 0.6731945276260376\n",
      "Epoch 33, batch 38 D Loss: 1.3884449005126953, G Loss: 0.6735512018203735\n",
      "Epoch 33, batch 39 D Loss: 1.3904204368591309, G Loss: 0.6765509247779846\n",
      "Epoch 33, batch 40 D Loss: 1.402270793914795, G Loss: 0.6619811058044434\n",
      "Epoch 33, batch 41 D Loss: 1.3836976289749146, G Loss: 0.6829938292503357\n",
      "Epoch 33, batch 42 D Loss: 1.3990797996520996, G Loss: 0.671241819858551\n",
      "Epoch 33, batch 43 D Loss: 1.4126834869384766, G Loss: 0.6693586707115173\n",
      "Epoch 33, batch 44 D Loss: 1.3631103038787842, G Loss: 0.691821813583374\n",
      "Epoch 33, batch 45 D Loss: 1.3842110633850098, G Loss: 0.6806997656822205\n",
      "Epoch 33, batch 46 D Loss: 1.4194145202636719, G Loss: 0.6623621582984924\n",
      "Epoch 33, batch 47 D Loss: 1.387107253074646, G Loss: 0.6857118010520935\n",
      "Epoch 33, batch 48 D Loss: 1.3842686414718628, G Loss: 0.6864316463470459\n",
      "Epoch 33, batch 49 D Loss: 1.3954966068267822, G Loss: 0.6851670145988464\n",
      "Epoch 33, batch 50 D Loss: 1.3827524185180664, G Loss: 0.7027111649513245\n",
      "Epoch 33, batch 51 D Loss: 1.3788173198699951, G Loss: 0.6946098208427429\n",
      "Epoch 33, batch 52 D Loss: 1.3813531398773193, G Loss: 0.7068999409675598\n",
      "Epoch 33, batch 53 D Loss: 1.3792773485183716, G Loss: 0.6940691471099854\n",
      "Epoch 33, batch 54 D Loss: 1.361720323562622, G Loss: 0.691698431968689\n",
      "Epoch 33, batch 55 D Loss: 1.3623855113983154, G Loss: 0.7127255797386169\n",
      "Epoch 33, batch 56 D Loss: 1.36588716506958, G Loss: 0.7078375816345215\n",
      "Epoch 33, batch 57 D Loss: 1.381706714630127, G Loss: 0.7303445339202881\n",
      "Epoch 33, batch 58 D Loss: 1.4085538387298584, G Loss: 0.6944043636322021\n",
      "Epoch 33, batch 59 D Loss: 1.3378965854644775, G Loss: 0.7338597774505615\n",
      "Epoch 33, batch 60 D Loss: 1.3882532119750977, G Loss: 0.7136404514312744\n",
      "Epoch 33, batch 61 D Loss: 1.402055025100708, G Loss: 0.6959476470947266\n",
      "Epoch 33, batch 62 D Loss: 1.42295241355896, G Loss: 0.6886386275291443\n",
      "Epoch 33, batch 63 D Loss: 1.3861169815063477, G Loss: 0.7010945677757263\n",
      "Epoch 33, batch 64 D Loss: 1.3898348808288574, G Loss: 0.7363267540931702\n",
      "Epoch 33, batch 65 D Loss: 1.4208577871322632, G Loss: 0.7028555870056152\n",
      "Epoch 33, batch 66 D Loss: 1.3771159648895264, G Loss: 0.7349436283111572\n",
      "Epoch 33, batch 67 D Loss: 1.4005355834960938, G Loss: 0.7015934586524963\n",
      "Epoch 33, batch 68 D Loss: 1.3750381469726562, G Loss: 0.7259306907653809\n",
      "Epoch 33, batch 69 D Loss: 1.377968668937683, G Loss: 0.7188529968261719\n",
      "Epoch 33, batch 70 D Loss: 1.430156946182251, G Loss: 0.7057641744613647\n",
      "Epoch 33, batch 71 D Loss: 1.4284393787384033, G Loss: 0.689185619354248\n",
      "Epoch 33, batch 72 D Loss: 1.3882455825805664, G Loss: 0.7252399921417236\n",
      "Epoch 33, batch 73 D Loss: 1.4176697731018066, G Loss: 0.7128053307533264\n",
      "Epoch 33, batch 74 D Loss: 1.3694944381713867, G Loss: 0.717183530330658\n",
      "Epoch 33, batch 75 D Loss: 1.4131999015808105, G Loss: 0.6796852350234985\n",
      "Epoch 33, batch 76 D Loss: 1.4357547760009766, G Loss: 0.7003473043441772\n",
      "Epoch 33, batch 77 D Loss: 1.3975517749786377, G Loss: 0.6982336640357971\n",
      "Epoch 33, batch 78 D Loss: 1.4094257354736328, G Loss: 0.7152053117752075\n",
      "Epoch 33, batch 79 D Loss: 1.4139856100082397, G Loss: 0.6884602904319763\n",
      "Epoch 33, batch 80 D Loss: 1.3747978210449219, G Loss: 0.7108115553855896\n",
      "Epoch 33, batch 81 D Loss: 1.4277089834213257, G Loss: 0.6738475561141968\n",
      "Epoch 33, batch 82 D Loss: 1.3981047868728638, G Loss: 0.692054808139801\n",
      "Epoch 33, batch 83 D Loss: 1.436023235321045, G Loss: 0.678046703338623\n",
      "Epoch 33, batch 84 D Loss: 1.4137499332427979, G Loss: 0.7149903774261475\n",
      "Epoch 33, batch 85 D Loss: 1.402190089225769, G Loss: 0.6972956657409668\n",
      "Epoch 33, batch 86 D Loss: 1.4076813459396362, G Loss: 0.7082011699676514\n",
      "Epoch 33, batch 87 D Loss: 1.3991758823394775, G Loss: 0.728236973285675\n",
      "Epoch 33, batch 88 D Loss: 1.4146835803985596, G Loss: 0.7141731381416321\n",
      "Epoch 33, batch 89 D Loss: 1.401775598526001, G Loss: 0.7102839946746826\n",
      "Epoch 33, batch 90 D Loss: 1.4356422424316406, G Loss: 0.6909027695655823\n",
      "Epoch 33, batch 91 D Loss: 1.3971068859100342, G Loss: 0.7207328677177429\n",
      "Epoch 33, batch 92 D Loss: 1.4043892621994019, G Loss: 0.7202222347259521\n",
      "Epoch 33, batch 93 D Loss: 1.40118408203125, G Loss: 0.7125412225723267\n",
      "Epoch 33, batch 94 D Loss: 1.396983027458191, G Loss: 0.7208459973335266\n",
      "Epoch 33, batch 95 D Loss: 1.3994128704071045, G Loss: 0.7241935729980469\n",
      "Epoch 33, batch 96 D Loss: 1.3866875171661377, G Loss: 0.7236289381980896\n",
      "Epoch 33, batch 97 D Loss: 1.4006606340408325, G Loss: 0.7185987234115601\n",
      "Epoch 33, batch 98 D Loss: 1.3959543704986572, G Loss: 0.7298925518989563\n",
      "Epoch 33, batch 99 D Loss: 1.391059398651123, G Loss: 0.7355419993400574\n",
      "Epoch 33, batch 100 D Loss: 1.361340045928955, G Loss: 0.753673791885376\n",
      "Epoch 33, batch 101 D Loss: 1.3897563219070435, G Loss: 0.7449528574943542\n",
      "Epoch 33, batch 102 D Loss: 1.4090421199798584, G Loss: 0.7277771830558777\n",
      "Epoch 33, batch 103 D Loss: 1.384064793586731, G Loss: 0.7562471628189087\n",
      "Epoch 33, batch 104 D Loss: 1.3884482383728027, G Loss: 0.7466577887535095\n",
      "Epoch 33, batch 105 D Loss: 1.4008934497833252, G Loss: 0.7390634417533875\n",
      "Epoch 33, batch 106 D Loss: 1.3763973712921143, G Loss: 0.7438374161720276\n",
      "Epoch 33, batch 107 D Loss: 1.4024744033813477, G Loss: 0.7384082674980164\n",
      "Epoch 33, batch 108 D Loss: 1.3852256536483765, G Loss: 0.7559422254562378\n",
      "Epoch 33, batch 109 D Loss: 1.3768310546875, G Loss: 0.7522708773612976\n",
      "Epoch 33, batch 110 D Loss: 1.3816373348236084, G Loss: 0.7601673007011414\n",
      "Epoch 33, batch 111 D Loss: 1.3684947490692139, G Loss: 0.7737276554107666\n",
      "Epoch 33, batch 112 D Loss: 1.3755123615264893, G Loss: 0.7650865912437439\n",
      "Epoch 33, batch 113 D Loss: 1.3643256425857544, G Loss: 0.7487708330154419\n",
      "Epoch 33, batch 114 D Loss: 1.3543658256530762, G Loss: 0.767347514629364\n",
      "Epoch 33, batch 115 D Loss: 1.3546411991119385, G Loss: 0.7750484347343445\n",
      "Epoch 33, batch 116 D Loss: 1.3704617023468018, G Loss: 0.764112114906311\n",
      "Epoch 33, batch 117 D Loss: 1.3815884590148926, G Loss: 0.772382915019989\n",
      "Epoch 33, batch 118 D Loss: 1.3648102283477783, G Loss: 0.7767168283462524\n",
      "Epoch 33, batch 119 D Loss: 1.3876709938049316, G Loss: 0.7505082488059998\n",
      "Epoch 33, batch 120 D Loss: 1.3925228118896484, G Loss: 0.7426542639732361\n",
      "Epoch 33, batch 121 D Loss: 1.377536416053772, G Loss: 0.7456179261207581\n",
      "Epoch 33, batch 122 D Loss: 1.3459625244140625, G Loss: 0.772398054599762\n",
      "Epoch 33, batch 123 D Loss: 1.380394697189331, G Loss: 0.7463406324386597\n",
      "Epoch 33, batch 124 D Loss: 1.3984029293060303, G Loss: 0.7387280464172363\n",
      "Epoch 33, batch 125 D Loss: 1.4028513431549072, G Loss: 0.7162420749664307\n",
      "Epoch 33, batch 126 D Loss: 1.4109959602355957, G Loss: 0.7145119309425354\n",
      "Epoch 33, batch 127 D Loss: 1.408766746520996, G Loss: 0.7147552371025085\n",
      "Epoch 33, batch 128 D Loss: 1.3998048305511475, G Loss: 0.7352643013000488\n",
      "Epoch 33, batch 129 D Loss: 1.4143218994140625, G Loss: 0.7266676425933838\n",
      "Epoch 33, batch 130 D Loss: 1.395257592201233, G Loss: 0.7257548570632935\n",
      "Epoch 33, batch 131 D Loss: 1.3720526695251465, G Loss: 0.7259044051170349\n",
      "Epoch 33, batch 132 D Loss: 1.4133374691009521, G Loss: 0.7131842970848083\n",
      "Epoch 33, batch 133 D Loss: 1.4088683128356934, G Loss: 0.7017553448677063\n",
      "Epoch 33, batch 134 D Loss: 1.400132179260254, G Loss: 0.7069717645645142\n",
      "Epoch 33, batch 135 D Loss: 1.39412522315979, G Loss: 0.715248167514801\n",
      "Epoch 33, batch 136 D Loss: 1.3783223628997803, G Loss: 0.7084625363349915\n",
      "Epoch 33, batch 137 D Loss: 1.403693437576294, G Loss: 0.695231556892395\n",
      "Epoch 33, batch 138 D Loss: 1.3888044357299805, G Loss: 0.7222442626953125\n",
      "Epoch 33, batch 139 D Loss: 1.3593227863311768, G Loss: 0.7203491926193237\n",
      "Epoch 33, batch 140 D Loss: 1.3685264587402344, G Loss: 0.7132704257965088\n",
      "Epoch 33, batch 141 D Loss: 1.407418966293335, G Loss: 0.685600996017456\n",
      "Epoch 33, batch 142 D Loss: 1.41074538230896, G Loss: 0.7012286186218262\n",
      "Epoch 33, batch 143 D Loss: 1.4205687046051025, G Loss: 0.6877864599227905\n",
      "Epoch 33, batch 144 D Loss: 1.4159765243530273, G Loss: 0.6848976612091064\n",
      "Epoch 33, batch 145 D Loss: 1.4163384437561035, G Loss: 0.6816615462303162\n",
      "Epoch 33, batch 146 D Loss: 1.446333646774292, G Loss: 0.6839691400527954\n",
      "Epoch 33, batch 147 D Loss: 1.4032344818115234, G Loss: 0.694110095500946\n",
      "Epoch 33, batch 148 D Loss: 1.417090654373169, G Loss: 0.6825976371765137\n",
      "Epoch 33, batch 149 D Loss: 1.3729157447814941, G Loss: 0.6972529888153076\n",
      "Epoch 33, batch 150 D Loss: 1.4112751483917236, G Loss: 0.6768978238105774\n",
      "Epoch 33, batch 151 D Loss: 1.4124974012374878, G Loss: 0.6813700795173645\n",
      "Epoch 33, batch 152 D Loss: 1.388904333114624, G Loss: 0.6885946393013\n",
      "Epoch 33, batch 153 D Loss: 1.3962419033050537, G Loss: 0.6908023357391357\n",
      "Epoch 33, batch 154 D Loss: 1.3967761993408203, G Loss: 0.6849932670593262\n",
      "Epoch 33, batch 155 D Loss: 1.384663701057434, G Loss: 0.6931875348091125\n",
      "Epoch 33, batch 156 D Loss: 1.3773798942565918, G Loss: 0.6981664896011353\n",
      "Epoch 33, batch 157 D Loss: 1.3792109489440918, G Loss: 0.7048510909080505\n",
      "Epoch 33, batch 158 D Loss: 1.3814767599105835, G Loss: 0.6957410573959351\n",
      "Epoch 33, batch 159 D Loss: 1.3899463415145874, G Loss: 0.6937400698661804\n",
      "Epoch 33, batch 160 D Loss: 1.3792048692703247, G Loss: 0.6952655911445618\n",
      "Epoch 33, batch 161 D Loss: 1.3737397193908691, G Loss: 0.7021173238754272\n",
      "Epoch 33, batch 162 D Loss: 1.3747748136520386, G Loss: 0.7009478211402893\n",
      "Epoch 33, batch 163 D Loss: 1.378435730934143, G Loss: 0.699379563331604\n",
      "Epoch 33, batch 164 D Loss: 1.3727993965148926, G Loss: 0.7078801989555359\n",
      "Epoch 33, batch 165 D Loss: 1.3760149478912354, G Loss: 0.6964362263679504\n",
      "Epoch 33, batch 166 D Loss: 1.367950439453125, G Loss: 0.7039459943771362\n",
      "Epoch 33, batch 167 D Loss: 1.362628698348999, G Loss: 0.7168592810630798\n",
      "Epoch 33, batch 168 D Loss: 1.3702495098114014, G Loss: 0.7115240693092346\n",
      "Epoch 33, batch 169 D Loss: 1.3685656785964966, G Loss: 0.7134116888046265\n",
      "Epoch 33, batch 170 D Loss: 1.3722155094146729, G Loss: 0.708916425704956\n",
      "Epoch 33, batch 171 D Loss: 1.3495780229568481, G Loss: 0.7256630659103394\n",
      "Epoch 33, batch 172 D Loss: 1.3587663173675537, G Loss: 0.716174304485321\n",
      "Epoch 33, batch 173 D Loss: 1.3678417205810547, G Loss: 0.7190064191818237\n",
      "Epoch 33, batch 174 D Loss: 1.3461986780166626, G Loss: 0.7326135039329529\n",
      "Epoch 33, batch 175 D Loss: 1.3712555170059204, G Loss: 0.7014619708061218\n",
      "Epoch 33, batch 176 D Loss: 1.3462202548980713, G Loss: 0.7230600714683533\n",
      "Epoch 33, batch 177 D Loss: 1.366403579711914, G Loss: 0.7096033692359924\n",
      "Epoch 33, batch 178 D Loss: 1.3518197536468506, G Loss: 0.7260358929634094\n",
      "Epoch 33, batch 179 D Loss: 1.3373980522155762, G Loss: 0.7121273875236511\n",
      "Epoch 33, batch 180 D Loss: 1.3916923999786377, G Loss: 0.6829628944396973\n",
      "Epoch 33, batch 181 D Loss: 1.3188412189483643, G Loss: 0.7490133047103882\n",
      "Epoch 33, batch 182 D Loss: 1.3286635875701904, G Loss: 0.7312700152397156\n",
      "Epoch 33, batch 183 D Loss: 1.353082537651062, G Loss: 0.7273012399673462\n",
      "Epoch 33, batch 184 D Loss: 1.3288471698760986, G Loss: 0.7358909845352173\n",
      "Epoch 33, batch 185 D Loss: 1.3270678520202637, G Loss: 0.7325448393821716\n",
      "Epoch 33, batch 186 D Loss: 1.3416553735733032, G Loss: 0.735226035118103\n",
      "Epoch 33, batch 187 D Loss: 1.3995718955993652, G Loss: 0.6850554943084717\n",
      "Epoch 33, batch 188 D Loss: 1.3494691848754883, G Loss: 0.7266280651092529\n",
      "Epoch 33, batch 189 D Loss: 1.4047188758850098, G Loss: 0.7158059477806091\n",
      "Epoch 33, batch 190 D Loss: 1.401113510131836, G Loss: 0.6879501342773438\n",
      "Epoch 33, batch 191 D Loss: 1.3985745906829834, G Loss: 0.6889474987983704\n",
      "Epoch 33, batch 192 D Loss: 1.4461266994476318, G Loss: 0.6785221695899963\n",
      "Epoch 33, batch 193 D Loss: 1.4117186069488525, G Loss: 0.6850081086158752\n",
      "Epoch 33, batch 194 D Loss: 1.4271461963653564, G Loss: 0.6849115490913391\n",
      "Epoch 33, batch 195 D Loss: 1.3895564079284668, G Loss: 0.6880182027816772\n",
      "Epoch 33, batch 196 D Loss: 1.362891435623169, G Loss: 0.7048416137695312\n",
      "Epoch 33, batch 197 D Loss: 1.3816112279891968, G Loss: 0.690186619758606\n",
      "Epoch 33, batch 198 D Loss: 1.3821368217468262, G Loss: 0.6742575168609619\n",
      "Epoch 33, batch 199 D Loss: 1.4323697090148926, G Loss: 0.6700668931007385\n",
      "Epoch 33, batch 200 D Loss: 1.3637712001800537, G Loss: 0.7073692083358765\n",
      "Epoch 34, batch 1 D Loss: 1.3969879150390625, G Loss: 0.6665442585945129\n",
      "Epoch 34, batch 2 D Loss: 1.3920702934265137, G Loss: 0.6562693119049072\n",
      "Epoch 34, batch 3 D Loss: 1.3926197290420532, G Loss: 0.6798107028007507\n",
      "Epoch 34, batch 4 D Loss: 1.4615883827209473, G Loss: 0.6654442548751831\n",
      "Epoch 34, batch 5 D Loss: 1.4349342584609985, G Loss: 0.6622259020805359\n",
      "Epoch 34, batch 6 D Loss: 1.3925139904022217, G Loss: 0.6909827589988708\n",
      "Epoch 34, batch 7 D Loss: 1.3788025379180908, G Loss: 0.687690794467926\n",
      "Epoch 34, batch 8 D Loss: 1.3945655822753906, G Loss: 0.6786065697669983\n",
      "Epoch 34, batch 9 D Loss: 1.4231325387954712, G Loss: 0.670769453048706\n",
      "Epoch 34, batch 10 D Loss: 1.3759117126464844, G Loss: 0.6833157539367676\n",
      "Epoch 34, batch 11 D Loss: 1.404448390007019, G Loss: 0.6845318675041199\n",
      "Epoch 34, batch 12 D Loss: 1.4152097702026367, G Loss: 0.6648131012916565\n",
      "Epoch 34, batch 13 D Loss: 1.4329417943954468, G Loss: 0.6650873422622681\n",
      "Epoch 34, batch 14 D Loss: 1.4191880226135254, G Loss: 0.6779100894927979\n",
      "Epoch 34, batch 15 D Loss: 1.3937917947769165, G Loss: 0.6721914410591125\n",
      "Epoch 34, batch 16 D Loss: 1.3996689319610596, G Loss: 0.6787438988685608\n",
      "Epoch 34, batch 17 D Loss: 1.3844062089920044, G Loss: 0.6854362487792969\n",
      "Epoch 34, batch 18 D Loss: 1.4252749681472778, G Loss: 0.6750689744949341\n",
      "Epoch 34, batch 19 D Loss: 1.3940768241882324, G Loss: 0.686923086643219\n",
      "Epoch 34, batch 20 D Loss: 1.4040343761444092, G Loss: 0.6737571954727173\n",
      "Epoch 34, batch 21 D Loss: 1.4064518213272095, G Loss: 0.6749679446220398\n",
      "Epoch 34, batch 22 D Loss: 1.4121122360229492, G Loss: 0.6691579222679138\n",
      "Epoch 34, batch 23 D Loss: 1.3888046741485596, G Loss: 0.6862789392471313\n",
      "Epoch 34, batch 24 D Loss: 1.4028973579406738, G Loss: 0.6947239637374878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, batch 25 D Loss: 1.3709487915039062, G Loss: 0.7002965807914734\n",
      "Epoch 34, batch 26 D Loss: 1.3773068189620972, G Loss: 0.6883493065834045\n",
      "Epoch 34, batch 27 D Loss: 1.3924353122711182, G Loss: 0.6899281144142151\n",
      "Epoch 34, batch 28 D Loss: 1.3822247982025146, G Loss: 0.6905487179756165\n",
      "Epoch 34, batch 29 D Loss: 1.372810959815979, G Loss: 0.7033803462982178\n",
      "Epoch 34, batch 30 D Loss: 1.380908727645874, G Loss: 0.696052074432373\n",
      "Epoch 34, batch 31 D Loss: 1.3716683387756348, G Loss: 0.7110179662704468\n",
      "Epoch 34, batch 32 D Loss: 1.3794653415679932, G Loss: 0.7119115591049194\n",
      "Epoch 34, batch 33 D Loss: 1.3828656673431396, G Loss: 0.6979072093963623\n",
      "Epoch 34, batch 34 D Loss: 1.3669687509536743, G Loss: 0.7089189887046814\n",
      "Epoch 34, batch 35 D Loss: 1.3595271110534668, G Loss: 0.718861997127533\n",
      "Epoch 34, batch 36 D Loss: 1.3712050914764404, G Loss: 0.7263613343238831\n",
      "Epoch 34, batch 37 D Loss: 1.3716113567352295, G Loss: 0.7066397666931152\n",
      "Epoch 34, batch 38 D Loss: 1.3585762977600098, G Loss: 0.7223107218742371\n",
      "Epoch 34, batch 39 D Loss: 1.3768999576568604, G Loss: 0.7042214274406433\n",
      "Epoch 34, batch 40 D Loss: 1.3848307132720947, G Loss: 0.7038302421569824\n",
      "Epoch 34, batch 41 D Loss: 1.376297950744629, G Loss: 0.7174636125564575\n",
      "Epoch 34, batch 42 D Loss: 1.388153314590454, G Loss: 0.6947690844535828\n",
      "Epoch 34, batch 43 D Loss: 1.361687183380127, G Loss: 0.7161181569099426\n",
      "Epoch 34, batch 44 D Loss: 1.338521122932434, G Loss: 0.7287459373474121\n",
      "Epoch 34, batch 45 D Loss: 1.3793174028396606, G Loss: 0.6939060688018799\n",
      "Epoch 34, batch 46 D Loss: 1.397114872932434, G Loss: 0.6896929144859314\n",
      "Epoch 34, batch 47 D Loss: 1.3538463115692139, G Loss: 0.7075039744377136\n",
      "Epoch 34, batch 48 D Loss: 1.3665053844451904, G Loss: 0.7013107538223267\n",
      "Epoch 34, batch 49 D Loss: 1.3969706296920776, G Loss: 0.7003549337387085\n",
      "Epoch 34, batch 50 D Loss: 1.3434351682662964, G Loss: 0.6959447264671326\n",
      "Epoch 34, batch 51 D Loss: 1.3380634784698486, G Loss: 0.7178518772125244\n",
      "Epoch 34, batch 52 D Loss: 1.3358508348464966, G Loss: 0.7176794409751892\n",
      "Epoch 34, batch 53 D Loss: 1.364806890487671, G Loss: 0.7032895088195801\n",
      "Epoch 34, batch 54 D Loss: 1.3590401411056519, G Loss: 0.6998324394226074\n",
      "Epoch 34, batch 55 D Loss: 1.3652784824371338, G Loss: 0.718532383441925\n",
      "Epoch 34, batch 56 D Loss: 1.3561897277832031, G Loss: 0.7040693759918213\n",
      "Epoch 34, batch 57 D Loss: 1.3047621250152588, G Loss: 0.7143784165382385\n",
      "Epoch 34, batch 58 D Loss: 1.3442392349243164, G Loss: 0.7088103294372559\n",
      "Epoch 34, batch 59 D Loss: 1.3595433235168457, G Loss: 0.7145521640777588\n",
      "Epoch 34, batch 60 D Loss: 1.3413608074188232, G Loss: 0.7241705060005188\n",
      "Epoch 34, batch 61 D Loss: 1.363837718963623, G Loss: 0.7048941254615784\n",
      "Epoch 34, batch 62 D Loss: 1.336565613746643, G Loss: 0.71239173412323\n",
      "Epoch 34, batch 63 D Loss: 1.3408238887786865, G Loss: 0.6981813311576843\n",
      "Epoch 34, batch 64 D Loss: 1.3560694456100464, G Loss: 0.6804384589195251\n",
      "Epoch 34, batch 65 D Loss: 1.3379120826721191, G Loss: 0.697716474533081\n",
      "Epoch 34, batch 66 D Loss: 1.3275671005249023, G Loss: 0.6908885836601257\n",
      "Epoch 34, batch 67 D Loss: 1.366114616394043, G Loss: 0.6842901110649109\n",
      "Epoch 34, batch 68 D Loss: 1.3649531602859497, G Loss: 0.6831509470939636\n",
      "Epoch 34, batch 69 D Loss: 1.3991047143936157, G Loss: 0.6610279679298401\n",
      "Epoch 34, batch 70 D Loss: 1.3904967308044434, G Loss: 0.6934601068496704\n",
      "Epoch 34, batch 71 D Loss: 1.4168304204940796, G Loss: 0.6785331964492798\n",
      "Epoch 34, batch 72 D Loss: 1.3927412033081055, G Loss: 0.6694322824478149\n",
      "Epoch 34, batch 73 D Loss: 1.3389532566070557, G Loss: 0.6840107440948486\n",
      "Epoch 34, batch 74 D Loss: 1.3831093311309814, G Loss: 0.6738437414169312\n",
      "Epoch 34, batch 75 D Loss: 1.3450406789779663, G Loss: 0.6781845092773438\n",
      "Epoch 34, batch 76 D Loss: 1.4034595489501953, G Loss: 0.6495805978775024\n",
      "Epoch 34, batch 77 D Loss: 1.3387442827224731, G Loss: 0.6832190752029419\n",
      "Epoch 34, batch 78 D Loss: 1.4095666408538818, G Loss: 0.6543422937393188\n",
      "Epoch 34, batch 79 D Loss: 1.368962287902832, G Loss: 0.6611311435699463\n",
      "Epoch 34, batch 80 D Loss: 1.437296986579895, G Loss: 0.641406774520874\n",
      "Epoch 34, batch 81 D Loss: 1.38810133934021, G Loss: 0.665357768535614\n",
      "Epoch 34, batch 82 D Loss: 1.3668491840362549, G Loss: 0.6681684851646423\n",
      "Epoch 34, batch 83 D Loss: 1.3780279159545898, G Loss: 0.6588817834854126\n",
      "Epoch 34, batch 84 D Loss: 1.4478521347045898, G Loss: 0.6201289296150208\n",
      "Epoch 34, batch 85 D Loss: 1.416057825088501, G Loss: 0.673458993434906\n",
      "Epoch 34, batch 86 D Loss: 1.4204612970352173, G Loss: 0.6483912467956543\n",
      "Epoch 34, batch 87 D Loss: 1.4631986618041992, G Loss: 0.6227654814720154\n",
      "Epoch 34, batch 88 D Loss: 1.4163246154785156, G Loss: 0.6363683342933655\n",
      "Epoch 34, batch 89 D Loss: 1.4427273273468018, G Loss: 0.6338561773300171\n",
      "Epoch 34, batch 90 D Loss: 1.4431452751159668, G Loss: 0.6439561247825623\n",
      "Epoch 34, batch 91 D Loss: 1.419912576675415, G Loss: 0.6417627930641174\n",
      "Epoch 34, batch 92 D Loss: 1.4121322631835938, G Loss: 0.6350083351135254\n",
      "Epoch 34, batch 93 D Loss: 1.4381587505340576, G Loss: 0.6296680569648743\n",
      "Epoch 34, batch 94 D Loss: 1.4377968311309814, G Loss: 0.6288201808929443\n",
      "Epoch 34, batch 95 D Loss: 1.3833551406860352, G Loss: 0.6541236042976379\n",
      "Epoch 34, batch 96 D Loss: 1.4790931940078735, G Loss: 0.6221166253089905\n",
      "Epoch 34, batch 97 D Loss: 1.4234964847564697, G Loss: 0.6334478855133057\n",
      "Epoch 34, batch 98 D Loss: 1.4168825149536133, G Loss: 0.6428879499435425\n",
      "Epoch 34, batch 99 D Loss: 1.4527819156646729, G Loss: 0.6392618417739868\n",
      "Epoch 34, batch 100 D Loss: 1.455507516860962, G Loss: 0.6232678890228271\n",
      "Epoch 34, batch 101 D Loss: 1.4415132999420166, G Loss: 0.6382037997245789\n",
      "Epoch 34, batch 102 D Loss: 1.3805421590805054, G Loss: 0.6685478687286377\n",
      "Epoch 34, batch 103 D Loss: 1.4296026229858398, G Loss: 0.646330714225769\n",
      "Epoch 34, batch 104 D Loss: 1.4020918607711792, G Loss: 0.6592007279396057\n",
      "Epoch 34, batch 105 D Loss: 1.4107999801635742, G Loss: 0.6693768501281738\n",
      "Epoch 34, batch 106 D Loss: 1.4276906251907349, G Loss: 0.6518532037734985\n",
      "Epoch 34, batch 107 D Loss: 1.4265024662017822, G Loss: 0.6469878554344177\n",
      "Epoch 34, batch 108 D Loss: 1.4135373830795288, G Loss: 0.6589154601097107\n",
      "Epoch 34, batch 109 D Loss: 1.388601541519165, G Loss: 0.6740635633468628\n",
      "Epoch 34, batch 110 D Loss: 1.3939306735992432, G Loss: 0.6802927255630493\n",
      "Epoch 34, batch 111 D Loss: 1.3715324401855469, G Loss: 0.6894172430038452\n",
      "Epoch 34, batch 112 D Loss: 1.3940634727478027, G Loss: 0.6786458492279053\n",
      "Epoch 34, batch 113 D Loss: 1.3906668424606323, G Loss: 0.6956077814102173\n",
      "Epoch 34, batch 114 D Loss: 1.3617184162139893, G Loss: 0.7153807282447815\n",
      "Epoch 34, batch 115 D Loss: 1.373197078704834, G Loss: 0.7103186845779419\n",
      "Epoch 34, batch 116 D Loss: 1.3591535091400146, G Loss: 0.7232776880264282\n",
      "Epoch 34, batch 117 D Loss: 1.3650962114334106, G Loss: 0.709774911403656\n",
      "Epoch 34, batch 118 D Loss: 1.3504314422607422, G Loss: 0.7111383080482483\n",
      "Epoch 34, batch 119 D Loss: 1.3187196254730225, G Loss: 0.7391732931137085\n",
      "Epoch 34, batch 120 D Loss: 1.3738410472869873, G Loss: 0.7355379462242126\n",
      "Epoch 34, batch 121 D Loss: 1.3696253299713135, G Loss: 0.713294267654419\n",
      "Epoch 34, batch 122 D Loss: 1.3439080715179443, G Loss: 0.739259660243988\n",
      "Epoch 34, batch 123 D Loss: 1.346078634262085, G Loss: 0.7382726073265076\n",
      "Epoch 34, batch 124 D Loss: 1.3732380867004395, G Loss: 0.735503613948822\n",
      "Epoch 34, batch 125 D Loss: 1.3276824951171875, G Loss: 0.7722781300544739\n",
      "Epoch 34, batch 126 D Loss: 1.4363579750061035, G Loss: 0.7056805491447449\n",
      "Epoch 34, batch 127 D Loss: 1.379506230354309, G Loss: 0.7048808336257935\n",
      "Epoch 34, batch 128 D Loss: 1.2994087934494019, G Loss: 0.8161978125572205\n",
      "Epoch 34, batch 129 D Loss: 1.3357822895050049, G Loss: 0.7797565460205078\n",
      "Epoch 34, batch 130 D Loss: 1.3387278318405151, G Loss: 0.7527037858963013\n",
      "Epoch 34, batch 131 D Loss: 1.3231019973754883, G Loss: 0.7806786298751831\n",
      "Epoch 34, batch 132 D Loss: 1.3500807285308838, G Loss: 0.7773416042327881\n",
      "Epoch 34, batch 133 D Loss: 1.3201769590377808, G Loss: 0.789367139339447\n",
      "Epoch 34, batch 134 D Loss: 1.3928459882736206, G Loss: 0.7817285060882568\n",
      "Epoch 34, batch 135 D Loss: 1.41370689868927, G Loss: 0.7390798926353455\n",
      "Epoch 34, batch 136 D Loss: 1.339510202407837, G Loss: 0.7756797671318054\n",
      "Epoch 34, batch 137 D Loss: 1.4212908744812012, G Loss: 0.7837972044944763\n",
      "Epoch 34, batch 138 D Loss: 1.3301281929016113, G Loss: 0.7822957634925842\n",
      "Epoch 34, batch 139 D Loss: 1.4374418258666992, G Loss: 0.7443845868110657\n",
      "Epoch 34, batch 140 D Loss: 1.3970587253570557, G Loss: 0.8075206875801086\n",
      "Epoch 34, batch 141 D Loss: 1.3659188747406006, G Loss: 0.7969622611999512\n",
      "Epoch 34, batch 142 D Loss: 1.421025037765503, G Loss: 0.7531934976577759\n",
      "Epoch 34, batch 143 D Loss: 1.4064502716064453, G Loss: 0.7654828429222107\n",
      "Epoch 34, batch 144 D Loss: 1.4608838558197021, G Loss: 0.7022111415863037\n",
      "Epoch 34, batch 145 D Loss: 1.4216711521148682, G Loss: 0.7461370229721069\n",
      "Epoch 34, batch 146 D Loss: 1.4432246685028076, G Loss: 0.7356290221214294\n",
      "Epoch 34, batch 147 D Loss: 1.3962894678115845, G Loss: 0.7207097411155701\n",
      "Epoch 34, batch 148 D Loss: 1.449800729751587, G Loss: 0.7087751626968384\n",
      "Epoch 34, batch 149 D Loss: 1.4179387092590332, G Loss: 0.7406375408172607\n",
      "Epoch 34, batch 150 D Loss: 1.438382625579834, G Loss: 0.7220605611801147\n",
      "Epoch 34, batch 151 D Loss: 1.344630479812622, G Loss: 0.7326956987380981\n",
      "Epoch 34, batch 152 D Loss: 1.403311014175415, G Loss: 0.7267834544181824\n",
      "Epoch 34, batch 153 D Loss: 1.4338157176971436, G Loss: 0.7458495497703552\n",
      "Epoch 34, batch 154 D Loss: 1.4517016410827637, G Loss: 0.7069680094718933\n",
      "Epoch 34, batch 155 D Loss: 1.4103646278381348, G Loss: 0.7094444036483765\n",
      "Epoch 34, batch 156 D Loss: 1.4045636653900146, G Loss: 0.7149473428726196\n",
      "Epoch 34, batch 157 D Loss: 1.4184342622756958, G Loss: 0.7208235263824463\n",
      "Epoch 34, batch 158 D Loss: 1.422118902206421, G Loss: 0.7085269689559937\n",
      "Epoch 34, batch 159 D Loss: 1.4643065929412842, G Loss: 0.6888263821601868\n",
      "Epoch 34, batch 160 D Loss: 1.4621931314468384, G Loss: 0.7008917927742004\n",
      "Epoch 34, batch 161 D Loss: 1.421114206314087, G Loss: 0.7367103695869446\n",
      "Epoch 34, batch 162 D Loss: 1.4027539491653442, G Loss: 0.7216273546218872\n",
      "Epoch 34, batch 163 D Loss: 1.3998674154281616, G Loss: 0.7364134788513184\n",
      "Epoch 34, batch 164 D Loss: 1.462660312652588, G Loss: 0.7014868259429932\n",
      "Epoch 34, batch 165 D Loss: 1.4207823276519775, G Loss: 0.7236168384552002\n",
      "Epoch 34, batch 166 D Loss: 1.407850980758667, G Loss: 0.7341461777687073\n",
      "Epoch 34, batch 167 D Loss: 1.410761833190918, G Loss: 0.732733964920044\n",
      "Epoch 34, batch 168 D Loss: 1.4068772792816162, G Loss: 0.7293161749839783\n",
      "Epoch 34, batch 169 D Loss: 1.432765007019043, G Loss: 0.7356393337249756\n",
      "Epoch 34, batch 170 D Loss: 1.4266624450683594, G Loss: 0.7235530614852905\n",
      "Epoch 34, batch 171 D Loss: 1.4037268161773682, G Loss: 0.7444668412208557\n",
      "Epoch 34, batch 172 D Loss: 1.3953713178634644, G Loss: 0.7407898902893066\n",
      "Epoch 34, batch 173 D Loss: 1.4294734001159668, G Loss: 0.7311033606529236\n",
      "Epoch 34, batch 174 D Loss: 1.4367737770080566, G Loss: 0.7248679995536804\n",
      "Epoch 34, batch 175 D Loss: 1.408953070640564, G Loss: 0.7361613512039185\n",
      "Epoch 34, batch 176 D Loss: 1.390580177307129, G Loss: 0.7505495548248291\n",
      "Epoch 34, batch 177 D Loss: 1.3912413120269775, G Loss: 0.7520378232002258\n",
      "Epoch 34, batch 178 D Loss: 1.4082978963851929, G Loss: 0.7484681606292725\n",
      "Epoch 34, batch 179 D Loss: 1.4017441272735596, G Loss: 0.7591989040374756\n",
      "Epoch 34, batch 180 D Loss: 1.381037950515747, G Loss: 0.7723002433776855\n",
      "Epoch 34, batch 181 D Loss: 1.3892101049423218, G Loss: 0.7688949108123779\n",
      "Epoch 34, batch 182 D Loss: 1.3898075819015503, G Loss: 0.7682651281356812\n",
      "Epoch 34, batch 183 D Loss: 1.3949530124664307, G Loss: 0.7726519107818604\n",
      "Epoch 34, batch 184 D Loss: 1.3922498226165771, G Loss: 0.7746467590332031\n",
      "Epoch 34, batch 185 D Loss: 1.3884613513946533, G Loss: 0.7720816731452942\n",
      "Epoch 34, batch 186 D Loss: 1.376810073852539, G Loss: 0.7859692573547363\n",
      "Epoch 34, batch 187 D Loss: 1.3766347169876099, G Loss: 0.7885037064552307\n",
      "Epoch 34, batch 188 D Loss: 1.3837947845458984, G Loss: 0.7859570384025574\n",
      "Epoch 34, batch 189 D Loss: 1.3599152565002441, G Loss: 0.8116979002952576\n",
      "Epoch 34, batch 190 D Loss: 1.3658125400543213, G Loss: 0.7978823184967041\n",
      "Epoch 34, batch 191 D Loss: 1.373482346534729, G Loss: 0.7897412776947021\n",
      "Epoch 34, batch 192 D Loss: 1.3653082847595215, G Loss: 0.8018351197242737\n",
      "Epoch 34, batch 193 D Loss: 1.360377311706543, G Loss: 0.796893835067749\n",
      "Epoch 34, batch 194 D Loss: 1.36016845703125, G Loss: 0.7928422689437866\n",
      "Epoch 34, batch 195 D Loss: 1.3549854755401611, G Loss: 0.8219020962715149\n",
      "Epoch 34, batch 196 D Loss: 1.3589997291564941, G Loss: 0.820517897605896\n",
      "Epoch 34, batch 197 D Loss: 1.3362538814544678, G Loss: 0.835457444190979\n",
      "Epoch 34, batch 198 D Loss: 1.3350722789764404, G Loss: 0.8275089859962463\n",
      "Epoch 34, batch 199 D Loss: 1.3541271686553955, G Loss: 0.8101455569267273\n",
      "Epoch 34, batch 200 D Loss: 1.3472247123718262, G Loss: 0.8090411424636841\n",
      "Epoch 35, batch 1 D Loss: 1.3672138452529907, G Loss: 0.804989755153656\n",
      "Epoch 35, batch 2 D Loss: 1.376446008682251, G Loss: 0.8062558770179749\n",
      "Epoch 35, batch 3 D Loss: 1.3440544605255127, G Loss: 0.793674111366272\n",
      "Epoch 35, batch 4 D Loss: 1.3412749767303467, G Loss: 0.8029587268829346\n",
      "Epoch 35, batch 5 D Loss: 1.377061367034912, G Loss: 0.8103580474853516\n",
      "Epoch 35, batch 6 D Loss: 1.372398853302002, G Loss: 0.8194125890731812\n",
      "Epoch 35, batch 7 D Loss: 1.383650541305542, G Loss: 0.7911229729652405\n",
      "Epoch 35, batch 8 D Loss: 1.32574462890625, G Loss: 0.8232688903808594\n",
      "Epoch 35, batch 9 D Loss: 1.3312723636627197, G Loss: 0.8029786944389343\n",
      "Epoch 35, batch 10 D Loss: 1.3710813522338867, G Loss: 0.7813608050346375\n",
      "Epoch 35, batch 11 D Loss: 1.3095641136169434, G Loss: 0.8187359571456909\n",
      "Epoch 35, batch 12 D Loss: 1.336582899093628, G Loss: 0.7715483903884888\n",
      "Epoch 35, batch 13 D Loss: 1.3493341207504272, G Loss: 0.7704908847808838\n",
      "Epoch 35, batch 14 D Loss: 1.383786678314209, G Loss: 0.7864471673965454\n",
      "Epoch 35, batch 15 D Loss: 1.3691747188568115, G Loss: 0.7606547474861145\n",
      "Epoch 35, batch 16 D Loss: 1.3431293964385986, G Loss: 0.7836391925811768\n",
      "Epoch 35, batch 17 D Loss: 1.3789424896240234, G Loss: 0.7439244985580444\n",
      "Epoch 35, batch 18 D Loss: 1.372927188873291, G Loss: 0.7540742754936218\n",
      "Epoch 35, batch 19 D Loss: 1.3797152042388916, G Loss: 0.7569385766983032\n",
      "Epoch 35, batch 20 D Loss: 1.3912901878356934, G Loss: 0.7349695563316345\n",
      "Epoch 35, batch 21 D Loss: 1.4159092903137207, G Loss: 0.7609947919845581\n",
      "Epoch 35, batch 22 D Loss: 1.390912413597107, G Loss: 0.7279430627822876\n",
      "Epoch 35, batch 23 D Loss: 1.4379643201828003, G Loss: 0.6888688802719116\n",
      "Epoch 35, batch 24 D Loss: 1.3421921730041504, G Loss: 0.7322467565536499\n",
      "Epoch 35, batch 25 D Loss: 1.3643250465393066, G Loss: 0.7170382738113403\n",
      "Epoch 35, batch 26 D Loss: 1.3682212829589844, G Loss: 0.7323426008224487\n",
      "Epoch 35, batch 27 D Loss: 1.4069173336029053, G Loss: 0.7213526368141174\n",
      "Epoch 35, batch 28 D Loss: 1.4318006038665771, G Loss: 0.7199949622154236\n",
      "Epoch 35, batch 29 D Loss: 1.3642299175262451, G Loss: 0.7333681583404541\n",
      "Epoch 35, batch 30 D Loss: 1.380052089691162, G Loss: 0.706422746181488\n",
      "Epoch 35, batch 31 D Loss: 1.3949005603790283, G Loss: 0.7021474242210388\n",
      "Epoch 35, batch 32 D Loss: 1.3556687831878662, G Loss: 0.7342166900634766\n",
      "Epoch 35, batch 33 D Loss: 1.426428198814392, G Loss: 0.6738357543945312\n",
      "Epoch 35, batch 34 D Loss: 1.432952880859375, G Loss: 0.6707141399383545\n",
      "Epoch 35, batch 35 D Loss: 1.388829231262207, G Loss: 0.7044669389724731\n",
      "Epoch 35, batch 36 D Loss: 1.3828715085983276, G Loss: 0.6853725910186768\n",
      "Epoch 35, batch 37 D Loss: 1.4225037097930908, G Loss: 0.6649639010429382\n",
      "Epoch 35, batch 38 D Loss: 1.431153416633606, G Loss: 0.6667996048927307\n",
      "Epoch 35, batch 39 D Loss: 1.469588279724121, G Loss: 0.6479709148406982\n",
      "Epoch 35, batch 40 D Loss: 1.4631811380386353, G Loss: 0.6313666105270386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, batch 41 D Loss: 1.4070415496826172, G Loss: 0.6687937378883362\n",
      "Epoch 35, batch 42 D Loss: 1.4129834175109863, G Loss: 0.6750540733337402\n",
      "Epoch 35, batch 43 D Loss: 1.4133557081222534, G Loss: 0.6487067341804504\n",
      "Epoch 35, batch 44 D Loss: 1.406750202178955, G Loss: 0.6692744493484497\n",
      "Epoch 35, batch 45 D Loss: 1.4017401933670044, G Loss: 0.6545356512069702\n",
      "Epoch 35, batch 46 D Loss: 1.421933650970459, G Loss: 0.6564614772796631\n",
      "Epoch 35, batch 47 D Loss: 1.4159409999847412, G Loss: 0.6460035443305969\n",
      "Epoch 35, batch 48 D Loss: 1.3919415473937988, G Loss: 0.6576874256134033\n",
      "Epoch 35, batch 49 D Loss: 1.3998408317565918, G Loss: 0.6617971658706665\n",
      "Epoch 35, batch 50 D Loss: 1.4057493209838867, G Loss: 0.6605876684188843\n",
      "Epoch 35, batch 51 D Loss: 1.4080836772918701, G Loss: 0.6437074542045593\n",
      "Epoch 35, batch 52 D Loss: 1.4007542133331299, G Loss: 0.653766930103302\n",
      "Epoch 35, batch 53 D Loss: 1.3856617212295532, G Loss: 0.658522367477417\n",
      "Epoch 35, batch 54 D Loss: 1.3804837465286255, G Loss: 0.6484426856040955\n",
      "Epoch 35, batch 55 D Loss: 1.3988733291625977, G Loss: 0.6672016382217407\n",
      "Epoch 35, batch 56 D Loss: 1.4236339330673218, G Loss: 0.6460896134376526\n",
      "Epoch 35, batch 57 D Loss: 1.3957281112670898, G Loss: 0.6538339257240295\n",
      "Epoch 35, batch 58 D Loss: 1.3996429443359375, G Loss: 0.6603236198425293\n",
      "Epoch 35, batch 59 D Loss: 1.3888461589813232, G Loss: 0.663727343082428\n",
      "Epoch 35, batch 60 D Loss: 1.3950426578521729, G Loss: 0.6606334447860718\n",
      "Epoch 35, batch 61 D Loss: 1.413379192352295, G Loss: 0.657000720500946\n",
      "Epoch 35, batch 62 D Loss: 1.4034616947174072, G Loss: 0.6577472686767578\n",
      "Epoch 35, batch 63 D Loss: 1.401841163635254, G Loss: 0.6569854617118835\n",
      "Epoch 35, batch 64 D Loss: 1.38498055934906, G Loss: 0.6670121550559998\n",
      "Epoch 35, batch 65 D Loss: 1.4008933305740356, G Loss: 0.6627002954483032\n",
      "Epoch 35, batch 66 D Loss: 1.3999683856964111, G Loss: 0.6609542369842529\n",
      "Epoch 35, batch 67 D Loss: 1.40413236618042, G Loss: 0.6590388417243958\n",
      "Epoch 35, batch 68 D Loss: 1.4008209705352783, G Loss: 0.6670436859130859\n",
      "Epoch 35, batch 69 D Loss: 1.3753316402435303, G Loss: 0.6741318702697754\n",
      "Epoch 35, batch 70 D Loss: 1.397453784942627, G Loss: 0.6579808592796326\n",
      "Epoch 35, batch 71 D Loss: 1.3921246528625488, G Loss: 0.6698835492134094\n",
      "Epoch 35, batch 72 D Loss: 1.3770830631256104, G Loss: 0.6800455451011658\n",
      "Epoch 35, batch 73 D Loss: 1.3968801498413086, G Loss: 0.670175313949585\n",
      "Epoch 35, batch 74 D Loss: 1.3845704793930054, G Loss: 0.6768205761909485\n",
      "Epoch 35, batch 75 D Loss: 1.3710486888885498, G Loss: 0.6897695660591125\n",
      "Epoch 35, batch 76 D Loss: 1.3902928829193115, G Loss: 0.6798423528671265\n",
      "Epoch 35, batch 77 D Loss: 1.3797305822372437, G Loss: 0.6750187873840332\n",
      "Epoch 35, batch 78 D Loss: 1.3952629566192627, G Loss: 0.6646537184715271\n",
      "Epoch 35, batch 79 D Loss: 1.3821784257888794, G Loss: 0.6757916808128357\n",
      "Epoch 35, batch 80 D Loss: 1.3719778060913086, G Loss: 0.6798317432403564\n",
      "Epoch 35, batch 81 D Loss: 1.3700766563415527, G Loss: 0.6683207154273987\n",
      "Epoch 35, batch 82 D Loss: 1.3731136322021484, G Loss: 0.6791152358055115\n",
      "Epoch 35, batch 83 D Loss: 1.3696653842926025, G Loss: 0.684050440788269\n",
      "Epoch 35, batch 84 D Loss: 1.3540068864822388, G Loss: 0.6928393840789795\n",
      "Epoch 35, batch 85 D Loss: 1.3616325855255127, G Loss: 0.6875522136688232\n",
      "Epoch 35, batch 86 D Loss: 1.3583104610443115, G Loss: 0.677365243434906\n",
      "Epoch 35, batch 87 D Loss: 1.3815464973449707, G Loss: 0.6686368584632874\n",
      "Epoch 35, batch 88 D Loss: 1.3654992580413818, G Loss: 0.669762134552002\n",
      "Epoch 35, batch 89 D Loss: 1.3815304040908813, G Loss: 0.674540102481842\n",
      "Epoch 35, batch 90 D Loss: 1.367147445678711, G Loss: 0.6744049787521362\n",
      "Epoch 35, batch 91 D Loss: 1.3712786436080933, G Loss: 0.6823652386665344\n",
      "Epoch 35, batch 92 D Loss: 1.3666534423828125, G Loss: 0.6681027412414551\n",
      "Epoch 35, batch 93 D Loss: 1.3635194301605225, G Loss: 0.6707241535186768\n",
      "Epoch 35, batch 94 D Loss: 1.378305196762085, G Loss: 0.6619210839271545\n",
      "Epoch 35, batch 95 D Loss: 1.3600962162017822, G Loss: 0.6694096922874451\n",
      "Epoch 35, batch 96 D Loss: 1.345253825187683, G Loss: 0.6732701659202576\n",
      "Epoch 35, batch 97 D Loss: 1.3827893733978271, G Loss: 0.6748325824737549\n",
      "Epoch 35, batch 98 D Loss: 1.3727784156799316, G Loss: 0.6773603558540344\n",
      "Epoch 35, batch 99 D Loss: 1.3697129487991333, G Loss: 0.6855372786521912\n",
      "Epoch 35, batch 100 D Loss: 1.3855388164520264, G Loss: 0.6581214666366577\n",
      "Epoch 35, batch 101 D Loss: 1.3768882751464844, G Loss: 0.668397843837738\n",
      "Epoch 35, batch 102 D Loss: 1.3571643829345703, G Loss: 0.6847020983695984\n",
      "Epoch 35, batch 103 D Loss: 1.3780360221862793, G Loss: 0.6709932088851929\n",
      "Epoch 35, batch 104 D Loss: 1.369390606880188, G Loss: 0.6627082824707031\n",
      "Epoch 35, batch 105 D Loss: 1.372680425643921, G Loss: 0.6639440059661865\n",
      "Epoch 35, batch 106 D Loss: 1.3521754741668701, G Loss: 0.684897780418396\n",
      "Epoch 35, batch 107 D Loss: 1.3619482517242432, G Loss: 0.6600437164306641\n",
      "Epoch 35, batch 108 D Loss: 1.391298532485962, G Loss: 0.6573939323425293\n",
      "Epoch 35, batch 109 D Loss: 1.3785183429718018, G Loss: 0.6634387373924255\n",
      "Epoch 35, batch 110 D Loss: 1.3637564182281494, G Loss: 0.670102596282959\n",
      "Epoch 35, batch 111 D Loss: 1.3939766883850098, G Loss: 0.6594274044036865\n",
      "Epoch 35, batch 112 D Loss: 1.3954694271087646, G Loss: 0.6602643728256226\n",
      "Epoch 35, batch 113 D Loss: 1.367864966392517, G Loss: 0.6715992093086243\n",
      "Epoch 35, batch 114 D Loss: 1.3543496131896973, G Loss: 0.6576337218284607\n",
      "Epoch 35, batch 115 D Loss: 1.356619119644165, G Loss: 0.6652145385742188\n",
      "Epoch 35, batch 116 D Loss: 1.3763293027877808, G Loss: 0.6519744992256165\n",
      "Epoch 35, batch 117 D Loss: 1.3893941640853882, G Loss: 0.6637727618217468\n",
      "Epoch 35, batch 118 D Loss: 1.3869524002075195, G Loss: 0.6709915995597839\n",
      "Epoch 35, batch 119 D Loss: 1.3845453262329102, G Loss: 0.6563395857810974\n",
      "Epoch 35, batch 120 D Loss: 1.3987021446228027, G Loss: 0.6549585461616516\n",
      "Epoch 35, batch 121 D Loss: 1.3862802982330322, G Loss: 0.6483750343322754\n",
      "Epoch 35, batch 122 D Loss: 1.3914597034454346, G Loss: 0.6523216366767883\n",
      "Epoch 35, batch 123 D Loss: 1.3710018396377563, G Loss: 0.6756154894828796\n",
      "Epoch 35, batch 124 D Loss: 1.408907413482666, G Loss: 0.6606498956680298\n",
      "Epoch 35, batch 125 D Loss: 1.3905043601989746, G Loss: 0.6491961479187012\n",
      "Epoch 35, batch 126 D Loss: 1.3780394792556763, G Loss: 0.683502733707428\n",
      "Epoch 35, batch 127 D Loss: 1.4368411302566528, G Loss: 0.6345617771148682\n",
      "Epoch 35, batch 128 D Loss: 1.4163670539855957, G Loss: 0.6533836126327515\n",
      "Epoch 35, batch 129 D Loss: 1.3778440952301025, G Loss: 0.6801505088806152\n",
      "Epoch 35, batch 130 D Loss: 1.3699432611465454, G Loss: 0.6860945224761963\n",
      "Epoch 35, batch 131 D Loss: 1.3972396850585938, G Loss: 0.659287691116333\n",
      "Epoch 35, batch 132 D Loss: 1.4168775081634521, G Loss: 0.6364492774009705\n",
      "Epoch 35, batch 133 D Loss: 1.372075080871582, G Loss: 0.6727068424224854\n",
      "Epoch 35, batch 134 D Loss: 1.423072338104248, G Loss: 0.6655767560005188\n",
      "Epoch 35, batch 135 D Loss: 1.4110296964645386, G Loss: 0.6625576019287109\n",
      "Epoch 35, batch 136 D Loss: 1.416576862335205, G Loss: 0.6825737953186035\n",
      "Epoch 35, batch 137 D Loss: 1.4025685787200928, G Loss: 0.6803442239761353\n",
      "Epoch 35, batch 138 D Loss: 1.4741010665893555, G Loss: 0.6350443363189697\n",
      "Epoch 35, batch 139 D Loss: 1.393751859664917, G Loss: 0.6721597909927368\n",
      "Epoch 35, batch 140 D Loss: 1.4261882305145264, G Loss: 0.6669517755508423\n",
      "Epoch 35, batch 141 D Loss: 1.4088304042816162, G Loss: 0.6792019605636597\n",
      "Epoch 35, batch 142 D Loss: 1.39983069896698, G Loss: 0.6781263947486877\n",
      "Epoch 35, batch 143 D Loss: 1.395941972732544, G Loss: 0.6982407569885254\n",
      "Epoch 35, batch 144 D Loss: 1.4315261840820312, G Loss: 0.6815888285636902\n",
      "Epoch 35, batch 145 D Loss: 1.4204596281051636, G Loss: 0.6796651482582092\n",
      "Epoch 35, batch 146 D Loss: 1.4230427742004395, G Loss: 0.675682544708252\n",
      "Epoch 35, batch 147 D Loss: 1.4015874862670898, G Loss: 0.7042851448059082\n",
      "Epoch 35, batch 148 D Loss: 1.4084751605987549, G Loss: 0.6926451325416565\n",
      "Epoch 35, batch 149 D Loss: 1.409338116645813, G Loss: 0.6836800575256348\n",
      "Epoch 35, batch 150 D Loss: 1.4228324890136719, G Loss: 0.6956231594085693\n",
      "Epoch 35, batch 151 D Loss: 1.3881075382232666, G Loss: 0.706697940826416\n",
      "Epoch 35, batch 152 D Loss: 1.412669062614441, G Loss: 0.7036179304122925\n",
      "Epoch 35, batch 153 D Loss: 1.4108288288116455, G Loss: 0.7065441608428955\n",
      "Epoch 35, batch 154 D Loss: 1.3880820274353027, G Loss: 0.7143760919570923\n",
      "Epoch 35, batch 155 D Loss: 1.4080653190612793, G Loss: 0.7082574367523193\n",
      "Epoch 35, batch 156 D Loss: 1.3954321146011353, G Loss: 0.7177783250808716\n",
      "Epoch 35, batch 157 D Loss: 1.4060561656951904, G Loss: 0.7148243188858032\n",
      "Epoch 35, batch 158 D Loss: 1.3966999053955078, G Loss: 0.7260153293609619\n",
      "Epoch 35, batch 159 D Loss: 1.3967056274414062, G Loss: 0.7285976409912109\n",
      "Epoch 35, batch 160 D Loss: 1.395699143409729, G Loss: 0.7259256839752197\n",
      "Epoch 35, batch 161 D Loss: 1.3942791223526, G Loss: 0.7278339862823486\n",
      "Epoch 35, batch 162 D Loss: 1.3950691223144531, G Loss: 0.7245134711265564\n",
      "Epoch 35, batch 163 D Loss: 1.3885124921798706, G Loss: 0.7345097064971924\n",
      "Epoch 35, batch 164 D Loss: 1.3919920921325684, G Loss: 0.7301670908927917\n",
      "Epoch 35, batch 165 D Loss: 1.383159875869751, G Loss: 0.7434605956077576\n",
      "Epoch 35, batch 166 D Loss: 1.3843441009521484, G Loss: 0.7391912341117859\n",
      "Epoch 35, batch 167 D Loss: 1.3825159072875977, G Loss: 0.750081479549408\n",
      "Epoch 35, batch 168 D Loss: 1.3890814781188965, G Loss: 0.7347618937492371\n",
      "Epoch 35, batch 169 D Loss: 1.3777341842651367, G Loss: 0.7472727298736572\n",
      "Epoch 35, batch 170 D Loss: 1.3828438520431519, G Loss: 0.7411665320396423\n",
      "Epoch 35, batch 171 D Loss: 1.4007623195648193, G Loss: 0.7491182088851929\n",
      "Epoch 35, batch 172 D Loss: 1.3866753578186035, G Loss: 0.7399947643280029\n",
      "Epoch 35, batch 173 D Loss: 1.3941712379455566, G Loss: 0.7336606383323669\n",
      "Epoch 35, batch 174 D Loss: 1.3753538131713867, G Loss: 0.7546440362930298\n",
      "Epoch 35, batch 175 D Loss: 1.3882473707199097, G Loss: 0.739858865737915\n",
      "Epoch 35, batch 176 D Loss: 1.3774876594543457, G Loss: 0.7424352169036865\n",
      "Epoch 35, batch 177 D Loss: 1.3688758611679077, G Loss: 0.7625797986984253\n",
      "Epoch 35, batch 178 D Loss: 1.3712408542633057, G Loss: 0.7582556009292603\n",
      "Epoch 35, batch 179 D Loss: 1.3800246715545654, G Loss: 0.7437226176261902\n",
      "Epoch 35, batch 180 D Loss: 1.3570826053619385, G Loss: 0.7768778800964355\n",
      "Epoch 35, batch 181 D Loss: 1.4033534526824951, G Loss: 0.7501619458198547\n",
      "Epoch 35, batch 182 D Loss: 1.388657569885254, G Loss: 0.7419828176498413\n",
      "Epoch 35, batch 183 D Loss: 1.3383207321166992, G Loss: 0.7963900566101074\n",
      "Epoch 35, batch 184 D Loss: 1.3832968473434448, G Loss: 0.7383343577384949\n",
      "Epoch 35, batch 185 D Loss: 1.367650032043457, G Loss: 0.741232693195343\n",
      "Epoch 35, batch 186 D Loss: 1.3899641036987305, G Loss: 0.7295477390289307\n",
      "Epoch 35, batch 187 D Loss: 1.382996916770935, G Loss: 0.7378700971603394\n",
      "Epoch 35, batch 188 D Loss: 1.3827279806137085, G Loss: 0.7445785403251648\n",
      "Epoch 35, batch 189 D Loss: 1.3783066272735596, G Loss: 0.7436564564704895\n",
      "Epoch 35, batch 190 D Loss: 1.3854042291641235, G Loss: 0.7420780658721924\n",
      "Epoch 35, batch 191 D Loss: 1.3874608278274536, G Loss: 0.7523913383483887\n",
      "Epoch 35, batch 192 D Loss: 1.378336787223816, G Loss: 0.7354512810707092\n",
      "Epoch 35, batch 193 D Loss: 1.3811819553375244, G Loss: 0.725029706954956\n",
      "Epoch 35, batch 194 D Loss: 1.3436343669891357, G Loss: 0.7627034783363342\n",
      "Epoch 35, batch 195 D Loss: 1.3848299980163574, G Loss: 0.7251496911048889\n",
      "Epoch 35, batch 196 D Loss: 1.4156484603881836, G Loss: 0.7065849900245667\n",
      "Epoch 35, batch 197 D Loss: 1.3634628057479858, G Loss: 0.7193361520767212\n",
      "Epoch 35, batch 198 D Loss: 1.3347713947296143, G Loss: 0.7550376653671265\n",
      "Epoch 35, batch 199 D Loss: 1.3707213401794434, G Loss: 0.7066924571990967\n",
      "Epoch 35, batch 200 D Loss: 1.3699616193771362, G Loss: 0.7159331440925598\n",
      "Epoch 36, batch 1 D Loss: 1.3862611055374146, G Loss: 0.7000730633735657\n",
      "Epoch 36, batch 2 D Loss: 1.3777616024017334, G Loss: 0.7288733124732971\n",
      "Epoch 36, batch 3 D Loss: 1.365609884262085, G Loss: 0.7329299449920654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, batch 4 D Loss: 1.3901894092559814, G Loss: 0.7012130618095398\n",
      "Epoch 36, batch 5 D Loss: 1.368180513381958, G Loss: 0.7222858667373657\n",
      "Epoch 36, batch 6 D Loss: 1.396754264831543, G Loss: 0.6973914504051208\n",
      "Epoch 36, batch 7 D Loss: 1.3816200494766235, G Loss: 0.7152047753334045\n",
      "Epoch 36, batch 8 D Loss: 1.3663334846496582, G Loss: 0.7158473134040833\n",
      "Epoch 36, batch 9 D Loss: 1.3739433288574219, G Loss: 0.7131339907646179\n",
      "Epoch 36, batch 10 D Loss: 1.3640038967132568, G Loss: 0.7175363302230835\n",
      "Epoch 36, batch 11 D Loss: 1.4171559810638428, G Loss: 0.6994135975837708\n",
      "Epoch 36, batch 12 D Loss: 1.388014554977417, G Loss: 0.6987907290458679\n",
      "Epoch 36, batch 13 D Loss: 1.3986462354660034, G Loss: 0.6963024735450745\n",
      "Epoch 36, batch 14 D Loss: 1.374076247215271, G Loss: 0.7001849412918091\n",
      "Epoch 36, batch 15 D Loss: 1.3837926387786865, G Loss: 0.6929479837417603\n",
      "Epoch 36, batch 16 D Loss: 1.3944065570831299, G Loss: 0.6795997619628906\n",
      "Epoch 36, batch 17 D Loss: 1.4103556871414185, G Loss: 0.6749169230461121\n",
      "Epoch 36, batch 18 D Loss: 1.378817081451416, G Loss: 0.6947574019432068\n",
      "Epoch 36, batch 19 D Loss: 1.368748426437378, G Loss: 0.6830689311027527\n",
      "Epoch 36, batch 20 D Loss: 1.392690896987915, G Loss: 0.6786447167396545\n",
      "Epoch 36, batch 21 D Loss: 1.3887653350830078, G Loss: 0.6849226355552673\n",
      "Epoch 36, batch 22 D Loss: 1.397061824798584, G Loss: 0.6790099143981934\n",
      "Epoch 36, batch 23 D Loss: 1.3809468746185303, G Loss: 0.6868220567703247\n",
      "Epoch 36, batch 24 D Loss: 1.3910517692565918, G Loss: 0.6737483143806458\n",
      "Epoch 36, batch 25 D Loss: 1.3805415630340576, G Loss: 0.6698859930038452\n",
      "Epoch 36, batch 26 D Loss: 1.3882355690002441, G Loss: 0.654591977596283\n",
      "Epoch 36, batch 27 D Loss: 1.4077253341674805, G Loss: 0.6465554237365723\n",
      "Epoch 36, batch 28 D Loss: 1.373589277267456, G Loss: 0.6795883178710938\n",
      "Epoch 36, batch 29 D Loss: 1.3863587379455566, G Loss: 0.6681306958198547\n",
      "Epoch 36, batch 30 D Loss: 1.3746044635772705, G Loss: 0.6560982465744019\n",
      "Epoch 36, batch 31 D Loss: 1.3646578788757324, G Loss: 0.6698986887931824\n",
      "Epoch 36, batch 32 D Loss: 1.368407964706421, G Loss: 0.678743302822113\n",
      "Epoch 36, batch 33 D Loss: 1.3438403606414795, G Loss: 0.6848547458648682\n",
      "Epoch 36, batch 34 D Loss: 1.3551872968673706, G Loss: 0.6837930083274841\n",
      "Epoch 36, batch 35 D Loss: 1.3905556201934814, G Loss: 0.6614210605621338\n",
      "Epoch 36, batch 36 D Loss: 1.3914533853530884, G Loss: 0.677752673625946\n",
      "Epoch 36, batch 37 D Loss: 1.3742141723632812, G Loss: 0.6721145510673523\n",
      "Epoch 36, batch 38 D Loss: 1.3793660402297974, G Loss: 0.6610990166664124\n",
      "Epoch 36, batch 39 D Loss: 1.3810505867004395, G Loss: 0.6585818529129028\n",
      "Epoch 36, batch 40 D Loss: 1.3684709072113037, G Loss: 0.6719802618026733\n",
      "Epoch 36, batch 41 D Loss: 1.3760902881622314, G Loss: 0.6703289747238159\n",
      "Epoch 36, batch 42 D Loss: 1.3630785942077637, G Loss: 0.6792324185371399\n",
      "Epoch 36, batch 43 D Loss: 1.4091191291809082, G Loss: 0.6527990102767944\n",
      "Epoch 36, batch 44 D Loss: 1.3878122568130493, G Loss: 0.6699342131614685\n",
      "Epoch 36, batch 45 D Loss: 1.3667666912078857, G Loss: 0.6757968664169312\n",
      "Epoch 36, batch 46 D Loss: 1.3830766677856445, G Loss: 0.6536003947257996\n",
      "Epoch 36, batch 47 D Loss: 1.3970065116882324, G Loss: 0.6583421230316162\n",
      "Epoch 36, batch 48 D Loss: 1.3829894065856934, G Loss: 0.666931688785553\n",
      "Epoch 36, batch 49 D Loss: 1.3516429662704468, G Loss: 0.6835344433784485\n",
      "Epoch 36, batch 50 D Loss: 1.411558747291565, G Loss: 0.6543115377426147\n",
      "Epoch 36, batch 51 D Loss: 1.3557438850402832, G Loss: 0.6782288551330566\n",
      "Epoch 36, batch 52 D Loss: 1.4011504650115967, G Loss: 0.6487482190132141\n",
      "Epoch 36, batch 53 D Loss: 1.391646385192871, G Loss: 0.6661323308944702\n",
      "Epoch 36, batch 54 D Loss: 1.3905293941497803, G Loss: 0.6644450426101685\n",
      "Epoch 36, batch 55 D Loss: 1.3495633602142334, G Loss: 0.6735498905181885\n",
      "Epoch 36, batch 56 D Loss: 1.3979657888412476, G Loss: 0.6487061977386475\n",
      "Epoch 36, batch 57 D Loss: 1.3764084577560425, G Loss: 0.6665558815002441\n",
      "Epoch 36, batch 58 D Loss: 1.3726780414581299, G Loss: 0.6700016856193542\n",
      "Epoch 36, batch 59 D Loss: 1.3794000148773193, G Loss: 0.6636444926261902\n",
      "Epoch 36, batch 60 D Loss: 1.4210079908370972, G Loss: 0.6472088694572449\n",
      "Epoch 36, batch 61 D Loss: 1.3758544921875, G Loss: 0.6596289277076721\n",
      "Epoch 36, batch 62 D Loss: 1.4074664115905762, G Loss: 0.6514114141464233\n",
      "Epoch 36, batch 63 D Loss: 1.4010727405548096, G Loss: 0.648393988609314\n",
      "Epoch 36, batch 64 D Loss: 1.4161112308502197, G Loss: 0.6610319018363953\n",
      "Epoch 36, batch 65 D Loss: 1.3993546962738037, G Loss: 0.6583921313285828\n",
      "Epoch 36, batch 66 D Loss: 1.4064743518829346, G Loss: 0.6587899923324585\n",
      "Epoch 36, batch 67 D Loss: 1.3675187826156616, G Loss: 0.6693683862686157\n",
      "Epoch 36, batch 68 D Loss: 1.3929367065429688, G Loss: 0.6576740741729736\n",
      "Epoch 36, batch 69 D Loss: 1.4001493453979492, G Loss: 0.6583058834075928\n",
      "Epoch 36, batch 70 D Loss: 1.390930414199829, G Loss: 0.6597706079483032\n",
      "Epoch 36, batch 71 D Loss: 1.379248857498169, G Loss: 0.6658739447593689\n",
      "Epoch 36, batch 72 D Loss: 1.3618295192718506, G Loss: 0.6740081310272217\n",
      "Epoch 36, batch 73 D Loss: 1.3755035400390625, G Loss: 0.6792827844619751\n",
      "Epoch 36, batch 74 D Loss: 1.3947057723999023, G Loss: 0.6628457903862\n",
      "Epoch 36, batch 75 D Loss: 1.4206570386886597, G Loss: 0.6556881666183472\n",
      "Epoch 36, batch 76 D Loss: 1.4054338932037354, G Loss: 0.6585795879364014\n",
      "Epoch 36, batch 77 D Loss: 1.4035230875015259, G Loss: 0.6561503410339355\n",
      "Epoch 36, batch 78 D Loss: 1.3881301879882812, G Loss: 0.6723299622535706\n",
      "Epoch 36, batch 79 D Loss: 1.4053051471710205, G Loss: 0.6670991778373718\n",
      "Epoch 36, batch 80 D Loss: 1.4038445949554443, G Loss: 0.6712908148765564\n",
      "Epoch 36, batch 81 D Loss: 1.402771234512329, G Loss: 0.6711310744285583\n",
      "Epoch 36, batch 82 D Loss: 1.3862175941467285, G Loss: 0.676520824432373\n",
      "Epoch 36, batch 83 D Loss: 1.4135282039642334, G Loss: 0.6683629751205444\n",
      "Epoch 36, batch 84 D Loss: 1.3945565223693848, G Loss: 0.6765280365943909\n",
      "Epoch 36, batch 85 D Loss: 1.387360692024231, G Loss: 0.6927527785301208\n",
      "Epoch 36, batch 86 D Loss: 1.3939051628112793, G Loss: 0.6905994415283203\n",
      "Epoch 36, batch 87 D Loss: 1.379644513130188, G Loss: 0.6957600116729736\n",
      "Epoch 36, batch 88 D Loss: 1.3747806549072266, G Loss: 0.6934173107147217\n",
      "Epoch 36, batch 89 D Loss: 1.3937115669250488, G Loss: 0.6929422616958618\n",
      "Epoch 36, batch 90 D Loss: 1.3734865188598633, G Loss: 0.7082897424697876\n",
      "Epoch 36, batch 91 D Loss: 1.384044885635376, G Loss: 0.7046269178390503\n",
      "Epoch 36, batch 92 D Loss: 1.380145788192749, G Loss: 0.70400071144104\n",
      "Epoch 36, batch 93 D Loss: 1.386204481124878, G Loss: 0.7079179286956787\n",
      "Epoch 36, batch 94 D Loss: 1.3879015445709229, G Loss: 0.7069224715232849\n",
      "Epoch 36, batch 95 D Loss: 1.3784441947937012, G Loss: 0.7108995318412781\n",
      "Epoch 36, batch 96 D Loss: 1.3658719062805176, G Loss: 0.7072170376777649\n",
      "Epoch 36, batch 97 D Loss: 1.3854972124099731, G Loss: 0.7189756631851196\n",
      "Epoch 36, batch 98 D Loss: 1.3754061460494995, G Loss: 0.7162796854972839\n",
      "Epoch 36, batch 99 D Loss: 1.3684885501861572, G Loss: 0.7209781408309937\n",
      "Epoch 36, batch 100 D Loss: 1.3665556907653809, G Loss: 0.7296473979949951\n",
      "Epoch 36, batch 101 D Loss: 1.3516631126403809, G Loss: 0.7340145707130432\n",
      "Epoch 36, batch 102 D Loss: 1.369805097579956, G Loss: 0.7304181456565857\n",
      "Epoch 36, batch 103 D Loss: 1.3844836950302124, G Loss: 0.7123584151268005\n",
      "Epoch 36, batch 104 D Loss: 1.3569869995117188, G Loss: 0.7276896834373474\n",
      "Epoch 36, batch 105 D Loss: 1.3349508047103882, G Loss: 0.7409572005271912\n",
      "Epoch 36, batch 106 D Loss: 1.3522415161132812, G Loss: 0.7434459924697876\n",
      "Epoch 36, batch 107 D Loss: 1.347704529762268, G Loss: 0.726132869720459\n",
      "Epoch 36, batch 108 D Loss: 1.4036915302276611, G Loss: 0.7101947069168091\n",
      "Epoch 36, batch 109 D Loss: 1.3416388034820557, G Loss: 0.7323826551437378\n",
      "Epoch 36, batch 110 D Loss: 1.3470354080200195, G Loss: 0.745430052280426\n",
      "Epoch 36, batch 111 D Loss: 1.3567640781402588, G Loss: 0.7332432270050049\n",
      "Epoch 36, batch 112 D Loss: 1.3438867330551147, G Loss: 0.7633733153343201\n",
      "Epoch 36, batch 113 D Loss: 1.3511605262756348, G Loss: 0.747688889503479\n",
      "Epoch 36, batch 114 D Loss: 1.3733855485916138, G Loss: 0.7627971768379211\n",
      "Epoch 36, batch 115 D Loss: 1.3817191123962402, G Loss: 0.7354777455329895\n",
      "Epoch 36, batch 116 D Loss: 1.3771111965179443, G Loss: 0.7553460597991943\n",
      "Epoch 36, batch 117 D Loss: 1.3459856510162354, G Loss: 0.7559550404548645\n",
      "Epoch 36, batch 118 D Loss: 1.3559443950653076, G Loss: 0.7782032489776611\n",
      "Epoch 36, batch 119 D Loss: 1.3754451274871826, G Loss: 0.7528365254402161\n",
      "Epoch 36, batch 120 D Loss: 1.328016996383667, G Loss: 0.7598456740379333\n",
      "Epoch 36, batch 121 D Loss: 1.3629677295684814, G Loss: 0.7505354881286621\n",
      "Epoch 36, batch 122 D Loss: 1.3673418760299683, G Loss: 0.7370043396949768\n",
      "Epoch 36, batch 123 D Loss: 1.4151511192321777, G Loss: 0.724456250667572\n",
      "Epoch 36, batch 124 D Loss: 1.3877787590026855, G Loss: 0.7423024773597717\n",
      "Epoch 36, batch 125 D Loss: 1.4010673761367798, G Loss: 0.7227270007133484\n",
      "Epoch 36, batch 126 D Loss: 1.3999497890472412, G Loss: 0.7211716175079346\n",
      "Epoch 36, batch 127 D Loss: 1.4442986249923706, G Loss: 0.7161136865615845\n",
      "Epoch 36, batch 128 D Loss: 1.3849939107894897, G Loss: 0.7153503894805908\n",
      "Epoch 36, batch 129 D Loss: 1.4086494445800781, G Loss: 0.7259977459907532\n",
      "Epoch 36, batch 130 D Loss: 1.4012235403060913, G Loss: 0.6955737471580505\n",
      "Epoch 36, batch 131 D Loss: 1.377062201499939, G Loss: 0.7040776014328003\n",
      "Epoch 36, batch 132 D Loss: 1.3871915340423584, G Loss: 0.7244055271148682\n",
      "Epoch 36, batch 133 D Loss: 1.4178848266601562, G Loss: 0.7233148217201233\n",
      "Epoch 36, batch 134 D Loss: 1.3892898559570312, G Loss: 0.726868212223053\n",
      "Epoch 36, batch 135 D Loss: 1.37174654006958, G Loss: 0.7422667741775513\n",
      "Epoch 36, batch 136 D Loss: 1.4220350980758667, G Loss: 0.6832283139228821\n",
      "Epoch 36, batch 137 D Loss: 1.3721688985824585, G Loss: 0.7339807152748108\n",
      "Epoch 36, batch 138 D Loss: 1.414353847503662, G Loss: 0.7149181365966797\n",
      "Epoch 36, batch 139 D Loss: 1.4134174585342407, G Loss: 0.6924617290496826\n",
      "Epoch 36, batch 140 D Loss: 1.3977720737457275, G Loss: 0.6946248412132263\n",
      "Epoch 36, batch 141 D Loss: 1.3921833038330078, G Loss: 0.7039937376976013\n",
      "Epoch 36, batch 142 D Loss: 1.4477050304412842, G Loss: 0.6872860789299011\n",
      "Epoch 36, batch 143 D Loss: 1.4133708477020264, G Loss: 0.7012282609939575\n",
      "Epoch 36, batch 144 D Loss: 1.4193637371063232, G Loss: 0.7075099945068359\n",
      "Epoch 36, batch 145 D Loss: 1.4214247465133667, G Loss: 0.7016457915306091\n",
      "Epoch 36, batch 146 D Loss: 1.3874380588531494, G Loss: 0.7011122107505798\n",
      "Epoch 36, batch 147 D Loss: 1.4032909870147705, G Loss: 0.6983585953712463\n",
      "Epoch 36, batch 148 D Loss: 1.4083279371261597, G Loss: 0.6842386722564697\n",
      "Epoch 36, batch 149 D Loss: 1.4208861589431763, G Loss: 0.6946872472763062\n",
      "Epoch 36, batch 150 D Loss: 1.4179167747497559, G Loss: 0.691831648349762\n",
      "Epoch 36, batch 151 D Loss: 1.424858808517456, G Loss: 0.6918479800224304\n",
      "Epoch 36, batch 152 D Loss: 1.432021141052246, G Loss: 0.6805760264396667\n",
      "Epoch 36, batch 153 D Loss: 1.4063990116119385, G Loss: 0.7061683535575867\n",
      "Epoch 36, batch 154 D Loss: 1.3998146057128906, G Loss: 0.7019070386886597\n",
      "Epoch 36, batch 155 D Loss: 1.4495668411254883, G Loss: 0.6745185852050781\n",
      "Epoch 36, batch 156 D Loss: 1.4091849327087402, G Loss: 0.7005023956298828\n",
      "Epoch 36, batch 157 D Loss: 1.4275524616241455, G Loss: 0.6943260431289673\n",
      "Epoch 36, batch 158 D Loss: 1.4051690101623535, G Loss: 0.7121850848197937\n",
      "Epoch 36, batch 159 D Loss: 1.4027621746063232, G Loss: 0.7125157117843628\n",
      "Epoch 36, batch 160 D Loss: 1.406637191772461, G Loss: 0.7040020823478699\n",
      "Epoch 36, batch 161 D Loss: 1.3967037200927734, G Loss: 0.7191042304039001\n",
      "Epoch 36, batch 162 D Loss: 1.4278438091278076, G Loss: 0.7052713632583618\n",
      "Epoch 36, batch 163 D Loss: 1.4126616716384888, G Loss: 0.7056826949119568\n",
      "Epoch 36, batch 164 D Loss: 1.4009816646575928, G Loss: 0.7159112691879272\n",
      "Epoch 36, batch 165 D Loss: 1.4133026599884033, G Loss: 0.7180171012878418\n",
      "Epoch 36, batch 166 D Loss: 1.3889517784118652, G Loss: 0.7248058319091797\n",
      "Epoch 36, batch 167 D Loss: 1.4066520929336548, G Loss: 0.7243006825447083\n",
      "Epoch 36, batch 168 D Loss: 1.3998689651489258, G Loss: 0.7273068428039551\n",
      "Epoch 36, batch 169 D Loss: 1.402472972869873, G Loss: 0.7187677025794983\n",
      "Epoch 36, batch 170 D Loss: 1.375328779220581, G Loss: 0.7540294528007507\n",
      "Epoch 36, batch 171 D Loss: 1.4030640125274658, G Loss: 0.7293011546134949\n",
      "Epoch 36, batch 172 D Loss: 1.376168966293335, G Loss: 0.7565590739250183\n",
      "Epoch 36, batch 173 D Loss: 1.383208155632019, G Loss: 0.7319009304046631\n",
      "Epoch 36, batch 174 D Loss: 1.3903968334197998, G Loss: 0.7336747050285339\n",
      "Epoch 36, batch 175 D Loss: 1.3895132541656494, G Loss: 0.7468231320381165\n",
      "Epoch 36, batch 176 D Loss: 1.374861717224121, G Loss: 0.750331699848175\n",
      "Epoch 36, batch 177 D Loss: 1.3772356510162354, G Loss: 0.7574865818023682\n",
      "Epoch 36, batch 178 D Loss: 1.380346417427063, G Loss: 0.7595572471618652\n",
      "Epoch 36, batch 179 D Loss: 1.3869946002960205, G Loss: 0.7615562677383423\n",
      "Epoch 36, batch 180 D Loss: 1.3880915641784668, G Loss: 0.7608017921447754\n",
      "Epoch 36, batch 181 D Loss: 1.3781945705413818, G Loss: 0.77069491147995\n",
      "Epoch 36, batch 182 D Loss: 1.361865758895874, G Loss: 0.7627576589584351\n",
      "Epoch 36, batch 183 D Loss: 1.3797972202301025, G Loss: 0.7710850238800049\n",
      "Epoch 36, batch 184 D Loss: 1.3604342937469482, G Loss: 0.7666668891906738\n",
      "Epoch 36, batch 185 D Loss: 1.3989150524139404, G Loss: 0.7585386633872986\n",
      "Epoch 36, batch 186 D Loss: 1.3626928329467773, G Loss: 0.7707875967025757\n",
      "Epoch 36, batch 187 D Loss: 1.3671574592590332, G Loss: 0.7579060196876526\n",
      "Epoch 36, batch 188 D Loss: 1.3371779918670654, G Loss: 0.8047714829444885\n",
      "Epoch 36, batch 189 D Loss: 1.3963239192962646, G Loss: 0.7600389719009399\n",
      "Epoch 36, batch 190 D Loss: 1.3827552795410156, G Loss: 0.7569916248321533\n",
      "Epoch 36, batch 191 D Loss: 1.3941092491149902, G Loss: 0.7468141317367554\n",
      "Epoch 36, batch 192 D Loss: 1.3655112981796265, G Loss: 0.7401706576347351\n",
      "Epoch 36, batch 193 D Loss: 1.3762009143829346, G Loss: 0.7734706997871399\n",
      "Epoch 36, batch 194 D Loss: 1.3812205791473389, G Loss: 0.763508141040802\n",
      "Epoch 36, batch 195 D Loss: 1.3895686864852905, G Loss: 0.7547286152839661\n",
      "Epoch 36, batch 196 D Loss: 1.3787426948547363, G Loss: 0.7254042029380798\n",
      "Epoch 36, batch 197 D Loss: 1.3698387145996094, G Loss: 0.74757981300354\n",
      "Epoch 36, batch 198 D Loss: 1.3938195705413818, G Loss: 0.7217977046966553\n",
      "Epoch 36, batch 199 D Loss: 1.3509843349456787, G Loss: 0.7697517275810242\n",
      "Epoch 36, batch 200 D Loss: 1.3784451484680176, G Loss: 0.7111427187919617\n",
      "Epoch 37, batch 1 D Loss: 1.4062374830245972, G Loss: 0.7102007865905762\n",
      "Epoch 37, batch 2 D Loss: 1.3675297498703003, G Loss: 0.7345221042633057\n",
      "Epoch 37, batch 3 D Loss: 1.373443603515625, G Loss: 0.7625170350074768\n",
      "Epoch 37, batch 4 D Loss: 1.3595308065414429, G Loss: 0.7409676313400269\n",
      "Epoch 37, batch 5 D Loss: 1.375554084777832, G Loss: 0.7315617203712463\n",
      "Epoch 37, batch 6 D Loss: 1.4030747413635254, G Loss: 0.7214114665985107\n",
      "Epoch 37, batch 7 D Loss: 1.370896577835083, G Loss: 0.7183139324188232\n",
      "Epoch 37, batch 8 D Loss: 1.3730818033218384, G Loss: 0.7089138627052307\n",
      "Epoch 37, batch 9 D Loss: 1.413962483406067, G Loss: 0.7177724242210388\n",
      "Epoch 37, batch 10 D Loss: 1.3642692565917969, G Loss: 0.7013266086578369\n",
      "Epoch 37, batch 11 D Loss: 1.3959128856658936, G Loss: 0.6993531584739685\n",
      "Epoch 37, batch 12 D Loss: 1.379844307899475, G Loss: 0.7045519948005676\n",
      "Epoch 37, batch 13 D Loss: 1.3880687952041626, G Loss: 0.7019489407539368\n",
      "Epoch 37, batch 14 D Loss: 1.4233078956604004, G Loss: 0.7049184441566467\n",
      "Epoch 37, batch 15 D Loss: 1.3953895568847656, G Loss: 0.7017404437065125\n",
      "Epoch 37, batch 16 D Loss: 1.3824385404586792, G Loss: 0.6931703090667725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, batch 17 D Loss: 1.3889853954315186, G Loss: 0.6936672329902649\n",
      "Epoch 37, batch 18 D Loss: 1.3888587951660156, G Loss: 0.6920773983001709\n",
      "Epoch 37, batch 19 D Loss: 1.406099557876587, G Loss: 0.6970353722572327\n",
      "Epoch 37, batch 20 D Loss: 1.380021333694458, G Loss: 0.7167710065841675\n",
      "Epoch 37, batch 21 D Loss: 1.3861660957336426, G Loss: 0.7132821679115295\n",
      "Epoch 37, batch 22 D Loss: 1.4018052816390991, G Loss: 0.6778109073638916\n",
      "Epoch 37, batch 23 D Loss: 1.3841431140899658, G Loss: 0.6873903870582581\n",
      "Epoch 37, batch 24 D Loss: 1.3579812049865723, G Loss: 0.7248214483261108\n",
      "Epoch 37, batch 25 D Loss: 1.3806384801864624, G Loss: 0.6928095817565918\n",
      "Epoch 37, batch 26 D Loss: 1.3805594444274902, G Loss: 0.7015764713287354\n",
      "Epoch 37, batch 27 D Loss: 1.3870397806167603, G Loss: 0.6885040402412415\n",
      "Epoch 37, batch 28 D Loss: 1.4386545419692993, G Loss: 0.686553418636322\n",
      "Epoch 37, batch 29 D Loss: 1.4037673473358154, G Loss: 0.6925621628761292\n",
      "Epoch 37, batch 30 D Loss: 1.4199135303497314, G Loss: 0.6942367553710938\n",
      "Epoch 37, batch 31 D Loss: 1.388024091720581, G Loss: 0.703410804271698\n",
      "Epoch 37, batch 32 D Loss: 1.414227843284607, G Loss: 0.6899923086166382\n",
      "Epoch 37, batch 33 D Loss: 1.3965418338775635, G Loss: 0.6883067488670349\n",
      "Epoch 37, batch 34 D Loss: 1.3864936828613281, G Loss: 0.6999731659889221\n",
      "Epoch 37, batch 35 D Loss: 1.3483192920684814, G Loss: 0.7278010845184326\n",
      "Epoch 37, batch 36 D Loss: 1.4100533723831177, G Loss: 0.6935898661613464\n",
      "Epoch 37, batch 37 D Loss: 1.388343334197998, G Loss: 0.7148866057395935\n",
      "Epoch 37, batch 38 D Loss: 1.4118163585662842, G Loss: 0.6932860612869263\n",
      "Epoch 37, batch 39 D Loss: 1.385798454284668, G Loss: 0.7207000255584717\n",
      "Epoch 37, batch 40 D Loss: 1.4064137935638428, G Loss: 0.6902086734771729\n",
      "Epoch 37, batch 41 D Loss: 1.3972177505493164, G Loss: 0.6967076063156128\n",
      "Epoch 37, batch 42 D Loss: 1.391371726989746, G Loss: 0.7122746109962463\n",
      "Epoch 37, batch 43 D Loss: 1.3890972137451172, G Loss: 0.7128965854644775\n",
      "Epoch 37, batch 44 D Loss: 1.4106481075286865, G Loss: 0.7036632299423218\n",
      "Epoch 37, batch 45 D Loss: 1.3927968740463257, G Loss: 0.7042961120605469\n",
      "Epoch 37, batch 46 D Loss: 1.37858247756958, G Loss: 0.7100785970687866\n",
      "Epoch 37, batch 47 D Loss: 1.3839390277862549, G Loss: 0.7228224277496338\n",
      "Epoch 37, batch 48 D Loss: 1.3571078777313232, G Loss: 0.7213714718818665\n",
      "Epoch 37, batch 49 D Loss: 1.3895962238311768, G Loss: 0.7077906727790833\n",
      "Epoch 37, batch 50 D Loss: 1.3944892883300781, G Loss: 0.6964936852455139\n",
      "Epoch 37, batch 51 D Loss: 1.3726997375488281, G Loss: 0.7095853686332703\n",
      "Epoch 37, batch 52 D Loss: 1.3915438652038574, G Loss: 0.6993207335472107\n",
      "Epoch 37, batch 53 D Loss: 1.369372844696045, G Loss: 0.718101978302002\n",
      "Epoch 37, batch 54 D Loss: 1.40297532081604, G Loss: 0.7086554169654846\n",
      "Epoch 37, batch 55 D Loss: 1.3716448545455933, G Loss: 0.715076208114624\n",
      "Epoch 37, batch 56 D Loss: 1.3999016284942627, G Loss: 0.7143192887306213\n",
      "Epoch 37, batch 57 D Loss: 1.376079797744751, G Loss: 0.7339817881584167\n",
      "Epoch 37, batch 58 D Loss: 1.3914504051208496, G Loss: 0.7112083435058594\n",
      "Epoch 37, batch 59 D Loss: 1.3830115795135498, G Loss: 0.7165586352348328\n",
      "Epoch 37, batch 60 D Loss: 1.3686689138412476, G Loss: 0.7343730926513672\n",
      "Epoch 37, batch 61 D Loss: 1.3650197982788086, G Loss: 0.7386218905448914\n",
      "Epoch 37, batch 62 D Loss: 1.3977224826812744, G Loss: 0.7060855627059937\n",
      "Epoch 37, batch 63 D Loss: 1.4011213779449463, G Loss: 0.7089314460754395\n",
      "Epoch 37, batch 64 D Loss: 1.3930301666259766, G Loss: 0.7184415459632874\n",
      "Epoch 37, batch 65 D Loss: 1.3728868961334229, G Loss: 0.7118200659751892\n",
      "Epoch 37, batch 66 D Loss: 1.4073796272277832, G Loss: 0.7059659361839294\n",
      "Epoch 37, batch 67 D Loss: 1.3769633769989014, G Loss: 0.7019121050834656\n",
      "Epoch 37, batch 68 D Loss: 1.3831422328948975, G Loss: 0.7052578926086426\n",
      "Epoch 37, batch 69 D Loss: 1.4012197256088257, G Loss: 0.7026171684265137\n",
      "Epoch 37, batch 70 D Loss: 1.4001102447509766, G Loss: 0.7012965679168701\n",
      "Epoch 37, batch 71 D Loss: 1.4177398681640625, G Loss: 0.6834585666656494\n",
      "Epoch 37, batch 72 D Loss: 1.390807867050171, G Loss: 0.6839271783828735\n",
      "Epoch 37, batch 73 D Loss: 1.4047527313232422, G Loss: 0.6881500482559204\n",
      "Epoch 37, batch 74 D Loss: 1.4189252853393555, G Loss: 0.6761476993560791\n",
      "Epoch 37, batch 75 D Loss: 1.401557207107544, G Loss: 0.6734004020690918\n",
      "Epoch 37, batch 76 D Loss: 1.394486904144287, G Loss: 0.6939740777015686\n",
      "Epoch 37, batch 77 D Loss: 1.384610652923584, G Loss: 0.6749472618103027\n",
      "Epoch 37, batch 78 D Loss: 1.399486780166626, G Loss: 0.6749825477600098\n",
      "Epoch 37, batch 79 D Loss: 1.3950228691101074, G Loss: 0.6697136163711548\n",
      "Epoch 37, batch 80 D Loss: 1.3923693895339966, G Loss: 0.6655021905899048\n",
      "Epoch 37, batch 81 D Loss: 1.377988338470459, G Loss: 0.6750486493110657\n",
      "Epoch 37, batch 82 D Loss: 1.4045300483703613, G Loss: 0.6639868021011353\n",
      "Epoch 37, batch 83 D Loss: 1.3900651931762695, G Loss: 0.6694812178611755\n",
      "Epoch 37, batch 84 D Loss: 1.3898515701293945, G Loss: 0.6649592518806458\n",
      "Epoch 37, batch 85 D Loss: 1.395735502243042, G Loss: 0.6541647911071777\n",
      "Epoch 37, batch 86 D Loss: 1.3620187044143677, G Loss: 0.6713396310806274\n",
      "Epoch 37, batch 87 D Loss: 1.364778757095337, G Loss: 0.6742085218429565\n",
      "Epoch 37, batch 88 D Loss: 1.3857965469360352, G Loss: 0.6496018767356873\n",
      "Epoch 37, batch 89 D Loss: 1.3670270442962646, G Loss: 0.6837044358253479\n",
      "Epoch 37, batch 90 D Loss: 1.3783600330352783, G Loss: 0.6518913507461548\n",
      "Epoch 37, batch 91 D Loss: 1.3994158506393433, G Loss: 0.6620112061500549\n",
      "Epoch 37, batch 92 D Loss: 1.3956269025802612, G Loss: 0.6518130302429199\n",
      "Epoch 37, batch 93 D Loss: 1.3902924060821533, G Loss: 0.6476603746414185\n",
      "Epoch 37, batch 94 D Loss: 1.3508994579315186, G Loss: 0.6734126806259155\n",
      "Epoch 37, batch 95 D Loss: 1.3822779655456543, G Loss: 0.6490057110786438\n",
      "Epoch 37, batch 96 D Loss: 1.3687479496002197, G Loss: 0.6704943776130676\n",
      "Epoch 37, batch 97 D Loss: 1.3697404861450195, G Loss: 0.6582333445549011\n",
      "Epoch 37, batch 98 D Loss: 1.4011982679367065, G Loss: 0.644880473613739\n",
      "Epoch 37, batch 99 D Loss: 1.3869587182998657, G Loss: 0.6624301075935364\n",
      "Epoch 37, batch 100 D Loss: 1.392512321472168, G Loss: 0.6534184813499451\n",
      "Epoch 37, batch 101 D Loss: 1.3773505687713623, G Loss: 0.6595349311828613\n",
      "Epoch 37, batch 102 D Loss: 1.3593964576721191, G Loss: 0.6828322410583496\n",
      "Epoch 37, batch 103 D Loss: 1.3825469017028809, G Loss: 0.6510807871818542\n",
      "Epoch 37, batch 104 D Loss: 1.3690168857574463, G Loss: 0.6662316918373108\n",
      "Epoch 37, batch 105 D Loss: 1.3723430633544922, G Loss: 0.6581600904464722\n",
      "Epoch 37, batch 106 D Loss: 1.3801100254058838, G Loss: 0.6611396074295044\n",
      "Epoch 37, batch 107 D Loss: 1.359653115272522, G Loss: 0.659080445766449\n",
      "Epoch 37, batch 108 D Loss: 1.3966584205627441, G Loss: 0.6405523419380188\n",
      "Epoch 37, batch 109 D Loss: 1.3677351474761963, G Loss: 0.6532391905784607\n",
      "Epoch 37, batch 110 D Loss: 1.3753409385681152, G Loss: 0.6445649862289429\n",
      "Epoch 37, batch 111 D Loss: 1.3671035766601562, G Loss: 0.6612083911895752\n",
      "Epoch 37, batch 112 D Loss: 1.4130456447601318, G Loss: 0.6290464401245117\n",
      "Epoch 37, batch 113 D Loss: 1.3919398784637451, G Loss: 0.6418418884277344\n",
      "Epoch 37, batch 114 D Loss: 1.397010326385498, G Loss: 0.6426851153373718\n",
      "Epoch 37, batch 115 D Loss: 1.3570619821548462, G Loss: 0.6599628329277039\n",
      "Epoch 37, batch 116 D Loss: 1.4087955951690674, G Loss: 0.630388617515564\n",
      "Epoch 37, batch 117 D Loss: 1.3992787599563599, G Loss: 0.647939920425415\n",
      "Epoch 37, batch 118 D Loss: 1.3536041975021362, G Loss: 0.6620679497718811\n",
      "Epoch 37, batch 119 D Loss: 1.3699240684509277, G Loss: 0.6575582027435303\n",
      "Epoch 37, batch 120 D Loss: 1.3596543073654175, G Loss: 0.6727036833763123\n",
      "Epoch 37, batch 121 D Loss: 1.3654804229736328, G Loss: 0.6471279859542847\n",
      "Epoch 37, batch 122 D Loss: 1.35654616355896, G Loss: 0.6687726378440857\n",
      "Epoch 37, batch 123 D Loss: 1.366664171218872, G Loss: 0.6605017185211182\n",
      "Epoch 37, batch 124 D Loss: 1.392228603363037, G Loss: 0.6406910419464111\n",
      "Epoch 37, batch 125 D Loss: 1.4021482467651367, G Loss: 0.6513198614120483\n",
      "Epoch 37, batch 126 D Loss: 1.3899221420288086, G Loss: 0.6537290215492249\n",
      "Epoch 37, batch 127 D Loss: 1.3705259561538696, G Loss: 0.6462791562080383\n",
      "Epoch 37, batch 128 D Loss: 1.3808403015136719, G Loss: 0.6455281972885132\n",
      "Epoch 37, batch 129 D Loss: 1.3572577238082886, G Loss: 0.6673079133033752\n",
      "Epoch 37, batch 130 D Loss: 1.3636242151260376, G Loss: 0.667792558670044\n",
      "Epoch 37, batch 131 D Loss: 1.3555961847305298, G Loss: 0.6720827221870422\n",
      "Epoch 37, batch 132 D Loss: 1.387662649154663, G Loss: 0.6436826586723328\n",
      "Epoch 37, batch 133 D Loss: 1.3755483627319336, G Loss: 0.6589784026145935\n",
      "Epoch 37, batch 134 D Loss: 1.3662958145141602, G Loss: 0.6650411486625671\n",
      "Epoch 37, batch 135 D Loss: 1.3614962100982666, G Loss: 0.6630046963691711\n",
      "Epoch 37, batch 136 D Loss: 1.353639841079712, G Loss: 0.6770177483558655\n",
      "Epoch 37, batch 137 D Loss: 1.3931384086608887, G Loss: 0.6541926860809326\n",
      "Epoch 37, batch 138 D Loss: 1.347185492515564, G Loss: 0.671733558177948\n",
      "Epoch 37, batch 139 D Loss: 1.3846664428710938, G Loss: 0.6594762206077576\n",
      "Epoch 37, batch 140 D Loss: 1.3876451253890991, G Loss: 0.6664271354675293\n",
      "Epoch 37, batch 141 D Loss: 1.3757954835891724, G Loss: 0.6731429100036621\n",
      "Epoch 37, batch 142 D Loss: 1.372974157333374, G Loss: 0.6663777232170105\n",
      "Epoch 37, batch 143 D Loss: 1.381075143814087, G Loss: 0.6608813405036926\n",
      "Epoch 37, batch 144 D Loss: 1.3692436218261719, G Loss: 0.6739177107810974\n",
      "Epoch 37, batch 145 D Loss: 1.3723244667053223, G Loss: 0.6768242120742798\n",
      "Epoch 37, batch 146 D Loss: 1.4025697708129883, G Loss: 0.6583507061004639\n",
      "Epoch 37, batch 147 D Loss: 1.376699447631836, G Loss: 0.663848876953125\n",
      "Epoch 37, batch 148 D Loss: 1.378801941871643, G Loss: 0.6785886287689209\n",
      "Epoch 37, batch 149 D Loss: 1.3746750354766846, G Loss: 0.6864379048347473\n",
      "Epoch 37, batch 150 D Loss: 1.3567636013031006, G Loss: 0.6768324375152588\n",
      "Epoch 37, batch 151 D Loss: 1.3883084058761597, G Loss: 0.6645298600196838\n",
      "Epoch 37, batch 152 D Loss: 1.3586015701293945, G Loss: 0.6719341278076172\n",
      "Epoch 37, batch 153 D Loss: 1.3753459453582764, G Loss: 0.6821771860122681\n",
      "Epoch 37, batch 154 D Loss: 1.4001473188400269, G Loss: 0.6499779224395752\n",
      "Epoch 37, batch 155 D Loss: 1.3630142211914062, G Loss: 0.6854537725448608\n",
      "Epoch 37, batch 156 D Loss: 1.3839987516403198, G Loss: 0.6876974701881409\n",
      "Epoch 37, batch 157 D Loss: 1.3586140871047974, G Loss: 0.6868926882743835\n",
      "Epoch 37, batch 158 D Loss: 1.350132703781128, G Loss: 0.6912261247634888\n",
      "Epoch 37, batch 159 D Loss: 1.4154640436172485, G Loss: 0.6643801927566528\n",
      "Epoch 37, batch 160 D Loss: 1.4354515075683594, G Loss: 0.6375519037246704\n",
      "Epoch 37, batch 161 D Loss: 1.406125783920288, G Loss: 0.6775357127189636\n",
      "Epoch 37, batch 162 D Loss: 1.3924462795257568, G Loss: 0.6793482303619385\n",
      "Epoch 37, batch 163 D Loss: 1.4792778491973877, G Loss: 0.6469785571098328\n",
      "Epoch 37, batch 164 D Loss: 1.4146711826324463, G Loss: 0.6666911244392395\n",
      "Epoch 37, batch 165 D Loss: 1.4056892395019531, G Loss: 0.6766220331192017\n",
      "Epoch 37, batch 166 D Loss: 1.3971765041351318, G Loss: 0.6949868202209473\n",
      "Epoch 37, batch 167 D Loss: 1.4424724578857422, G Loss: 0.6652752757072449\n",
      "Epoch 37, batch 168 D Loss: 1.4174094200134277, G Loss: 0.6847143769264221\n",
      "Epoch 37, batch 169 D Loss: 1.4412552118301392, G Loss: 0.6611706018447876\n",
      "Epoch 37, batch 170 D Loss: 1.3975715637207031, G Loss: 0.702495276927948\n",
      "Epoch 37, batch 171 D Loss: 1.4221304655075073, G Loss: 0.6850764751434326\n",
      "Epoch 37, batch 172 D Loss: 1.392162561416626, G Loss: 0.7228936553001404\n",
      "Epoch 37, batch 173 D Loss: 1.4055538177490234, G Loss: 0.7188750505447388\n",
      "Epoch 37, batch 174 D Loss: 1.4027783870697021, G Loss: 0.7202818989753723\n",
      "Epoch 37, batch 175 D Loss: 1.426494836807251, G Loss: 0.7001764178276062\n",
      "Epoch 37, batch 176 D Loss: 1.3786965608596802, G Loss: 0.7350221276283264\n",
      "Epoch 37, batch 177 D Loss: 1.4181487560272217, G Loss: 0.7130172848701477\n",
      "Epoch 37, batch 178 D Loss: 1.4061827659606934, G Loss: 0.7284156680107117\n",
      "Epoch 37, batch 179 D Loss: 1.4069342613220215, G Loss: 0.7272214293479919\n",
      "Epoch 37, batch 180 D Loss: 1.4247453212738037, G Loss: 0.7261074781417847\n",
      "Epoch 37, batch 181 D Loss: 1.411015272140503, G Loss: 0.7411287426948547\n",
      "Epoch 37, batch 182 D Loss: 1.3949531316757202, G Loss: 0.747549295425415\n",
      "Epoch 37, batch 183 D Loss: 1.4019145965576172, G Loss: 0.7470481395721436\n",
      "Epoch 37, batch 184 D Loss: 1.4105238914489746, G Loss: 0.7491986751556396\n",
      "Epoch 37, batch 185 D Loss: 1.39919114112854, G Loss: 0.7653642296791077\n",
      "Epoch 37, batch 186 D Loss: 1.372180461883545, G Loss: 0.7787404656410217\n",
      "Epoch 37, batch 187 D Loss: 1.400550127029419, G Loss: 0.7611020803451538\n",
      "Epoch 37, batch 188 D Loss: 1.3854172229766846, G Loss: 0.779238224029541\n",
      "Epoch 37, batch 189 D Loss: 1.401247262954712, G Loss: 0.7752264142036438\n",
      "Epoch 37, batch 190 D Loss: 1.376176118850708, G Loss: 0.7850044369697571\n",
      "Epoch 37, batch 191 D Loss: 1.3792669773101807, G Loss: 0.7962994575500488\n",
      "Epoch 37, batch 192 D Loss: 1.3949322700500488, G Loss: 0.7894685864448547\n",
      "Epoch 37, batch 193 D Loss: 1.3953192234039307, G Loss: 0.7919471263885498\n",
      "Epoch 37, batch 194 D Loss: 1.3713722229003906, G Loss: 0.8093709349632263\n",
      "Epoch 37, batch 195 D Loss: 1.3838101625442505, G Loss: 0.7938440442085266\n",
      "Epoch 37, batch 196 D Loss: 1.3648912906646729, G Loss: 0.7997586727142334\n",
      "Epoch 37, batch 197 D Loss: 1.356323480606079, G Loss: 0.8168577551841736\n",
      "Epoch 37, batch 198 D Loss: 1.370194911956787, G Loss: 0.8051574230194092\n",
      "Epoch 37, batch 199 D Loss: 1.381096601486206, G Loss: 0.8107596635818481\n",
      "Epoch 37, batch 200 D Loss: 1.3802118301391602, G Loss: 0.8070054650306702\n",
      "Epoch 38, batch 1 D Loss: 1.3842664957046509, G Loss: 0.7973398566246033\n",
      "Epoch 38, batch 2 D Loss: 1.3893771171569824, G Loss: 0.8315195441246033\n",
      "Epoch 38, batch 3 D Loss: 1.380530595779419, G Loss: 0.8330941200256348\n",
      "Epoch 38, batch 4 D Loss: 1.3490099906921387, G Loss: 0.829649806022644\n",
      "Epoch 38, batch 5 D Loss: 1.3814902305603027, G Loss: 0.8132019639015198\n",
      "Epoch 38, batch 6 D Loss: 1.3608981370925903, G Loss: 0.8187313675880432\n",
      "Epoch 38, batch 7 D Loss: 1.3890323638916016, G Loss: 0.8214385509490967\n",
      "Epoch 38, batch 8 D Loss: 1.3703421354293823, G Loss: 0.8019679188728333\n",
      "Epoch 38, batch 9 D Loss: 1.3595256805419922, G Loss: 0.8272998929023743\n",
      "Epoch 38, batch 10 D Loss: 1.369583010673523, G Loss: 0.7915708422660828\n",
      "Epoch 38, batch 11 D Loss: 1.4019436836242676, G Loss: 0.7840946316719055\n",
      "Epoch 38, batch 12 D Loss: 1.356595754623413, G Loss: 0.8128313422203064\n",
      "Epoch 38, batch 13 D Loss: 1.3488966226577759, G Loss: 0.7934643030166626\n",
      "Epoch 38, batch 14 D Loss: 1.359158992767334, G Loss: 0.7889194488525391\n",
      "Epoch 38, batch 15 D Loss: 1.3570756912231445, G Loss: 0.8363742232322693\n",
      "Epoch 38, batch 16 D Loss: 1.3959122896194458, G Loss: 0.792400062084198\n",
      "Epoch 38, batch 17 D Loss: 1.3889156579971313, G Loss: 0.761638343334198\n",
      "Epoch 38, batch 18 D Loss: 1.41984224319458, G Loss: 0.7539446949958801\n",
      "Epoch 38, batch 19 D Loss: 1.3378746509552002, G Loss: 0.7893638014793396\n",
      "Epoch 38, batch 20 D Loss: 1.3203811645507812, G Loss: 0.8178351521492004\n",
      "Epoch 38, batch 21 D Loss: 1.4015071392059326, G Loss: 0.7413148283958435\n",
      "Epoch 38, batch 22 D Loss: 1.3632276058197021, G Loss: 0.78184974193573\n",
      "Epoch 38, batch 23 D Loss: 1.39288330078125, G Loss: 0.7564869523048401\n",
      "Epoch 38, batch 24 D Loss: 1.403916835784912, G Loss: 0.7307422161102295\n",
      "Epoch 38, batch 25 D Loss: 1.3775629997253418, G Loss: 0.7819666266441345\n",
      "Epoch 38, batch 26 D Loss: 1.336031198501587, G Loss: 0.76109778881073\n",
      "Epoch 38, batch 27 D Loss: 1.3950421810150146, G Loss: 0.7526872158050537\n",
      "Epoch 38, batch 28 D Loss: 1.4017698764801025, G Loss: 0.7340543866157532\n",
      "Epoch 38, batch 29 D Loss: 1.3939142227172852, G Loss: 0.7289465069770813\n",
      "Epoch 38, batch 30 D Loss: 1.3781626224517822, G Loss: 0.729813814163208\n",
      "Epoch 38, batch 31 D Loss: 1.3866732120513916, G Loss: 0.7294924259185791\n",
      "Epoch 38, batch 32 D Loss: 1.3921988010406494, G Loss: 0.7147772312164307\n",
      "Epoch 38, batch 33 D Loss: 1.445183277130127, G Loss: 0.6948010325431824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, batch 34 D Loss: 1.3811286687850952, G Loss: 0.7294142246246338\n",
      "Epoch 38, batch 35 D Loss: 1.3643255233764648, G Loss: 0.7393345832824707\n",
      "Epoch 38, batch 36 D Loss: 1.393968105316162, G Loss: 0.704223096370697\n",
      "Epoch 38, batch 37 D Loss: 1.4011015892028809, G Loss: 0.7099153995513916\n",
      "Epoch 38, batch 38 D Loss: 1.399886131286621, G Loss: 0.6927791833877563\n",
      "Epoch 38, batch 39 D Loss: 1.4075655937194824, G Loss: 0.7062934637069702\n",
      "Epoch 38, batch 40 D Loss: 1.4157220125198364, G Loss: 0.6867945790290833\n",
      "Epoch 38, batch 41 D Loss: 1.3719770908355713, G Loss: 0.7088636159896851\n",
      "Epoch 38, batch 42 D Loss: 1.4067959785461426, G Loss: 0.6878246068954468\n",
      "Epoch 38, batch 43 D Loss: 1.4051064252853394, G Loss: 0.673496425151825\n",
      "Epoch 38, batch 44 D Loss: 1.4160813093185425, G Loss: 0.6805320978164673\n",
      "Epoch 38, batch 45 D Loss: 1.377150535583496, G Loss: 0.6960744261741638\n",
      "Epoch 38, batch 46 D Loss: 1.3816866874694824, G Loss: 0.6933419108390808\n",
      "Epoch 38, batch 47 D Loss: 1.4154847860336304, G Loss: 0.6727554798126221\n",
      "Epoch 38, batch 48 D Loss: 1.410866141319275, G Loss: 0.6683530211448669\n",
      "Epoch 38, batch 49 D Loss: 1.4133100509643555, G Loss: 0.6782901883125305\n",
      "Epoch 38, batch 50 D Loss: 1.4294995069503784, G Loss: 0.6531407237052917\n",
      "Epoch 38, batch 51 D Loss: 1.3896675109863281, G Loss: 0.6907534003257751\n",
      "Epoch 38, batch 52 D Loss: 1.407662272453308, G Loss: 0.6735951900482178\n",
      "Epoch 38, batch 53 D Loss: 1.4077634811401367, G Loss: 0.6808624863624573\n",
      "Epoch 38, batch 54 D Loss: 1.4179953336715698, G Loss: 0.6623254418373108\n",
      "Epoch 38, batch 55 D Loss: 1.4228100776672363, G Loss: 0.6607409119606018\n",
      "Epoch 38, batch 56 D Loss: 1.4188563823699951, G Loss: 0.6668540835380554\n",
      "Epoch 38, batch 57 D Loss: 1.4288196563720703, G Loss: 0.6669712662696838\n",
      "Epoch 38, batch 58 D Loss: 1.4007232189178467, G Loss: 0.6785637736320496\n",
      "Epoch 38, batch 59 D Loss: 1.3947703838348389, G Loss: 0.682437539100647\n",
      "Epoch 38, batch 60 D Loss: 1.3934636116027832, G Loss: 0.6737085580825806\n",
      "Epoch 38, batch 61 D Loss: 1.3935174942016602, G Loss: 0.6788941025733948\n",
      "Epoch 38, batch 62 D Loss: 1.3959157466888428, G Loss: 0.6730627417564392\n",
      "Epoch 38, batch 63 D Loss: 1.400583267211914, G Loss: 0.6762020587921143\n",
      "Epoch 38, batch 64 D Loss: 1.4088817834854126, G Loss: 0.6735464334487915\n",
      "Epoch 38, batch 65 D Loss: 1.4090179204940796, G Loss: 0.6677858233451843\n",
      "Epoch 38, batch 66 D Loss: 1.3998668193817139, G Loss: 0.6749817728996277\n",
      "Epoch 38, batch 67 D Loss: 1.397588849067688, G Loss: 0.6758013963699341\n",
      "Epoch 38, batch 68 D Loss: 1.4009774923324585, G Loss: 0.6778761148452759\n",
      "Epoch 38, batch 69 D Loss: 1.399067997932434, G Loss: 0.6783624887466431\n",
      "Epoch 38, batch 70 D Loss: 1.3930705785751343, G Loss: 0.6755012273788452\n",
      "Epoch 38, batch 71 D Loss: 1.389634370803833, G Loss: 0.6831727623939514\n",
      "Epoch 38, batch 72 D Loss: 1.39109206199646, G Loss: 0.6773849725723267\n",
      "Epoch 38, batch 73 D Loss: 1.3877620697021484, G Loss: 0.6844921708106995\n",
      "Epoch 38, batch 74 D Loss: 1.3871119022369385, G Loss: 0.6823296546936035\n",
      "Epoch 38, batch 75 D Loss: 1.3897509574890137, G Loss: 0.6794472336769104\n",
      "Epoch 38, batch 76 D Loss: 1.3831920623779297, G Loss: 0.6860939264297485\n",
      "Epoch 38, batch 77 D Loss: 1.381642460823059, G Loss: 0.6825343370437622\n",
      "Epoch 38, batch 78 D Loss: 1.3781678676605225, G Loss: 0.6875998973846436\n",
      "Epoch 38, batch 79 D Loss: 1.3788979053497314, G Loss: 0.6869030594825745\n",
      "Epoch 38, batch 80 D Loss: 1.3770527839660645, G Loss: 0.6883600354194641\n",
      "Epoch 38, batch 81 D Loss: 1.3789026737213135, G Loss: 0.6803169846534729\n",
      "Epoch 38, batch 82 D Loss: 1.3762261867523193, G Loss: 0.6913516521453857\n",
      "Epoch 38, batch 83 D Loss: 1.3627721071243286, G Loss: 0.6962092518806458\n",
      "Epoch 38, batch 84 D Loss: 1.3701121807098389, G Loss: 0.6931801438331604\n",
      "Epoch 38, batch 85 D Loss: 1.3737256526947021, G Loss: 0.6872610449790955\n",
      "Epoch 38, batch 86 D Loss: 1.3814349174499512, G Loss: 0.6862571239471436\n",
      "Epoch 38, batch 87 D Loss: 1.377493977546692, G Loss: 0.6838287115097046\n",
      "Epoch 38, batch 88 D Loss: 1.3646273612976074, G Loss: 0.6954893469810486\n",
      "Epoch 38, batch 89 D Loss: 1.3594590425491333, G Loss: 0.6981408596038818\n",
      "Epoch 38, batch 90 D Loss: 1.364202857017517, G Loss: 0.687390923500061\n",
      "Epoch 38, batch 91 D Loss: 1.3740642070770264, G Loss: 0.6908782720565796\n",
      "Epoch 38, batch 92 D Loss: 1.3742179870605469, G Loss: 0.6949948668479919\n",
      "Epoch 38, batch 93 D Loss: 1.355978012084961, G Loss: 0.6885797381401062\n",
      "Epoch 38, batch 94 D Loss: 1.3622722625732422, G Loss: 0.6876322031021118\n",
      "Epoch 38, batch 95 D Loss: 1.3656561374664307, G Loss: 0.6879996657371521\n",
      "Epoch 38, batch 96 D Loss: 1.3792996406555176, G Loss: 0.6810980439186096\n",
      "Epoch 38, batch 97 D Loss: 1.3540856838226318, G Loss: 0.7037114500999451\n",
      "Epoch 38, batch 98 D Loss: 1.3822357654571533, G Loss: 0.6917219758033752\n",
      "Epoch 38, batch 99 D Loss: 1.3714569807052612, G Loss: 0.678191602230072\n",
      "Epoch 38, batch 100 D Loss: 1.3636021614074707, G Loss: 0.6818863749504089\n",
      "Epoch 38, batch 101 D Loss: 1.3591258525848389, G Loss: 0.6890831589698792\n",
      "Epoch 38, batch 102 D Loss: 1.368882417678833, G Loss: 0.6890028119087219\n",
      "Epoch 38, batch 103 D Loss: 1.3607563972473145, G Loss: 0.6956499218940735\n",
      "Epoch 38, batch 104 D Loss: 1.343454122543335, G Loss: 0.6964985728263855\n",
      "Epoch 38, batch 105 D Loss: 1.3628168106079102, G Loss: 0.7116958498954773\n",
      "Epoch 38, batch 106 D Loss: 1.354917287826538, G Loss: 0.6993480920791626\n",
      "Epoch 38, batch 107 D Loss: 1.3700828552246094, G Loss: 0.6866384744644165\n",
      "Epoch 38, batch 108 D Loss: 1.3638442754745483, G Loss: 0.6874662041664124\n",
      "Epoch 38, batch 109 D Loss: 1.3724569082260132, G Loss: 0.681093692779541\n",
      "Epoch 38, batch 110 D Loss: 1.384972333908081, G Loss: 0.6679041981697083\n",
      "Epoch 38, batch 111 D Loss: 1.3654348850250244, G Loss: 0.6852385997772217\n",
      "Epoch 38, batch 112 D Loss: 1.3784029483795166, G Loss: 0.6803608536720276\n",
      "Epoch 38, batch 113 D Loss: 1.3806017637252808, G Loss: 0.6791411638259888\n",
      "Epoch 38, batch 114 D Loss: 1.3983638286590576, G Loss: 0.6623519062995911\n",
      "Epoch 38, batch 115 D Loss: 1.3962750434875488, G Loss: 0.6698077917098999\n",
      "Epoch 38, batch 116 D Loss: 1.4000056982040405, G Loss: 0.6529624462127686\n",
      "Epoch 38, batch 117 D Loss: 1.3602941036224365, G Loss: 0.6944639086723328\n",
      "Epoch 38, batch 118 D Loss: 1.3541960716247559, G Loss: 0.699298083782196\n",
      "Epoch 38, batch 119 D Loss: 1.3740938901901245, G Loss: 0.6863515377044678\n",
      "Epoch 38, batch 120 D Loss: 1.3711342811584473, G Loss: 0.6833691596984863\n",
      "Epoch 38, batch 121 D Loss: 1.3792459964752197, G Loss: 0.6823316216468811\n",
      "Epoch 38, batch 122 D Loss: 1.354201316833496, G Loss: 0.70628821849823\n",
      "Epoch 38, batch 123 D Loss: 1.3582249879837036, G Loss: 0.7061484456062317\n",
      "Epoch 38, batch 124 D Loss: 1.4241544008255005, G Loss: 0.6648878455162048\n",
      "Epoch 38, batch 125 D Loss: 1.3510656356811523, G Loss: 0.6964833736419678\n",
      "Epoch 38, batch 126 D Loss: 1.3864941596984863, G Loss: 0.6881868243217468\n",
      "Epoch 38, batch 127 D Loss: 1.3576343059539795, G Loss: 0.686306357383728\n",
      "Epoch 38, batch 128 D Loss: 1.445939302444458, G Loss: 0.6564282774925232\n",
      "Epoch 38, batch 129 D Loss: 1.377476692199707, G Loss: 0.6798328161239624\n",
      "Epoch 38, batch 130 D Loss: 1.4355463981628418, G Loss: 0.6636248826980591\n",
      "Epoch 38, batch 131 D Loss: 1.3956044912338257, G Loss: 0.6699052453041077\n",
      "Epoch 38, batch 132 D Loss: 1.4432010650634766, G Loss: 0.6486860513687134\n",
      "Epoch 38, batch 133 D Loss: 1.394706130027771, G Loss: 0.6879432797431946\n",
      "Epoch 38, batch 134 D Loss: 1.4023823738098145, G Loss: 0.6739171743392944\n",
      "Epoch 38, batch 135 D Loss: 1.3941292762756348, G Loss: 0.6826587915420532\n",
      "Epoch 38, batch 136 D Loss: 1.3797199726104736, G Loss: 0.6876175403594971\n",
      "Epoch 38, batch 137 D Loss: 1.449167251586914, G Loss: 0.662333071231842\n",
      "Epoch 38, batch 138 D Loss: 1.3606488704681396, G Loss: 0.7043366432189941\n",
      "Epoch 38, batch 139 D Loss: 1.412100076675415, G Loss: 0.6822292804718018\n",
      "Epoch 38, batch 140 D Loss: 1.4132435321807861, G Loss: 0.6539373993873596\n",
      "Epoch 38, batch 141 D Loss: 1.404278039932251, G Loss: 0.6806234121322632\n",
      "Epoch 38, batch 142 D Loss: 1.3782382011413574, G Loss: 0.6893919110298157\n",
      "Epoch 38, batch 143 D Loss: 1.4264836311340332, G Loss: 0.6841491460800171\n",
      "Epoch 38, batch 144 D Loss: 1.4332892894744873, G Loss: 0.6599489450454712\n",
      "Epoch 38, batch 145 D Loss: 1.4087539911270142, G Loss: 0.6760025024414062\n",
      "Epoch 38, batch 146 D Loss: 1.4224200248718262, G Loss: 0.6570658683776855\n",
      "Epoch 38, batch 147 D Loss: 1.4123529195785522, G Loss: 0.6736193895339966\n",
      "Epoch 38, batch 148 D Loss: 1.3959333896636963, G Loss: 0.6889779567718506\n",
      "Epoch 38, batch 149 D Loss: 1.398962378501892, G Loss: 0.6962266564369202\n",
      "Epoch 38, batch 150 D Loss: 1.4189479351043701, G Loss: 0.6837223172187805\n",
      "Epoch 38, batch 151 D Loss: 1.4255332946777344, G Loss: 0.6726917028427124\n",
      "Epoch 38, batch 152 D Loss: 1.3860135078430176, G Loss: 0.700507402420044\n",
      "Epoch 38, batch 153 D Loss: 1.396177053451538, G Loss: 0.6928834319114685\n",
      "Epoch 38, batch 154 D Loss: 1.4128118753433228, G Loss: 0.6882520318031311\n",
      "Epoch 38, batch 155 D Loss: 1.3964226245880127, G Loss: 0.6969314813613892\n",
      "Epoch 38, batch 156 D Loss: 1.3799171447753906, G Loss: 0.7034236788749695\n",
      "Epoch 38, batch 157 D Loss: 1.4114038944244385, G Loss: 0.6876657009124756\n",
      "Epoch 38, batch 158 D Loss: 1.4070758819580078, G Loss: 0.6968896985054016\n",
      "Epoch 38, batch 159 D Loss: 1.3878405094146729, G Loss: 0.7107636332511902\n",
      "Epoch 38, batch 160 D Loss: 1.4052611589431763, G Loss: 0.7018169164657593\n",
      "Epoch 38, batch 161 D Loss: 1.393791675567627, G Loss: 0.7084600925445557\n",
      "Epoch 38, batch 162 D Loss: 1.395836353302002, G Loss: 0.7055088877677917\n",
      "Epoch 38, batch 163 D Loss: 1.384209156036377, G Loss: 0.7160655856132507\n",
      "Epoch 38, batch 164 D Loss: 1.3946359157562256, G Loss: 0.7156493663787842\n",
      "Epoch 38, batch 165 D Loss: 1.3935883045196533, G Loss: 0.7209335565567017\n",
      "Epoch 38, batch 166 D Loss: 1.390638828277588, G Loss: 0.7192440629005432\n",
      "Epoch 38, batch 167 D Loss: 1.3974826335906982, G Loss: 0.728026807308197\n",
      "Epoch 38, batch 168 D Loss: 1.3638875484466553, G Loss: 0.7356786131858826\n",
      "Epoch 38, batch 169 D Loss: 1.3801648616790771, G Loss: 0.7284306883811951\n",
      "Epoch 38, batch 170 D Loss: 1.378770351409912, G Loss: 0.7339199185371399\n",
      "Epoch 38, batch 171 D Loss: 1.3804289102554321, G Loss: 0.7520241737365723\n",
      "Epoch 38, batch 172 D Loss: 1.3908360004425049, G Loss: 0.7283451557159424\n",
      "Epoch 38, batch 173 D Loss: 1.3703367710113525, G Loss: 0.7366828918457031\n",
      "Epoch 38, batch 174 D Loss: 1.3783349990844727, G Loss: 0.7395175695419312\n",
      "Epoch 38, batch 175 D Loss: 1.3912699222564697, G Loss: 0.7359760403633118\n",
      "Epoch 38, batch 176 D Loss: 1.3893241882324219, G Loss: 0.7347385287284851\n",
      "Epoch 38, batch 177 D Loss: 1.38447904586792, G Loss: 0.7393679022789001\n",
      "Epoch 38, batch 178 D Loss: 1.3821953535079956, G Loss: 0.7307012677192688\n",
      "Epoch 38, batch 179 D Loss: 1.3743674755096436, G Loss: 0.7318117022514343\n",
      "Epoch 38, batch 180 D Loss: 1.3815796375274658, G Loss: 0.7366328239440918\n",
      "Epoch 38, batch 181 D Loss: 1.3713966608047485, G Loss: 0.7428659200668335\n",
      "Epoch 38, batch 182 D Loss: 1.4038183689117432, G Loss: 0.7341265082359314\n",
      "Epoch 38, batch 183 D Loss: 1.3673228025436401, G Loss: 0.7400912642478943\n",
      "Epoch 38, batch 184 D Loss: 1.3924638032913208, G Loss: 0.7308236956596375\n",
      "Epoch 38, batch 185 D Loss: 1.388073205947876, G Loss: 0.723171055316925\n",
      "Epoch 38, batch 186 D Loss: 1.3791835308074951, G Loss: 0.7259027361869812\n",
      "Epoch 38, batch 187 D Loss: 1.3644955158233643, G Loss: 0.7494741678237915\n",
      "Epoch 38, batch 188 D Loss: 1.400437593460083, G Loss: 0.7138870358467102\n",
      "Epoch 38, batch 189 D Loss: 1.3911333084106445, G Loss: 0.7215576767921448\n",
      "Epoch 38, batch 190 D Loss: 1.3869155645370483, G Loss: 0.7348341941833496\n",
      "Epoch 38, batch 191 D Loss: 1.3715827465057373, G Loss: 0.728290855884552\n",
      "Epoch 38, batch 192 D Loss: 1.389986276626587, G Loss: 0.7266550660133362\n",
      "Epoch 38, batch 193 D Loss: 1.3719782829284668, G Loss: 0.7204811573028564\n",
      "Epoch 38, batch 194 D Loss: 1.371978759765625, G Loss: 0.7269543409347534\n",
      "Epoch 38, batch 195 D Loss: 1.3854848146438599, G Loss: 0.7244439125061035\n",
      "Epoch 38, batch 196 D Loss: 1.3735382556915283, G Loss: 0.723755955696106\n",
      "Epoch 38, batch 197 D Loss: 1.3929388523101807, G Loss: 0.7027367353439331\n",
      "Epoch 38, batch 198 D Loss: 1.388868808746338, G Loss: 0.7179140448570251\n",
      "Epoch 38, batch 199 D Loss: 1.3809161186218262, G Loss: 0.7134237885475159\n",
      "Epoch 38, batch 200 D Loss: 1.3704798221588135, G Loss: 0.7154905796051025\n",
      "Epoch 39, batch 1 D Loss: 1.3913581371307373, G Loss: 0.7103399634361267\n",
      "Epoch 39, batch 2 D Loss: 1.3827221393585205, G Loss: 0.717518150806427\n",
      "Epoch 39, batch 3 D Loss: 1.3600965738296509, G Loss: 0.7390074133872986\n",
      "Epoch 39, batch 4 D Loss: 1.381007432937622, G Loss: 0.723041296005249\n",
      "Epoch 39, batch 5 D Loss: 1.4080032110214233, G Loss: 0.6906512975692749\n",
      "Epoch 39, batch 6 D Loss: 1.388253927230835, G Loss: 0.7129848003387451\n",
      "Epoch 39, batch 7 D Loss: 1.3962724208831787, G Loss: 0.6968158483505249\n",
      "Epoch 39, batch 8 D Loss: 1.401702642440796, G Loss: 0.7106876373291016\n",
      "Epoch 39, batch 9 D Loss: 1.3920691013336182, G Loss: 0.6980087161064148\n",
      "Epoch 39, batch 10 D Loss: 1.3919850587844849, G Loss: 0.7004467248916626\n",
      "Epoch 39, batch 11 D Loss: 1.3785090446472168, G Loss: 0.7070852518081665\n",
      "Epoch 39, batch 12 D Loss: 1.3973770141601562, G Loss: 0.6974784135818481\n",
      "Epoch 39, batch 13 D Loss: 1.3865509033203125, G Loss: 0.6945480108261108\n",
      "Epoch 39, batch 14 D Loss: 1.4071087837219238, G Loss: 0.6924716830253601\n",
      "Epoch 39, batch 15 D Loss: 1.3734811544418335, G Loss: 0.7019667029380798\n",
      "Epoch 39, batch 16 D Loss: 1.4311940670013428, G Loss: 0.6739643216133118\n",
      "Epoch 39, batch 17 D Loss: 1.3823570013046265, G Loss: 0.6870275139808655\n",
      "Epoch 39, batch 18 D Loss: 1.392255187034607, G Loss: 0.6977861523628235\n",
      "Epoch 39, batch 19 D Loss: 1.3731755018234253, G Loss: 0.7008833885192871\n",
      "Epoch 39, batch 20 D Loss: 1.4043810367584229, G Loss: 0.6864692568778992\n",
      "Epoch 39, batch 21 D Loss: 1.399104118347168, G Loss: 0.6820448040962219\n",
      "Epoch 39, batch 22 D Loss: 1.3869032859802246, G Loss: 0.687747597694397\n",
      "Epoch 39, batch 23 D Loss: 1.410602331161499, G Loss: 0.6853545308113098\n",
      "Epoch 39, batch 24 D Loss: 1.396662950515747, G Loss: 0.6842055320739746\n",
      "Epoch 39, batch 25 D Loss: 1.3901678323745728, G Loss: 0.6831559538841248\n",
      "Epoch 39, batch 26 D Loss: 1.4016915559768677, G Loss: 0.6879574656486511\n",
      "Epoch 39, batch 27 D Loss: 1.402294635772705, G Loss: 0.6729600429534912\n",
      "Epoch 39, batch 28 D Loss: 1.367624282836914, G Loss: 0.6905192732810974\n",
      "Epoch 39, batch 29 D Loss: 1.3751306533813477, G Loss: 0.6850227117538452\n",
      "Epoch 39, batch 30 D Loss: 1.393912672996521, G Loss: 0.6773374080657959\n",
      "Epoch 39, batch 31 D Loss: 1.3889999389648438, G Loss: 0.6843788623809814\n",
      "Epoch 39, batch 32 D Loss: 1.3915472030639648, G Loss: 0.6907978057861328\n",
      "Epoch 39, batch 33 D Loss: 1.3876824378967285, G Loss: 0.6778497099876404\n",
      "Epoch 39, batch 34 D Loss: 1.3803532123565674, G Loss: 0.6872732639312744\n",
      "Epoch 39, batch 35 D Loss: 1.399255394935608, G Loss: 0.6773914098739624\n",
      "Epoch 39, batch 36 D Loss: 1.380051612854004, G Loss: 0.6746095418930054\n",
      "Epoch 39, batch 37 D Loss: 1.3774816989898682, G Loss: 0.6781728267669678\n",
      "Epoch 39, batch 38 D Loss: 1.387299656867981, G Loss: 0.6738401651382446\n",
      "Epoch 39, batch 39 D Loss: 1.3725966215133667, G Loss: 0.6802505254745483\n",
      "Epoch 39, batch 40 D Loss: 1.383814811706543, G Loss: 0.6799032688140869\n",
      "Epoch 39, batch 41 D Loss: 1.391682744026184, G Loss: 0.6844530701637268\n",
      "Epoch 39, batch 42 D Loss: 1.3840885162353516, G Loss: 0.6840267181396484\n",
      "Epoch 39, batch 43 D Loss: 1.3622442483901978, G Loss: 0.6847364902496338\n",
      "Epoch 39, batch 44 D Loss: 1.3772056102752686, G Loss: 0.6730877161026001\n",
      "Epoch 39, batch 45 D Loss: 1.3794538974761963, G Loss: 0.682443380355835\n",
      "Epoch 39, batch 46 D Loss: 1.389678716659546, G Loss: 0.6808761358261108\n",
      "Epoch 39, batch 47 D Loss: 1.385209560394287, G Loss: 0.6765414476394653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, batch 48 D Loss: 1.372252106666565, G Loss: 0.6828583478927612\n",
      "Epoch 39, batch 49 D Loss: 1.3804386854171753, G Loss: 0.6761413812637329\n",
      "Epoch 39, batch 50 D Loss: 1.3753244876861572, G Loss: 0.6809683442115784\n",
      "Epoch 39, batch 51 D Loss: 1.3824150562286377, G Loss: 0.672607421875\n",
      "Epoch 39, batch 52 D Loss: 1.3752622604370117, G Loss: 0.6763880252838135\n",
      "Epoch 39, batch 53 D Loss: 1.3737847805023193, G Loss: 0.6752417087554932\n",
      "Epoch 39, batch 54 D Loss: 1.3720518350601196, G Loss: 0.6845201849937439\n",
      "Epoch 39, batch 55 D Loss: 1.366868257522583, G Loss: 0.6807672381401062\n",
      "Epoch 39, batch 56 D Loss: 1.3676670789718628, G Loss: 0.6846648454666138\n",
      "Epoch 39, batch 57 D Loss: 1.387468934059143, G Loss: 0.6731447577476501\n",
      "Epoch 39, batch 58 D Loss: 1.3784152269363403, G Loss: 0.677164614200592\n",
      "Epoch 39, batch 59 D Loss: 1.3673534393310547, G Loss: 0.686795711517334\n",
      "Epoch 39, batch 60 D Loss: 1.3808791637420654, G Loss: 0.6825014352798462\n",
      "Epoch 39, batch 61 D Loss: 1.369559407234192, G Loss: 0.6763095855712891\n",
      "Epoch 39, batch 62 D Loss: 1.3734214305877686, G Loss: 0.6719027161598206\n",
      "Epoch 39, batch 63 D Loss: 1.3647119998931885, G Loss: 0.6681459546089172\n",
      "Epoch 39, batch 64 D Loss: 1.3640053272247314, G Loss: 0.6926241517066956\n",
      "Epoch 39, batch 65 D Loss: 1.4018707275390625, G Loss: 0.6744459271430969\n",
      "Epoch 39, batch 66 D Loss: 1.343523383140564, G Loss: 0.6797127723693848\n",
      "Epoch 39, batch 67 D Loss: 1.3937084674835205, G Loss: 0.6680164337158203\n",
      "Epoch 39, batch 68 D Loss: 1.3579232692718506, G Loss: 0.6889558434486389\n",
      "Epoch 39, batch 69 D Loss: 1.3698986768722534, G Loss: 0.6909342408180237\n",
      "Epoch 39, batch 70 D Loss: 1.3730502128601074, G Loss: 0.6782927513122559\n",
      "Epoch 39, batch 71 D Loss: 1.384626865386963, G Loss: 0.6688216924667358\n",
      "Epoch 39, batch 72 D Loss: 1.3761318922042847, G Loss: 0.6783730387687683\n",
      "Epoch 39, batch 73 D Loss: 1.3801032304763794, G Loss: 0.6760521531105042\n",
      "Epoch 39, batch 74 D Loss: 1.3815891742706299, G Loss: 0.6859052777290344\n",
      "Epoch 39, batch 75 D Loss: 1.3783854246139526, G Loss: 0.6726744771003723\n",
      "Epoch 39, batch 76 D Loss: 1.379996418952942, G Loss: 0.6600692272186279\n",
      "Epoch 39, batch 77 D Loss: 1.3760432004928589, G Loss: 0.685973048210144\n",
      "Epoch 39, batch 78 D Loss: 1.3570122718811035, G Loss: 0.6772053241729736\n",
      "Epoch 39, batch 79 D Loss: 1.3803762197494507, G Loss: 0.6793124675750732\n",
      "Epoch 39, batch 80 D Loss: 1.3604001998901367, G Loss: 0.6832360625267029\n",
      "Epoch 39, batch 81 D Loss: 1.382776141166687, G Loss: 0.6636612415313721\n",
      "Epoch 39, batch 82 D Loss: 1.355471134185791, G Loss: 0.6844589710235596\n",
      "Epoch 39, batch 83 D Loss: 1.3678052425384521, G Loss: 0.6726109981536865\n",
      "Epoch 39, batch 84 D Loss: 1.3574223518371582, G Loss: 0.682694673538208\n",
      "Epoch 39, batch 85 D Loss: 1.3708200454711914, G Loss: 0.6686290502548218\n",
      "Epoch 39, batch 86 D Loss: 1.3815397024154663, G Loss: 0.6733250617980957\n",
      "Epoch 39, batch 87 D Loss: 1.3552378416061401, G Loss: 0.6770434379577637\n",
      "Epoch 39, batch 88 D Loss: 1.4018123149871826, G Loss: 0.6555262804031372\n",
      "Epoch 39, batch 89 D Loss: 1.394571304321289, G Loss: 0.6666932702064514\n",
      "Epoch 39, batch 90 D Loss: 1.3897557258605957, G Loss: 0.6639546751976013\n",
      "Epoch 39, batch 91 D Loss: 1.3855366706848145, G Loss: 0.6569561958312988\n",
      "Epoch 39, batch 92 D Loss: 1.4032764434814453, G Loss: 0.6491134166717529\n",
      "Epoch 39, batch 93 D Loss: 1.3972523212432861, G Loss: 0.6561868190765381\n",
      "Epoch 39, batch 94 D Loss: 1.3696081638336182, G Loss: 0.666138768196106\n",
      "Epoch 39, batch 95 D Loss: 1.3920522928237915, G Loss: 0.6599282622337341\n",
      "Epoch 39, batch 96 D Loss: 1.3861708641052246, G Loss: 0.6706058979034424\n",
      "Epoch 39, batch 97 D Loss: 1.380509614944458, G Loss: 0.6653316020965576\n",
      "Epoch 39, batch 98 D Loss: 1.4180419445037842, G Loss: 0.6567752361297607\n",
      "Epoch 39, batch 99 D Loss: 1.3857359886169434, G Loss: 0.6563146710395813\n",
      "Epoch 39, batch 100 D Loss: 1.4046800136566162, G Loss: 0.6509059071540833\n",
      "Epoch 39, batch 101 D Loss: 1.4135377407073975, G Loss: 0.6548600792884827\n",
      "Epoch 39, batch 102 D Loss: 1.3884532451629639, G Loss: 0.6580808162689209\n",
      "Epoch 39, batch 103 D Loss: 1.4224128723144531, G Loss: 0.6469557881355286\n",
      "Epoch 39, batch 104 D Loss: 1.397979497909546, G Loss: 0.6504023671150208\n",
      "Epoch 39, batch 105 D Loss: 1.4296715259552002, G Loss: 0.6401962041854858\n",
      "Epoch 39, batch 106 D Loss: 1.3953518867492676, G Loss: 0.6705140471458435\n",
      "Epoch 39, batch 107 D Loss: 1.3979196548461914, G Loss: 0.6627570986747742\n",
      "Epoch 39, batch 108 D Loss: 1.3960626125335693, G Loss: 0.6622012853622437\n",
      "Epoch 39, batch 109 D Loss: 1.395517349243164, G Loss: 0.6573525071144104\n",
      "Epoch 39, batch 110 D Loss: 1.4090077877044678, G Loss: 0.6588342189788818\n",
      "Epoch 39, batch 111 D Loss: 1.4031716585159302, G Loss: 0.6593289375305176\n",
      "Epoch 39, batch 112 D Loss: 1.391862392425537, G Loss: 0.6602125763893127\n",
      "Epoch 39, batch 113 D Loss: 1.398737907409668, G Loss: 0.6710656881332397\n",
      "Epoch 39, batch 114 D Loss: 1.390932559967041, G Loss: 0.6811621189117432\n",
      "Epoch 39, batch 115 D Loss: 1.4070155620574951, G Loss: 0.667869508266449\n",
      "Epoch 39, batch 116 D Loss: 1.4036104679107666, G Loss: 0.6680813431739807\n",
      "Epoch 39, batch 117 D Loss: 1.4001824855804443, G Loss: 0.6710429191589355\n",
      "Epoch 39, batch 118 D Loss: 1.3995994329452515, G Loss: 0.6794068813323975\n",
      "Epoch 39, batch 119 D Loss: 1.406292200088501, G Loss: 0.674210786819458\n",
      "Epoch 39, batch 120 D Loss: 1.423506736755371, G Loss: 0.6587123870849609\n",
      "Epoch 39, batch 121 D Loss: 1.4134283065795898, G Loss: 0.6648245453834534\n",
      "Epoch 39, batch 122 D Loss: 1.396672248840332, G Loss: 0.6751479506492615\n",
      "Epoch 39, batch 123 D Loss: 1.3731731176376343, G Loss: 0.6869286298751831\n",
      "Epoch 39, batch 124 D Loss: 1.4084703922271729, G Loss: 0.6791645288467407\n",
      "Epoch 39, batch 125 D Loss: 1.3762784004211426, G Loss: 0.6850036382675171\n",
      "Epoch 39, batch 126 D Loss: 1.3912122249603271, G Loss: 0.6781657338142395\n",
      "Epoch 39, batch 127 D Loss: 1.4100124835968018, G Loss: 0.6661597490310669\n",
      "Epoch 39, batch 128 D Loss: 1.4011024236679077, G Loss: 0.6729724407196045\n",
      "Epoch 39, batch 129 D Loss: 1.4005863666534424, G Loss: 0.6745930314064026\n",
      "Epoch 39, batch 130 D Loss: 1.3981629610061646, G Loss: 0.6842514872550964\n",
      "Epoch 39, batch 131 D Loss: 1.3946046829223633, G Loss: 0.6754575371742249\n",
      "Epoch 39, batch 132 D Loss: 1.3791612386703491, G Loss: 0.6791418194770813\n",
      "Epoch 39, batch 133 D Loss: 1.404470682144165, G Loss: 0.6867683529853821\n",
      "Epoch 39, batch 134 D Loss: 1.3855524063110352, G Loss: 0.6993234753608704\n",
      "Epoch 39, batch 135 D Loss: 1.402696967124939, G Loss: 0.6836494207382202\n",
      "Epoch 39, batch 136 D Loss: 1.3847825527191162, G Loss: 0.6986640095710754\n",
      "Epoch 39, batch 137 D Loss: 1.382521152496338, G Loss: 0.6933242678642273\n",
      "Epoch 39, batch 138 D Loss: 1.4047648906707764, G Loss: 0.6830760836601257\n",
      "Epoch 39, batch 139 D Loss: 1.3841183185577393, G Loss: 0.6990857720375061\n",
      "Epoch 39, batch 140 D Loss: 1.397310733795166, G Loss: 0.6832566261291504\n",
      "Epoch 39, batch 141 D Loss: 1.3843755722045898, G Loss: 0.7067828178405762\n",
      "Epoch 39, batch 142 D Loss: 1.4077041149139404, G Loss: 0.6922630071640015\n",
      "Epoch 39, batch 143 D Loss: 1.3761234283447266, G Loss: 0.710070013999939\n",
      "Epoch 39, batch 144 D Loss: 1.3867276906967163, G Loss: 0.7053964138031006\n",
      "Epoch 39, batch 145 D Loss: 1.3810648918151855, G Loss: 0.7133789658546448\n",
      "Epoch 39, batch 146 D Loss: 1.3718600273132324, G Loss: 0.721210777759552\n",
      "Epoch 39, batch 147 D Loss: 1.394017219543457, G Loss: 0.7036617994308472\n",
      "Epoch 39, batch 148 D Loss: 1.3618061542510986, G Loss: 0.7222131490707397\n",
      "Epoch 39, batch 149 D Loss: 1.3769726753234863, G Loss: 0.7164396643638611\n",
      "Epoch 39, batch 150 D Loss: 1.380434274673462, G Loss: 0.7212854027748108\n",
      "Epoch 39, batch 151 D Loss: 1.391599178314209, G Loss: 0.7155168652534485\n",
      "Epoch 39, batch 152 D Loss: 1.3976556062698364, G Loss: 0.7167834639549255\n",
      "Epoch 39, batch 153 D Loss: 1.356278896331787, G Loss: 0.7298047542572021\n",
      "Epoch 39, batch 154 D Loss: 1.3921478986740112, G Loss: 0.7199941277503967\n",
      "Epoch 39, batch 155 D Loss: 1.3849129676818848, G Loss: 0.7140472531318665\n",
      "Epoch 39, batch 156 D Loss: 1.3774092197418213, G Loss: 0.7303406000137329\n",
      "Epoch 39, batch 157 D Loss: 1.3645081520080566, G Loss: 0.7266456484794617\n",
      "Epoch 39, batch 158 D Loss: 1.3661160469055176, G Loss: 0.7308667898178101\n",
      "Epoch 39, batch 159 D Loss: 1.379402756690979, G Loss: 0.7287417650222778\n",
      "Epoch 39, batch 160 D Loss: 1.3863062858581543, G Loss: 0.7352958917617798\n",
      "Epoch 39, batch 161 D Loss: 1.358771800994873, G Loss: 0.7590171098709106\n",
      "Epoch 39, batch 162 D Loss: 1.3845620155334473, G Loss: 0.7219711542129517\n",
      "Epoch 39, batch 163 D Loss: 1.3646926879882812, G Loss: 0.7435251474380493\n",
      "Epoch 39, batch 164 D Loss: 1.3502840995788574, G Loss: 0.7616655230522156\n",
      "Epoch 39, batch 165 D Loss: 1.3613200187683105, G Loss: 0.7629177570343018\n",
      "Epoch 39, batch 166 D Loss: 1.3672080039978027, G Loss: 0.7632867693901062\n",
      "Epoch 39, batch 167 D Loss: 1.3797497749328613, G Loss: 0.7325941324234009\n",
      "Epoch 39, batch 168 D Loss: 1.3838660717010498, G Loss: 0.713641881942749\n",
      "Epoch 39, batch 169 D Loss: 1.419564962387085, G Loss: 0.7138243317604065\n",
      "Epoch 39, batch 170 D Loss: 1.3926796913146973, G Loss: 0.7434383630752563\n",
      "Epoch 39, batch 171 D Loss: 1.3866381645202637, G Loss: 0.7278252243995667\n",
      "Epoch 39, batch 172 D Loss: 1.3909186124801636, G Loss: 0.7348877191543579\n",
      "Epoch 39, batch 173 D Loss: 1.349031925201416, G Loss: 0.7783771753311157\n",
      "Epoch 39, batch 174 D Loss: 1.3828332424163818, G Loss: 0.7623226642608643\n",
      "Epoch 39, batch 175 D Loss: 1.3993358612060547, G Loss: 0.7478294372558594\n",
      "Epoch 39, batch 176 D Loss: 1.3786507844924927, G Loss: 0.718964695930481\n",
      "Epoch 39, batch 177 D Loss: 1.3884778022766113, G Loss: 0.7246221899986267\n",
      "Epoch 39, batch 178 D Loss: 1.3682329654693604, G Loss: 0.7381629943847656\n",
      "Epoch 39, batch 179 D Loss: 1.3845735788345337, G Loss: 0.727928638458252\n",
      "Epoch 39, batch 180 D Loss: 1.3812873363494873, G Loss: 0.7167016863822937\n",
      "Epoch 39, batch 181 D Loss: 1.3986971378326416, G Loss: 0.730233907699585\n",
      "Epoch 39, batch 182 D Loss: 1.3981826305389404, G Loss: 0.7346025705337524\n",
      "Epoch 39, batch 183 D Loss: 1.4229316711425781, G Loss: 0.7039825320243835\n",
      "Epoch 39, batch 184 D Loss: 1.3881982564926147, G Loss: 0.7405644059181213\n",
      "Epoch 39, batch 185 D Loss: 1.3864951133728027, G Loss: 0.7224307060241699\n",
      "Epoch 39, batch 186 D Loss: 1.3718008995056152, G Loss: 0.7465693950653076\n",
      "Epoch 39, batch 187 D Loss: 1.389155626296997, G Loss: 0.7208299040794373\n",
      "Epoch 39, batch 188 D Loss: 1.4276679754257202, G Loss: 0.7198572754859924\n",
      "Epoch 39, batch 189 D Loss: 1.3751506805419922, G Loss: 0.720164954662323\n",
      "Epoch 39, batch 190 D Loss: 1.364910364151001, G Loss: 0.7373721599578857\n",
      "Epoch 39, batch 191 D Loss: 1.3919715881347656, G Loss: 0.7273924350738525\n",
      "Epoch 39, batch 192 D Loss: 1.4185476303100586, G Loss: 0.7000354528427124\n",
      "Epoch 39, batch 193 D Loss: 1.4074337482452393, G Loss: 0.7221241593360901\n",
      "Epoch 39, batch 194 D Loss: 1.412085771560669, G Loss: 0.7119624614715576\n",
      "Epoch 39, batch 195 D Loss: 1.405522346496582, G Loss: 0.7039299607276917\n",
      "Epoch 39, batch 196 D Loss: 1.3848581314086914, G Loss: 0.7213037014007568\n",
      "Epoch 39, batch 197 D Loss: 1.4095027446746826, G Loss: 0.7163183093070984\n",
      "Epoch 39, batch 198 D Loss: 1.3950819969177246, G Loss: 0.7134498357772827\n",
      "Epoch 39, batch 199 D Loss: 1.4062561988830566, G Loss: 0.7122066020965576\n",
      "Epoch 39, batch 200 D Loss: 1.4058759212493896, G Loss: 0.7187681794166565\n",
      "Epoch 40, batch 1 D Loss: 1.3681212663650513, G Loss: 0.7163113355636597\n",
      "Epoch 40, batch 2 D Loss: 1.406113862991333, G Loss: 0.7189342379570007\n",
      "Epoch 40, batch 3 D Loss: 1.393405556678772, G Loss: 0.7209451198577881\n",
      "Epoch 40, batch 4 D Loss: 1.3960423469543457, G Loss: 0.7374629378318787\n",
      "Epoch 40, batch 5 D Loss: 1.3902331590652466, G Loss: 0.7224869728088379\n",
      "Epoch 40, batch 6 D Loss: 1.3817169666290283, G Loss: 0.7214277386665344\n",
      "Epoch 40, batch 7 D Loss: 1.3885570764541626, G Loss: 0.7112853527069092\n",
      "Epoch 40, batch 8 D Loss: 1.3913837671279907, G Loss: 0.7065992951393127\n",
      "Epoch 40, batch 9 D Loss: 1.384307861328125, G Loss: 0.7269328832626343\n",
      "Epoch 40, batch 10 D Loss: 1.3947476148605347, G Loss: 0.714780867099762\n",
      "Epoch 40, batch 11 D Loss: 1.387556552886963, G Loss: 0.7057374715805054\n",
      "Epoch 40, batch 12 D Loss: 1.3991053104400635, G Loss: 0.7216640710830688\n",
      "Epoch 40, batch 13 D Loss: 1.4045250415802002, G Loss: 0.714323103427887\n",
      "Epoch 40, batch 14 D Loss: 1.4038407802581787, G Loss: 0.7135778665542603\n",
      "Epoch 40, batch 15 D Loss: 1.3842582702636719, G Loss: 0.7163763642311096\n",
      "Epoch 40, batch 16 D Loss: 1.3625948429107666, G Loss: 0.7343918085098267\n",
      "Epoch 40, batch 17 D Loss: 1.4059336185455322, G Loss: 0.7108293175697327\n",
      "Epoch 40, batch 18 D Loss: 1.398611068725586, G Loss: 0.7126463055610657\n",
      "Epoch 40, batch 19 D Loss: 1.3764195442199707, G Loss: 0.7277677655220032\n",
      "Epoch 40, batch 20 D Loss: 1.395677089691162, G Loss: 0.7273537516593933\n",
      "Epoch 40, batch 21 D Loss: 1.3909281492233276, G Loss: 0.7312817573547363\n",
      "Epoch 40, batch 22 D Loss: 1.4089696407318115, G Loss: 0.6985682845115662\n",
      "Epoch 40, batch 23 D Loss: 1.3781529664993286, G Loss: 0.7258742451667786\n",
      "Epoch 40, batch 24 D Loss: 1.4092514514923096, G Loss: 0.7197986841201782\n",
      "Epoch 40, batch 25 D Loss: 1.3923592567443848, G Loss: 0.7132737040519714\n",
      "Epoch 40, batch 26 D Loss: 1.3958756923675537, G Loss: 0.72962486743927\n",
      "Epoch 40, batch 27 D Loss: 1.4003605842590332, G Loss: 0.7207452654838562\n",
      "Epoch 40, batch 28 D Loss: 1.3995859622955322, G Loss: 0.7344253659248352\n",
      "Epoch 40, batch 29 D Loss: 1.3936288356781006, G Loss: 0.7296026349067688\n",
      "Epoch 40, batch 30 D Loss: 1.3876361846923828, G Loss: 0.7246140241622925\n",
      "Epoch 40, batch 31 D Loss: 1.3876128196716309, G Loss: 0.7232835292816162\n",
      "Epoch 40, batch 32 D Loss: 1.3767576217651367, G Loss: 0.7409498691558838\n",
      "Epoch 40, batch 33 D Loss: 1.3822481632232666, G Loss: 0.7413555979728699\n",
      "Epoch 40, batch 34 D Loss: 1.3819499015808105, G Loss: 0.7393420338630676\n",
      "Epoch 40, batch 35 D Loss: 1.3973357677459717, G Loss: 0.7328807711601257\n",
      "Epoch 40, batch 36 D Loss: 1.3927854299545288, G Loss: 0.7297567129135132\n",
      "Epoch 40, batch 37 D Loss: 1.3859689235687256, G Loss: 0.738159716129303\n",
      "Epoch 40, batch 38 D Loss: 1.3977034091949463, G Loss: 0.7385475635528564\n",
      "Epoch 40, batch 39 D Loss: 1.3817492723464966, G Loss: 0.7382790446281433\n",
      "Epoch 40, batch 40 D Loss: 1.3903865814208984, G Loss: 0.7238588929176331\n",
      "Epoch 40, batch 41 D Loss: 1.3863403797149658, G Loss: 0.7244998812675476\n",
      "Epoch 40, batch 42 D Loss: 1.3881723880767822, G Loss: 0.7286527156829834\n",
      "Epoch 40, batch 43 D Loss: 1.3838876485824585, G Loss: 0.7408902645111084\n",
      "Epoch 40, batch 44 D Loss: 1.4007503986358643, G Loss: 0.7340174913406372\n",
      "Epoch 40, batch 45 D Loss: 1.37350332736969, G Loss: 0.7480732202529907\n",
      "Epoch 40, batch 46 D Loss: 1.410551905632019, G Loss: 0.7266320586204529\n",
      "Epoch 40, batch 47 D Loss: 1.3772163391113281, G Loss: 0.7410415410995483\n",
      "Epoch 40, batch 48 D Loss: 1.3782360553741455, G Loss: 0.7383622527122498\n",
      "Epoch 40, batch 49 D Loss: 1.4014253616333008, G Loss: 0.7209520936012268\n",
      "Epoch 40, batch 50 D Loss: 1.3717286586761475, G Loss: 0.7382556200027466\n",
      "Epoch 40, batch 51 D Loss: 1.386141061782837, G Loss: 0.7390170097351074\n",
      "Epoch 40, batch 52 D Loss: 1.3962492942810059, G Loss: 0.7294908761978149\n",
      "Epoch 40, batch 53 D Loss: 1.390636920928955, G Loss: 0.7325270771980286\n",
      "Epoch 40, batch 54 D Loss: 1.3727376461029053, G Loss: 0.7428945302963257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, batch 55 D Loss: 1.383531093597412, G Loss: 0.7372661828994751\n",
      "Epoch 40, batch 56 D Loss: 1.3746659755706787, G Loss: 0.7340450882911682\n",
      "Epoch 40, batch 57 D Loss: 1.3817720413208008, G Loss: 0.7214418649673462\n",
      "Epoch 40, batch 58 D Loss: 1.3757147789001465, G Loss: 0.7259207963943481\n",
      "Epoch 40, batch 59 D Loss: 1.3938889503479004, G Loss: 0.7144241333007812\n",
      "Epoch 40, batch 60 D Loss: 1.3792600631713867, G Loss: 0.7250691056251526\n",
      "Epoch 40, batch 61 D Loss: 1.383278250694275, G Loss: 0.7350438833236694\n",
      "Epoch 40, batch 62 D Loss: 1.3820762634277344, G Loss: 0.7249050736427307\n",
      "Epoch 40, batch 63 D Loss: 1.3607964515686035, G Loss: 0.7453163862228394\n",
      "Epoch 40, batch 64 D Loss: 1.3681225776672363, G Loss: 0.7197151184082031\n",
      "Epoch 40, batch 65 D Loss: 1.3651866912841797, G Loss: 0.7350874543190002\n",
      "Epoch 40, batch 66 D Loss: 1.3691645860671997, G Loss: 0.7255247235298157\n",
      "Epoch 40, batch 67 D Loss: 1.358144998550415, G Loss: 0.7219420075416565\n",
      "Epoch 40, batch 68 D Loss: 1.3605965375900269, G Loss: 0.7519568800926208\n",
      "Epoch 40, batch 69 D Loss: 1.3808050155639648, G Loss: 0.7353149652481079\n",
      "Epoch 40, batch 70 D Loss: 1.4103753566741943, G Loss: 0.7171611785888672\n",
      "Epoch 40, batch 71 D Loss: 1.3780577182769775, G Loss: 0.7134785652160645\n",
      "Epoch 40, batch 72 D Loss: 1.3783769607543945, G Loss: 0.722073495388031\n",
      "Epoch 40, batch 73 D Loss: 1.3655602931976318, G Loss: 0.7239123582839966\n",
      "Epoch 40, batch 74 D Loss: 1.3805203437805176, G Loss: 0.7085017561912537\n",
      "Epoch 40, batch 75 D Loss: 1.3866345882415771, G Loss: 0.7084124088287354\n",
      "Epoch 40, batch 76 D Loss: 1.398886799812317, G Loss: 0.6989514231681824\n",
      "Epoch 40, batch 77 D Loss: 1.3850626945495605, G Loss: 0.6874423027038574\n",
      "Epoch 40, batch 78 D Loss: 1.3419909477233887, G Loss: 0.7147369384765625\n",
      "Epoch 40, batch 79 D Loss: 1.3921210765838623, G Loss: 0.7144770622253418\n",
      "Epoch 40, batch 80 D Loss: 1.3787834644317627, G Loss: 0.720611572265625\n",
      "Epoch 40, batch 81 D Loss: 1.390939712524414, G Loss: 0.7120134234428406\n",
      "Epoch 40, batch 82 D Loss: 1.3808534145355225, G Loss: 0.7038614749908447\n",
      "Epoch 40, batch 83 D Loss: 1.3788982629776, G Loss: 0.7041798233985901\n",
      "Epoch 40, batch 84 D Loss: 1.3643620014190674, G Loss: 0.7060263156890869\n",
      "Epoch 40, batch 85 D Loss: 1.3628911972045898, G Loss: 0.7058121562004089\n",
      "Epoch 40, batch 86 D Loss: 1.3789801597595215, G Loss: 0.7143165469169617\n",
      "Epoch 40, batch 87 D Loss: 1.4071044921875, G Loss: 0.6760976314544678\n",
      "Epoch 40, batch 88 D Loss: 1.3840519189834595, G Loss: 0.6869081258773804\n",
      "Epoch 40, batch 89 D Loss: 1.397113561630249, G Loss: 0.697865903377533\n",
      "Epoch 40, batch 90 D Loss: 1.366477131843567, G Loss: 0.6969895958900452\n",
      "Epoch 40, batch 91 D Loss: 1.3625465631484985, G Loss: 0.6898060441017151\n",
      "Epoch 40, batch 92 D Loss: 1.3922734260559082, G Loss: 0.6848114132881165\n",
      "Epoch 40, batch 93 D Loss: 1.394402027130127, G Loss: 0.67937171459198\n",
      "Epoch 40, batch 94 D Loss: 1.3823955059051514, G Loss: 0.6776179671287537\n",
      "Epoch 40, batch 95 D Loss: 1.36368727684021, G Loss: 0.7179753184318542\n",
      "Epoch 40, batch 96 D Loss: 1.4000155925750732, G Loss: 0.670586109161377\n",
      "Epoch 40, batch 97 D Loss: 1.3851993083953857, G Loss: 0.6891223788261414\n",
      "Epoch 40, batch 98 D Loss: 1.373838186264038, G Loss: 0.6893509030342102\n",
      "Epoch 40, batch 99 D Loss: 1.3704626560211182, G Loss: 0.6949518322944641\n",
      "Epoch 40, batch 100 D Loss: 1.3969097137451172, G Loss: 0.6595166921615601\n",
      "Epoch 40, batch 101 D Loss: 1.3789680004119873, G Loss: 0.69051194190979\n",
      "Epoch 40, batch 102 D Loss: 1.3991734981536865, G Loss: 0.6653308868408203\n",
      "Epoch 40, batch 103 D Loss: 1.3958320617675781, G Loss: 0.6745699048042297\n",
      "Epoch 40, batch 104 D Loss: 1.3823096752166748, G Loss: 0.6768825650215149\n",
      "Epoch 40, batch 105 D Loss: 1.4040460586547852, G Loss: 0.6609496474266052\n",
      "Epoch 40, batch 106 D Loss: 1.3995789289474487, G Loss: 0.6703397631645203\n",
      "Epoch 40, batch 107 D Loss: 1.3990083932876587, G Loss: 0.6657382249832153\n",
      "Epoch 40, batch 108 D Loss: 1.4202570915222168, G Loss: 0.6574996113777161\n",
      "Epoch 40, batch 109 D Loss: 1.3652409315109253, G Loss: 0.6926538348197937\n",
      "Epoch 40, batch 110 D Loss: 1.4206876754760742, G Loss: 0.6529841423034668\n",
      "Epoch 40, batch 111 D Loss: 1.3957489728927612, G Loss: 0.6659706234931946\n",
      "Epoch 40, batch 112 D Loss: 1.422458291053772, G Loss: 0.6587939262390137\n",
      "Epoch 40, batch 113 D Loss: 1.3671221733093262, G Loss: 0.6794143915176392\n",
      "Epoch 40, batch 114 D Loss: 1.4155664443969727, G Loss: 0.6599658727645874\n",
      "Epoch 40, batch 115 D Loss: 1.407031774520874, G Loss: 0.6540908813476562\n",
      "Epoch 40, batch 116 D Loss: 1.3843090534210205, G Loss: 0.6669290661811829\n",
      "Epoch 40, batch 117 D Loss: 1.4214329719543457, G Loss: 0.6472032070159912\n",
      "Epoch 40, batch 118 D Loss: 1.3957178592681885, G Loss: 0.6641345024108887\n",
      "Epoch 40, batch 119 D Loss: 1.3943120241165161, G Loss: 0.6626145243644714\n",
      "Epoch 40, batch 120 D Loss: 1.3893742561340332, G Loss: 0.661516010761261\n",
      "Epoch 40, batch 121 D Loss: 1.3843426704406738, G Loss: 0.6641248464584351\n",
      "Epoch 40, batch 122 D Loss: 1.3896808624267578, G Loss: 0.6631706357002258\n",
      "Epoch 40, batch 123 D Loss: 1.3820565938949585, G Loss: 0.6661789417266846\n",
      "Epoch 40, batch 124 D Loss: 1.401821494102478, G Loss: 0.6598824858665466\n",
      "Epoch 40, batch 125 D Loss: 1.398076057434082, G Loss: 0.661857008934021\n",
      "Epoch 40, batch 126 D Loss: 1.394212245941162, G Loss: 0.6602404713630676\n",
      "Epoch 40, batch 127 D Loss: 1.4043203592300415, G Loss: 0.664804220199585\n",
      "Epoch 40, batch 128 D Loss: 1.4039993286132812, G Loss: 0.6460636854171753\n",
      "Epoch 40, batch 129 D Loss: 1.3829193115234375, G Loss: 0.6671168804168701\n",
      "Epoch 40, batch 130 D Loss: 1.4007551670074463, G Loss: 0.6532167792320251\n",
      "Epoch 40, batch 131 D Loss: 1.3931677341461182, G Loss: 0.6628772020339966\n",
      "Epoch 40, batch 132 D Loss: 1.4008934497833252, G Loss: 0.6503352522850037\n",
      "Epoch 40, batch 133 D Loss: 1.3915636539459229, G Loss: 0.6592861413955688\n",
      "Epoch 40, batch 134 D Loss: 1.401646375656128, G Loss: 0.6594352722167969\n",
      "Epoch 40, batch 135 D Loss: 1.3932785987854004, G Loss: 0.6626487970352173\n",
      "Epoch 40, batch 136 D Loss: 1.4003101587295532, G Loss: 0.6650058627128601\n",
      "Epoch 40, batch 137 D Loss: 1.403048038482666, G Loss: 0.6499903202056885\n",
      "Epoch 40, batch 138 D Loss: 1.3910365104675293, G Loss: 0.6676346659660339\n",
      "Epoch 40, batch 139 D Loss: 1.4050488471984863, G Loss: 0.6534683704376221\n",
      "Epoch 40, batch 140 D Loss: 1.3936611413955688, G Loss: 0.6621415019035339\n",
      "Epoch 40, batch 141 D Loss: 1.3871805667877197, G Loss: 0.6705453991889954\n",
      "Epoch 40, batch 142 D Loss: 1.3954310417175293, G Loss: 0.6657452583312988\n",
      "Epoch 40, batch 143 D Loss: 1.4030365943908691, G Loss: 0.6614603400230408\n",
      "Epoch 40, batch 144 D Loss: 1.3878934383392334, G Loss: 0.6601467132568359\n",
      "Epoch 40, batch 145 D Loss: 1.3908982276916504, G Loss: 0.6611083745956421\n",
      "Epoch 40, batch 146 D Loss: 1.400039792060852, G Loss: 0.6624314188957214\n",
      "Epoch 40, batch 147 D Loss: 1.390430212020874, G Loss: 0.66823410987854\n",
      "Epoch 40, batch 148 D Loss: 1.380228042602539, G Loss: 0.6721229553222656\n",
      "Epoch 40, batch 149 D Loss: 1.3876953125, G Loss: 0.6646824479103088\n",
      "Epoch 40, batch 150 D Loss: 1.3883612155914307, G Loss: 0.6671224236488342\n",
      "Epoch 40, batch 151 D Loss: 1.3739138841629028, G Loss: 0.6752088069915771\n",
      "Epoch 40, batch 152 D Loss: 1.3820589780807495, G Loss: 0.6689140796661377\n",
      "Epoch 40, batch 153 D Loss: 1.3846261501312256, G Loss: 0.6733965277671814\n",
      "Epoch 40, batch 154 D Loss: 1.3859360218048096, G Loss: 0.6664879322052002\n",
      "Epoch 40, batch 155 D Loss: 1.380469799041748, G Loss: 0.6671984195709229\n",
      "Epoch 40, batch 156 D Loss: 1.3737447261810303, G Loss: 0.6708025336265564\n",
      "Epoch 40, batch 157 D Loss: 1.376025915145874, G Loss: 0.6752135753631592\n",
      "Epoch 40, batch 158 D Loss: 1.377711296081543, G Loss: 0.6727588176727295\n",
      "Epoch 40, batch 159 D Loss: 1.3689086437225342, G Loss: 0.6767202019691467\n",
      "Epoch 40, batch 160 D Loss: 1.3740804195404053, G Loss: 0.6779422163963318\n",
      "Epoch 40, batch 161 D Loss: 1.3773082494735718, G Loss: 0.6764878034591675\n",
      "Epoch 40, batch 162 D Loss: 1.3764002323150635, G Loss: 0.6700583100318909\n",
      "Epoch 40, batch 163 D Loss: 1.3756273984909058, G Loss: 0.6790311932563782\n",
      "Epoch 40, batch 164 D Loss: 1.37949800491333, G Loss: 0.6708146929740906\n",
      "Epoch 40, batch 165 D Loss: 1.3648704290390015, G Loss: 0.6815120577812195\n",
      "Epoch 40, batch 166 D Loss: 1.3748219013214111, G Loss: 0.675325334072113\n",
      "Epoch 40, batch 167 D Loss: 1.3681795597076416, G Loss: 0.6725877523422241\n",
      "Epoch 40, batch 168 D Loss: 1.3714792728424072, G Loss: 0.6857516765594482\n",
      "Epoch 40, batch 169 D Loss: 1.391120195388794, G Loss: 0.6723434329032898\n",
      "Epoch 40, batch 170 D Loss: 1.392820119857788, G Loss: 0.6694047451019287\n",
      "Epoch 40, batch 171 D Loss: 1.3897106647491455, G Loss: 0.6653013825416565\n",
      "Epoch 40, batch 172 D Loss: 1.3887712955474854, G Loss: 0.6773439049720764\n",
      "Epoch 40, batch 173 D Loss: 1.3733513355255127, G Loss: 0.6814390420913696\n",
      "Epoch 40, batch 174 D Loss: 1.3761404752731323, G Loss: 0.6858578324317932\n",
      "Epoch 40, batch 175 D Loss: 1.3635153770446777, G Loss: 0.6871868371963501\n",
      "Epoch 40, batch 176 D Loss: 1.370179533958435, G Loss: 0.6827934980392456\n",
      "Epoch 40, batch 177 D Loss: 1.3753180503845215, G Loss: 0.6778700351715088\n",
      "Epoch 40, batch 178 D Loss: 1.3809195756912231, G Loss: 0.6855891942977905\n",
      "Epoch 40, batch 179 D Loss: 1.366256833076477, G Loss: 0.6855255961418152\n",
      "Epoch 40, batch 180 D Loss: 1.369554042816162, G Loss: 0.6784011125564575\n",
      "Epoch 40, batch 181 D Loss: 1.3567869663238525, G Loss: 0.689801812171936\n",
      "Epoch 40, batch 182 D Loss: 1.3742258548736572, G Loss: 0.6863027811050415\n",
      "Epoch 40, batch 183 D Loss: 1.3665324449539185, G Loss: 0.6933009624481201\n",
      "Epoch 40, batch 184 D Loss: 1.3797069787979126, G Loss: 0.6847166419029236\n",
      "Epoch 40, batch 185 D Loss: 1.3263778686523438, G Loss: 0.712995707988739\n",
      "Epoch 40, batch 186 D Loss: 1.3745561838150024, G Loss: 0.6838743686676025\n",
      "Epoch 40, batch 187 D Loss: 1.3617210388183594, G Loss: 0.691697895526886\n",
      "Epoch 40, batch 188 D Loss: 1.390164852142334, G Loss: 0.6697531342506409\n",
      "Epoch 40, batch 189 D Loss: 1.3716648817062378, G Loss: 0.686998188495636\n",
      "Epoch 40, batch 190 D Loss: 1.3787131309509277, G Loss: 0.6771215200424194\n",
      "Epoch 40, batch 191 D Loss: 1.3399245738983154, G Loss: 0.6990997195243835\n",
      "Epoch 40, batch 192 D Loss: 1.3714334964752197, G Loss: 0.7030053734779358\n",
      "Epoch 40, batch 193 D Loss: 1.3667209148406982, G Loss: 0.6953891515731812\n",
      "Epoch 40, batch 194 D Loss: 1.3580775260925293, G Loss: 0.6857696771621704\n",
      "Epoch 40, batch 195 D Loss: 1.3710925579071045, G Loss: 0.6812365651130676\n",
      "Epoch 40, batch 196 D Loss: 1.3655211925506592, G Loss: 0.6999427676200867\n",
      "Epoch 40, batch 197 D Loss: 1.4103485345840454, G Loss: 0.67132568359375\n",
      "Epoch 40, batch 198 D Loss: 1.3779866695404053, G Loss: 0.6890937089920044\n",
      "Epoch 40, batch 199 D Loss: 1.3915863037109375, G Loss: 0.6912267804145813\n",
      "Epoch 40, batch 200 D Loss: 1.3680362701416016, G Loss: 0.7020714282989502\n",
      "Epoch 41, batch 1 D Loss: 1.4012142419815063, G Loss: 0.6652873754501343\n",
      "Epoch 41, batch 2 D Loss: 1.4001407623291016, G Loss: 0.6841933727264404\n",
      "Epoch 41, batch 3 D Loss: 1.3957021236419678, G Loss: 0.6806786060333252\n",
      "Epoch 41, batch 4 D Loss: 1.3673765659332275, G Loss: 0.6841158866882324\n",
      "Epoch 41, batch 5 D Loss: 1.3650932312011719, G Loss: 0.6760994791984558\n",
      "Epoch 41, batch 6 D Loss: 1.363814115524292, G Loss: 0.6853259801864624\n",
      "Epoch 41, batch 7 D Loss: 1.362196445465088, G Loss: 0.6913801431655884\n",
      "Epoch 41, batch 8 D Loss: 1.415306568145752, G Loss: 0.6713529825210571\n",
      "Epoch 41, batch 9 D Loss: 1.3843436241149902, G Loss: 0.6887337565422058\n",
      "Epoch 41, batch 10 D Loss: 1.3925724029541016, G Loss: 0.6768828630447388\n",
      "Epoch 41, batch 11 D Loss: 1.3574583530426025, G Loss: 0.6998785138130188\n",
      "Epoch 41, batch 12 D Loss: 1.3905503749847412, G Loss: 0.6751998066902161\n",
      "Epoch 41, batch 13 D Loss: 1.4188456535339355, G Loss: 0.685585618019104\n",
      "Epoch 41, batch 14 D Loss: 1.3813889026641846, G Loss: 0.6975314617156982\n",
      "Epoch 41, batch 15 D Loss: 1.3750901222229004, G Loss: 0.6862314343452454\n",
      "Epoch 41, batch 16 D Loss: 1.3946049213409424, G Loss: 0.6862860321998596\n",
      "Epoch 41, batch 17 D Loss: 1.3905773162841797, G Loss: 0.6811169981956482\n",
      "Epoch 41, batch 18 D Loss: 1.4033031463623047, G Loss: 0.6718196868896484\n",
      "Epoch 41, batch 19 D Loss: 1.4013420343399048, G Loss: 0.6930389404296875\n",
      "Epoch 41, batch 20 D Loss: 1.4074091911315918, G Loss: 0.6947702765464783\n",
      "Epoch 41, batch 21 D Loss: 1.409533977508545, G Loss: 0.6800290942192078\n",
      "Epoch 41, batch 22 D Loss: 1.4123250246047974, G Loss: 0.6826179623603821\n",
      "Epoch 41, batch 23 D Loss: 1.3899669647216797, G Loss: 0.6870743036270142\n",
      "Epoch 41, batch 24 D Loss: 1.3859007358551025, G Loss: 0.7072145342826843\n",
      "Epoch 41, batch 25 D Loss: 1.3956866264343262, G Loss: 0.6949394345283508\n",
      "Epoch 41, batch 26 D Loss: 1.3679051399230957, G Loss: 0.702833354473114\n",
      "Epoch 41, batch 27 D Loss: 1.4004491567611694, G Loss: 0.6798333525657654\n",
      "Epoch 41, batch 28 D Loss: 1.4027974605560303, G Loss: 0.6904855370521545\n",
      "Epoch 41, batch 29 D Loss: 1.3916709423065186, G Loss: 0.6924872398376465\n",
      "Epoch 41, batch 30 D Loss: 1.3903329372406006, G Loss: 0.6905106902122498\n",
      "Epoch 41, batch 31 D Loss: 1.424694538116455, G Loss: 0.6913939118385315\n",
      "Epoch 41, batch 32 D Loss: 1.3862707614898682, G Loss: 0.7091808915138245\n",
      "Epoch 41, batch 33 D Loss: 1.4213635921478271, G Loss: 0.6918432116508484\n",
      "Epoch 41, batch 34 D Loss: 1.3792362213134766, G Loss: 0.709105372428894\n",
      "Epoch 41, batch 35 D Loss: 1.4040234088897705, G Loss: 0.6980358958244324\n",
      "Epoch 41, batch 36 D Loss: 1.4159855842590332, G Loss: 0.6875153183937073\n",
      "Epoch 41, batch 37 D Loss: 1.379075050354004, G Loss: 0.7101982831954956\n",
      "Epoch 41, batch 38 D Loss: 1.4031667709350586, G Loss: 0.6963558793067932\n",
      "Epoch 41, batch 39 D Loss: 1.3994452953338623, G Loss: 0.6998847126960754\n",
      "Epoch 41, batch 40 D Loss: 1.408578634262085, G Loss: 0.7018224596977234\n",
      "Epoch 41, batch 41 D Loss: 1.4171360731124878, G Loss: 0.6896256804466248\n",
      "Epoch 41, batch 42 D Loss: 1.382253646850586, G Loss: 0.705544114112854\n",
      "Epoch 41, batch 43 D Loss: 1.4191977977752686, G Loss: 0.6900678873062134\n",
      "Epoch 41, batch 44 D Loss: 1.4091167449951172, G Loss: 0.6900317668914795\n",
      "Epoch 41, batch 45 D Loss: 1.4019994735717773, G Loss: 0.6938811540603638\n",
      "Epoch 41, batch 46 D Loss: 1.4024715423583984, G Loss: 0.7060767412185669\n",
      "Epoch 41, batch 47 D Loss: 1.3979692459106445, G Loss: 0.7059473395347595\n",
      "Epoch 41, batch 48 D Loss: 1.4193270206451416, G Loss: 0.6970584392547607\n",
      "Epoch 41, batch 49 D Loss: 1.394511103630066, G Loss: 0.7137386202812195\n",
      "Epoch 41, batch 50 D Loss: 1.3973608016967773, G Loss: 0.7041552662849426\n",
      "Epoch 41, batch 51 D Loss: 1.3972277641296387, G Loss: 0.7161603569984436\n",
      "Epoch 41, batch 52 D Loss: 1.4129338264465332, G Loss: 0.7011862993240356\n",
      "Epoch 41, batch 53 D Loss: 1.4045220613479614, G Loss: 0.7058271765708923\n",
      "Epoch 41, batch 54 D Loss: 1.4137110710144043, G Loss: 0.6983410120010376\n",
      "Epoch 41, batch 55 D Loss: 1.392157793045044, G Loss: 0.7042161822319031\n",
      "Epoch 41, batch 56 D Loss: 1.4103485345840454, G Loss: 0.7015464901924133\n",
      "Epoch 41, batch 57 D Loss: 1.3835227489471436, G Loss: 0.7189419269561768\n",
      "Epoch 41, batch 58 D Loss: 1.3912584781646729, G Loss: 0.7181860208511353\n",
      "Epoch 41, batch 59 D Loss: 1.3951215744018555, G Loss: 0.710938036441803\n",
      "Epoch 41, batch 60 D Loss: 1.4074621200561523, G Loss: 0.7072288393974304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, batch 61 D Loss: 1.3938841819763184, G Loss: 0.709530234336853\n",
      "Epoch 41, batch 62 D Loss: 1.3876031637191772, G Loss: 0.7242096066474915\n",
      "Epoch 41, batch 63 D Loss: 1.3842616081237793, G Loss: 0.7209171056747437\n",
      "Epoch 41, batch 64 D Loss: 1.385385513305664, G Loss: 0.7187315225601196\n",
      "Epoch 41, batch 65 D Loss: 1.3872926235198975, G Loss: 0.7204955816268921\n",
      "Epoch 41, batch 66 D Loss: 1.3932000398635864, G Loss: 0.7197630405426025\n",
      "Epoch 41, batch 67 D Loss: 1.3901138305664062, G Loss: 0.7188547253608704\n",
      "Epoch 41, batch 68 D Loss: 1.3979988098144531, G Loss: 0.7191838622093201\n",
      "Epoch 41, batch 69 D Loss: 1.3892121315002441, G Loss: 0.7255204916000366\n",
      "Epoch 41, batch 70 D Loss: 1.3967442512512207, G Loss: 0.7236257195472717\n",
      "Epoch 41, batch 71 D Loss: 1.3882641792297363, G Loss: 0.7220139503479004\n",
      "Epoch 41, batch 72 D Loss: 1.3884443044662476, G Loss: 0.721070408821106\n",
      "Epoch 41, batch 73 D Loss: 1.3827489614486694, G Loss: 0.7271823287010193\n",
      "Epoch 41, batch 74 D Loss: 1.3882161378860474, G Loss: 0.722724974155426\n",
      "Epoch 41, batch 75 D Loss: 1.3844107389450073, G Loss: 0.7324258685112\n",
      "Epoch 41, batch 76 D Loss: 1.3896634578704834, G Loss: 0.7227804064750671\n",
      "Epoch 41, batch 77 D Loss: 1.3875064849853516, G Loss: 0.7303346395492554\n",
      "Epoch 41, batch 78 D Loss: 1.3876184225082397, G Loss: 0.7287584543228149\n",
      "Epoch 41, batch 79 D Loss: 1.3828778266906738, G Loss: 0.7312628030776978\n",
      "Epoch 41, batch 80 D Loss: 1.3698389530181885, G Loss: 0.7415043115615845\n",
      "Epoch 41, batch 81 D Loss: 1.3799283504486084, G Loss: 0.7337714433670044\n",
      "Epoch 41, batch 82 D Loss: 1.39923095703125, G Loss: 0.7253278493881226\n",
      "Epoch 41, batch 83 D Loss: 1.3880879878997803, G Loss: 0.7326692342758179\n",
      "Epoch 41, batch 84 D Loss: 1.386278510093689, G Loss: 0.734809160232544\n",
      "Epoch 41, batch 85 D Loss: 1.3784841299057007, G Loss: 0.7349777221679688\n",
      "Epoch 41, batch 86 D Loss: 1.3827624320983887, G Loss: 0.7341352701187134\n",
      "Epoch 41, batch 87 D Loss: 1.3821113109588623, G Loss: 0.7344542145729065\n",
      "Epoch 41, batch 88 D Loss: 1.3745434284210205, G Loss: 0.7389984130859375\n",
      "Epoch 41, batch 89 D Loss: 1.3882628679275513, G Loss: 0.7307608723640442\n",
      "Epoch 41, batch 90 D Loss: 1.3551404476165771, G Loss: 0.7519550919532776\n",
      "Epoch 41, batch 91 D Loss: 1.3742456436157227, G Loss: 0.7372967600822449\n",
      "Epoch 41, batch 92 D Loss: 1.376460075378418, G Loss: 0.7428545951843262\n",
      "Epoch 41, batch 93 D Loss: 1.3662853240966797, G Loss: 0.7479404211044312\n",
      "Epoch 41, batch 94 D Loss: 1.3805609941482544, G Loss: 0.7300039529800415\n",
      "Epoch 41, batch 95 D Loss: 1.3841278553009033, G Loss: 0.7295523285865784\n",
      "Epoch 41, batch 96 D Loss: 1.3612678050994873, G Loss: 0.7532821893692017\n",
      "Epoch 41, batch 97 D Loss: 1.3697984218597412, G Loss: 0.7412611246109009\n",
      "Epoch 41, batch 98 D Loss: 1.366544485092163, G Loss: 0.75396329164505\n",
      "Epoch 41, batch 99 D Loss: 1.35091233253479, G Loss: 0.7588308453559875\n",
      "Epoch 41, batch 100 D Loss: 1.3572701215744019, G Loss: 0.7582396864891052\n",
      "Epoch 41, batch 101 D Loss: 1.3796725273132324, G Loss: 0.7296208143234253\n",
      "Epoch 41, batch 102 D Loss: 1.3680853843688965, G Loss: 0.7446278929710388\n",
      "Epoch 41, batch 103 D Loss: 1.3604326248168945, G Loss: 0.7632092833518982\n",
      "Epoch 41, batch 104 D Loss: 1.3867037296295166, G Loss: 0.7297384738922119\n",
      "Epoch 41, batch 105 D Loss: 1.3763585090637207, G Loss: 0.7539272904396057\n",
      "Epoch 41, batch 106 D Loss: 1.39015531539917, G Loss: 0.7280988097190857\n",
      "Epoch 41, batch 107 D Loss: 1.3874773979187012, G Loss: 0.7383165955543518\n",
      "Epoch 41, batch 108 D Loss: 1.3703222274780273, G Loss: 0.7677993774414062\n",
      "Epoch 41, batch 109 D Loss: 1.3521904945373535, G Loss: 0.7596326470375061\n",
      "Epoch 41, batch 110 D Loss: 1.3708548545837402, G Loss: 0.7546432018280029\n",
      "Epoch 41, batch 111 D Loss: 1.3634529113769531, G Loss: 0.7367139458656311\n",
      "Epoch 41, batch 112 D Loss: 1.3716633319854736, G Loss: 0.7406815886497498\n",
      "Epoch 41, batch 113 D Loss: 1.36906099319458, G Loss: 0.7567535638809204\n",
      "Epoch 41, batch 114 D Loss: 1.3480103015899658, G Loss: 0.7381328344345093\n",
      "Epoch 41, batch 115 D Loss: 1.356247901916504, G Loss: 0.7536454796791077\n",
      "Epoch 41, batch 116 D Loss: 1.3775254487991333, G Loss: 0.7303754687309265\n",
      "Epoch 41, batch 117 D Loss: 1.3762946128845215, G Loss: 0.7366732954978943\n",
      "Epoch 41, batch 118 D Loss: 1.3847031593322754, G Loss: 0.7271243333816528\n",
      "Epoch 41, batch 119 D Loss: 1.3897207975387573, G Loss: 0.7195273041725159\n",
      "Epoch 41, batch 120 D Loss: 1.3954992294311523, G Loss: 0.7274817824363708\n",
      "Epoch 41, batch 121 D Loss: 1.351479172706604, G Loss: 0.7671500444412231\n",
      "Epoch 41, batch 122 D Loss: 1.3667324781417847, G Loss: 0.7402331829071045\n",
      "Epoch 41, batch 123 D Loss: 1.375624656677246, G Loss: 0.6970605254173279\n",
      "Epoch 41, batch 124 D Loss: 1.3811705112457275, G Loss: 0.7216214537620544\n",
      "Epoch 41, batch 125 D Loss: 1.3847887516021729, G Loss: 0.726003885269165\n",
      "Epoch 41, batch 126 D Loss: 1.3491538763046265, G Loss: 0.7426785230636597\n",
      "Epoch 41, batch 127 D Loss: 1.406349778175354, G Loss: 0.7135694622993469\n",
      "Epoch 41, batch 128 D Loss: 1.3874722719192505, G Loss: 0.721286952495575\n",
      "Epoch 41, batch 129 D Loss: 1.3751695156097412, G Loss: 0.7052547931671143\n",
      "Epoch 41, batch 130 D Loss: 1.4089446067810059, G Loss: 0.7067939639091492\n",
      "Epoch 41, batch 131 D Loss: 1.385291337966919, G Loss: 0.7147712707519531\n",
      "Epoch 41, batch 132 D Loss: 1.3705039024353027, G Loss: 0.714832603931427\n",
      "Epoch 41, batch 133 D Loss: 1.3921535015106201, G Loss: 0.7115069627761841\n",
      "Epoch 41, batch 134 D Loss: 1.4031765460968018, G Loss: 0.6948186755180359\n",
      "Epoch 41, batch 135 D Loss: 1.3960782289505005, G Loss: 0.6978121399879456\n",
      "Epoch 41, batch 136 D Loss: 1.4031121730804443, G Loss: 0.6965356469154358\n",
      "Epoch 41, batch 137 D Loss: 1.3935558795928955, G Loss: 0.6889670491218567\n",
      "Epoch 41, batch 138 D Loss: 1.4148099422454834, G Loss: 0.6827353835105896\n",
      "Epoch 41, batch 139 D Loss: 1.4009678363800049, G Loss: 0.6902945041656494\n",
      "Epoch 41, batch 140 D Loss: 1.3784838914871216, G Loss: 0.6970544457435608\n",
      "Epoch 41, batch 141 D Loss: 1.3862881660461426, G Loss: 0.6924627423286438\n",
      "Epoch 41, batch 142 D Loss: 1.3745681047439575, G Loss: 0.6937214136123657\n",
      "Epoch 41, batch 143 D Loss: 1.3945105075836182, G Loss: 0.699207603931427\n",
      "Epoch 41, batch 144 D Loss: 1.3917280435562134, G Loss: 0.7034574151039124\n",
      "Epoch 41, batch 145 D Loss: 1.3732366561889648, G Loss: 0.6895299553871155\n",
      "Epoch 41, batch 146 D Loss: 1.3918085098266602, G Loss: 0.6868836879730225\n",
      "Epoch 41, batch 147 D Loss: 1.3802227973937988, G Loss: 0.6884661316871643\n",
      "Epoch 41, batch 148 D Loss: 1.3727209568023682, G Loss: 0.6886460185050964\n",
      "Epoch 41, batch 149 D Loss: 1.3910393714904785, G Loss: 0.6871060132980347\n",
      "Epoch 41, batch 150 D Loss: 1.386005163192749, G Loss: 0.6875528693199158\n",
      "Epoch 41, batch 151 D Loss: 1.404666781425476, G Loss: 0.6822918653488159\n",
      "Epoch 41, batch 152 D Loss: 1.364906907081604, G Loss: 0.6958208680152893\n",
      "Epoch 41, batch 153 D Loss: 1.397920846939087, G Loss: 0.6810043454170227\n",
      "Epoch 41, batch 154 D Loss: 1.3814311027526855, G Loss: 0.6915586590766907\n",
      "Epoch 41, batch 155 D Loss: 1.4095444679260254, G Loss: 0.6678807139396667\n",
      "Epoch 41, batch 156 D Loss: 1.4064241647720337, G Loss: 0.6720348596572876\n",
      "Epoch 41, batch 157 D Loss: 1.390608310699463, G Loss: 0.6828868985176086\n",
      "Epoch 41, batch 158 D Loss: 1.3864216804504395, G Loss: 0.6894586682319641\n",
      "Epoch 41, batch 159 D Loss: 1.4063808917999268, G Loss: 0.6677502989768982\n",
      "Epoch 41, batch 160 D Loss: 1.3797643184661865, G Loss: 0.6786695122718811\n",
      "Epoch 41, batch 161 D Loss: 1.3972489833831787, G Loss: 0.6700218319892883\n",
      "Epoch 41, batch 162 D Loss: 1.3910433053970337, G Loss: 0.6787387728691101\n",
      "Epoch 41, batch 163 D Loss: 1.3828487396240234, G Loss: 0.6820313930511475\n",
      "Epoch 41, batch 164 D Loss: 1.4102299213409424, G Loss: 0.6704232096672058\n",
      "Epoch 41, batch 165 D Loss: 1.4114261865615845, G Loss: 0.6647817492485046\n",
      "Epoch 41, batch 166 D Loss: 1.3851542472839355, G Loss: 0.6719592809677124\n",
      "Epoch 41, batch 167 D Loss: 1.388136625289917, G Loss: 0.6739146709442139\n",
      "Epoch 41, batch 168 D Loss: 1.3985991477966309, G Loss: 0.6776360869407654\n",
      "Epoch 41, batch 169 D Loss: 1.3901667594909668, G Loss: 0.679879367351532\n",
      "Epoch 41, batch 170 D Loss: 1.3778917789459229, G Loss: 0.6761676073074341\n",
      "Epoch 41, batch 171 D Loss: 1.413804531097412, G Loss: 0.6536399126052856\n",
      "Epoch 41, batch 172 D Loss: 1.4059197902679443, G Loss: 0.6672207713127136\n",
      "Epoch 41, batch 173 D Loss: 1.3958635330200195, G Loss: 0.6670687198638916\n",
      "Epoch 41, batch 174 D Loss: 1.3883047103881836, G Loss: 0.6732578277587891\n",
      "Epoch 41, batch 175 D Loss: 1.394139051437378, G Loss: 0.6681962609291077\n",
      "Epoch 41, batch 176 D Loss: 1.404866337776184, G Loss: 0.6670752167701721\n",
      "Epoch 41, batch 177 D Loss: 1.3912293910980225, G Loss: 0.6747800707817078\n",
      "Epoch 41, batch 178 D Loss: 1.4069287776947021, G Loss: 0.6683042049407959\n",
      "Epoch 41, batch 179 D Loss: 1.4101841449737549, G Loss: 0.6698700189590454\n",
      "Epoch 41, batch 180 D Loss: 1.3891267776489258, G Loss: 0.6815334558486938\n",
      "Epoch 41, batch 181 D Loss: 1.3933302164077759, G Loss: 0.6698451042175293\n",
      "Epoch 41, batch 182 D Loss: 1.4008457660675049, G Loss: 0.6718971133232117\n",
      "Epoch 41, batch 183 D Loss: 1.3934526443481445, G Loss: 0.6795665621757507\n",
      "Epoch 41, batch 184 D Loss: 1.418280839920044, G Loss: 0.6578559279441833\n",
      "Epoch 41, batch 185 D Loss: 1.3829898834228516, G Loss: 0.6838662028312683\n",
      "Epoch 41, batch 186 D Loss: 1.3977715969085693, G Loss: 0.668541431427002\n",
      "Epoch 41, batch 187 D Loss: 1.3969776630401611, G Loss: 0.6774356961250305\n",
      "Epoch 41, batch 188 D Loss: 1.377504825592041, G Loss: 0.6808398962020874\n",
      "Epoch 41, batch 189 D Loss: 1.3904900550842285, G Loss: 0.6801263689994812\n",
      "Epoch 41, batch 190 D Loss: 1.3946788311004639, G Loss: 0.668887197971344\n",
      "Epoch 41, batch 191 D Loss: 1.405982494354248, G Loss: 0.6633079648017883\n",
      "Epoch 41, batch 192 D Loss: 1.3865647315979004, G Loss: 0.6893953680992126\n",
      "Epoch 41, batch 193 D Loss: 1.3925435543060303, G Loss: 0.6739058494567871\n",
      "Epoch 41, batch 194 D Loss: 1.3863266706466675, G Loss: 0.6875075697898865\n",
      "Epoch 41, batch 195 D Loss: 1.3885326385498047, G Loss: 0.6793203949928284\n",
      "Epoch 41, batch 196 D Loss: 1.3882639408111572, G Loss: 0.6824504137039185\n",
      "Epoch 41, batch 197 D Loss: 1.3979703187942505, G Loss: 0.6838564872741699\n",
      "Epoch 41, batch 198 D Loss: 1.4037692546844482, G Loss: 0.6856874823570251\n",
      "Epoch 41, batch 199 D Loss: 1.385136365890503, G Loss: 0.6909514665603638\n",
      "Epoch 41, batch 200 D Loss: 1.3823904991149902, G Loss: 0.6844356656074524\n",
      "Epoch 42, batch 1 D Loss: 1.3989589214324951, G Loss: 0.6773736476898193\n",
      "Epoch 42, batch 2 D Loss: 1.3969347476959229, G Loss: 0.6825438141822815\n",
      "Epoch 42, batch 3 D Loss: 1.381340742111206, G Loss: 0.6898298859596252\n",
      "Epoch 42, batch 4 D Loss: 1.3897883892059326, G Loss: 0.6902978420257568\n",
      "Epoch 42, batch 5 D Loss: 1.3985193967819214, G Loss: 0.6763702630996704\n",
      "Epoch 42, batch 6 D Loss: 1.390010118484497, G Loss: 0.6841385364532471\n",
      "Epoch 42, batch 7 D Loss: 1.3940143585205078, G Loss: 0.6786532402038574\n",
      "Epoch 42, batch 8 D Loss: 1.3872714042663574, G Loss: 0.6853283643722534\n",
      "Epoch 42, batch 9 D Loss: 1.3837306499481201, G Loss: 0.6896511912345886\n",
      "Epoch 42, batch 10 D Loss: 1.377036213874817, G Loss: 0.6907332539558411\n",
      "Epoch 42, batch 11 D Loss: 1.384439468383789, G Loss: 0.689075767993927\n",
      "Epoch 42, batch 12 D Loss: 1.3912618160247803, G Loss: 0.691968560218811\n",
      "Epoch 42, batch 13 D Loss: 1.382915735244751, G Loss: 0.697084903717041\n",
      "Epoch 42, batch 14 D Loss: 1.3902446031570435, G Loss: 0.6901841759681702\n",
      "Epoch 42, batch 15 D Loss: 1.378147840499878, G Loss: 0.6989212036132812\n",
      "Epoch 42, batch 16 D Loss: 1.373767614364624, G Loss: 0.6925380229949951\n",
      "Epoch 42, batch 17 D Loss: 1.3897504806518555, G Loss: 0.6844752430915833\n",
      "Epoch 42, batch 18 D Loss: 1.3906373977661133, G Loss: 0.6829177737236023\n",
      "Epoch 42, batch 19 D Loss: 1.3760311603546143, G Loss: 0.6924216747283936\n",
      "Epoch 42, batch 20 D Loss: 1.3916263580322266, G Loss: 0.7035596370697021\n",
      "Epoch 42, batch 21 D Loss: 1.3867714405059814, G Loss: 0.6973913311958313\n",
      "Epoch 42, batch 22 D Loss: 1.3764171600341797, G Loss: 0.6975624561309814\n",
      "Epoch 42, batch 23 D Loss: 1.3838034868240356, G Loss: 0.689369261264801\n",
      "Epoch 42, batch 24 D Loss: 1.4024107456207275, G Loss: 0.6761575937271118\n",
      "Epoch 42, batch 25 D Loss: 1.3894617557525635, G Loss: 0.6845399737358093\n",
      "Epoch 42, batch 26 D Loss: 1.3802132606506348, G Loss: 0.7026873230934143\n",
      "Epoch 42, batch 27 D Loss: 1.3835747241973877, G Loss: 0.6950315237045288\n",
      "Epoch 42, batch 28 D Loss: 1.3753635883331299, G Loss: 0.6987622976303101\n",
      "Epoch 42, batch 29 D Loss: 1.3858356475830078, G Loss: 0.6877529621124268\n",
      "Epoch 42, batch 30 D Loss: 1.3612135648727417, G Loss: 0.7109121680259705\n",
      "Epoch 42, batch 31 D Loss: 1.3779908418655396, G Loss: 0.6898159980773926\n",
      "Epoch 42, batch 32 D Loss: 1.379932165145874, G Loss: 0.6908906698226929\n",
      "Epoch 42, batch 33 D Loss: 1.3720879554748535, G Loss: 0.6987084150314331\n",
      "Epoch 42, batch 34 D Loss: 1.3729116916656494, G Loss: 0.7084219455718994\n",
      "Epoch 42, batch 35 D Loss: 1.3697707653045654, G Loss: 0.6981140375137329\n",
      "Epoch 42, batch 36 D Loss: 1.3875372409820557, G Loss: 0.6955031752586365\n",
      "Epoch 42, batch 37 D Loss: 1.3725907802581787, G Loss: 0.7031113505363464\n",
      "Epoch 42, batch 38 D Loss: 1.3687965869903564, G Loss: 0.6904465556144714\n",
      "Epoch 42, batch 39 D Loss: 1.3696272373199463, G Loss: 0.7019405961036682\n",
      "Epoch 42, batch 40 D Loss: 1.3916609287261963, G Loss: 0.6910513043403625\n",
      "Epoch 42, batch 41 D Loss: 1.3759257793426514, G Loss: 0.6911953687667847\n",
      "Epoch 42, batch 42 D Loss: 1.3652745485305786, G Loss: 0.704864501953125\n",
      "Epoch 42, batch 43 D Loss: 1.369927167892456, G Loss: 0.7003642916679382\n",
      "Epoch 42, batch 44 D Loss: 1.3585724830627441, G Loss: 0.710768461227417\n",
      "Epoch 42, batch 45 D Loss: 1.38020658493042, G Loss: 0.7027386426925659\n",
      "Epoch 42, batch 46 D Loss: 1.3814632892608643, G Loss: 0.7044721841812134\n",
      "Epoch 42, batch 47 D Loss: 1.3683300018310547, G Loss: 0.7054339051246643\n",
      "Epoch 42, batch 48 D Loss: 1.3785762786865234, G Loss: 0.709675133228302\n",
      "Epoch 42, batch 49 D Loss: 1.3829553127288818, G Loss: 0.7017365097999573\n",
      "Epoch 42, batch 50 D Loss: 1.3794360160827637, G Loss: 0.6929680109024048\n",
      "Epoch 42, batch 51 D Loss: 1.3610361814498901, G Loss: 0.7081964015960693\n",
      "Epoch 42, batch 52 D Loss: 1.3599309921264648, G Loss: 0.7020407319068909\n",
      "Epoch 42, batch 53 D Loss: 1.3811969757080078, G Loss: 0.697411060333252\n",
      "Epoch 42, batch 54 D Loss: 1.357335090637207, G Loss: 0.7167715430259705\n",
      "Epoch 42, batch 55 D Loss: 1.363389015197754, G Loss: 0.7035132646560669\n",
      "Epoch 42, batch 56 D Loss: 1.3729522228240967, G Loss: 0.6889112591743469\n",
      "Epoch 42, batch 57 D Loss: 1.3811566829681396, G Loss: 0.6868335008621216\n",
      "Epoch 42, batch 58 D Loss: 1.364537239074707, G Loss: 0.6965907216072083\n",
      "Epoch 42, batch 59 D Loss: 1.366150140762329, G Loss: 0.6959757804870605\n",
      "Epoch 42, batch 60 D Loss: 1.3778886795043945, G Loss: 0.6972728967666626\n",
      "Epoch 42, batch 61 D Loss: 1.373071551322937, G Loss: 0.7090826630592346\n",
      "Epoch 42, batch 62 D Loss: 1.3677181005477905, G Loss: 0.7050941586494446\n",
      "Epoch 42, batch 63 D Loss: 1.3698649406433105, G Loss: 0.7037151455879211\n",
      "Epoch 42, batch 64 D Loss: 1.3612996339797974, G Loss: 0.6972827315330505\n",
      "Epoch 42, batch 65 D Loss: 1.3634867668151855, G Loss: 0.712425708770752\n",
      "Epoch 42, batch 66 D Loss: 1.38594388961792, G Loss: 0.6783027052879333\n",
      "Epoch 42, batch 67 D Loss: 1.3765265941619873, G Loss: 0.6895495653152466\n",
      "Epoch 42, batch 68 D Loss: 1.3796067237854004, G Loss: 0.6891713738441467\n",
      "Epoch 42, batch 69 D Loss: 1.3987784385681152, G Loss: 0.6650545597076416\n",
      "Epoch 42, batch 70 D Loss: 1.3717358112335205, G Loss: 0.7043845653533936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, batch 71 D Loss: 1.3998606204986572, G Loss: 0.6778133511543274\n",
      "Epoch 42, batch 72 D Loss: 1.3780345916748047, G Loss: 0.6855593919754028\n",
      "Epoch 42, batch 73 D Loss: 1.3845300674438477, G Loss: 0.7009416818618774\n",
      "Epoch 42, batch 74 D Loss: 1.368398904800415, G Loss: 0.7068688869476318\n",
      "Epoch 42, batch 75 D Loss: 1.406001329421997, G Loss: 0.673602283000946\n",
      "Epoch 42, batch 76 D Loss: 1.3830068111419678, G Loss: 0.696376621723175\n",
      "Epoch 42, batch 77 D Loss: 1.3938912153244019, G Loss: 0.688534677028656\n",
      "Epoch 42, batch 78 D Loss: 1.396803617477417, G Loss: 0.6964715719223022\n",
      "Epoch 42, batch 79 D Loss: 1.395989179611206, G Loss: 0.6853981614112854\n",
      "Epoch 42, batch 80 D Loss: 1.382720947265625, G Loss: 0.6945142149925232\n",
      "Epoch 42, batch 81 D Loss: 1.3856472969055176, G Loss: 0.68822181224823\n",
      "Epoch 42, batch 82 D Loss: 1.4233875274658203, G Loss: 0.6635312438011169\n",
      "Epoch 42, batch 83 D Loss: 1.396028757095337, G Loss: 0.6819363236427307\n",
      "Epoch 42, batch 84 D Loss: 1.3815970420837402, G Loss: 0.6967009902000427\n",
      "Epoch 42, batch 85 D Loss: 1.3965299129486084, G Loss: 0.6940163373947144\n",
      "Epoch 42, batch 86 D Loss: 1.3814072608947754, G Loss: 0.673572838306427\n",
      "Epoch 42, batch 87 D Loss: 1.3786554336547852, G Loss: 0.6937476396560669\n",
      "Epoch 42, batch 88 D Loss: 1.373600721359253, G Loss: 0.6856520771980286\n",
      "Epoch 42, batch 89 D Loss: 1.403594970703125, G Loss: 0.6842024326324463\n",
      "Epoch 42, batch 90 D Loss: 1.4069445133209229, G Loss: 0.6984968781471252\n",
      "Epoch 42, batch 91 D Loss: 1.3891944885253906, G Loss: 0.6939581036567688\n",
      "Epoch 42, batch 92 D Loss: 1.3906278610229492, G Loss: 0.6871401071548462\n",
      "Epoch 42, batch 93 D Loss: 1.3885635137557983, G Loss: 0.6928237676620483\n",
      "Epoch 42, batch 94 D Loss: 1.4057538509368896, G Loss: 0.683387279510498\n",
      "Epoch 42, batch 95 D Loss: 1.4243659973144531, G Loss: 0.685336709022522\n",
      "Epoch 42, batch 96 D Loss: 1.3973946571350098, G Loss: 0.6942418813705444\n",
      "Epoch 42, batch 97 D Loss: 1.4056884050369263, G Loss: 0.6938591003417969\n",
      "Epoch 42, batch 98 D Loss: 1.3907577991485596, G Loss: 0.7011808156967163\n",
      "Epoch 42, batch 99 D Loss: 1.394503116607666, G Loss: 0.7000946998596191\n",
      "Epoch 42, batch 100 D Loss: 1.3713641166687012, G Loss: 0.6921054720878601\n",
      "Epoch 42, batch 101 D Loss: 1.3927583694458008, G Loss: 0.6942988038063049\n",
      "Epoch 42, batch 102 D Loss: 1.3860621452331543, G Loss: 0.6976668834686279\n",
      "Epoch 42, batch 103 D Loss: 1.4064698219299316, G Loss: 0.6813134551048279\n",
      "Epoch 42, batch 104 D Loss: 1.4008493423461914, G Loss: 0.6940872669219971\n",
      "Epoch 42, batch 105 D Loss: 1.4027433395385742, G Loss: 0.6996327042579651\n",
      "Epoch 42, batch 106 D Loss: 1.4323660135269165, G Loss: 0.6654170155525208\n",
      "Epoch 42, batch 107 D Loss: 1.3995425701141357, G Loss: 0.6934021711349487\n",
      "Epoch 42, batch 108 D Loss: 1.3900678157806396, G Loss: 0.6895451545715332\n",
      "Epoch 42, batch 109 D Loss: 1.4108288288116455, G Loss: 0.6861425638198853\n",
      "Epoch 42, batch 110 D Loss: 1.4129207134246826, G Loss: 0.6827261447906494\n",
      "Epoch 42, batch 111 D Loss: 1.4220902919769287, G Loss: 0.6879317760467529\n",
      "Epoch 42, batch 112 D Loss: 1.3845853805541992, G Loss: 0.6896087527275085\n",
      "Epoch 42, batch 113 D Loss: 1.387356162071228, G Loss: 0.6969034075737\n",
      "Epoch 42, batch 114 D Loss: 1.4169402122497559, G Loss: 0.6765463948249817\n",
      "Epoch 42, batch 115 D Loss: 1.4380695819854736, G Loss: 0.6715041995048523\n",
      "Epoch 42, batch 116 D Loss: 1.4078489542007446, G Loss: 0.6810384392738342\n",
      "Epoch 42, batch 117 D Loss: 1.3988990783691406, G Loss: 0.6901329159736633\n",
      "Epoch 42, batch 118 D Loss: 1.4075305461883545, G Loss: 0.6879650354385376\n",
      "Epoch 42, batch 119 D Loss: 1.4053232669830322, G Loss: 0.6838456988334656\n",
      "Epoch 42, batch 120 D Loss: 1.4083483219146729, G Loss: 0.6825946569442749\n",
      "Epoch 42, batch 121 D Loss: 1.3992317914962769, G Loss: 0.689532458782196\n",
      "Epoch 42, batch 122 D Loss: 1.4017412662506104, G Loss: 0.6910284161567688\n",
      "Epoch 42, batch 123 D Loss: 1.394280195236206, G Loss: 0.6930431127548218\n",
      "Epoch 42, batch 124 D Loss: 1.4036661386489868, G Loss: 0.6924678683280945\n",
      "Epoch 42, batch 125 D Loss: 1.397153615951538, G Loss: 0.69908607006073\n",
      "Epoch 42, batch 126 D Loss: 1.3872754573822021, G Loss: 0.7059780955314636\n",
      "Epoch 42, batch 127 D Loss: 1.4044954776763916, G Loss: 0.6938669085502625\n",
      "Epoch 42, batch 128 D Loss: 1.395552396774292, G Loss: 0.7007611989974976\n",
      "Epoch 42, batch 129 D Loss: 1.3927557468414307, G Loss: 0.6981781125068665\n",
      "Epoch 42, batch 130 D Loss: 1.4020345211029053, G Loss: 0.7032814025878906\n",
      "Epoch 42, batch 131 D Loss: 1.3989061117172241, G Loss: 0.6955068707466125\n",
      "Epoch 42, batch 132 D Loss: 1.3997172117233276, G Loss: 0.6966020464897156\n",
      "Epoch 42, batch 133 D Loss: 1.3908181190490723, G Loss: 0.7056165933609009\n",
      "Epoch 42, batch 134 D Loss: 1.4037693738937378, G Loss: 0.7042486667633057\n",
      "Epoch 42, batch 135 D Loss: 1.3946017026901245, G Loss: 0.6979238986968994\n",
      "Epoch 42, batch 136 D Loss: 1.3968451023101807, G Loss: 0.7019099593162537\n",
      "Epoch 42, batch 137 D Loss: 1.404134750366211, G Loss: 0.7002031803131104\n",
      "Epoch 42, batch 138 D Loss: 1.397707462310791, G Loss: 0.7016469836235046\n",
      "Epoch 42, batch 139 D Loss: 1.388141393661499, G Loss: 0.7040925621986389\n",
      "Epoch 42, batch 140 D Loss: 1.3820855617523193, G Loss: 0.7081354260444641\n",
      "Epoch 42, batch 141 D Loss: 1.3856441974639893, G Loss: 0.7064059376716614\n",
      "Epoch 42, batch 142 D Loss: 1.3976107835769653, G Loss: 0.700676679611206\n",
      "Epoch 42, batch 143 D Loss: 1.3920857906341553, G Loss: 0.703339159488678\n",
      "Epoch 42, batch 144 D Loss: 1.3931976556777954, G Loss: 0.707627534866333\n",
      "Epoch 42, batch 145 D Loss: 1.3820371627807617, G Loss: 0.7105562090873718\n",
      "Epoch 42, batch 146 D Loss: 1.3820600509643555, G Loss: 0.7112596035003662\n",
      "Epoch 42, batch 147 D Loss: 1.3959896564483643, G Loss: 0.709505558013916\n",
      "Epoch 42, batch 148 D Loss: 1.3903782367706299, G Loss: 0.7043136358261108\n",
      "Epoch 42, batch 149 D Loss: 1.3923563957214355, G Loss: 0.7035418748855591\n",
      "Epoch 42, batch 150 D Loss: 1.3934142589569092, G Loss: 0.7035456299781799\n",
      "Epoch 42, batch 151 D Loss: 1.3889052867889404, G Loss: 0.7120465636253357\n",
      "Epoch 42, batch 152 D Loss: 1.3845101594924927, G Loss: 0.7097040414810181\n",
      "Epoch 42, batch 153 D Loss: 1.3796899318695068, G Loss: 0.7142890691757202\n",
      "Epoch 42, batch 154 D Loss: 1.3781378269195557, G Loss: 0.7115886807441711\n",
      "Epoch 42, batch 155 D Loss: 1.3852248191833496, G Loss: 0.7169070243835449\n",
      "Epoch 42, batch 156 D Loss: 1.3777482509613037, G Loss: 0.7101143598556519\n",
      "Epoch 42, batch 157 D Loss: 1.3861443996429443, G Loss: 0.7099701166152954\n",
      "Epoch 42, batch 158 D Loss: 1.37373685836792, G Loss: 0.7140363454818726\n",
      "Epoch 42, batch 159 D Loss: 1.3796067237854004, G Loss: 0.7159445285797119\n",
      "Epoch 42, batch 160 D Loss: 1.3846817016601562, G Loss: 0.7130343914031982\n",
      "Epoch 42, batch 161 D Loss: 1.3841298818588257, G Loss: 0.7125549912452698\n",
      "Epoch 42, batch 162 D Loss: 1.389012336730957, G Loss: 0.7081639170646667\n",
      "Epoch 42, batch 163 D Loss: 1.3832213878631592, G Loss: 0.7157633900642395\n",
      "Epoch 42, batch 164 D Loss: 1.3782343864440918, G Loss: 0.7207048535346985\n",
      "Epoch 42, batch 165 D Loss: 1.3752610683441162, G Loss: 0.7167186737060547\n",
      "Epoch 42, batch 166 D Loss: 1.386282205581665, G Loss: 0.7138023376464844\n",
      "Epoch 42, batch 167 D Loss: 1.3786128759384155, G Loss: 0.7175902724266052\n",
      "Epoch 42, batch 168 D Loss: 1.3779499530792236, G Loss: 0.7173663377761841\n",
      "Epoch 42, batch 169 D Loss: 1.3764424324035645, G Loss: 0.7215522527694702\n",
      "Epoch 42, batch 170 D Loss: 1.3683421611785889, G Loss: 0.7230589389801025\n",
      "Epoch 42, batch 171 D Loss: 1.3836228847503662, G Loss: 0.7111096382141113\n",
      "Epoch 42, batch 172 D Loss: 1.374121904373169, G Loss: 0.719138503074646\n",
      "Epoch 42, batch 173 D Loss: 1.3749065399169922, G Loss: 0.7226254343986511\n",
      "Epoch 42, batch 174 D Loss: 1.3800458908081055, G Loss: 0.7119810581207275\n",
      "Epoch 42, batch 175 D Loss: 1.365851879119873, G Loss: 0.7188394665718079\n",
      "Epoch 42, batch 176 D Loss: 1.3807421922683716, G Loss: 0.7180633544921875\n",
      "Epoch 42, batch 177 D Loss: 1.3743948936462402, G Loss: 0.7137911319732666\n",
      "Epoch 42, batch 178 D Loss: 1.3823127746582031, G Loss: 0.7097617983818054\n",
      "Epoch 42, batch 179 D Loss: 1.3739125728607178, G Loss: 0.7222242951393127\n",
      "Epoch 42, batch 180 D Loss: 1.3790230751037598, G Loss: 0.7173395752906799\n",
      "Epoch 42, batch 181 D Loss: 1.3772356510162354, G Loss: 0.710182249546051\n",
      "Epoch 42, batch 182 D Loss: 1.374173879623413, G Loss: 0.7277417182922363\n",
      "Epoch 42, batch 183 D Loss: 1.3741354942321777, G Loss: 0.7161290645599365\n",
      "Epoch 42, batch 184 D Loss: 1.3857040405273438, G Loss: 0.7057265639305115\n",
      "Epoch 42, batch 185 D Loss: 1.3819732666015625, G Loss: 0.7121231555938721\n",
      "Epoch 42, batch 186 D Loss: 1.36918306350708, G Loss: 0.7208061814308167\n",
      "Epoch 42, batch 187 D Loss: 1.377012014389038, G Loss: 0.7225286960601807\n",
      "Epoch 42, batch 188 D Loss: 1.3668503761291504, G Loss: 0.7176202535629272\n",
      "Epoch 42, batch 189 D Loss: 1.3716484308242798, G Loss: 0.7242000699043274\n",
      "Epoch 42, batch 190 D Loss: 1.37198805809021, G Loss: 0.7116841673851013\n",
      "Epoch 42, batch 191 D Loss: 1.3744146823883057, G Loss: 0.7142641544342041\n",
      "Epoch 42, batch 192 D Loss: 1.3572595119476318, G Loss: 0.7216331958770752\n",
      "Epoch 42, batch 193 D Loss: 1.3786540031433105, G Loss: 0.7119567394256592\n",
      "Epoch 42, batch 194 D Loss: 1.3589611053466797, G Loss: 0.7208265066146851\n",
      "Epoch 42, batch 195 D Loss: 1.3777241706848145, G Loss: 0.7141819596290588\n",
      "Epoch 42, batch 196 D Loss: 1.3658283948898315, G Loss: 0.7283199429512024\n",
      "Epoch 42, batch 197 D Loss: 1.365822434425354, G Loss: 0.7198392748832703\n",
      "Epoch 42, batch 198 D Loss: 1.348535180091858, G Loss: 0.7194772362709045\n",
      "Epoch 42, batch 199 D Loss: 1.3526800870895386, G Loss: 0.7268941402435303\n",
      "Epoch 42, batch 200 D Loss: 1.3731322288513184, G Loss: 0.7141510844230652\n",
      "Epoch 43, batch 1 D Loss: 1.352329969406128, G Loss: 0.7361721992492676\n",
      "Epoch 43, batch 2 D Loss: 1.360026240348816, G Loss: 0.7231898307800293\n",
      "Epoch 43, batch 3 D Loss: 1.3619965314865112, G Loss: 0.7197431921958923\n",
      "Epoch 43, batch 4 D Loss: 1.3760359287261963, G Loss: 0.7037490606307983\n",
      "Epoch 43, batch 5 D Loss: 1.3617923259735107, G Loss: 0.7273943424224854\n",
      "Epoch 43, batch 6 D Loss: 1.3726472854614258, G Loss: 0.7221735119819641\n",
      "Epoch 43, batch 7 D Loss: 1.3872195482254028, G Loss: 0.7144537568092346\n",
      "Epoch 43, batch 8 D Loss: 1.3615522384643555, G Loss: 0.7136046886444092\n",
      "Epoch 43, batch 9 D Loss: 1.3426889181137085, G Loss: 0.7345693707466125\n",
      "Epoch 43, batch 10 D Loss: 1.3784897327423096, G Loss: 0.7089582681655884\n",
      "Epoch 43, batch 11 D Loss: 1.3871841430664062, G Loss: 0.7065060138702393\n",
      "Epoch 43, batch 12 D Loss: 1.3526229858398438, G Loss: 0.7261199951171875\n",
      "Epoch 43, batch 13 D Loss: 1.3797199726104736, G Loss: 0.7018473744392395\n",
      "Epoch 43, batch 14 D Loss: 1.3590935468673706, G Loss: 0.7124001383781433\n",
      "Epoch 43, batch 15 D Loss: 1.3834381103515625, G Loss: 0.6956028938293457\n",
      "Epoch 43, batch 16 D Loss: 1.3755569458007812, G Loss: 0.7149312496185303\n",
      "Epoch 43, batch 17 D Loss: 1.3924756050109863, G Loss: 0.6972160339355469\n",
      "Epoch 43, batch 18 D Loss: 1.3868566751480103, G Loss: 0.7010644674301147\n",
      "Epoch 43, batch 19 D Loss: 1.3607399463653564, G Loss: 0.7127212285995483\n",
      "Epoch 43, batch 20 D Loss: 1.3715702295303345, G Loss: 0.6978906989097595\n",
      "Epoch 43, batch 21 D Loss: 1.373128890991211, G Loss: 0.7037051320075989\n",
      "Epoch 43, batch 22 D Loss: 1.3687620162963867, G Loss: 0.7025592923164368\n",
      "Epoch 43, batch 23 D Loss: 1.4029595851898193, G Loss: 0.6876201033592224\n",
      "Epoch 43, batch 24 D Loss: 1.3644689321517944, G Loss: 0.6947140693664551\n",
      "Epoch 43, batch 25 D Loss: 1.3775839805603027, G Loss: 0.7076705694198608\n",
      "Epoch 43, batch 26 D Loss: 1.3788511753082275, G Loss: 0.7045426964759827\n",
      "Epoch 43, batch 27 D Loss: 1.3739643096923828, G Loss: 0.7086840867996216\n",
      "Epoch 43, batch 28 D Loss: 1.393934965133667, G Loss: 0.6899182796478271\n",
      "Epoch 43, batch 29 D Loss: 1.3678901195526123, G Loss: 0.7045818567276001\n",
      "Epoch 43, batch 30 D Loss: 1.3700367212295532, G Loss: 0.7092231512069702\n",
      "Epoch 43, batch 31 D Loss: 1.3803738355636597, G Loss: 0.6830103397369385\n",
      "Epoch 43, batch 32 D Loss: 1.3925411701202393, G Loss: 0.6954732537269592\n",
      "Epoch 43, batch 33 D Loss: 1.3569061756134033, G Loss: 0.70176100730896\n",
      "Epoch 43, batch 34 D Loss: 1.382028341293335, G Loss: 0.6858447194099426\n",
      "Epoch 43, batch 35 D Loss: 1.406863808631897, G Loss: 0.6770488619804382\n",
      "Epoch 43, batch 36 D Loss: 1.38205885887146, G Loss: 0.6871215105056763\n",
      "Epoch 43, batch 37 D Loss: 1.3792405128479004, G Loss: 0.683637797832489\n",
      "Epoch 43, batch 38 D Loss: 1.382893443107605, G Loss: 0.6781353950500488\n",
      "Epoch 43, batch 39 D Loss: 1.3834201097488403, G Loss: 0.693912148475647\n",
      "Epoch 43, batch 40 D Loss: 1.3920867443084717, G Loss: 0.6679765582084656\n",
      "Epoch 43, batch 41 D Loss: 1.3866500854492188, G Loss: 0.6748151183128357\n",
      "Epoch 43, batch 42 D Loss: 1.4001524448394775, G Loss: 0.6762633323669434\n",
      "Epoch 43, batch 43 D Loss: 1.3860419988632202, G Loss: 0.6890105605125427\n",
      "Epoch 43, batch 44 D Loss: 1.412785530090332, G Loss: 0.6704663038253784\n",
      "Epoch 43, batch 45 D Loss: 1.4187462329864502, G Loss: 0.6630451679229736\n",
      "Epoch 43, batch 46 D Loss: 1.4194588661193848, G Loss: 0.6500601172447205\n",
      "Epoch 43, batch 47 D Loss: 1.3870360851287842, G Loss: 0.6740114688873291\n",
      "Epoch 43, batch 48 D Loss: 1.3994189500808716, G Loss: 0.6698872447013855\n",
      "Epoch 43, batch 49 D Loss: 1.381873607635498, G Loss: 0.6723888516426086\n",
      "Epoch 43, batch 50 D Loss: 1.3973729610443115, G Loss: 0.6644270420074463\n",
      "Epoch 43, batch 51 D Loss: 1.3904632329940796, G Loss: 0.6815829277038574\n",
      "Epoch 43, batch 52 D Loss: 1.3761086463928223, G Loss: 0.6774287223815918\n",
      "Epoch 43, batch 53 D Loss: 1.4100852012634277, G Loss: 0.6663681268692017\n",
      "Epoch 43, batch 54 D Loss: 1.4157843589782715, G Loss: 0.6608905792236328\n",
      "Epoch 43, batch 55 D Loss: 1.417364239692688, G Loss: 0.6654053330421448\n",
      "Epoch 43, batch 56 D Loss: 1.4038279056549072, G Loss: 0.6610291004180908\n",
      "Epoch 43, batch 57 D Loss: 1.4090814590454102, G Loss: 0.6673184037208557\n",
      "Epoch 43, batch 58 D Loss: 1.4142413139343262, G Loss: 0.6581748723983765\n",
      "Epoch 43, batch 59 D Loss: 1.3983392715454102, G Loss: 0.6575286984443665\n",
      "Epoch 43, batch 60 D Loss: 1.4155586957931519, G Loss: 0.6704494953155518\n",
      "Epoch 43, batch 61 D Loss: 1.4136539697647095, G Loss: 0.6603931188583374\n",
      "Epoch 43, batch 62 D Loss: 1.4063193798065186, G Loss: 0.657905101776123\n",
      "Epoch 43, batch 63 D Loss: 1.3961937427520752, G Loss: 0.6701174378395081\n",
      "Epoch 43, batch 64 D Loss: 1.3899636268615723, G Loss: 0.6681582927703857\n",
      "Epoch 43, batch 65 D Loss: 1.3899896144866943, G Loss: 0.6703251600265503\n",
      "Epoch 43, batch 66 D Loss: 1.4085500240325928, G Loss: 0.6609443426132202\n",
      "Epoch 43, batch 67 D Loss: 1.3990658521652222, G Loss: 0.6611402034759521\n",
      "Epoch 43, batch 68 D Loss: 1.4196012020111084, G Loss: 0.6445358395576477\n",
      "Epoch 43, batch 69 D Loss: 1.4106321334838867, G Loss: 0.6594337224960327\n",
      "Epoch 43, batch 70 D Loss: 1.4176461696624756, G Loss: 0.6466366052627563\n",
      "Epoch 43, batch 71 D Loss: 1.4027533531188965, G Loss: 0.663479208946228\n",
      "Epoch 43, batch 72 D Loss: 1.4102648496627808, G Loss: 0.6636241674423218\n",
      "Epoch 43, batch 73 D Loss: 1.3880574703216553, G Loss: 0.6761013865470886\n",
      "Epoch 43, batch 74 D Loss: 1.4231362342834473, G Loss: 0.6522272229194641\n",
      "Epoch 43, batch 75 D Loss: 1.3884711265563965, G Loss: 0.6731646060943604\n",
      "Epoch 43, batch 76 D Loss: 1.4222514629364014, G Loss: 0.659224271774292\n",
      "Epoch 43, batch 77 D Loss: 1.4088760614395142, G Loss: 0.6628973484039307\n",
      "Epoch 43, batch 78 D Loss: 1.3985344171524048, G Loss: 0.6641832590103149\n",
      "Epoch 43, batch 79 D Loss: 1.3904696702957153, G Loss: 0.6678486466407776\n",
      "Epoch 43, batch 80 D Loss: 1.4195046424865723, G Loss: 0.6585975885391235\n",
      "Epoch 43, batch 81 D Loss: 1.403510332107544, G Loss: 0.6715996265411377\n",
      "Epoch 43, batch 82 D Loss: 1.411480188369751, G Loss: 0.6653631329536438\n",
      "Epoch 43, batch 83 D Loss: 1.3939592838287354, G Loss: 0.6770142912864685\n",
      "Epoch 43, batch 84 D Loss: 1.402248501777649, G Loss: 0.6712040901184082\n",
      "Epoch 43, batch 85 D Loss: 1.4128941297531128, G Loss: 0.6606506109237671\n",
      "Epoch 43, batch 86 D Loss: 1.4073028564453125, G Loss: 0.6656339764595032\n",
      "Epoch 43, batch 87 D Loss: 1.4156692028045654, G Loss: 0.6604868173599243\n",
      "Epoch 43, batch 88 D Loss: 1.4100254774093628, G Loss: 0.6712665557861328\n",
      "Epoch 43, batch 89 D Loss: 1.3896372318267822, G Loss: 0.6846848130226135\n",
      "Epoch 43, batch 90 D Loss: 1.3641377687454224, G Loss: 0.6857819557189941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, batch 91 D Loss: 1.3956775665283203, G Loss: 0.6773515343666077\n",
      "Epoch 43, batch 92 D Loss: 1.410158395767212, G Loss: 0.6701123118400574\n",
      "Epoch 43, batch 93 D Loss: 1.389331340789795, G Loss: 0.6830707788467407\n",
      "Epoch 43, batch 94 D Loss: 1.3880398273468018, G Loss: 0.68548184633255\n",
      "Epoch 43, batch 95 D Loss: 1.389052152633667, G Loss: 0.6852261424064636\n",
      "Epoch 43, batch 96 D Loss: 1.3913154602050781, G Loss: 0.6858947277069092\n",
      "Epoch 43, batch 97 D Loss: 1.3799173831939697, G Loss: 0.6981891393661499\n",
      "Epoch 43, batch 98 D Loss: 1.3996496200561523, G Loss: 0.6792904138565063\n",
      "Epoch 43, batch 99 D Loss: 1.3914048671722412, G Loss: 0.681477963924408\n",
      "Epoch 43, batch 100 D Loss: 1.375826358795166, G Loss: 0.6944426894187927\n",
      "Epoch 43, batch 101 D Loss: 1.3818504810333252, G Loss: 0.6942058801651001\n",
      "Epoch 43, batch 102 D Loss: 1.3914916515350342, G Loss: 0.6840846538543701\n",
      "Epoch 43, batch 103 D Loss: 1.3839629888534546, G Loss: 0.6927700638771057\n",
      "Epoch 43, batch 104 D Loss: 1.3888351917266846, G Loss: 0.6947082281112671\n",
      "Epoch 43, batch 105 D Loss: 1.3764742612838745, G Loss: 0.697959303855896\n",
      "Epoch 43, batch 106 D Loss: 1.3854900598526, G Loss: 0.6962615251541138\n",
      "Epoch 43, batch 107 D Loss: 1.366464376449585, G Loss: 0.7005537152290344\n",
      "Epoch 43, batch 108 D Loss: 1.3715319633483887, G Loss: 0.7001765966415405\n",
      "Epoch 43, batch 109 D Loss: 1.3579983711242676, G Loss: 0.7128563523292542\n",
      "Epoch 43, batch 110 D Loss: 1.37875235080719, G Loss: 0.6974907517433167\n",
      "Epoch 43, batch 111 D Loss: 1.3948694467544556, G Loss: 0.6830927133560181\n",
      "Epoch 43, batch 112 D Loss: 1.3812251091003418, G Loss: 0.703042209148407\n",
      "Epoch 43, batch 113 D Loss: 1.3714205026626587, G Loss: 0.7048449516296387\n",
      "Epoch 43, batch 114 D Loss: 1.3657076358795166, G Loss: 0.7208177447319031\n",
      "Epoch 43, batch 115 D Loss: 1.3823407888412476, G Loss: 0.6977057456970215\n",
      "Epoch 43, batch 116 D Loss: 1.3798980712890625, G Loss: 0.7047603130340576\n",
      "Epoch 43, batch 117 D Loss: 1.354225993156433, G Loss: 0.7183167338371277\n",
      "Epoch 43, batch 118 D Loss: 1.3516044616699219, G Loss: 0.7132200002670288\n",
      "Epoch 43, batch 119 D Loss: 1.3813350200653076, G Loss: 0.6990594267845154\n",
      "Epoch 43, batch 120 D Loss: 1.3587920665740967, G Loss: 0.7125676870346069\n",
      "Epoch 43, batch 121 D Loss: 1.3802530765533447, G Loss: 0.7025148868560791\n",
      "Epoch 43, batch 122 D Loss: 1.3829554319381714, G Loss: 0.7077754139900208\n",
      "Epoch 43, batch 123 D Loss: 1.365158200263977, G Loss: 0.7106931805610657\n",
      "Epoch 43, batch 124 D Loss: 1.3611507415771484, G Loss: 0.7148969769477844\n",
      "Epoch 43, batch 125 D Loss: 1.3727564811706543, G Loss: 0.709237277507782\n",
      "Epoch 43, batch 126 D Loss: 1.3695008754730225, G Loss: 0.70566725730896\n",
      "Epoch 43, batch 127 D Loss: 1.381669282913208, G Loss: 0.709746241569519\n",
      "Epoch 43, batch 128 D Loss: 1.3863887786865234, G Loss: 0.6941701769828796\n",
      "Epoch 43, batch 129 D Loss: 1.378607153892517, G Loss: 0.7046941518783569\n",
      "Epoch 43, batch 130 D Loss: 1.3847217559814453, G Loss: 0.703768789768219\n",
      "Epoch 43, batch 131 D Loss: 1.378483772277832, G Loss: 0.710196316242218\n",
      "Epoch 43, batch 132 D Loss: 1.374396800994873, G Loss: 0.7032954692840576\n",
      "Epoch 43, batch 133 D Loss: 1.3702868223190308, G Loss: 0.7083187699317932\n",
      "Epoch 43, batch 134 D Loss: 1.364436388015747, G Loss: 0.7129511833190918\n",
      "Epoch 43, batch 135 D Loss: 1.3775413036346436, G Loss: 0.7034109234809875\n",
      "Epoch 43, batch 136 D Loss: 1.3930652141571045, G Loss: 0.700822651386261\n",
      "Epoch 43, batch 137 D Loss: 1.3687458038330078, G Loss: 0.7235342264175415\n",
      "Epoch 43, batch 138 D Loss: 1.394314169883728, G Loss: 0.6926670074462891\n",
      "Epoch 43, batch 139 D Loss: 1.3600101470947266, G Loss: 0.7132358551025391\n",
      "Epoch 43, batch 140 D Loss: 1.3778642416000366, G Loss: 0.7066469788551331\n",
      "Epoch 43, batch 141 D Loss: 1.3706581592559814, G Loss: 0.7180127501487732\n",
      "Epoch 43, batch 142 D Loss: 1.4056050777435303, G Loss: 0.6915332674980164\n",
      "Epoch 43, batch 143 D Loss: 1.4119489192962646, G Loss: 0.6858847141265869\n",
      "Epoch 43, batch 144 D Loss: 1.3647656440734863, G Loss: 0.7083806395530701\n",
      "Epoch 43, batch 145 D Loss: 1.3643027544021606, G Loss: 0.7171570658683777\n",
      "Epoch 43, batch 146 D Loss: 1.3811564445495605, G Loss: 0.7032623291015625\n",
      "Epoch 43, batch 147 D Loss: 1.3899364471435547, G Loss: 0.6933778524398804\n",
      "Epoch 43, batch 148 D Loss: 1.3749713897705078, G Loss: 0.6865499019622803\n",
      "Epoch 43, batch 149 D Loss: 1.3678187131881714, G Loss: 0.6962242722511292\n",
      "Epoch 43, batch 150 D Loss: 1.3938024044036865, G Loss: 0.7046626210212708\n",
      "Epoch 43, batch 151 D Loss: 1.4007278680801392, G Loss: 0.673255980014801\n",
      "Epoch 43, batch 152 D Loss: 1.3754756450653076, G Loss: 0.6879600286483765\n",
      "Epoch 43, batch 153 D Loss: 1.3571326732635498, G Loss: 0.7057673931121826\n",
      "Epoch 43, batch 154 D Loss: 1.4052073955535889, G Loss: 0.6777940988540649\n",
      "Epoch 43, batch 155 D Loss: 1.3691153526306152, G Loss: 0.6948169469833374\n",
      "Epoch 43, batch 156 D Loss: 1.403792142868042, G Loss: 0.6914226412773132\n",
      "Epoch 43, batch 157 D Loss: 1.4185467958450317, G Loss: 0.6852365732192993\n",
      "Epoch 43, batch 158 D Loss: 1.3921477794647217, G Loss: 0.6852066516876221\n",
      "Epoch 43, batch 159 D Loss: 1.4078198671340942, G Loss: 0.6823734045028687\n",
      "Epoch 43, batch 160 D Loss: 1.3932335376739502, G Loss: 0.6780106425285339\n",
      "Epoch 43, batch 161 D Loss: 1.3984838724136353, G Loss: 0.6746919751167297\n",
      "Epoch 43, batch 162 D Loss: 1.4147722721099854, G Loss: 0.6914394497871399\n",
      "Epoch 43, batch 163 D Loss: 1.432167649269104, G Loss: 0.683266282081604\n",
      "Epoch 43, batch 164 D Loss: 1.416853666305542, G Loss: 0.6684566736221313\n",
      "Epoch 43, batch 165 D Loss: 1.4123108386993408, G Loss: 0.6727621555328369\n",
      "Epoch 43, batch 166 D Loss: 1.3916361331939697, G Loss: 0.6842951774597168\n",
      "Epoch 43, batch 167 D Loss: 1.3846845626831055, G Loss: 0.6997116804122925\n",
      "Epoch 43, batch 168 D Loss: 1.4016869068145752, G Loss: 0.6910003423690796\n",
      "Epoch 43, batch 169 D Loss: 1.4109169244766235, G Loss: 0.675869882106781\n",
      "Epoch 43, batch 170 D Loss: 1.390299677848816, G Loss: 0.6911711692810059\n",
      "Epoch 43, batch 171 D Loss: 1.4005759954452515, G Loss: 0.6821064949035645\n",
      "Epoch 43, batch 172 D Loss: 1.3939253091812134, G Loss: 0.6886327862739563\n",
      "Epoch 43, batch 173 D Loss: 1.3982503414154053, G Loss: 0.6804575324058533\n",
      "Epoch 43, batch 174 D Loss: 1.4100375175476074, G Loss: 0.6839089393615723\n",
      "Epoch 43, batch 175 D Loss: 1.4070701599121094, G Loss: 0.6869670152664185\n",
      "Epoch 43, batch 176 D Loss: 1.3970240354537964, G Loss: 0.684003472328186\n",
      "Epoch 43, batch 177 D Loss: 1.4042298793792725, G Loss: 0.6981138586997986\n",
      "Epoch 43, batch 178 D Loss: 1.4144799709320068, G Loss: 0.6810747385025024\n",
      "Epoch 43, batch 179 D Loss: 1.3912122249603271, G Loss: 0.6901156902313232\n",
      "Epoch 43, batch 180 D Loss: 1.3889529705047607, G Loss: 0.6905229091644287\n",
      "Epoch 43, batch 181 D Loss: 1.3900907039642334, G Loss: 0.6968337893486023\n",
      "Epoch 43, batch 182 D Loss: 1.3975019454956055, G Loss: 0.6978996992111206\n",
      "Epoch 43, batch 183 D Loss: 1.3827683925628662, G Loss: 0.6962761878967285\n",
      "Epoch 43, batch 184 D Loss: 1.4030323028564453, G Loss: 0.6848475933074951\n",
      "Epoch 43, batch 185 D Loss: 1.3880314826965332, G Loss: 0.6976712942123413\n",
      "Epoch 43, batch 186 D Loss: 1.4083375930786133, G Loss: 0.6815637350082397\n",
      "Epoch 43, batch 187 D Loss: 1.3966858386993408, G Loss: 0.6839198470115662\n",
      "Epoch 43, batch 188 D Loss: 1.4104433059692383, G Loss: 0.6842466592788696\n",
      "Epoch 43, batch 189 D Loss: 1.4000798463821411, G Loss: 0.6910119652748108\n",
      "Epoch 43, batch 190 D Loss: 1.3942310810089111, G Loss: 0.6873972415924072\n",
      "Epoch 43, batch 191 D Loss: 1.3945125341415405, G Loss: 0.6958242654800415\n",
      "Epoch 43, batch 192 D Loss: 1.3933582305908203, G Loss: 0.6934319138526917\n",
      "Epoch 43, batch 193 D Loss: 1.397045612335205, G Loss: 0.7020947933197021\n",
      "Epoch 43, batch 194 D Loss: 1.4055793285369873, G Loss: 0.6974958181381226\n",
      "Epoch 43, batch 195 D Loss: 1.3974394798278809, G Loss: 0.6998339891433716\n",
      "Epoch 43, batch 196 D Loss: 1.4027440547943115, G Loss: 0.6962108016014099\n",
      "Epoch 43, batch 197 D Loss: 1.3940479755401611, G Loss: 0.6994583010673523\n",
      "Epoch 43, batch 198 D Loss: 1.3994555473327637, G Loss: 0.6972790360450745\n",
      "Epoch 43, batch 199 D Loss: 1.4045865535736084, G Loss: 0.6944119334220886\n",
      "Epoch 43, batch 200 D Loss: 1.3965048789978027, G Loss: 0.6986904144287109\n",
      "Epoch 44, batch 1 D Loss: 1.3878045082092285, G Loss: 0.701256513595581\n",
      "Epoch 44, batch 2 D Loss: 1.3855469226837158, G Loss: 0.7045568227767944\n",
      "Epoch 44, batch 3 D Loss: 1.3905515670776367, G Loss: 0.705950915813446\n",
      "Epoch 44, batch 4 D Loss: 1.396477460861206, G Loss: 0.7000578045845032\n",
      "Epoch 44, batch 5 D Loss: 1.3886165618896484, G Loss: 0.7074658274650574\n",
      "Epoch 44, batch 6 D Loss: 1.4143986701965332, G Loss: 0.6972209215164185\n",
      "Epoch 44, batch 7 D Loss: 1.390618085861206, G Loss: 0.7063764333724976\n",
      "Epoch 44, batch 8 D Loss: 1.3881181478500366, G Loss: 0.7075274586677551\n",
      "Epoch 44, batch 9 D Loss: 1.3938400745391846, G Loss: 0.7048639059066772\n",
      "Epoch 44, batch 10 D Loss: 1.3907837867736816, G Loss: 0.7072176337242126\n",
      "Epoch 44, batch 11 D Loss: 1.3963944911956787, G Loss: 0.7053563594818115\n",
      "Epoch 44, batch 12 D Loss: 1.3897393941879272, G Loss: 0.7098578810691833\n",
      "Epoch 44, batch 13 D Loss: 1.3944286108016968, G Loss: 0.7070881128311157\n",
      "Epoch 44, batch 14 D Loss: 1.3911733627319336, G Loss: 0.7135026454925537\n",
      "Epoch 44, batch 15 D Loss: 1.3900835514068604, G Loss: 0.7108039259910583\n",
      "Epoch 44, batch 16 D Loss: 1.396981954574585, G Loss: 0.7109021544456482\n",
      "Epoch 44, batch 17 D Loss: 1.3939875364303589, G Loss: 0.7080355882644653\n",
      "Epoch 44, batch 18 D Loss: 1.3920576572418213, G Loss: 0.7090432047843933\n",
      "Epoch 44, batch 19 D Loss: 1.3868656158447266, G Loss: 0.7130372524261475\n",
      "Epoch 44, batch 20 D Loss: 1.3880033493041992, G Loss: 0.7145077586174011\n",
      "Epoch 44, batch 21 D Loss: 1.385573387145996, G Loss: 0.713453471660614\n",
      "Epoch 44, batch 22 D Loss: 1.3865971565246582, G Loss: 0.7129660844802856\n",
      "Epoch 44, batch 23 D Loss: 1.3849291801452637, G Loss: 0.7122710347175598\n",
      "Epoch 44, batch 24 D Loss: 1.3892405033111572, G Loss: 0.7114661931991577\n",
      "Epoch 44, batch 25 D Loss: 1.3810480833053589, G Loss: 0.718000590801239\n",
      "Epoch 44, batch 26 D Loss: 1.3912057876586914, G Loss: 0.7121418714523315\n",
      "Epoch 44, batch 27 D Loss: 1.3870073556900024, G Loss: 0.7147203087806702\n",
      "Epoch 44, batch 28 D Loss: 1.386376142501831, G Loss: 0.713172972202301\n",
      "Epoch 44, batch 29 D Loss: 1.387882947921753, G Loss: 0.713423490524292\n",
      "Epoch 44, batch 30 D Loss: 1.384901762008667, G Loss: 0.713879406452179\n",
      "Epoch 44, batch 31 D Loss: 1.3862841129302979, G Loss: 0.7142994403839111\n",
      "Epoch 44, batch 32 D Loss: 1.3828572034835815, G Loss: 0.7178781032562256\n",
      "Epoch 44, batch 33 D Loss: 1.3858928680419922, G Loss: 0.7151204943656921\n",
      "Epoch 44, batch 34 D Loss: 1.388441562652588, G Loss: 0.713642418384552\n",
      "Epoch 44, batch 35 D Loss: 1.3808894157409668, G Loss: 0.7202743291854858\n",
      "Epoch 44, batch 36 D Loss: 1.385594129562378, G Loss: 0.7193698287010193\n",
      "Epoch 44, batch 37 D Loss: 1.3857290744781494, G Loss: 0.7185423374176025\n",
      "Epoch 44, batch 38 D Loss: 1.389807939529419, G Loss: 0.7177307605743408\n",
      "Epoch 44, batch 39 D Loss: 1.383042573928833, G Loss: 0.7178682684898376\n",
      "Epoch 44, batch 40 D Loss: 1.378558874130249, G Loss: 0.7214643359184265\n",
      "Epoch 44, batch 41 D Loss: 1.381904125213623, G Loss: 0.7173750400543213\n",
      "Epoch 44, batch 42 D Loss: 1.3847146034240723, G Loss: 0.7165501117706299\n",
      "Epoch 44, batch 43 D Loss: 1.3818154335021973, G Loss: 0.7169249653816223\n",
      "Epoch 44, batch 44 D Loss: 1.380314826965332, G Loss: 0.7242885828018188\n",
      "Epoch 44, batch 45 D Loss: 1.3842096328735352, G Loss: 0.7153722643852234\n",
      "Epoch 44, batch 46 D Loss: 1.382751703262329, G Loss: 0.724429190158844\n",
      "Epoch 44, batch 47 D Loss: 1.3763115406036377, G Loss: 0.7214016914367676\n",
      "Epoch 44, batch 48 D Loss: 1.3804161548614502, G Loss: 0.719258189201355\n",
      "Epoch 44, batch 49 D Loss: 1.38395357131958, G Loss: 0.7213470935821533\n",
      "Epoch 44, batch 50 D Loss: 1.365716814994812, G Loss: 0.7351826429367065\n",
      "Epoch 44, batch 51 D Loss: 1.3699069023132324, G Loss: 0.7334069013595581\n",
      "Epoch 44, batch 52 D Loss: 1.3833892345428467, G Loss: 0.7221099734306335\n",
      "Epoch 44, batch 53 D Loss: 1.3680503368377686, G Loss: 0.7277673482894897\n",
      "Epoch 44, batch 54 D Loss: 1.3721952438354492, G Loss: 0.7273638844490051\n",
      "Epoch 44, batch 55 D Loss: 1.3798646926879883, G Loss: 0.7379859685897827\n",
      "Epoch 44, batch 56 D Loss: 1.3874390125274658, G Loss: 0.720432460308075\n",
      "Epoch 44, batch 57 D Loss: 1.368004322052002, G Loss: 0.745033860206604\n",
      "Epoch 44, batch 58 D Loss: 1.3760762214660645, G Loss: 0.72979336977005\n",
      "Epoch 44, batch 59 D Loss: 1.380392074584961, G Loss: 0.7349692583084106\n",
      "Epoch 44, batch 60 D Loss: 1.380387544631958, G Loss: 0.7291998863220215\n",
      "Epoch 44, batch 61 D Loss: 1.3829782009124756, G Loss: 0.7300812005996704\n",
      "Epoch 44, batch 62 D Loss: 1.3838341236114502, G Loss: 0.720587968826294\n",
      "Epoch 44, batch 63 D Loss: 1.380184531211853, G Loss: 0.7346411347389221\n",
      "Epoch 44, batch 64 D Loss: 1.368295431137085, G Loss: 0.7411953210830688\n",
      "Epoch 44, batch 65 D Loss: 1.3693002462387085, G Loss: 0.7374331951141357\n",
      "Epoch 44, batch 66 D Loss: 1.3628120422363281, G Loss: 0.7380957007408142\n",
      "Epoch 44, batch 67 D Loss: 1.3803634643554688, G Loss: 0.7322738766670227\n",
      "Epoch 44, batch 68 D Loss: 1.364624261856079, G Loss: 0.7278873920440674\n",
      "Epoch 44, batch 69 D Loss: 1.3840737342834473, G Loss: 0.7290722131729126\n",
      "Epoch 44, batch 70 D Loss: 1.3715742826461792, G Loss: 0.7259774804115295\n",
      "Epoch 44, batch 71 D Loss: 1.373100996017456, G Loss: 0.7340585589408875\n",
      "Epoch 44, batch 72 D Loss: 1.3710253238677979, G Loss: 0.7272943258285522\n",
      "Epoch 44, batch 73 D Loss: 1.3702380657196045, G Loss: 0.7430351376533508\n",
      "Epoch 44, batch 74 D Loss: 1.382450819015503, G Loss: 0.7162081003189087\n",
      "Epoch 44, batch 75 D Loss: 1.3789689540863037, G Loss: 0.722035825252533\n",
      "Epoch 44, batch 76 D Loss: 1.3691024780273438, G Loss: 0.7309200763702393\n",
      "Epoch 44, batch 77 D Loss: 1.3672425746917725, G Loss: 0.7345279455184937\n",
      "Epoch 44, batch 78 D Loss: 1.3864442110061646, G Loss: 0.7200149297714233\n",
      "Epoch 44, batch 79 D Loss: 1.3749364614486694, G Loss: 0.7239575982093811\n",
      "Epoch 44, batch 80 D Loss: 1.3863329887390137, G Loss: 0.7317658066749573\n",
      "Epoch 44, batch 81 D Loss: 1.3582217693328857, G Loss: 0.7301832437515259\n",
      "Epoch 44, batch 82 D Loss: 1.3987236022949219, G Loss: 0.7164777517318726\n",
      "Epoch 44, batch 83 D Loss: 1.3547179698944092, G Loss: 0.7345752716064453\n",
      "Epoch 44, batch 84 D Loss: 1.3677384853363037, G Loss: 0.731594443321228\n",
      "Epoch 44, batch 85 D Loss: 1.3864965438842773, G Loss: 0.7192699313163757\n",
      "Epoch 44, batch 86 D Loss: 1.39259672164917, G Loss: 0.714824378490448\n",
      "Epoch 44, batch 87 D Loss: 1.3829290866851807, G Loss: 0.7222196459770203\n",
      "Epoch 44, batch 88 D Loss: 1.3593132495880127, G Loss: 0.721534788608551\n",
      "Epoch 44, batch 89 D Loss: 1.386169672012329, G Loss: 0.7090818881988525\n",
      "Epoch 44, batch 90 D Loss: 1.3626562356948853, G Loss: 0.715046763420105\n",
      "Epoch 44, batch 91 D Loss: 1.375012755393982, G Loss: 0.7148359417915344\n",
      "Epoch 44, batch 92 D Loss: 1.3835694789886475, G Loss: 0.7111770510673523\n",
      "Epoch 44, batch 93 D Loss: 1.3674206733703613, G Loss: 0.7135385870933533\n",
      "Epoch 44, batch 94 D Loss: 1.3478620052337646, G Loss: 0.7119306325912476\n",
      "Epoch 44, batch 95 D Loss: 1.3818801641464233, G Loss: 0.7037416100502014\n",
      "Epoch 44, batch 96 D Loss: 1.3682135343551636, G Loss: 0.7085325717926025\n",
      "Epoch 44, batch 97 D Loss: 1.3563075065612793, G Loss: 0.7058203220367432\n",
      "Epoch 44, batch 98 D Loss: 1.3803963661193848, G Loss: 0.7085295915603638\n",
      "Epoch 44, batch 99 D Loss: 1.3818159103393555, G Loss: 0.6982262134552002\n",
      "Epoch 44, batch 100 D Loss: 1.384558916091919, G Loss: 0.6980707049369812\n",
      "Epoch 44, batch 101 D Loss: 1.365675449371338, G Loss: 0.6997275352478027\n",
      "Epoch 44, batch 102 D Loss: 1.3749330043792725, G Loss: 0.7113882303237915\n",
      "Epoch 44, batch 103 D Loss: 1.3711392879486084, G Loss: 0.6979165077209473\n",
      "Epoch 44, batch 104 D Loss: 1.3646798133850098, G Loss: 0.696547269821167\n",
      "Epoch 44, batch 105 D Loss: 1.3700352907180786, G Loss: 0.6937990784645081\n",
      "Epoch 44, batch 106 D Loss: 1.387160062789917, G Loss: 0.690645694732666\n",
      "Epoch 44, batch 107 D Loss: 1.3625112771987915, G Loss: 0.7017385959625244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, batch 108 D Loss: 1.3949713706970215, G Loss: 0.6882812976837158\n",
      "Epoch 44, batch 109 D Loss: 1.3857147693634033, G Loss: 0.6886590719223022\n",
      "Epoch 44, batch 110 D Loss: 1.3896487951278687, G Loss: 0.6935664415359497\n",
      "Epoch 44, batch 111 D Loss: 1.3928236961364746, G Loss: 0.6746517419815063\n",
      "Epoch 44, batch 112 D Loss: 1.423872709274292, G Loss: 0.6708586812019348\n",
      "Epoch 44, batch 113 D Loss: 1.3748400211334229, G Loss: 0.6691058874130249\n",
      "Epoch 44, batch 114 D Loss: 1.3928403854370117, G Loss: 0.6738067865371704\n",
      "Epoch 44, batch 115 D Loss: 1.3723242282867432, G Loss: 0.6926360130310059\n",
      "Epoch 44, batch 116 D Loss: 1.3775184154510498, G Loss: 0.6817183494567871\n",
      "Epoch 44, batch 117 D Loss: 1.4020438194274902, G Loss: 0.6636306047439575\n",
      "Epoch 44, batch 118 D Loss: 1.4015064239501953, G Loss: 0.6762900352478027\n",
      "Epoch 44, batch 119 D Loss: 1.3857109546661377, G Loss: 0.6699091196060181\n",
      "Epoch 44, batch 120 D Loss: 1.3696556091308594, G Loss: 0.6771393418312073\n",
      "Epoch 44, batch 121 D Loss: 1.4198964834213257, G Loss: 0.657582700252533\n",
      "Epoch 44, batch 122 D Loss: 1.3812005519866943, G Loss: 0.6854694485664368\n",
      "Epoch 44, batch 123 D Loss: 1.3836603164672852, G Loss: 0.6745866537094116\n",
      "Epoch 44, batch 124 D Loss: 1.3939599990844727, G Loss: 0.6773220300674438\n",
      "Epoch 44, batch 125 D Loss: 1.385718584060669, G Loss: 0.668123185634613\n",
      "Epoch 44, batch 126 D Loss: 1.408456802368164, G Loss: 0.6565420627593994\n",
      "Epoch 44, batch 127 D Loss: 1.3996670246124268, G Loss: 0.6700347661972046\n",
      "Epoch 44, batch 128 D Loss: 1.4061341285705566, G Loss: 0.6598219275474548\n",
      "Epoch 44, batch 129 D Loss: 1.405149221420288, G Loss: 0.6595315337181091\n",
      "Epoch 44, batch 130 D Loss: 1.42252516746521, G Loss: 0.6492438316345215\n",
      "Epoch 44, batch 131 D Loss: 1.4127211570739746, G Loss: 0.6592963933944702\n",
      "Epoch 44, batch 132 D Loss: 1.4143941402435303, G Loss: 0.644148588180542\n",
      "Epoch 44, batch 133 D Loss: 1.4215238094329834, G Loss: 0.6512055397033691\n",
      "Epoch 44, batch 134 D Loss: 1.4145236015319824, G Loss: 0.6568962931632996\n",
      "Epoch 44, batch 135 D Loss: 1.3853342533111572, G Loss: 0.6741728782653809\n",
      "Epoch 44, batch 136 D Loss: 1.4058303833007812, G Loss: 0.6545515656471252\n",
      "Epoch 44, batch 137 D Loss: 1.4133979082107544, G Loss: 0.6583865880966187\n",
      "Epoch 44, batch 138 D Loss: 1.3994624614715576, G Loss: 0.6656924486160278\n",
      "Epoch 44, batch 139 D Loss: 1.4149608612060547, G Loss: 0.6547138690948486\n",
      "Epoch 44, batch 140 D Loss: 1.410146713256836, G Loss: 0.6525846719741821\n",
      "Epoch 44, batch 141 D Loss: 1.420080542564392, G Loss: 0.6538981795310974\n",
      "Epoch 44, batch 142 D Loss: 1.4083082675933838, G Loss: 0.6649898290634155\n",
      "Epoch 44, batch 143 D Loss: 1.4137442111968994, G Loss: 0.6575809717178345\n",
      "Epoch 44, batch 144 D Loss: 1.4205052852630615, G Loss: 0.6526301503181458\n",
      "Epoch 44, batch 145 D Loss: 1.4264419078826904, G Loss: 0.651634931564331\n",
      "Epoch 44, batch 146 D Loss: 1.4058295488357544, G Loss: 0.6670597195625305\n",
      "Epoch 44, batch 147 D Loss: 1.3987987041473389, G Loss: 0.6709270477294922\n",
      "Epoch 44, batch 148 D Loss: 1.4090864658355713, G Loss: 0.658294141292572\n",
      "Epoch 44, batch 149 D Loss: 1.3911981582641602, G Loss: 0.6712646484375\n",
      "Epoch 44, batch 150 D Loss: 1.4188565015792847, G Loss: 0.6568572521209717\n",
      "Epoch 44, batch 151 D Loss: 1.411043405532837, G Loss: 0.6645618677139282\n",
      "Epoch 44, batch 152 D Loss: 1.4200782775878906, G Loss: 0.6604921221733093\n",
      "Epoch 44, batch 153 D Loss: 1.4206652641296387, G Loss: 0.6577450633049011\n",
      "Epoch 44, batch 154 D Loss: 1.3969531059265137, G Loss: 0.6713621020317078\n",
      "Epoch 44, batch 155 D Loss: 1.405897855758667, G Loss: 0.6684210896492004\n",
      "Epoch 44, batch 156 D Loss: 1.4023282527923584, G Loss: 0.6736459136009216\n",
      "Epoch 44, batch 157 D Loss: 1.3951020240783691, G Loss: 0.6778486371040344\n",
      "Epoch 44, batch 158 D Loss: 1.403498649597168, G Loss: 0.6734834313392639\n",
      "Epoch 44, batch 159 D Loss: 1.4110605716705322, G Loss: 0.6691142916679382\n",
      "Epoch 44, batch 160 D Loss: 1.396580457687378, G Loss: 0.6753973960876465\n",
      "Epoch 44, batch 161 D Loss: 1.3992512226104736, G Loss: 0.6719798445701599\n",
      "Epoch 44, batch 162 D Loss: 1.398486852645874, G Loss: 0.6762325167655945\n",
      "Epoch 44, batch 163 D Loss: 1.3938815593719482, G Loss: 0.676581621170044\n",
      "Epoch 44, batch 164 D Loss: 1.3874080181121826, G Loss: 0.6832830309867859\n",
      "Epoch 44, batch 165 D Loss: 1.4020819664001465, G Loss: 0.6751489043235779\n",
      "Epoch 44, batch 166 D Loss: 1.3903892040252686, G Loss: 0.6842485070228577\n",
      "Epoch 44, batch 167 D Loss: 1.393664836883545, G Loss: 0.6813220977783203\n",
      "Epoch 44, batch 168 D Loss: 1.3949851989746094, G Loss: 0.6802165508270264\n",
      "Epoch 44, batch 169 D Loss: 1.393343210220337, G Loss: 0.6843544840812683\n",
      "Epoch 44, batch 170 D Loss: 1.3868449926376343, G Loss: 0.6880348324775696\n",
      "Epoch 44, batch 171 D Loss: 1.3855328559875488, G Loss: 0.6891915202140808\n",
      "Epoch 44, batch 172 D Loss: 1.3857834339141846, G Loss: 0.6871457695960999\n",
      "Epoch 44, batch 173 D Loss: 1.3844751119613647, G Loss: 0.6879525184631348\n",
      "Epoch 44, batch 174 D Loss: 1.3876376152038574, G Loss: 0.6880732774734497\n",
      "Epoch 44, batch 175 D Loss: 1.3824455738067627, G Loss: 0.6887315511703491\n",
      "Epoch 44, batch 176 D Loss: 1.380767583847046, G Loss: 0.6892123222351074\n",
      "Epoch 44, batch 177 D Loss: 1.384084939956665, G Loss: 0.6938068866729736\n",
      "Epoch 44, batch 178 D Loss: 1.3796398639678955, G Loss: 0.6901371479034424\n",
      "Epoch 44, batch 179 D Loss: 1.3812040090560913, G Loss: 0.6929684281349182\n",
      "Epoch 44, batch 180 D Loss: 1.367849588394165, G Loss: 0.6956539750099182\n",
      "Epoch 44, batch 181 D Loss: 1.3734776973724365, G Loss: 0.6947963833808899\n",
      "Epoch 44, batch 182 D Loss: 1.3715949058532715, G Loss: 0.691626787185669\n",
      "Epoch 44, batch 183 D Loss: 1.3748595714569092, G Loss: 0.6980629563331604\n",
      "Epoch 44, batch 184 D Loss: 1.37932288646698, G Loss: 0.6959493160247803\n",
      "Epoch 44, batch 185 D Loss: 1.3744330406188965, G Loss: 0.6943928003311157\n",
      "Epoch 44, batch 186 D Loss: 1.376702070236206, G Loss: 0.696419358253479\n",
      "Epoch 44, batch 187 D Loss: 1.375097632408142, G Loss: 0.6959801316261292\n",
      "Epoch 44, batch 188 D Loss: 1.371523141860962, G Loss: 0.6946973204612732\n",
      "Epoch 44, batch 189 D Loss: 1.3605928421020508, G Loss: 0.7040168046951294\n",
      "Epoch 44, batch 190 D Loss: 1.3444249629974365, G Loss: 0.7214697003364563\n",
      "Epoch 44, batch 191 D Loss: 1.370259165763855, G Loss: 0.6969885230064392\n",
      "Epoch 44, batch 192 D Loss: 1.365427851676941, G Loss: 0.697115957736969\n",
      "Epoch 44, batch 193 D Loss: 1.363964557647705, G Loss: 0.7057938575744629\n",
      "Epoch 44, batch 194 D Loss: 1.3639214038848877, G Loss: 0.691069483757019\n",
      "Epoch 44, batch 195 D Loss: 1.3536126613616943, G Loss: 0.7039337754249573\n",
      "Epoch 44, batch 196 D Loss: 1.3590326309204102, G Loss: 0.7014582753181458\n",
      "Epoch 44, batch 197 D Loss: 1.3522427082061768, G Loss: 0.7055683732032776\n",
      "Epoch 44, batch 198 D Loss: 1.3614139556884766, G Loss: 0.6934225559234619\n",
      "Epoch 44, batch 199 D Loss: 1.3535816669464111, G Loss: 0.7046207189559937\n",
      "Epoch 44, batch 200 D Loss: 1.3632463216781616, G Loss: 0.6858015656471252\n",
      "Epoch 45, batch 1 D Loss: 1.3597145080566406, G Loss: 0.700248122215271\n",
      "Epoch 45, batch 2 D Loss: 1.3638592958450317, G Loss: 0.6860409379005432\n",
      "Epoch 45, batch 3 D Loss: 1.3804857730865479, G Loss: 0.6750974059104919\n",
      "Epoch 45, batch 4 D Loss: 1.3483667373657227, G Loss: 0.7008908987045288\n",
      "Epoch 45, batch 5 D Loss: 1.3663244247436523, G Loss: 0.6947953701019287\n",
      "Epoch 45, batch 6 D Loss: 1.3687788248062134, G Loss: 0.6965790390968323\n",
      "Epoch 45, batch 7 D Loss: 1.3644267320632935, G Loss: 0.6829097270965576\n",
      "Epoch 45, batch 8 D Loss: 1.3535816669464111, G Loss: 0.6955450177192688\n",
      "Epoch 45, batch 9 D Loss: 1.3655506372451782, G Loss: 0.6877920627593994\n",
      "Epoch 45, batch 10 D Loss: 1.356563925743103, G Loss: 0.7091737389564514\n",
      "Epoch 45, batch 11 D Loss: 1.3669445514678955, G Loss: 0.6881420016288757\n",
      "Epoch 45, batch 12 D Loss: 1.3378169536590576, G Loss: 0.7158324718475342\n",
      "Epoch 45, batch 13 D Loss: 1.3418333530426025, G Loss: 0.6979801654815674\n",
      "Epoch 45, batch 14 D Loss: 1.3704595565795898, G Loss: 0.6810998320579529\n",
      "Epoch 45, batch 15 D Loss: 1.3844242095947266, G Loss: 0.6833302974700928\n",
      "Epoch 45, batch 16 D Loss: 1.3897837400436401, G Loss: 0.6727660894393921\n",
      "Epoch 45, batch 17 D Loss: 1.3694453239440918, G Loss: 0.6777002811431885\n",
      "Epoch 45, batch 18 D Loss: 1.326963186264038, G Loss: 0.7089917063713074\n",
      "Epoch 45, batch 19 D Loss: 1.363438367843628, G Loss: 0.692389726638794\n",
      "Epoch 45, batch 20 D Loss: 1.363976001739502, G Loss: 0.6819865703582764\n",
      "Epoch 45, batch 21 D Loss: 1.3670666217803955, G Loss: 0.6795070767402649\n",
      "Epoch 45, batch 22 D Loss: 1.376131296157837, G Loss: 0.6689771413803101\n",
      "Epoch 45, batch 23 D Loss: 1.3720355033874512, G Loss: 0.6863389611244202\n",
      "Epoch 45, batch 24 D Loss: 1.3473504781723022, G Loss: 0.6779574751853943\n",
      "Epoch 45, batch 25 D Loss: 1.330687165260315, G Loss: 0.6894738674163818\n",
      "Epoch 45, batch 26 D Loss: 1.3975611925125122, G Loss: 0.666252851486206\n",
      "Epoch 45, batch 27 D Loss: 1.3697171211242676, G Loss: 0.6669864654541016\n",
      "Epoch 45, batch 28 D Loss: 1.3484313488006592, G Loss: 0.686352550983429\n",
      "Epoch 45, batch 29 D Loss: 1.3854467868804932, G Loss: 0.6541662812232971\n",
      "Epoch 45, batch 30 D Loss: 1.3948259353637695, G Loss: 0.6483456492424011\n",
      "Epoch 45, batch 31 D Loss: 1.38331937789917, G Loss: 0.6653298735618591\n",
      "Epoch 45, batch 32 D Loss: 1.3807146549224854, G Loss: 0.6575338840484619\n",
      "Epoch 45, batch 33 D Loss: 1.388432264328003, G Loss: 0.6632463335990906\n",
      "Epoch 45, batch 34 D Loss: 1.346224069595337, G Loss: 0.6744948625564575\n",
      "Epoch 45, batch 35 D Loss: 1.3581416606903076, G Loss: 0.6700428128242493\n",
      "Epoch 45, batch 36 D Loss: 1.379807710647583, G Loss: 0.6711179614067078\n",
      "Epoch 45, batch 37 D Loss: 1.3693907260894775, G Loss: 0.6747283339500427\n",
      "Epoch 45, batch 38 D Loss: 1.3715128898620605, G Loss: 0.6571028232574463\n",
      "Epoch 45, batch 39 D Loss: 1.386513113975525, G Loss: 0.6747901439666748\n",
      "Epoch 45, batch 40 D Loss: 1.4056400060653687, G Loss: 0.6525360345840454\n",
      "Epoch 45, batch 41 D Loss: 1.3910961151123047, G Loss: 0.6680167317390442\n",
      "Epoch 45, batch 42 D Loss: 1.392619252204895, G Loss: 0.6569433808326721\n",
      "Epoch 45, batch 43 D Loss: 1.4134328365325928, G Loss: 0.6467762589454651\n",
      "Epoch 45, batch 44 D Loss: 1.393975019454956, G Loss: 0.6630756258964539\n",
      "Epoch 45, batch 45 D Loss: 1.4119901657104492, G Loss: 0.6483108997344971\n",
      "Epoch 45, batch 46 D Loss: 1.4222338199615479, G Loss: 0.6550167202949524\n",
      "Epoch 45, batch 47 D Loss: 1.4099972248077393, G Loss: 0.6648973822593689\n",
      "Epoch 45, batch 48 D Loss: 1.4253660440444946, G Loss: 0.6352539658546448\n",
      "Epoch 45, batch 49 D Loss: 1.420055627822876, G Loss: 0.6528764963150024\n",
      "Epoch 45, batch 50 D Loss: 1.447068691253662, G Loss: 0.6517223119735718\n",
      "Epoch 45, batch 51 D Loss: 1.4434452056884766, G Loss: 0.6394893527030945\n",
      "Epoch 45, batch 52 D Loss: 1.4005796909332275, G Loss: 0.6734460592269897\n",
      "Epoch 45, batch 53 D Loss: 1.4023363590240479, G Loss: 0.6622238159179688\n",
      "Epoch 45, batch 54 D Loss: 1.4290318489074707, G Loss: 0.6618047952651978\n",
      "Epoch 45, batch 55 D Loss: 1.429764986038208, G Loss: 0.649939775466919\n",
      "Epoch 45, batch 56 D Loss: 1.4001824855804443, G Loss: 0.6765525937080383\n",
      "Epoch 45, batch 57 D Loss: 1.397496223449707, G Loss: 0.6743106842041016\n",
      "Epoch 45, batch 58 D Loss: 1.3917663097381592, G Loss: 0.6821682453155518\n",
      "Epoch 45, batch 59 D Loss: 1.4041813611984253, G Loss: 0.6698253154754639\n",
      "Epoch 45, batch 60 D Loss: 1.399320125579834, G Loss: 0.6851532459259033\n",
      "Epoch 45, batch 61 D Loss: 1.419809341430664, G Loss: 0.6771146655082703\n",
      "Epoch 45, batch 62 D Loss: 1.3783552646636963, G Loss: 0.6913452744483948\n",
      "Epoch 45, batch 63 D Loss: 1.434969186782837, G Loss: 0.6655647158622742\n",
      "Epoch 45, batch 64 D Loss: 1.433474063873291, G Loss: 0.680732250213623\n",
      "Epoch 45, batch 65 D Loss: 1.434277057647705, G Loss: 0.6739983558654785\n",
      "Epoch 45, batch 66 D Loss: 1.4127490520477295, G Loss: 0.6949953436851501\n",
      "Epoch 45, batch 67 D Loss: 1.4193501472473145, G Loss: 0.681998074054718\n",
      "Epoch 45, batch 68 D Loss: 1.4039745330810547, G Loss: 0.701824963092804\n",
      "Epoch 45, batch 69 D Loss: 1.428687334060669, G Loss: 0.6750190258026123\n",
      "Epoch 45, batch 70 D Loss: 1.4110435247421265, G Loss: 0.699679434299469\n",
      "Epoch 45, batch 71 D Loss: 1.4144576787948608, G Loss: 0.6991732120513916\n",
      "Epoch 45, batch 72 D Loss: 1.4165689945220947, G Loss: 0.6981566548347473\n",
      "Epoch 45, batch 73 D Loss: 1.4149549007415771, G Loss: 0.6915538311004639\n",
      "Epoch 45, batch 74 D Loss: 1.4291331768035889, G Loss: 0.7024410367012024\n",
      "Epoch 45, batch 75 D Loss: 1.408766508102417, G Loss: 0.7089558243751526\n",
      "Epoch 45, batch 76 D Loss: 1.4009320735931396, G Loss: 0.7135751247406006\n",
      "Epoch 45, batch 77 D Loss: 1.4011346101760864, G Loss: 0.7149391174316406\n",
      "Epoch 45, batch 78 D Loss: 1.3925395011901855, G Loss: 0.7219891548156738\n",
      "Epoch 45, batch 79 D Loss: 1.3954161405563354, G Loss: 0.7227436304092407\n",
      "Epoch 45, batch 80 D Loss: 1.4052941799163818, G Loss: 0.7194979786872864\n",
      "Epoch 45, batch 81 D Loss: 1.4096145629882812, G Loss: 0.7167870998382568\n",
      "Epoch 45, batch 82 D Loss: 1.3963720798492432, G Loss: 0.7253924608230591\n",
      "Epoch 45, batch 83 D Loss: 1.3895801305770874, G Loss: 0.7274483442306519\n",
      "Epoch 45, batch 84 D Loss: 1.3943997621536255, G Loss: 0.7261506915092468\n",
      "Epoch 45, batch 85 D Loss: 1.3892874717712402, G Loss: 0.7299976944923401\n",
      "Epoch 45, batch 86 D Loss: 1.4018937349319458, G Loss: 0.7221858501434326\n",
      "Epoch 45, batch 87 D Loss: 1.3972269296646118, G Loss: 0.725430965423584\n",
      "Epoch 45, batch 88 D Loss: 1.3907084465026855, G Loss: 0.7308377623558044\n",
      "Epoch 45, batch 89 D Loss: 1.3957597017288208, G Loss: 0.728234052658081\n",
      "Epoch 45, batch 90 D Loss: 1.3962807655334473, G Loss: 0.7291162014007568\n",
      "Epoch 45, batch 91 D Loss: 1.3872387409210205, G Loss: 0.7364444136619568\n",
      "Epoch 45, batch 92 D Loss: 1.3905160427093506, G Loss: 0.7316850423812866\n",
      "Epoch 45, batch 93 D Loss: 1.389182209968567, G Loss: 0.7378151416778564\n",
      "Epoch 45, batch 94 D Loss: 1.4008549451828003, G Loss: 0.7293000817298889\n",
      "Epoch 45, batch 95 D Loss: 1.388641357421875, G Loss: 0.7341471314430237\n",
      "Epoch 45, batch 96 D Loss: 1.3812029361724854, G Loss: 0.7452577948570251\n",
      "Epoch 45, batch 97 D Loss: 1.3829913139343262, G Loss: 0.7431002259254456\n",
      "Epoch 45, batch 98 D Loss: 1.3779985904693604, G Loss: 0.7487134337425232\n",
      "Epoch 45, batch 99 D Loss: 1.383453607559204, G Loss: 0.7423732280731201\n",
      "Epoch 45, batch 100 D Loss: 1.3839937448501587, G Loss: 0.7415684461593628\n",
      "Epoch 45, batch 101 D Loss: 1.3762965202331543, G Loss: 0.7517929673194885\n",
      "Epoch 45, batch 102 D Loss: 1.3797101974487305, G Loss: 0.7521057724952698\n",
      "Epoch 45, batch 103 D Loss: 1.378993272781372, G Loss: 0.7510284185409546\n",
      "Epoch 45, batch 104 D Loss: 1.3775279521942139, G Loss: 0.7481107115745544\n",
      "Epoch 45, batch 105 D Loss: 1.3708727359771729, G Loss: 0.7582375407218933\n",
      "Epoch 45, batch 106 D Loss: 1.3844733238220215, G Loss: 0.7430217266082764\n",
      "Epoch 45, batch 107 D Loss: 1.3590850830078125, G Loss: 0.7818820476531982\n",
      "Epoch 45, batch 108 D Loss: 1.388261318206787, G Loss: 0.7419717311859131\n",
      "Epoch 45, batch 109 D Loss: 1.3705646991729736, G Loss: 0.7622506022453308\n",
      "Epoch 45, batch 110 D Loss: 1.3620507717132568, G Loss: 0.7766883373260498\n",
      "Epoch 45, batch 111 D Loss: 1.3640520572662354, G Loss: 0.7821240425109863\n",
      "Epoch 45, batch 112 D Loss: 1.4010379314422607, G Loss: 0.7445693016052246\n",
      "Epoch 45, batch 113 D Loss: 1.3789540529251099, G Loss: 0.7690036296844482\n",
      "Epoch 45, batch 114 D Loss: 1.3541676998138428, G Loss: 0.7782037854194641\n",
      "Epoch 45, batch 115 D Loss: 1.3404439687728882, G Loss: 0.7934678792953491\n",
      "Epoch 45, batch 116 D Loss: 1.3775138854980469, G Loss: 0.7596951127052307\n",
      "Epoch 45, batch 117 D Loss: 1.3855197429656982, G Loss: 0.7699877023696899\n",
      "Epoch 45, batch 118 D Loss: 1.366922378540039, G Loss: 0.7659381031990051\n",
      "Epoch 45, batch 119 D Loss: 1.3701553344726562, G Loss: 0.7562744617462158\n",
      "Epoch 45, batch 120 D Loss: 1.3624992370605469, G Loss: 0.7623533010482788\n",
      "Epoch 45, batch 121 D Loss: 1.388012409210205, G Loss: 0.7562071084976196\n",
      "Epoch 45, batch 122 D Loss: 1.4014976024627686, G Loss: 0.755269467830658\n",
      "Epoch 45, batch 123 D Loss: 1.3779044151306152, G Loss: 0.7407495379447937\n",
      "Epoch 45, batch 124 D Loss: 1.3658447265625, G Loss: 0.7598989605903625\n",
      "Epoch 45, batch 125 D Loss: 1.3708691596984863, G Loss: 0.7598918676376343\n",
      "Epoch 45, batch 126 D Loss: 1.347644329071045, G Loss: 0.770141065120697\n",
      "Epoch 45, batch 127 D Loss: 1.3678730726242065, G Loss: 0.7586236000061035\n",
      "Epoch 45, batch 128 D Loss: 1.3748424053192139, G Loss: 0.7402761578559875\n",
      "Epoch 45, batch 129 D Loss: 1.4018232822418213, G Loss: 0.7341148853302002\n",
      "Epoch 45, batch 130 D Loss: 1.3933072090148926, G Loss: 0.7553921341896057\n",
      "Epoch 45, batch 131 D Loss: 1.3653982877731323, G Loss: 0.7499645352363586\n",
      "Epoch 45, batch 132 D Loss: 1.364654302597046, G Loss: 0.7593399882316589\n",
      "Epoch 45, batch 133 D Loss: 1.3990821838378906, G Loss: 0.7430779337882996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, batch 134 D Loss: 1.3617100715637207, G Loss: 0.7534894347190857\n",
      "Epoch 45, batch 135 D Loss: 1.3909633159637451, G Loss: 0.7670409679412842\n",
      "Epoch 45, batch 136 D Loss: 1.3626441955566406, G Loss: 0.7724657654762268\n",
      "Epoch 45, batch 137 D Loss: 1.3691095113754272, G Loss: 0.7806906700134277\n",
      "Epoch 45, batch 138 D Loss: 1.3961474895477295, G Loss: 0.7571858167648315\n",
      "Epoch 45, batch 139 D Loss: 1.3711016178131104, G Loss: 0.7570485472679138\n",
      "Epoch 45, batch 140 D Loss: 1.3628408908843994, G Loss: 0.7560002207756042\n",
      "Epoch 45, batch 141 D Loss: 1.380868911743164, G Loss: 0.7450887560844421\n",
      "Epoch 45, batch 142 D Loss: 1.3920636177062988, G Loss: 0.7246242761611938\n",
      "Epoch 45, batch 143 D Loss: 1.3901243209838867, G Loss: 0.7404981255531311\n",
      "Epoch 45, batch 144 D Loss: 1.374711513519287, G Loss: 0.7681081891059875\n",
      "Epoch 45, batch 145 D Loss: 1.3983447551727295, G Loss: 0.7429373860359192\n",
      "Epoch 45, batch 146 D Loss: 1.371260404586792, G Loss: 0.733064591884613\n",
      "Epoch 45, batch 147 D Loss: 1.3843562602996826, G Loss: 0.7471572756767273\n",
      "Epoch 45, batch 148 D Loss: 1.3755347728729248, G Loss: 0.7305996417999268\n",
      "Epoch 45, batch 149 D Loss: 1.4047048091888428, G Loss: 0.7164811491966248\n",
      "Epoch 45, batch 150 D Loss: 1.3975701332092285, G Loss: 0.7123602032661438\n",
      "Epoch 45, batch 151 D Loss: 1.4018735885620117, G Loss: 0.730721116065979\n",
      "Epoch 45, batch 152 D Loss: 1.3752844333648682, G Loss: 0.735429584980011\n",
      "Epoch 45, batch 153 D Loss: 1.4204721450805664, G Loss: 0.7144652605056763\n",
      "Epoch 45, batch 154 D Loss: 1.3629556894302368, G Loss: 0.7322114706039429\n",
      "Epoch 45, batch 155 D Loss: 1.3978931903839111, G Loss: 0.7256172299385071\n",
      "Epoch 45, batch 156 D Loss: 1.379241704940796, G Loss: 0.710127055644989\n",
      "Epoch 45, batch 157 D Loss: 1.3747389316558838, G Loss: 0.7164160013198853\n",
      "Epoch 45, batch 158 D Loss: 1.3856427669525146, G Loss: 0.7159426212310791\n",
      "Epoch 45, batch 159 D Loss: 1.4037444591522217, G Loss: 0.7045858502388\n",
      "Epoch 45, batch 160 D Loss: 1.389644980430603, G Loss: 0.7039975523948669\n",
      "Epoch 45, batch 161 D Loss: 1.3975059986114502, G Loss: 0.7072365283966064\n",
      "Epoch 45, batch 162 D Loss: 1.3832545280456543, G Loss: 0.7104377746582031\n",
      "Epoch 45, batch 163 D Loss: 1.3778355121612549, G Loss: 0.7139890789985657\n",
      "Epoch 45, batch 164 D Loss: 1.3708889484405518, G Loss: 0.7176498174667358\n",
      "Epoch 45, batch 165 D Loss: 1.390470266342163, G Loss: 0.7004913091659546\n",
      "Epoch 45, batch 166 D Loss: 1.3786420822143555, G Loss: 0.6994953155517578\n",
      "Epoch 45, batch 167 D Loss: 1.3960742950439453, G Loss: 0.7126011848449707\n",
      "Epoch 45, batch 168 D Loss: 1.3798274993896484, G Loss: 0.6926291584968567\n",
      "Epoch 45, batch 169 D Loss: 1.406956672668457, G Loss: 0.6912188529968262\n",
      "Epoch 45, batch 170 D Loss: 1.3767105340957642, G Loss: 0.7064952254295349\n",
      "Epoch 45, batch 171 D Loss: 1.3934037685394287, G Loss: 0.7062357068061829\n",
      "Epoch 45, batch 172 D Loss: 1.3970632553100586, G Loss: 0.6933640837669373\n",
      "Epoch 45, batch 173 D Loss: 1.3990122079849243, G Loss: 0.6898237466812134\n",
      "Epoch 45, batch 174 D Loss: 1.401342511177063, G Loss: 0.6837706565856934\n",
      "Epoch 45, batch 175 D Loss: 1.377030372619629, G Loss: 0.7083178758621216\n",
      "Epoch 45, batch 176 D Loss: 1.4132825136184692, G Loss: 0.6940819621086121\n",
      "Epoch 45, batch 177 D Loss: 1.3870749473571777, G Loss: 0.6804566383361816\n",
      "Epoch 45, batch 178 D Loss: 1.3775889873504639, G Loss: 0.701567530632019\n",
      "Epoch 45, batch 179 D Loss: 1.385624647140503, G Loss: 0.6953086853027344\n",
      "Epoch 45, batch 180 D Loss: 1.3945820331573486, G Loss: 0.6791831851005554\n",
      "Epoch 45, batch 181 D Loss: 1.4003710746765137, G Loss: 0.6870766282081604\n",
      "Epoch 45, batch 182 D Loss: 1.4019598960876465, G Loss: 0.6859651207923889\n",
      "Epoch 45, batch 183 D Loss: 1.3948700428009033, G Loss: 0.6826933026313782\n",
      "Epoch 45, batch 184 D Loss: 1.3798167705535889, G Loss: 0.6895543932914734\n",
      "Epoch 45, batch 185 D Loss: 1.395607352256775, G Loss: 0.6776533722877502\n",
      "Epoch 45, batch 186 D Loss: 1.3884999752044678, G Loss: 0.6837977766990662\n",
      "Epoch 45, batch 187 D Loss: 1.38902747631073, G Loss: 0.6889559030532837\n",
      "Epoch 45, batch 188 D Loss: 1.3862957954406738, G Loss: 0.6907650232315063\n",
      "Epoch 45, batch 189 D Loss: 1.3726972341537476, G Loss: 0.6922414898872375\n",
      "Epoch 45, batch 190 D Loss: 1.376158356666565, G Loss: 0.6895373463630676\n",
      "Epoch 45, batch 191 D Loss: 1.3809330463409424, G Loss: 0.6834975481033325\n",
      "Epoch 45, batch 192 D Loss: 1.3756111860275269, G Loss: 0.6806010603904724\n",
      "Epoch 45, batch 193 D Loss: 1.3875192403793335, G Loss: 0.6803750395774841\n",
      "Epoch 45, batch 194 D Loss: 1.3905367851257324, G Loss: 0.6797253489494324\n",
      "Epoch 45, batch 195 D Loss: 1.3789225816726685, G Loss: 0.6805918216705322\n",
      "Epoch 45, batch 196 D Loss: 1.386496901512146, G Loss: 0.6742411851882935\n",
      "Epoch 45, batch 197 D Loss: 1.3902771472930908, G Loss: 0.6849450469017029\n",
      "Epoch 45, batch 198 D Loss: 1.3953711986541748, G Loss: 0.6784613132476807\n",
      "Epoch 45, batch 199 D Loss: 1.3923124074935913, G Loss: 0.6809244751930237\n",
      "Epoch 45, batch 200 D Loss: 1.3801475763320923, G Loss: 0.6842309832572937\n",
      "Epoch 46, batch 1 D Loss: 1.3847190141677856, G Loss: 0.6843413710594177\n",
      "Epoch 46, batch 2 D Loss: 1.3922176361083984, G Loss: 0.6747904419898987\n",
      "Epoch 46, batch 3 D Loss: 1.3841787576675415, G Loss: 0.6792986392974854\n",
      "Epoch 46, batch 4 D Loss: 1.3844995498657227, G Loss: 0.6747144460678101\n",
      "Epoch 46, batch 5 D Loss: 1.386790156364441, G Loss: 0.6781889200210571\n",
      "Epoch 46, batch 6 D Loss: 1.3904963731765747, G Loss: 0.6773198843002319\n",
      "Epoch 46, batch 7 D Loss: 1.386190414428711, G Loss: 0.6790651679039001\n",
      "Epoch 46, batch 8 D Loss: 1.3963159322738647, G Loss: 0.6654447913169861\n",
      "Epoch 46, batch 9 D Loss: 1.3911837339401245, G Loss: 0.6687918901443481\n",
      "Epoch 46, batch 10 D Loss: 1.3887883424758911, G Loss: 0.6741518974304199\n",
      "Epoch 46, batch 11 D Loss: 1.3898977041244507, G Loss: 0.6725897192955017\n",
      "Epoch 46, batch 12 D Loss: 1.3770058155059814, G Loss: 0.6760690212249756\n",
      "Epoch 46, batch 13 D Loss: 1.386200189590454, G Loss: 0.6733778119087219\n",
      "Epoch 46, batch 14 D Loss: 1.3978419303894043, G Loss: 0.6643617153167725\n",
      "Epoch 46, batch 15 D Loss: 1.3883705139160156, G Loss: 0.6800479292869568\n",
      "Epoch 46, batch 16 D Loss: 1.3828963041305542, G Loss: 0.675235390663147\n",
      "Epoch 46, batch 17 D Loss: 1.3957483768463135, G Loss: 0.6691872477531433\n",
      "Epoch 46, batch 18 D Loss: 1.3836863040924072, G Loss: 0.6752224564552307\n",
      "Epoch 46, batch 19 D Loss: 1.3859658241271973, G Loss: 0.6758619546890259\n",
      "Epoch 46, batch 20 D Loss: 1.3838469982147217, G Loss: 0.6744146943092346\n",
      "Epoch 46, batch 21 D Loss: 1.3867154121398926, G Loss: 0.674561619758606\n",
      "Epoch 46, batch 22 D Loss: 1.3838714361190796, G Loss: 0.6724989414215088\n",
      "Epoch 46, batch 23 D Loss: 1.3843084573745728, G Loss: 0.6756162047386169\n",
      "Epoch 46, batch 24 D Loss: 1.3866013288497925, G Loss: 0.6714003086090088\n",
      "Epoch 46, batch 25 D Loss: 1.3901517391204834, G Loss: 0.6694507598876953\n",
      "Epoch 46, batch 26 D Loss: 1.3960070610046387, G Loss: 0.6679178476333618\n",
      "Epoch 46, batch 27 D Loss: 1.3878605365753174, G Loss: 0.6696460247039795\n",
      "Epoch 46, batch 28 D Loss: 1.386925220489502, G Loss: 0.6706122756004333\n",
      "Epoch 46, batch 29 D Loss: 1.3881651163101196, G Loss: 0.6667652130126953\n",
      "Epoch 46, batch 30 D Loss: 1.3901753425598145, G Loss: 0.6696501970291138\n",
      "Epoch 46, batch 31 D Loss: 1.3838714361190796, G Loss: 0.6707319021224976\n",
      "Epoch 46, batch 32 D Loss: 1.3868505954742432, G Loss: 0.6695085167884827\n",
      "Epoch 46, batch 33 D Loss: 1.3902950286865234, G Loss: 0.6714578866958618\n",
      "Epoch 46, batch 34 D Loss: 1.3880963325500488, G Loss: 0.6716446876525879\n",
      "Epoch 46, batch 35 D Loss: 1.376002311706543, G Loss: 0.675455629825592\n",
      "Epoch 46, batch 36 D Loss: 1.3853647708892822, G Loss: 0.6673362851142883\n",
      "Epoch 46, batch 37 D Loss: 1.3716728687286377, G Loss: 0.6731672883033752\n",
      "Epoch 46, batch 38 D Loss: 1.386777400970459, G Loss: 0.6727118492126465\n",
      "Epoch 46, batch 39 D Loss: 1.3770079612731934, G Loss: 0.6740065217018127\n",
      "Epoch 46, batch 40 D Loss: 1.3970966339111328, G Loss: 0.668224036693573\n",
      "Epoch 46, batch 41 D Loss: 1.3797551393508911, G Loss: 0.6681922674179077\n",
      "Epoch 46, batch 42 D Loss: 1.374711513519287, G Loss: 0.674339771270752\n",
      "Epoch 46, batch 43 D Loss: 1.3792192935943604, G Loss: 0.6753436923027039\n",
      "Epoch 46, batch 44 D Loss: 1.38959801197052, G Loss: 0.6659892201423645\n",
      "Epoch 46, batch 45 D Loss: 1.3896554708480835, G Loss: 0.6722325086593628\n",
      "Epoch 46, batch 46 D Loss: 1.3793530464172363, G Loss: 0.6740524768829346\n",
      "Epoch 46, batch 47 D Loss: 1.3821098804473877, G Loss: 0.674237847328186\n",
      "Epoch 46, batch 48 D Loss: 1.3959050178527832, G Loss: 0.6621732115745544\n",
      "Epoch 46, batch 49 D Loss: 1.3797918558120728, G Loss: 0.6748000979423523\n",
      "Epoch 46, batch 50 D Loss: 1.3914458751678467, G Loss: 0.6724806427955627\n",
      "Epoch 46, batch 51 D Loss: 1.3825414180755615, G Loss: 0.6703512072563171\n",
      "Epoch 46, batch 52 D Loss: 1.383531928062439, G Loss: 0.6722784638404846\n",
      "Epoch 46, batch 53 D Loss: 1.3749128580093384, G Loss: 0.6719722747802734\n",
      "Epoch 46, batch 54 D Loss: 1.3802162408828735, G Loss: 0.6726922392845154\n",
      "Epoch 46, batch 55 D Loss: 1.3825106620788574, G Loss: 0.6775673031806946\n",
      "Epoch 46, batch 56 D Loss: 1.3727319240570068, G Loss: 0.6752907633781433\n",
      "Epoch 46, batch 57 D Loss: 1.3820459842681885, G Loss: 0.6674734354019165\n",
      "Epoch 46, batch 58 D Loss: 1.3857094049453735, G Loss: 0.6667191982269287\n",
      "Epoch 46, batch 59 D Loss: 1.37892746925354, G Loss: 0.6756418347358704\n",
      "Epoch 46, batch 60 D Loss: 1.384119987487793, G Loss: 0.6734079122543335\n",
      "Epoch 46, batch 61 D Loss: 1.3770322799682617, G Loss: 0.6737118363380432\n",
      "Epoch 46, batch 62 D Loss: 1.3724009990692139, G Loss: 0.6759349703788757\n",
      "Epoch 46, batch 63 D Loss: 1.3782739639282227, G Loss: 0.6747334003448486\n",
      "Epoch 46, batch 64 D Loss: 1.3814852237701416, G Loss: 0.6713931560516357\n",
      "Epoch 46, batch 65 D Loss: 1.3887280225753784, G Loss: 0.6699347496032715\n",
      "Epoch 46, batch 66 D Loss: 1.3658206462860107, G Loss: 0.670441746711731\n",
      "Epoch 46, batch 67 D Loss: 1.3621554374694824, G Loss: 0.6741644144058228\n",
      "Epoch 46, batch 68 D Loss: 1.3922977447509766, G Loss: 0.6678932309150696\n",
      "Epoch 46, batch 69 D Loss: 1.3811063766479492, G Loss: 0.6743829250335693\n",
      "Epoch 46, batch 70 D Loss: 1.3811733722686768, G Loss: 0.6647791266441345\n",
      "Epoch 46, batch 71 D Loss: 1.3772459030151367, G Loss: 0.6732398271560669\n",
      "Epoch 46, batch 72 D Loss: 1.3762433528900146, G Loss: 0.6707190871238708\n",
      "Epoch 46, batch 73 D Loss: 1.374945878982544, G Loss: 0.6750783324241638\n",
      "Epoch 46, batch 74 D Loss: 1.3681609630584717, G Loss: 0.6750360727310181\n",
      "Epoch 46, batch 75 D Loss: 1.3654972314834595, G Loss: 0.6745109558105469\n",
      "Epoch 46, batch 76 D Loss: 1.3678112030029297, G Loss: 0.6767918467521667\n",
      "Epoch 46, batch 77 D Loss: 1.3729050159454346, G Loss: 0.6752249002456665\n",
      "Epoch 46, batch 78 D Loss: 1.3772006034851074, G Loss: 0.6683027148246765\n",
      "Epoch 46, batch 79 D Loss: 1.3863177299499512, G Loss: 0.6711994409561157\n",
      "Epoch 46, batch 80 D Loss: 1.3688242435455322, G Loss: 0.6717721819877625\n",
      "Epoch 46, batch 81 D Loss: 1.369345784187317, G Loss: 0.6761514544487\n",
      "Epoch 46, batch 82 D Loss: 1.3760364055633545, G Loss: 0.6668477654457092\n",
      "Epoch 46, batch 83 D Loss: 1.3832529783248901, G Loss: 0.6701205372810364\n",
      "Epoch 46, batch 84 D Loss: 1.3635293245315552, G Loss: 0.6725614070892334\n",
      "Epoch 46, batch 85 D Loss: 1.384427785873413, G Loss: 0.6652278304100037\n",
      "Epoch 46, batch 86 D Loss: 1.379431962966919, G Loss: 0.6748862266540527\n",
      "Epoch 46, batch 87 D Loss: 1.3606128692626953, G Loss: 0.6781863570213318\n",
      "Epoch 46, batch 88 D Loss: 1.3728892803192139, G Loss: 0.6751025915145874\n",
      "Epoch 46, batch 89 D Loss: 1.3821353912353516, G Loss: 0.6682596802711487\n",
      "Epoch 46, batch 90 D Loss: 1.3876855373382568, G Loss: 0.661608874797821\n",
      "Epoch 46, batch 91 D Loss: 1.377118706703186, G Loss: 0.6838322281837463\n",
      "Epoch 46, batch 92 D Loss: 1.3773019313812256, G Loss: 0.6820670366287231\n",
      "Epoch 46, batch 93 D Loss: 1.3930613994598389, G Loss: 0.6823828220367432\n",
      "Epoch 46, batch 94 D Loss: 1.3717024326324463, G Loss: 0.6662760972976685\n",
      "Epoch 46, batch 95 D Loss: 1.3724720478057861, G Loss: 0.6716189384460449\n",
      "Epoch 46, batch 96 D Loss: 1.370331048965454, G Loss: 0.6779801249504089\n",
      "Epoch 46, batch 97 D Loss: 1.3674862384796143, G Loss: 0.6671743988990784\n",
      "Epoch 46, batch 98 D Loss: 1.3518513441085815, G Loss: 0.6794151067733765\n",
      "Epoch 46, batch 99 D Loss: 1.368645429611206, G Loss: 0.6830120086669922\n",
      "Epoch 46, batch 100 D Loss: 1.3952419757843018, G Loss: 0.6651926636695862\n",
      "Epoch 46, batch 101 D Loss: 1.3728489875793457, G Loss: 0.6786627769470215\n",
      "Epoch 46, batch 102 D Loss: 1.3860081434249878, G Loss: 0.6888660192489624\n",
      "Epoch 46, batch 103 D Loss: 1.3891611099243164, G Loss: 0.6632527709007263\n",
      "Epoch 46, batch 104 D Loss: 1.3822026252746582, G Loss: 0.6950702667236328\n",
      "Epoch 46, batch 105 D Loss: 1.3861627578735352, G Loss: 0.6751213073730469\n",
      "Epoch 46, batch 106 D Loss: 1.3572947978973389, G Loss: 0.6764983534812927\n",
      "Epoch 46, batch 107 D Loss: 1.3670456409454346, G Loss: 0.6909368634223938\n",
      "Epoch 46, batch 108 D Loss: 1.388747215270996, G Loss: 0.6805472373962402\n",
      "Epoch 46, batch 109 D Loss: 1.387211561203003, G Loss: 0.6767435669898987\n",
      "Epoch 46, batch 110 D Loss: 1.3714189529418945, G Loss: 0.6944522857666016\n",
      "Epoch 46, batch 111 D Loss: 1.374891996383667, G Loss: 0.6780117750167847\n",
      "Epoch 46, batch 112 D Loss: 1.3850421905517578, G Loss: 0.6706575155258179\n",
      "Epoch 46, batch 113 D Loss: 1.3815770149230957, G Loss: 0.6774529814720154\n",
      "Epoch 46, batch 114 D Loss: 1.3537781238555908, G Loss: 0.6819657683372498\n",
      "Epoch 46, batch 115 D Loss: 1.3849025964736938, G Loss: 0.6825264692306519\n",
      "Epoch 46, batch 116 D Loss: 1.3923331499099731, G Loss: 0.6646983623504639\n",
      "Epoch 46, batch 117 D Loss: 1.3906810283660889, G Loss: 0.6797559261322021\n",
      "Epoch 46, batch 118 D Loss: 1.4185476303100586, G Loss: 0.6591770052909851\n",
      "Epoch 46, batch 119 D Loss: 1.3697478771209717, G Loss: 0.6828888058662415\n",
      "Epoch 46, batch 120 D Loss: 1.4036223888397217, G Loss: 0.6750656366348267\n",
      "Epoch 46, batch 121 D Loss: 1.4073257446289062, G Loss: 0.6669578552246094\n",
      "Epoch 46, batch 122 D Loss: 1.4025628566741943, G Loss: 0.6752113103866577\n",
      "Epoch 46, batch 123 D Loss: 1.3745858669281006, G Loss: 0.6937828063964844\n",
      "Epoch 46, batch 124 D Loss: 1.4184904098510742, G Loss: 0.6715330481529236\n",
      "Epoch 46, batch 125 D Loss: 1.3997483253479004, G Loss: 0.669940710067749\n",
      "Epoch 46, batch 126 D Loss: 1.378095269203186, G Loss: 0.6860939264297485\n",
      "Epoch 46, batch 127 D Loss: 1.4024020433425903, G Loss: 0.6752647161483765\n",
      "Epoch 46, batch 128 D Loss: 1.3897671699523926, G Loss: 0.695618212223053\n",
      "Epoch 46, batch 129 D Loss: 1.3711156845092773, G Loss: 0.7060026526451111\n",
      "Epoch 46, batch 130 D Loss: 1.419772744178772, G Loss: 0.6687585711479187\n",
      "Epoch 46, batch 131 D Loss: 1.38460111618042, G Loss: 0.6908849477767944\n",
      "Epoch 46, batch 132 D Loss: 1.4375507831573486, G Loss: 0.6614060401916504\n",
      "Epoch 46, batch 133 D Loss: 1.3775074481964111, G Loss: 0.7118217349052429\n",
      "Epoch 46, batch 134 D Loss: 1.3875086307525635, G Loss: 0.6958991885185242\n",
      "Epoch 46, batch 135 D Loss: 1.4231562614440918, G Loss: 0.6759033203125\n",
      "Epoch 46, batch 136 D Loss: 1.4107856750488281, G Loss: 0.6898618340492249\n",
      "Epoch 46, batch 137 D Loss: 1.3834444284439087, G Loss: 0.7041696310043335\n",
      "Epoch 46, batch 138 D Loss: 1.410146951675415, G Loss: 0.7046541571617126\n",
      "Epoch 46, batch 139 D Loss: 1.3639464378356934, G Loss: 0.7237979173660278\n",
      "Epoch 46, batch 140 D Loss: 1.3845069408416748, G Loss: 0.7135632038116455\n",
      "Epoch 46, batch 141 D Loss: 1.387878179550171, G Loss: 0.7141618132591248\n",
      "Epoch 46, batch 142 D Loss: 1.3992185592651367, G Loss: 0.7110084295272827\n",
      "Epoch 46, batch 143 D Loss: 1.4174429178237915, G Loss: 0.6983969807624817\n",
      "Epoch 46, batch 144 D Loss: 1.3804758787155151, G Loss: 0.7098108530044556\n",
      "Epoch 46, batch 145 D Loss: 1.4022598266601562, G Loss: 0.7191603779792786\n",
      "Epoch 46, batch 146 D Loss: 1.4009791612625122, G Loss: 0.7130463123321533\n",
      "Epoch 46, batch 147 D Loss: 1.3699259757995605, G Loss: 0.7323381304740906\n",
      "Epoch 46, batch 148 D Loss: 1.3785860538482666, G Loss: 0.7193872332572937\n",
      "Epoch 46, batch 149 D Loss: 1.3980624675750732, G Loss: 0.7300791144371033\n",
      "Epoch 46, batch 150 D Loss: 1.4133529663085938, G Loss: 0.7068774700164795\n",
      "Epoch 46, batch 151 D Loss: 1.4142146110534668, G Loss: 0.7134754657745361\n",
      "Epoch 46, batch 152 D Loss: 1.401291847229004, G Loss: 0.719144344329834\n",
      "Epoch 46, batch 153 D Loss: 1.4166789054870605, G Loss: 0.7102674245834351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, batch 154 D Loss: 1.4033639430999756, G Loss: 0.7202861905097961\n",
      "Epoch 46, batch 155 D Loss: 1.4052863121032715, G Loss: 0.7260693311691284\n",
      "Epoch 46, batch 156 D Loss: 1.3985815048217773, G Loss: 0.7264211773872375\n",
      "Epoch 46, batch 157 D Loss: 1.3938987255096436, G Loss: 0.7326762676239014\n",
      "Epoch 46, batch 158 D Loss: 1.39005708694458, G Loss: 0.7313591241836548\n",
      "Epoch 46, batch 159 D Loss: 1.3822308778762817, G Loss: 0.7416248917579651\n",
      "Epoch 46, batch 160 D Loss: 1.3906259536743164, G Loss: 0.7321918606758118\n",
      "Epoch 46, batch 161 D Loss: 1.3891324996948242, G Loss: 0.7380896806716919\n",
      "Epoch 46, batch 162 D Loss: 1.3983458280563354, G Loss: 0.7304083108901978\n",
      "Epoch 46, batch 163 D Loss: 1.3916714191436768, G Loss: 0.737430214881897\n",
      "Epoch 46, batch 164 D Loss: 1.3924717903137207, G Loss: 0.7448943853378296\n",
      "Epoch 46, batch 165 D Loss: 1.3869906663894653, G Loss: 0.7473872303962708\n",
      "Epoch 46, batch 166 D Loss: 1.3932749032974243, G Loss: 0.7361223101615906\n",
      "Epoch 46, batch 167 D Loss: 1.3942878246307373, G Loss: 0.7367842197418213\n",
      "Epoch 46, batch 168 D Loss: 1.384992241859436, G Loss: 0.7404493689537048\n",
      "Epoch 46, batch 169 D Loss: 1.4003510475158691, G Loss: 0.7394197583198547\n",
      "Epoch 46, batch 170 D Loss: 1.3966186046600342, G Loss: 0.7462241649627686\n",
      "Epoch 46, batch 171 D Loss: 1.3883315324783325, G Loss: 0.7449580430984497\n",
      "Epoch 46, batch 172 D Loss: 1.3883309364318848, G Loss: 0.742676854133606\n",
      "Epoch 46, batch 173 D Loss: 1.3926455974578857, G Loss: 0.7454018592834473\n",
      "Epoch 46, batch 174 D Loss: 1.3930375576019287, G Loss: 0.7460458278656006\n",
      "Epoch 46, batch 175 D Loss: 1.3900268077850342, G Loss: 0.7432460188865662\n",
      "Epoch 46, batch 176 D Loss: 1.39461088180542, G Loss: 0.7437208294868469\n",
      "Epoch 46, batch 177 D Loss: 1.3778125047683716, G Loss: 0.7523487210273743\n",
      "Epoch 46, batch 178 D Loss: 1.383254885673523, G Loss: 0.752051830291748\n",
      "Epoch 46, batch 179 D Loss: 1.3889966011047363, G Loss: 0.7435510754585266\n",
      "Epoch 46, batch 180 D Loss: 1.38612961769104, G Loss: 0.7469834685325623\n",
      "Epoch 46, batch 181 D Loss: 1.388959288597107, G Loss: 0.7486048936843872\n",
      "Epoch 46, batch 182 D Loss: 1.385314702987671, G Loss: 0.7495231628417969\n",
      "Epoch 46, batch 183 D Loss: 1.3926849365234375, G Loss: 0.7546842098236084\n",
      "Epoch 46, batch 184 D Loss: 1.382103681564331, G Loss: 0.7508350610733032\n",
      "Epoch 46, batch 185 D Loss: 1.3792710304260254, G Loss: 0.7537902593612671\n",
      "Epoch 46, batch 186 D Loss: 1.3837918043136597, G Loss: 0.7650125026702881\n",
      "Epoch 46, batch 187 D Loss: 1.371091365814209, G Loss: 0.763472318649292\n",
      "Epoch 46, batch 188 D Loss: 1.3818626403808594, G Loss: 0.7533535957336426\n",
      "Epoch 46, batch 189 D Loss: 1.3910119533538818, G Loss: 0.7576305866241455\n",
      "Epoch 46, batch 190 D Loss: 1.3859349489212036, G Loss: 0.7456724643707275\n",
      "Epoch 46, batch 191 D Loss: 1.3782340288162231, G Loss: 0.7637365460395813\n",
      "Epoch 46, batch 192 D Loss: 1.3853451013565063, G Loss: 0.7513776421546936\n",
      "Epoch 46, batch 193 D Loss: 1.377450704574585, G Loss: 0.7574918866157532\n",
      "Epoch 46, batch 194 D Loss: 1.3903460502624512, G Loss: 0.76041579246521\n",
      "Epoch 46, batch 195 D Loss: 1.3781825304031372, G Loss: 0.7590972781181335\n",
      "Epoch 46, batch 196 D Loss: 1.3767151832580566, G Loss: 0.7609552145004272\n",
      "Epoch 46, batch 197 D Loss: 1.3915008306503296, G Loss: 0.7424488663673401\n",
      "Epoch 46, batch 198 D Loss: 1.3969314098358154, G Loss: 0.746340274810791\n",
      "Epoch 46, batch 199 D Loss: 1.388077974319458, G Loss: 0.7615688443183899\n",
      "Epoch 46, batch 200 D Loss: 1.3847723007202148, G Loss: 0.7468818426132202\n",
      "Epoch 47, batch 1 D Loss: 1.3782436847686768, G Loss: 0.7650017738342285\n",
      "Epoch 47, batch 2 D Loss: 1.3881155252456665, G Loss: 0.7551955580711365\n",
      "Epoch 47, batch 3 D Loss: 1.3983904123306274, G Loss: 0.7520889043807983\n",
      "Epoch 47, batch 4 D Loss: 1.389106035232544, G Loss: 0.7540662288665771\n",
      "Epoch 47, batch 5 D Loss: 1.3906753063201904, G Loss: 0.7464542388916016\n",
      "Epoch 47, batch 6 D Loss: 1.3751370906829834, G Loss: 0.7654867768287659\n",
      "Epoch 47, batch 7 D Loss: 1.3800663948059082, G Loss: 0.736703634262085\n",
      "Epoch 47, batch 8 D Loss: 1.3545637130737305, G Loss: 0.7739360928535461\n",
      "Epoch 47, batch 9 D Loss: 1.3814797401428223, G Loss: 0.776710033416748\n",
      "Epoch 47, batch 10 D Loss: 1.3983361721038818, G Loss: 0.7317826747894287\n",
      "Epoch 47, batch 11 D Loss: 1.3741374015808105, G Loss: 0.7407915592193604\n",
      "Epoch 47, batch 12 D Loss: 1.3944647312164307, G Loss: 0.7444658875465393\n",
      "Epoch 47, batch 13 D Loss: 1.3668122291564941, G Loss: 0.7674363851547241\n",
      "Epoch 47, batch 14 D Loss: 1.378051519393921, G Loss: 0.7635217905044556\n",
      "Epoch 47, batch 15 D Loss: 1.3994619846343994, G Loss: 0.7251339554786682\n",
      "Epoch 47, batch 16 D Loss: 1.3819364309310913, G Loss: 0.7387797832489014\n",
      "Epoch 47, batch 17 D Loss: 1.4015454053878784, G Loss: 0.7281593084335327\n",
      "Epoch 47, batch 18 D Loss: 1.352703332901001, G Loss: 0.7563819885253906\n",
      "Epoch 47, batch 19 D Loss: 1.386275053024292, G Loss: 0.7431749105453491\n",
      "Epoch 47, batch 20 D Loss: 1.4173197746276855, G Loss: 0.7150680422782898\n",
      "Epoch 47, batch 21 D Loss: 1.399561882019043, G Loss: 0.7449746131896973\n",
      "Epoch 47, batch 22 D Loss: 1.3883020877838135, G Loss: 0.7285099625587463\n",
      "Epoch 47, batch 23 D Loss: 1.3941006660461426, G Loss: 0.7455769181251526\n",
      "Epoch 47, batch 24 D Loss: 1.390350580215454, G Loss: 0.7332682609558105\n",
      "Epoch 47, batch 25 D Loss: 1.3866440057754517, G Loss: 0.7263414263725281\n",
      "Epoch 47, batch 26 D Loss: 1.3953907489776611, G Loss: 0.716265857219696\n",
      "Epoch 47, batch 27 D Loss: 1.397831916809082, G Loss: 0.7181974649429321\n",
      "Epoch 47, batch 28 D Loss: 1.4193615913391113, G Loss: 0.7176014184951782\n",
      "Epoch 47, batch 29 D Loss: 1.3975121974945068, G Loss: 0.7079036235809326\n",
      "Epoch 47, batch 30 D Loss: 1.4055787324905396, G Loss: 0.7156883478164673\n",
      "Epoch 47, batch 31 D Loss: 1.4078876972198486, G Loss: 0.7117255330085754\n",
      "Epoch 47, batch 32 D Loss: 1.3941824436187744, G Loss: 0.7087475657463074\n",
      "Epoch 47, batch 33 D Loss: 1.4026997089385986, G Loss: 0.6929664611816406\n",
      "Epoch 47, batch 34 D Loss: 1.3870036602020264, G Loss: 0.7055233716964722\n",
      "Epoch 47, batch 35 D Loss: 1.409104585647583, G Loss: 0.6917927265167236\n",
      "Epoch 47, batch 36 D Loss: 1.3775142431259155, G Loss: 0.7174267768859863\n",
      "Epoch 47, batch 37 D Loss: 1.3850584030151367, G Loss: 0.6967087388038635\n",
      "Epoch 47, batch 38 D Loss: 1.384467363357544, G Loss: 0.6993778944015503\n",
      "Epoch 47, batch 39 D Loss: 1.3999481201171875, G Loss: 0.6952665448188782\n",
      "Epoch 47, batch 40 D Loss: 1.3968825340270996, G Loss: 0.6899267435073853\n",
      "Epoch 47, batch 41 D Loss: 1.397418737411499, G Loss: 0.6903210282325745\n",
      "Epoch 47, batch 42 D Loss: 1.3977034091949463, G Loss: 0.6876828074455261\n",
      "Epoch 47, batch 43 D Loss: 1.4070847034454346, G Loss: 0.6791692972183228\n",
      "Epoch 47, batch 44 D Loss: 1.4048514366149902, G Loss: 0.6763370037078857\n",
      "Epoch 47, batch 45 D Loss: 1.3959705829620361, G Loss: 0.688261091709137\n",
      "Epoch 47, batch 46 D Loss: 1.3953347206115723, G Loss: 0.6841439604759216\n",
      "Epoch 47, batch 47 D Loss: 1.397562026977539, G Loss: 0.6805928945541382\n",
      "Epoch 47, batch 48 D Loss: 1.3897807598114014, G Loss: 0.6845877766609192\n",
      "Epoch 47, batch 49 D Loss: 1.3924589157104492, G Loss: 0.686462938785553\n",
      "Epoch 47, batch 50 D Loss: 1.3942320346832275, G Loss: 0.6886649131774902\n",
      "Epoch 47, batch 51 D Loss: 1.3917694091796875, G Loss: 0.6860611438751221\n",
      "Epoch 47, batch 52 D Loss: 1.390277624130249, G Loss: 0.6803232431411743\n",
      "Epoch 47, batch 53 D Loss: 1.3941519260406494, G Loss: 0.6775959134101868\n",
      "Epoch 47, batch 54 D Loss: 1.400064468383789, G Loss: 0.6704676747322083\n",
      "Epoch 47, batch 55 D Loss: 1.3960163593292236, G Loss: 0.676608145236969\n",
      "Epoch 47, batch 56 D Loss: 1.3886656761169434, G Loss: 0.6792681217193604\n",
      "Epoch 47, batch 57 D Loss: 1.3907668590545654, G Loss: 0.6754311919212341\n",
      "Epoch 47, batch 58 D Loss: 1.389582633972168, G Loss: 0.6788122057914734\n",
      "Epoch 47, batch 59 D Loss: 1.3827970027923584, G Loss: 0.6801342964172363\n",
      "Epoch 47, batch 60 D Loss: 1.3914707899093628, G Loss: 0.6713889837265015\n",
      "Epoch 47, batch 61 D Loss: 1.3978188037872314, G Loss: 0.6771169900894165\n",
      "Epoch 47, batch 62 D Loss: 1.3933006525039673, G Loss: 0.6718919277191162\n",
      "Epoch 47, batch 63 D Loss: 1.3885202407836914, G Loss: 0.6760205626487732\n",
      "Epoch 47, batch 64 D Loss: 1.3907079696655273, G Loss: 0.6743250489234924\n",
      "Epoch 47, batch 65 D Loss: 1.3957479000091553, G Loss: 0.6690905690193176\n",
      "Epoch 47, batch 66 D Loss: 1.3922456502914429, G Loss: 0.6728608012199402\n",
      "Epoch 47, batch 67 D Loss: 1.3885095119476318, G Loss: 0.6719579100608826\n",
      "Epoch 47, batch 68 D Loss: 1.3897790908813477, G Loss: 0.6742388010025024\n",
      "Epoch 47, batch 69 D Loss: 1.392038106918335, G Loss: 0.6727821230888367\n",
      "Epoch 47, batch 70 D Loss: 1.39021897315979, G Loss: 0.672284722328186\n",
      "Epoch 47, batch 71 D Loss: 1.3853113651275635, G Loss: 0.6733417510986328\n",
      "Epoch 47, batch 72 D Loss: 1.3902580738067627, G Loss: 0.6722016334533691\n",
      "Epoch 47, batch 73 D Loss: 1.389662265777588, G Loss: 0.6710501313209534\n",
      "Epoch 47, batch 74 D Loss: 1.385085105895996, G Loss: 0.672954261302948\n",
      "Epoch 47, batch 75 D Loss: 1.385390281677246, G Loss: 0.6728147268295288\n",
      "Epoch 47, batch 76 D Loss: 1.3885226249694824, G Loss: 0.6716575026512146\n",
      "Epoch 47, batch 77 D Loss: 1.388124942779541, G Loss: 0.6720659732818604\n",
      "Epoch 47, batch 78 D Loss: 1.3887507915496826, G Loss: 0.6705975532531738\n",
      "Epoch 47, batch 79 D Loss: 1.3848387002944946, G Loss: 0.673075795173645\n",
      "Epoch 47, batch 80 D Loss: 1.3849111795425415, G Loss: 0.6725835204124451\n",
      "Epoch 47, batch 81 D Loss: 1.3906365633010864, G Loss: 0.6704516410827637\n",
      "Epoch 47, batch 82 D Loss: 1.3863028287887573, G Loss: 0.670199453830719\n",
      "Epoch 47, batch 83 D Loss: 1.38649320602417, G Loss: 0.6712039709091187\n",
      "Epoch 47, batch 84 D Loss: 1.3860390186309814, G Loss: 0.6692483425140381\n",
      "Epoch 47, batch 85 D Loss: 1.3864200115203857, G Loss: 0.6712360382080078\n",
      "Epoch 47, batch 86 D Loss: 1.387282371520996, G Loss: 0.670016884803772\n",
      "Epoch 47, batch 87 D Loss: 1.3858892917633057, G Loss: 0.6707640290260315\n",
      "Epoch 47, batch 88 D Loss: 1.386932373046875, G Loss: 0.6707785129547119\n",
      "Epoch 47, batch 89 D Loss: 1.3849632740020752, G Loss: 0.6691124439239502\n",
      "Epoch 47, batch 90 D Loss: 1.3850841522216797, G Loss: 0.6691789031028748\n",
      "Epoch 47, batch 91 D Loss: 1.3874258995056152, G Loss: 0.6704488396644592\n",
      "Epoch 47, batch 92 D Loss: 1.3863036632537842, G Loss: 0.6689491271972656\n",
      "Epoch 47, batch 93 D Loss: 1.3842252492904663, G Loss: 0.6711951494216919\n",
      "Epoch 47, batch 94 D Loss: 1.3816814422607422, G Loss: 0.6700717806816101\n",
      "Epoch 47, batch 95 D Loss: 1.38776695728302, G Loss: 0.6689397692680359\n",
      "Epoch 47, batch 96 D Loss: 1.3820195198059082, G Loss: 0.6704753041267395\n",
      "Epoch 47, batch 97 D Loss: 1.3817864656448364, G Loss: 0.6701347827911377\n",
      "Epoch 47, batch 98 D Loss: 1.378668189048767, G Loss: 0.6745834350585938\n",
      "Epoch 47, batch 99 D Loss: 1.3838988542556763, G Loss: 0.6714170575141907\n",
      "Epoch 47, batch 100 D Loss: 1.3804539442062378, G Loss: 0.6723915338516235\n",
      "Epoch 47, batch 101 D Loss: 1.388282060623169, G Loss: 0.6699501872062683\n",
      "Epoch 47, batch 102 D Loss: 1.384049654006958, G Loss: 0.6734174489974976\n",
      "Epoch 47, batch 103 D Loss: 1.385270118713379, G Loss: 0.6722987294197083\n",
      "Epoch 47, batch 104 D Loss: 1.3751397132873535, G Loss: 0.6729130744934082\n",
      "Epoch 47, batch 105 D Loss: 1.3790100812911987, G Loss: 0.6761279106140137\n",
      "Epoch 47, batch 106 D Loss: 1.3818809986114502, G Loss: 0.6728907227516174\n",
      "Epoch 47, batch 107 D Loss: 1.384535551071167, G Loss: 0.6665907502174377\n",
      "Epoch 47, batch 108 D Loss: 1.3814889192581177, G Loss: 0.6696605682373047\n",
      "Epoch 47, batch 109 D Loss: 1.3786251544952393, G Loss: 0.6734208464622498\n",
      "Epoch 47, batch 110 D Loss: 1.379805326461792, G Loss: 0.6727305054664612\n",
      "Epoch 47, batch 111 D Loss: 1.3785876035690308, G Loss: 0.6751850843429565\n",
      "Epoch 47, batch 112 D Loss: 1.383569359779358, G Loss: 0.6685851216316223\n",
      "Epoch 47, batch 113 D Loss: 1.3848154544830322, G Loss: 0.6707563996315002\n",
      "Epoch 47, batch 114 D Loss: 1.3803467750549316, G Loss: 0.6710628271102905\n",
      "Epoch 47, batch 115 D Loss: 1.3830897808074951, G Loss: 0.669736385345459\n",
      "Epoch 47, batch 116 D Loss: 1.37776780128479, G Loss: 0.6743531823158264\n",
      "Epoch 47, batch 117 D Loss: 1.3792202472686768, G Loss: 0.6690020561218262\n",
      "Epoch 47, batch 118 D Loss: 1.3842322826385498, G Loss: 0.6717479825019836\n",
      "Epoch 47, batch 119 D Loss: 1.3868861198425293, G Loss: 0.6710041761398315\n",
      "Epoch 47, batch 120 D Loss: 1.3775320053100586, G Loss: 0.665741503238678\n",
      "Epoch 47, batch 121 D Loss: 1.3741679191589355, G Loss: 0.6725434064865112\n",
      "Epoch 47, batch 122 D Loss: 1.376781702041626, G Loss: 0.6704689264297485\n",
      "Epoch 47, batch 123 D Loss: 1.3851909637451172, G Loss: 0.6656573414802551\n",
      "Epoch 47, batch 124 D Loss: 1.3830223083496094, G Loss: 0.6691622734069824\n",
      "Epoch 47, batch 125 D Loss: 1.380821704864502, G Loss: 0.6753847599029541\n",
      "Epoch 47, batch 126 D Loss: 1.3787363767623901, G Loss: 0.674773097038269\n",
      "Epoch 47, batch 127 D Loss: 1.3743847608566284, G Loss: 0.6732403039932251\n",
      "Epoch 47, batch 128 D Loss: 1.3807024955749512, G Loss: 0.6722165942192078\n",
      "Epoch 47, batch 129 D Loss: 1.3945047855377197, G Loss: 0.6653974056243896\n",
      "Epoch 47, batch 130 D Loss: 1.3769757747650146, G Loss: 0.6733679175376892\n",
      "Epoch 47, batch 131 D Loss: 1.377091646194458, G Loss: 0.6734246611595154\n",
      "Epoch 47, batch 132 D Loss: 1.3837547302246094, G Loss: 0.6736577749252319\n",
      "Epoch 47, batch 133 D Loss: 1.3835375308990479, G Loss: 0.6757354140281677\n",
      "Epoch 47, batch 134 D Loss: 1.3938292264938354, G Loss: 0.6639499664306641\n",
      "Epoch 47, batch 135 D Loss: 1.379319667816162, G Loss: 0.6759285926818848\n",
      "Epoch 47, batch 136 D Loss: 1.3805677890777588, G Loss: 0.6738882660865784\n",
      "Epoch 47, batch 137 D Loss: 1.3640846014022827, G Loss: 0.6876494884490967\n",
      "Epoch 47, batch 138 D Loss: 1.3719502687454224, G Loss: 0.6727417707443237\n",
      "Epoch 47, batch 139 D Loss: 1.3918359279632568, G Loss: 0.6716623902320862\n",
      "Epoch 47, batch 140 D Loss: 1.3725535869598389, G Loss: 0.6733996868133545\n",
      "Epoch 47, batch 141 D Loss: 1.3872286081314087, G Loss: 0.6694448590278625\n",
      "Epoch 47, batch 142 D Loss: 1.3867379426956177, G Loss: 0.6701907515525818\n",
      "Epoch 47, batch 143 D Loss: 1.3974194526672363, G Loss: 0.6619277000427246\n",
      "Epoch 47, batch 144 D Loss: 1.3793907165527344, G Loss: 0.680134117603302\n",
      "Epoch 47, batch 145 D Loss: 1.3801970481872559, G Loss: 0.6709864139556885\n",
      "Epoch 47, batch 146 D Loss: 1.3725042343139648, G Loss: 0.6831814050674438\n",
      "Epoch 47, batch 147 D Loss: 1.3807822465896606, G Loss: 0.6698733568191528\n",
      "Epoch 47, batch 148 D Loss: 1.3872740268707275, G Loss: 0.6713839173316956\n",
      "Epoch 47, batch 149 D Loss: 1.4079349040985107, G Loss: 0.6677414178848267\n",
      "Epoch 47, batch 150 D Loss: 1.3760511875152588, G Loss: 0.6832566857337952\n",
      "Epoch 47, batch 151 D Loss: 1.3781342506408691, G Loss: 0.679730236530304\n",
      "Epoch 47, batch 152 D Loss: 1.3861207962036133, G Loss: 0.6736335754394531\n",
      "Epoch 47, batch 153 D Loss: 1.379036545753479, G Loss: 0.6782752871513367\n",
      "Epoch 47, batch 154 D Loss: 1.3964357376098633, G Loss: 0.6701948642730713\n",
      "Epoch 47, batch 155 D Loss: 1.3949326276779175, G Loss: 0.6756933331489563\n",
      "Epoch 47, batch 156 D Loss: 1.383545160293579, G Loss: 0.680951714515686\n",
      "Epoch 47, batch 157 D Loss: 1.3966188430786133, G Loss: 0.6695371270179749\n",
      "Epoch 47, batch 158 D Loss: 1.3756296634674072, G Loss: 0.678585410118103\n",
      "Epoch 47, batch 159 D Loss: 1.4033706188201904, G Loss: 0.6750959753990173\n",
      "Epoch 47, batch 160 D Loss: 1.3821511268615723, G Loss: 0.685701847076416\n",
      "Epoch 47, batch 161 D Loss: 1.3720557689666748, G Loss: 0.6874951124191284\n",
      "Epoch 47, batch 162 D Loss: 1.3728251457214355, G Loss: 0.6891737580299377\n",
      "Epoch 47, batch 163 D Loss: 1.3943588733673096, G Loss: 0.6810445189476013\n",
      "Epoch 47, batch 164 D Loss: 1.3893301486968994, G Loss: 0.6845219135284424\n",
      "Epoch 47, batch 165 D Loss: 1.391409158706665, G Loss: 0.6803871989250183\n",
      "Epoch 47, batch 166 D Loss: 1.3828396797180176, G Loss: 0.6828153729438782\n",
      "Epoch 47, batch 167 D Loss: 1.3903032541275024, G Loss: 0.6823932528495789\n",
      "Epoch 47, batch 168 D Loss: 1.3834160566329956, G Loss: 0.6912719011306763\n",
      "Epoch 47, batch 169 D Loss: 1.3795673847198486, G Loss: 0.686634361743927\n",
      "Epoch 47, batch 170 D Loss: 1.3758180141448975, G Loss: 0.6969135403633118\n",
      "Epoch 47, batch 171 D Loss: 1.384810209274292, G Loss: 0.6859087944030762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, batch 172 D Loss: 1.3904284238815308, G Loss: 0.6853857636451721\n",
      "Epoch 47, batch 173 D Loss: 1.3992266654968262, G Loss: 0.6790849566459656\n",
      "Epoch 47, batch 174 D Loss: 1.3861987590789795, G Loss: 0.6916991472244263\n",
      "Epoch 47, batch 175 D Loss: 1.3826806545257568, G Loss: 0.6860359311103821\n",
      "Epoch 47, batch 176 D Loss: 1.401591420173645, G Loss: 0.679310142993927\n",
      "Epoch 47, batch 177 D Loss: 1.3789950609207153, G Loss: 0.6974797248840332\n",
      "Epoch 47, batch 178 D Loss: 1.377211332321167, G Loss: 0.6900877952575684\n",
      "Epoch 47, batch 179 D Loss: 1.3897180557250977, G Loss: 0.691224217414856\n",
      "Epoch 47, batch 180 D Loss: 1.3842111825942993, G Loss: 0.6949682831764221\n",
      "Epoch 47, batch 181 D Loss: 1.3838145732879639, G Loss: 0.6912548542022705\n",
      "Epoch 47, batch 182 D Loss: 1.3885661363601685, G Loss: 0.6932207345962524\n",
      "Epoch 47, batch 183 D Loss: 1.3830275535583496, G Loss: 0.6943092346191406\n",
      "Epoch 47, batch 184 D Loss: 1.3812406063079834, G Loss: 0.6967551708221436\n",
      "Epoch 47, batch 185 D Loss: 1.3910222053527832, G Loss: 0.6914994716644287\n",
      "Epoch 47, batch 186 D Loss: 1.3915982246398926, G Loss: 0.69464111328125\n",
      "Epoch 47, batch 187 D Loss: 1.3877261877059937, G Loss: 0.6978808641433716\n",
      "Epoch 47, batch 188 D Loss: 1.3869781494140625, G Loss: 0.691498339176178\n",
      "Epoch 47, batch 189 D Loss: 1.388758897781372, G Loss: 0.6908000111579895\n",
      "Epoch 47, batch 190 D Loss: 1.389091968536377, G Loss: 0.6910443305969238\n",
      "Epoch 47, batch 191 D Loss: 1.37911057472229, G Loss: 0.7013295888900757\n",
      "Epoch 47, batch 192 D Loss: 1.388432264328003, G Loss: 0.6983102560043335\n",
      "Epoch 47, batch 193 D Loss: 1.3855397701263428, G Loss: 0.6957840919494629\n",
      "Epoch 47, batch 194 D Loss: 1.3908997774124146, G Loss: 0.6910457015037537\n",
      "Epoch 47, batch 195 D Loss: 1.3842651844024658, G Loss: 0.6966027617454529\n",
      "Epoch 47, batch 196 D Loss: 1.3787363767623901, G Loss: 0.6979892253875732\n",
      "Epoch 47, batch 197 D Loss: 1.3909597396850586, G Loss: 0.6934282183647156\n",
      "Epoch 47, batch 198 D Loss: 1.384580135345459, G Loss: 0.7005915641784668\n",
      "Epoch 47, batch 199 D Loss: 1.3821697235107422, G Loss: 0.7012437582015991\n",
      "Epoch 47, batch 200 D Loss: 1.3852064609527588, G Loss: 0.7039953470230103\n",
      "Epoch 48, batch 1 D Loss: 1.3826180696487427, G Loss: 0.7016922235488892\n",
      "Epoch 48, batch 2 D Loss: 1.3890278339385986, G Loss: 0.6976526379585266\n",
      "Epoch 48, batch 3 D Loss: 1.3962831497192383, G Loss: 0.6967027187347412\n",
      "Epoch 48, batch 4 D Loss: 1.387634038925171, G Loss: 0.6966342926025391\n",
      "Epoch 48, batch 5 D Loss: 1.3943803310394287, G Loss: 0.6989337801933289\n",
      "Epoch 48, batch 6 D Loss: 1.3882980346679688, G Loss: 0.6992552876472473\n",
      "Epoch 48, batch 7 D Loss: 1.3813140392303467, G Loss: 0.7035118341445923\n",
      "Epoch 48, batch 8 D Loss: 1.3867236375808716, G Loss: 0.7037622928619385\n",
      "Epoch 48, batch 9 D Loss: 1.385288953781128, G Loss: 0.701646089553833\n",
      "Epoch 48, batch 10 D Loss: 1.3826203346252441, G Loss: 0.7054960131645203\n",
      "Epoch 48, batch 11 D Loss: 1.3861947059631348, G Loss: 0.7048904895782471\n",
      "Epoch 48, batch 12 D Loss: 1.3846102952957153, G Loss: 0.7047956585884094\n",
      "Epoch 48, batch 13 D Loss: 1.3828423023223877, G Loss: 0.7092865109443665\n",
      "Epoch 48, batch 14 D Loss: 1.38187575340271, G Loss: 0.7092713713645935\n",
      "Epoch 48, batch 15 D Loss: 1.3848309516906738, G Loss: 0.7051348090171814\n",
      "Epoch 48, batch 16 D Loss: 1.3875946998596191, G Loss: 0.707440972328186\n",
      "Epoch 48, batch 17 D Loss: 1.3878593444824219, G Loss: 0.7086282968521118\n",
      "Epoch 48, batch 18 D Loss: 1.3859782218933105, G Loss: 0.7091047167778015\n",
      "Epoch 48, batch 19 D Loss: 1.3833389282226562, G Loss: 0.7107008099555969\n",
      "Epoch 48, batch 20 D Loss: 1.3826375007629395, G Loss: 0.7118445634841919\n",
      "Epoch 48, batch 21 D Loss: 1.3861699104309082, G Loss: 0.7092617154121399\n",
      "Epoch 48, batch 22 D Loss: 1.3897244930267334, G Loss: 0.7065081596374512\n",
      "Epoch 48, batch 23 D Loss: 1.3835091590881348, G Loss: 0.7155333757400513\n",
      "Epoch 48, batch 24 D Loss: 1.3785046339035034, G Loss: 0.7166173458099365\n",
      "Epoch 48, batch 25 D Loss: 1.3824889659881592, G Loss: 0.7087964415550232\n",
      "Epoch 48, batch 26 D Loss: 1.3836389780044556, G Loss: 0.715999960899353\n",
      "Epoch 48, batch 27 D Loss: 1.3939685821533203, G Loss: 0.7161252498626709\n",
      "Epoch 48, batch 28 D Loss: 1.3818684816360474, G Loss: 0.7138147950172424\n",
      "Epoch 48, batch 29 D Loss: 1.3835320472717285, G Loss: 0.7140995264053345\n",
      "Epoch 48, batch 30 D Loss: 1.3823251724243164, G Loss: 0.7187454104423523\n",
      "Epoch 48, batch 31 D Loss: 1.3820912837982178, G Loss: 0.7135781049728394\n",
      "Epoch 48, batch 32 D Loss: 1.3845419883728027, G Loss: 0.7130391597747803\n",
      "Epoch 48, batch 33 D Loss: 1.3986666202545166, G Loss: 0.7088930010795593\n",
      "Epoch 48, batch 34 D Loss: 1.3826959133148193, G Loss: 0.716235876083374\n",
      "Epoch 48, batch 35 D Loss: 1.3799004554748535, G Loss: 0.7189381122589111\n",
      "Epoch 48, batch 36 D Loss: 1.3851137161254883, G Loss: 0.7166763544082642\n",
      "Epoch 48, batch 37 D Loss: 1.3847012519836426, G Loss: 0.7185332775115967\n",
      "Epoch 48, batch 38 D Loss: 1.3859505653381348, G Loss: 0.7148038744926453\n",
      "Epoch 48, batch 39 D Loss: 1.385675311088562, G Loss: 0.7194263935089111\n",
      "Epoch 48, batch 40 D Loss: 1.3888885974884033, G Loss: 0.7170366644859314\n",
      "Epoch 48, batch 41 D Loss: 1.3761855363845825, G Loss: 0.7237529158592224\n",
      "Epoch 48, batch 42 D Loss: 1.3796926736831665, G Loss: 0.7155201435089111\n",
      "Epoch 48, batch 43 D Loss: 1.3805989027023315, G Loss: 0.7171271443367004\n",
      "Epoch 48, batch 44 D Loss: 1.3780107498168945, G Loss: 0.7193034291267395\n",
      "Epoch 48, batch 45 D Loss: 1.3792258501052856, G Loss: 0.7309741377830505\n",
      "Epoch 48, batch 46 D Loss: 1.3689907789230347, G Loss: 0.7282147407531738\n",
      "Epoch 48, batch 47 D Loss: 1.3778681755065918, G Loss: 0.7194442749023438\n",
      "Epoch 48, batch 48 D Loss: 1.3860054016113281, G Loss: 0.7223728895187378\n",
      "Epoch 48, batch 49 D Loss: 1.3860063552856445, G Loss: 0.7171857953071594\n",
      "Epoch 48, batch 50 D Loss: 1.3807374238967896, G Loss: 0.7243662476539612\n",
      "Epoch 48, batch 51 D Loss: 1.3718700408935547, G Loss: 0.7290841937065125\n",
      "Epoch 48, batch 52 D Loss: 1.383805513381958, G Loss: 0.7285873889923096\n",
      "Epoch 48, batch 53 D Loss: 1.3883354663848877, G Loss: 0.7280003428459167\n",
      "Epoch 48, batch 54 D Loss: 1.3834201097488403, G Loss: 0.7266843914985657\n",
      "Epoch 48, batch 55 D Loss: 1.395648717880249, G Loss: 0.7203176021575928\n",
      "Epoch 48, batch 56 D Loss: 1.389634132385254, G Loss: 0.7249199151992798\n",
      "Epoch 48, batch 57 D Loss: 1.391913890838623, G Loss: 0.7225925326347351\n",
      "Epoch 48, batch 58 D Loss: 1.3835914134979248, G Loss: 0.7406060695648193\n",
      "Epoch 48, batch 59 D Loss: 1.3715784549713135, G Loss: 0.7358837127685547\n",
      "Epoch 48, batch 60 D Loss: 1.3851664066314697, G Loss: 0.7292900681495667\n",
      "Epoch 48, batch 61 D Loss: 1.386306881904602, G Loss: 0.7287339568138123\n",
      "Epoch 48, batch 62 D Loss: 1.385648488998413, G Loss: 0.7247839570045471\n",
      "Epoch 48, batch 63 D Loss: 1.3880943059921265, G Loss: 0.7176696062088013\n",
      "Epoch 48, batch 64 D Loss: 1.387573003768921, G Loss: 0.7191756963729858\n",
      "Epoch 48, batch 65 D Loss: 1.4006171226501465, G Loss: 0.7252731323242188\n",
      "Epoch 48, batch 66 D Loss: 1.3779127597808838, G Loss: 0.7296483516693115\n",
      "Epoch 48, batch 67 D Loss: 1.3840312957763672, G Loss: 0.7135884165763855\n",
      "Epoch 48, batch 68 D Loss: 1.3729431629180908, G Loss: 0.7250993251800537\n",
      "Epoch 48, batch 69 D Loss: 1.3861448764801025, G Loss: 0.707542359828949\n",
      "Epoch 48, batch 70 D Loss: 1.3971301317214966, G Loss: 0.7121954560279846\n",
      "Epoch 48, batch 71 D Loss: 1.3852027654647827, G Loss: 0.7201404571533203\n",
      "Epoch 48, batch 72 D Loss: 1.3944109678268433, G Loss: 0.7088775038719177\n",
      "Epoch 48, batch 73 D Loss: 1.3994625806808472, G Loss: 0.7104843258857727\n",
      "Epoch 48, batch 74 D Loss: 1.4066431522369385, G Loss: 0.7049170732498169\n",
      "Epoch 48, batch 75 D Loss: 1.4027607440948486, G Loss: 0.703403115272522\n",
      "Epoch 48, batch 76 D Loss: 1.399829387664795, G Loss: 0.7092940807342529\n",
      "Epoch 48, batch 77 D Loss: 1.39076566696167, G Loss: 0.7038915157318115\n",
      "Epoch 48, batch 78 D Loss: 1.4075685739517212, G Loss: 0.6954130530357361\n",
      "Epoch 48, batch 79 D Loss: 1.3877925872802734, G Loss: 0.7111462354660034\n",
      "Epoch 48, batch 80 D Loss: 1.4043540954589844, G Loss: 0.7018261551856995\n",
      "Epoch 48, batch 81 D Loss: 1.3837066888809204, G Loss: 0.7083477973937988\n",
      "Epoch 48, batch 82 D Loss: 1.4062420129776, G Loss: 0.6940122842788696\n",
      "Epoch 48, batch 83 D Loss: 1.3847806453704834, G Loss: 0.7059366106987\n",
      "Epoch 48, batch 84 D Loss: 1.3951168060302734, G Loss: 0.7023407220840454\n",
      "Epoch 48, batch 85 D Loss: 1.3900799751281738, G Loss: 0.6984633803367615\n",
      "Epoch 48, batch 86 D Loss: 1.385771632194519, G Loss: 0.704852819442749\n",
      "Epoch 48, batch 87 D Loss: 1.3942677974700928, G Loss: 0.7044752240180969\n",
      "Epoch 48, batch 88 D Loss: 1.394113302230835, G Loss: 0.6991950273513794\n",
      "Epoch 48, batch 89 D Loss: 1.3964273929595947, G Loss: 0.6973980069160461\n",
      "Epoch 48, batch 90 D Loss: 1.4028559923171997, G Loss: 0.699097752571106\n",
      "Epoch 48, batch 91 D Loss: 1.388959527015686, G Loss: 0.6958533525466919\n",
      "Epoch 48, batch 92 D Loss: 1.395082712173462, G Loss: 0.6947073340415955\n",
      "Epoch 48, batch 93 D Loss: 1.39633047580719, G Loss: 0.6932855248451233\n",
      "Epoch 48, batch 94 D Loss: 1.3941233158111572, G Loss: 0.6998122334480286\n",
      "Epoch 48, batch 95 D Loss: 1.4031226634979248, G Loss: 0.6860278844833374\n",
      "Epoch 48, batch 96 D Loss: 1.391777753829956, G Loss: 0.6935217976570129\n",
      "Epoch 48, batch 97 D Loss: 1.388063669204712, G Loss: 0.6950436234474182\n",
      "Epoch 48, batch 98 D Loss: 1.3911020755767822, G Loss: 0.6941967010498047\n",
      "Epoch 48, batch 99 D Loss: 1.3943307399749756, G Loss: 0.6928755044937134\n",
      "Epoch 48, batch 100 D Loss: 1.3964226245880127, G Loss: 0.6946097612380981\n",
      "Epoch 48, batch 101 D Loss: 1.3964436054229736, G Loss: 0.6889867186546326\n",
      "Epoch 48, batch 102 D Loss: 1.3896071910858154, G Loss: 0.6921287775039673\n",
      "Epoch 48, batch 103 D Loss: 1.3942327499389648, G Loss: 0.6914089918136597\n",
      "Epoch 48, batch 104 D Loss: 1.3919854164123535, G Loss: 0.6940737962722778\n",
      "Epoch 48, batch 105 D Loss: 1.394087791442871, G Loss: 0.6925705671310425\n",
      "Epoch 48, batch 106 D Loss: 1.3884811401367188, G Loss: 0.6938928961753845\n",
      "Epoch 48, batch 107 D Loss: 1.3997087478637695, G Loss: 0.6869593858718872\n",
      "Epoch 48, batch 108 D Loss: 1.390658974647522, G Loss: 0.6917332410812378\n",
      "Epoch 48, batch 109 D Loss: 1.3852059841156006, G Loss: 0.6904520988464355\n",
      "Epoch 48, batch 110 D Loss: 1.3856122493743896, G Loss: 0.6914284229278564\n",
      "Epoch 48, batch 111 D Loss: 1.3891446590423584, G Loss: 0.6924597024917603\n",
      "Epoch 48, batch 112 D Loss: 1.390765905380249, G Loss: 0.6872375011444092\n",
      "Epoch 48, batch 113 D Loss: 1.3863065242767334, G Loss: 0.6904577612876892\n",
      "Epoch 48, batch 114 D Loss: 1.3924696445465088, G Loss: 0.6885707974433899\n",
      "Epoch 48, batch 115 D Loss: 1.3884241580963135, G Loss: 0.6922866106033325\n",
      "Epoch 48, batch 116 D Loss: 1.392989158630371, G Loss: 0.6902134418487549\n",
      "Epoch 48, batch 117 D Loss: 1.3904144763946533, G Loss: 0.6938738226890564\n",
      "Epoch 48, batch 118 D Loss: 1.391049861907959, G Loss: 0.690072238445282\n",
      "Epoch 48, batch 119 D Loss: 1.3920371532440186, G Loss: 0.6896098256111145\n",
      "Epoch 48, batch 120 D Loss: 1.3885449171066284, G Loss: 0.6920220851898193\n",
      "Epoch 48, batch 121 D Loss: 1.393537998199463, G Loss: 0.6893185973167419\n",
      "Epoch 48, batch 122 D Loss: 1.3883212804794312, G Loss: 0.6944112181663513\n",
      "Epoch 48, batch 123 D Loss: 1.3865971565246582, G Loss: 0.6908516883850098\n",
      "Epoch 48, batch 124 D Loss: 1.3940606117248535, G Loss: 0.6922508478164673\n",
      "Epoch 48, batch 125 D Loss: 1.3912811279296875, G Loss: 0.6912564635276794\n",
      "Epoch 48, batch 126 D Loss: 1.3909838199615479, G Loss: 0.6896461248397827\n",
      "Epoch 48, batch 127 D Loss: 1.3840968608856201, G Loss: 0.6959443688392639\n",
      "Epoch 48, batch 128 D Loss: 1.3922760486602783, G Loss: 0.6911075115203857\n",
      "Epoch 48, batch 129 D Loss: 1.3882548809051514, G Loss: 0.6938469409942627\n",
      "Epoch 48, batch 130 D Loss: 1.387786626815796, G Loss: 0.688795268535614\n",
      "Epoch 48, batch 131 D Loss: 1.3865396976470947, G Loss: 0.692381739616394\n",
      "Epoch 48, batch 132 D Loss: 1.389317512512207, G Loss: 0.6923631429672241\n",
      "Epoch 48, batch 133 D Loss: 1.3879351615905762, G Loss: 0.6924738883972168\n",
      "Epoch 48, batch 134 D Loss: 1.3851897716522217, G Loss: 0.6927585005760193\n",
      "Epoch 48, batch 135 D Loss: 1.386380910873413, G Loss: 0.6915910840034485\n",
      "Epoch 48, batch 136 D Loss: 1.3873730897903442, G Loss: 0.691274106502533\n",
      "Epoch 48, batch 137 D Loss: 1.388237714767456, G Loss: 0.6906091570854187\n",
      "Epoch 48, batch 138 D Loss: 1.3863980770111084, G Loss: 0.6917756795883179\n",
      "Epoch 48, batch 139 D Loss: 1.3848907947540283, G Loss: 0.6914669275283813\n",
      "Epoch 48, batch 140 D Loss: 1.3881169557571411, G Loss: 0.6895016431808472\n",
      "Epoch 48, batch 141 D Loss: 1.385955810546875, G Loss: 0.6903240084648132\n",
      "Epoch 48, batch 142 D Loss: 1.3870878219604492, G Loss: 0.6899856328964233\n",
      "Epoch 48, batch 143 D Loss: 1.3871464729309082, G Loss: 0.6926005482673645\n",
      "Epoch 48, batch 144 D Loss: 1.3855371475219727, G Loss: 0.6918094158172607\n",
      "Epoch 48, batch 145 D Loss: 1.38336181640625, G Loss: 0.6937634348869324\n",
      "Epoch 48, batch 146 D Loss: 1.3793340921401978, G Loss: 0.69926917552948\n",
      "Epoch 48, batch 147 D Loss: 1.3860130310058594, G Loss: 0.6912990808486938\n",
      "Epoch 48, batch 148 D Loss: 1.38797926902771, G Loss: 0.6913477182388306\n",
      "Epoch 48, batch 149 D Loss: 1.3827345371246338, G Loss: 0.6935182213783264\n",
      "Epoch 48, batch 150 D Loss: 1.3823463916778564, G Loss: 0.6941525936126709\n",
      "Epoch 48, batch 151 D Loss: 1.3914740085601807, G Loss: 0.6894899010658264\n",
      "Epoch 48, batch 152 D Loss: 1.3821604251861572, G Loss: 0.6951863765716553\n",
      "Epoch 48, batch 153 D Loss: 1.379230260848999, G Loss: 0.6977792978286743\n",
      "Epoch 48, batch 154 D Loss: 1.3787143230438232, G Loss: 0.6970345973968506\n",
      "Epoch 48, batch 155 D Loss: 1.384838581085205, G Loss: 0.6910769939422607\n",
      "Epoch 48, batch 156 D Loss: 1.379631519317627, G Loss: 0.6961543560028076\n",
      "Epoch 48, batch 157 D Loss: 1.3786735534667969, G Loss: 0.6961147785186768\n",
      "Epoch 48, batch 158 D Loss: 1.3781309127807617, G Loss: 0.6990352869033813\n",
      "Epoch 48, batch 159 D Loss: 1.3815720081329346, G Loss: 0.6956357359886169\n",
      "Epoch 48, batch 160 D Loss: 1.3825366497039795, G Loss: 0.6944129467010498\n",
      "Epoch 48, batch 161 D Loss: 1.3833072185516357, G Loss: 0.6948246955871582\n",
      "Epoch 48, batch 162 D Loss: 1.3763724565505981, G Loss: 0.7024368047714233\n",
      "Epoch 48, batch 163 D Loss: 1.3735995292663574, G Loss: 0.7042125463485718\n",
      "Epoch 48, batch 164 D Loss: 1.37361741065979, G Loss: 0.7052967548370361\n",
      "Epoch 48, batch 165 D Loss: 1.3758468627929688, G Loss: 0.7015961408615112\n",
      "Epoch 48, batch 166 D Loss: 1.3859124183654785, G Loss: 0.6934661865234375\n",
      "Epoch 48, batch 167 D Loss: 1.3880274295806885, G Loss: 0.6947996020317078\n",
      "Epoch 48, batch 168 D Loss: 1.3832488059997559, G Loss: 0.6974163055419922\n",
      "Epoch 48, batch 169 D Loss: 1.3870658874511719, G Loss: 0.6932426691055298\n",
      "Epoch 48, batch 170 D Loss: 1.3809289932250977, G Loss: 0.6935166716575623\n",
      "Epoch 48, batch 171 D Loss: 1.3731603622436523, G Loss: 0.7011263370513916\n",
      "Epoch 48, batch 172 D Loss: 1.3814539909362793, G Loss: 0.6940873861312866\n",
      "Epoch 48, batch 173 D Loss: 1.3908641338348389, G Loss: 0.6964410543441772\n",
      "Epoch 48, batch 174 D Loss: 1.3837697505950928, G Loss: 0.692085862159729\n",
      "Epoch 48, batch 175 D Loss: 1.3886635303497314, G Loss: 0.6957833170890808\n",
      "Epoch 48, batch 176 D Loss: 1.3792039155960083, G Loss: 0.6975045800209045\n",
      "Epoch 48, batch 177 D Loss: 1.3760936260223389, G Loss: 0.6960663795471191\n",
      "Epoch 48, batch 178 D Loss: 1.378427267074585, G Loss: 0.6951202154159546\n",
      "Epoch 48, batch 179 D Loss: 1.3826195001602173, G Loss: 0.6918359994888306\n",
      "Epoch 48, batch 180 D Loss: 1.38039231300354, G Loss: 0.6963160634040833\n",
      "Epoch 48, batch 181 D Loss: 1.3822516202926636, G Loss: 0.6946708559989929\n",
      "Epoch 48, batch 182 D Loss: 1.3831067085266113, G Loss: 0.6891852021217346\n",
      "Epoch 48, batch 183 D Loss: 1.3816988468170166, G Loss: 0.7102914452552795\n",
      "Epoch 48, batch 184 D Loss: 1.3839516639709473, G Loss: 0.6910358667373657\n",
      "Epoch 48, batch 185 D Loss: 1.3814527988433838, G Loss: 0.6937205791473389\n",
      "Epoch 48, batch 186 D Loss: 1.3860673904418945, G Loss: 0.6947337985038757\n",
      "Epoch 48, batch 187 D Loss: 1.3916707038879395, G Loss: 0.6892329454421997\n",
      "Epoch 48, batch 188 D Loss: 1.377209186553955, G Loss: 0.6944198608398438\n",
      "Epoch 48, batch 189 D Loss: 1.383760690689087, G Loss: 0.6958727836608887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, batch 190 D Loss: 1.377650260925293, G Loss: 0.6938386559486389\n",
      "Epoch 48, batch 191 D Loss: 1.3852355480194092, G Loss: 0.6905505657196045\n",
      "Epoch 48, batch 192 D Loss: 1.3793041706085205, G Loss: 0.6966549754142761\n",
      "Epoch 48, batch 193 D Loss: 1.3910270929336548, G Loss: 0.6906784772872925\n",
      "Epoch 48, batch 194 D Loss: 1.3879879713058472, G Loss: 0.6940996646881104\n",
      "Epoch 48, batch 195 D Loss: 1.3919739723205566, G Loss: 0.6888917684555054\n",
      "Epoch 48, batch 196 D Loss: 1.3832926750183105, G Loss: 0.6860865950584412\n",
      "Epoch 48, batch 197 D Loss: 1.3901269435882568, G Loss: 0.6892568469047546\n",
      "Epoch 48, batch 198 D Loss: 1.386378288269043, G Loss: 0.6959890723228455\n",
      "Epoch 48, batch 199 D Loss: 1.390831470489502, G Loss: 0.6881030201911926\n",
      "Epoch 48, batch 200 D Loss: 1.3838801383972168, G Loss: 0.6897173523902893\n",
      "Epoch 49, batch 1 D Loss: 1.3847196102142334, G Loss: 0.6979362964630127\n",
      "Epoch 49, batch 2 D Loss: 1.3904242515563965, G Loss: 0.6879402995109558\n",
      "Epoch 49, batch 3 D Loss: 1.3915119171142578, G Loss: 0.6880830526351929\n",
      "Epoch 49, batch 4 D Loss: 1.375106692314148, G Loss: 0.6967728137969971\n",
      "Epoch 49, batch 5 D Loss: 1.3925790786743164, G Loss: 0.6847332119941711\n",
      "Epoch 49, batch 6 D Loss: 1.4004276990890503, G Loss: 0.6860391497612\n",
      "Epoch 49, batch 7 D Loss: 1.38128662109375, G Loss: 0.693499743938446\n",
      "Epoch 49, batch 8 D Loss: 1.3953616619110107, G Loss: 0.6797670722007751\n",
      "Epoch 49, batch 9 D Loss: 1.3823014497756958, G Loss: 0.6916123032569885\n",
      "Epoch 49, batch 10 D Loss: 1.386427640914917, G Loss: 0.6883093118667603\n",
      "Epoch 49, batch 11 D Loss: 1.394347906112671, G Loss: 0.6882275342941284\n",
      "Epoch 49, batch 12 D Loss: 1.4039585590362549, G Loss: 0.6892034411430359\n",
      "Epoch 49, batch 13 D Loss: 1.3779473304748535, G Loss: 0.6883324384689331\n",
      "Epoch 49, batch 14 D Loss: 1.383795976638794, G Loss: 0.6867687702178955\n",
      "Epoch 49, batch 15 D Loss: 1.3939173221588135, G Loss: 0.6814100742340088\n",
      "Epoch 49, batch 16 D Loss: 1.384941577911377, G Loss: 0.6868891716003418\n",
      "Epoch 49, batch 17 D Loss: 1.3764843940734863, G Loss: 0.6899327039718628\n",
      "Epoch 49, batch 18 D Loss: 1.3815364837646484, G Loss: 0.6924075484275818\n",
      "Epoch 49, batch 19 D Loss: 1.384648323059082, G Loss: 0.686572253704071\n",
      "Epoch 49, batch 20 D Loss: 1.3920997381210327, G Loss: 0.6829313039779663\n",
      "Epoch 49, batch 21 D Loss: 1.3862214088439941, G Loss: 0.6822226643562317\n",
      "Epoch 49, batch 22 D Loss: 1.3818418979644775, G Loss: 0.6852424144744873\n",
      "Epoch 49, batch 23 D Loss: 1.3816853761672974, G Loss: 0.6855313181877136\n",
      "Epoch 49, batch 24 D Loss: 1.3852999210357666, G Loss: 0.6827256679534912\n",
      "Epoch 49, batch 25 D Loss: 1.3839895725250244, G Loss: 0.688129723072052\n",
      "Epoch 49, batch 26 D Loss: 1.3814961910247803, G Loss: 0.6807984709739685\n",
      "Epoch 49, batch 27 D Loss: 1.384263277053833, G Loss: 0.6839126348495483\n",
      "Epoch 49, batch 28 D Loss: 1.3787007331848145, G Loss: 0.6860598921775818\n",
      "Epoch 49, batch 29 D Loss: 1.3917603492736816, G Loss: 0.6803845763206482\n",
      "Epoch 49, batch 30 D Loss: 1.3877835273742676, G Loss: 0.6856947541236877\n",
      "Epoch 49, batch 31 D Loss: 1.389073371887207, G Loss: 0.6799495816230774\n",
      "Epoch 49, batch 32 D Loss: 1.3918917179107666, G Loss: 0.6793169379234314\n",
      "Epoch 49, batch 33 D Loss: 1.389437198638916, G Loss: 0.6794038414955139\n",
      "Epoch 49, batch 34 D Loss: 1.3901686668395996, G Loss: 0.6774673461914062\n",
      "Epoch 49, batch 35 D Loss: 1.3933265209197998, G Loss: 0.6757146716117859\n",
      "Epoch 49, batch 36 D Loss: 1.3732576370239258, G Loss: 0.6826000213623047\n",
      "Epoch 49, batch 37 D Loss: 1.3817176818847656, G Loss: 0.6814556121826172\n",
      "Epoch 49, batch 38 D Loss: 1.3844798803329468, G Loss: 0.6752310395240784\n",
      "Epoch 49, batch 39 D Loss: 1.3849554061889648, G Loss: 0.684746503829956\n",
      "Epoch 49, batch 40 D Loss: 1.3804478645324707, G Loss: 0.6815983653068542\n",
      "Epoch 49, batch 41 D Loss: 1.3880665302276611, G Loss: 0.6779881119728088\n",
      "Epoch 49, batch 42 D Loss: 1.3854811191558838, G Loss: 0.6849343776702881\n",
      "Epoch 49, batch 43 D Loss: 1.3844012022018433, G Loss: 0.6853954792022705\n",
      "Epoch 49, batch 44 D Loss: 1.3940258026123047, G Loss: 0.6779359579086304\n",
      "Epoch 49, batch 45 D Loss: 1.3881592750549316, G Loss: 0.6830446124076843\n",
      "Epoch 49, batch 46 D Loss: 1.381287932395935, G Loss: 0.6855970621109009\n",
      "Epoch 49, batch 47 D Loss: 1.3876943588256836, G Loss: 0.6841264963150024\n",
      "Epoch 49, batch 48 D Loss: 1.3793556690216064, G Loss: 0.6901052594184875\n",
      "Epoch 49, batch 49 D Loss: 1.3851492404937744, G Loss: 0.6847729682922363\n",
      "Epoch 49, batch 50 D Loss: 1.3805360794067383, G Loss: 0.6864620447158813\n",
      "Epoch 49, batch 51 D Loss: 1.385738730430603, G Loss: 0.6823323965072632\n",
      "Epoch 49, batch 52 D Loss: 1.3904129266738892, G Loss: 0.6829820275306702\n",
      "Epoch 49, batch 53 D Loss: 1.3930656909942627, G Loss: 0.6768834590911865\n",
      "Epoch 49, batch 54 D Loss: 1.3796308040618896, G Loss: 0.6871139407157898\n",
      "Epoch 49, batch 55 D Loss: 1.3835090398788452, G Loss: 0.6757597327232361\n",
      "Epoch 49, batch 56 D Loss: 1.3827435970306396, G Loss: 0.6807270646095276\n",
      "Epoch 49, batch 57 D Loss: 1.3882145881652832, G Loss: 0.6746727228164673\n",
      "Epoch 49, batch 58 D Loss: 1.3839993476867676, G Loss: 0.6851111054420471\n",
      "Epoch 49, batch 59 D Loss: 1.3933329582214355, G Loss: 0.6784341335296631\n",
      "Epoch 49, batch 60 D Loss: 1.3866236209869385, G Loss: 0.6815106868743896\n",
      "Epoch 49, batch 61 D Loss: 1.3854562044143677, G Loss: 0.6869569420814514\n",
      "Epoch 49, batch 62 D Loss: 1.3858689069747925, G Loss: 0.6884152889251709\n",
      "Epoch 49, batch 63 D Loss: 1.391257882118225, G Loss: 0.6781290173530579\n",
      "Epoch 49, batch 64 D Loss: 1.3820180892944336, G Loss: 0.6840760111808777\n",
      "Epoch 49, batch 65 D Loss: 1.3776066303253174, G Loss: 0.6880494952201843\n",
      "Epoch 49, batch 66 D Loss: 1.3842675685882568, G Loss: 0.6830406188964844\n",
      "Epoch 49, batch 67 D Loss: 1.3694192171096802, G Loss: 0.6885109543800354\n",
      "Epoch 49, batch 68 D Loss: 1.3783605098724365, G Loss: 0.6846264004707336\n",
      "Epoch 49, batch 69 D Loss: 1.3814845085144043, G Loss: 0.6830207109451294\n",
      "Epoch 49, batch 70 D Loss: 1.3726887702941895, G Loss: 0.6833944916725159\n",
      "Epoch 49, batch 71 D Loss: 1.396135926246643, G Loss: 0.6739223599433899\n",
      "Epoch 49, batch 72 D Loss: 1.3783377408981323, G Loss: 0.6850430965423584\n",
      "Epoch 49, batch 73 D Loss: 1.382114291191101, G Loss: 0.6738523840904236\n",
      "Epoch 49, batch 74 D Loss: 1.369452714920044, G Loss: 0.6868795156478882\n",
      "Epoch 49, batch 75 D Loss: 1.3729066848754883, G Loss: 0.6834602952003479\n",
      "Epoch 49, batch 76 D Loss: 1.3758323192596436, G Loss: 0.6797093152999878\n",
      "Epoch 49, batch 77 D Loss: 1.3871986865997314, G Loss: 0.6746363043785095\n",
      "Epoch 49, batch 78 D Loss: 1.3848233222961426, G Loss: 0.6820800304412842\n",
      "Epoch 49, batch 79 D Loss: 1.3755989074707031, G Loss: 0.6836436986923218\n",
      "Epoch 49, batch 80 D Loss: 1.3622055053710938, G Loss: 0.6868569254875183\n",
      "Epoch 49, batch 81 D Loss: 1.387366533279419, G Loss: 0.6717302203178406\n",
      "Epoch 49, batch 82 D Loss: 1.3724126815795898, G Loss: 0.6798880696296692\n",
      "Epoch 49, batch 83 D Loss: 1.3721847534179688, G Loss: 0.6817815899848938\n",
      "Epoch 49, batch 84 D Loss: 1.3657989501953125, G Loss: 0.6781071424484253\n",
      "Epoch 49, batch 85 D Loss: 1.3764910697937012, G Loss: 0.678933322429657\n",
      "Epoch 49, batch 86 D Loss: 1.3884457349777222, G Loss: 0.6675750613212585\n",
      "Epoch 49, batch 87 D Loss: 1.3787684440612793, G Loss: 0.6740787029266357\n",
      "Epoch 49, batch 88 D Loss: 1.3841891288757324, G Loss: 0.6772163510322571\n",
      "Epoch 49, batch 89 D Loss: 1.3738391399383545, G Loss: 0.6764466166496277\n",
      "Epoch 49, batch 90 D Loss: 1.3826555013656616, G Loss: 0.6739596724510193\n",
      "Epoch 49, batch 91 D Loss: 1.3734444379806519, G Loss: 0.6730597615242004\n",
      "Epoch 49, batch 92 D Loss: 1.3825232982635498, G Loss: 0.682902991771698\n",
      "Epoch 49, batch 93 D Loss: 1.388866662979126, G Loss: 0.6694554090499878\n",
      "Epoch 49, batch 94 D Loss: 1.361483097076416, G Loss: 0.6848382353782654\n",
      "Epoch 49, batch 95 D Loss: 1.383718490600586, G Loss: 0.6716147065162659\n",
      "Epoch 49, batch 96 D Loss: 1.3812062740325928, G Loss: 0.6763479709625244\n",
      "Epoch 49, batch 97 D Loss: 1.3800175189971924, G Loss: 0.6779171824455261\n",
      "Epoch 49, batch 98 D Loss: 1.3871190547943115, G Loss: 0.6745875477790833\n",
      "Epoch 49, batch 99 D Loss: 1.3748846054077148, G Loss: 0.6786604523658752\n",
      "Epoch 49, batch 100 D Loss: 1.382042646408081, G Loss: 0.6761283278465271\n",
      "Epoch 49, batch 101 D Loss: 1.4137718677520752, G Loss: 0.6555846333503723\n",
      "Epoch 49, batch 102 D Loss: 1.4086966514587402, G Loss: 0.6620611548423767\n",
      "Epoch 49, batch 103 D Loss: 1.4068689346313477, G Loss: 0.6589862108230591\n",
      "Epoch 49, batch 104 D Loss: 1.3967514038085938, G Loss: 0.6629312038421631\n",
      "Epoch 49, batch 105 D Loss: 1.4078528881072998, G Loss: 0.6642960906028748\n",
      "Epoch 49, batch 106 D Loss: 1.399253010749817, G Loss: 0.6699352264404297\n",
      "Epoch 49, batch 107 D Loss: 1.4273227453231812, G Loss: 0.653730034828186\n",
      "Epoch 49, batch 108 D Loss: 1.3894026279449463, G Loss: 0.6755186319351196\n",
      "Epoch 49, batch 109 D Loss: 1.3688371181488037, G Loss: 0.6904220581054688\n",
      "Epoch 49, batch 110 D Loss: 1.3777469396591187, G Loss: 0.6911459565162659\n",
      "Epoch 49, batch 111 D Loss: 1.4119634628295898, G Loss: 0.6700246930122375\n",
      "Epoch 49, batch 112 D Loss: 1.3948330879211426, G Loss: 0.677585780620575\n",
      "Epoch 49, batch 113 D Loss: 1.402230143547058, G Loss: 0.6728435754776001\n",
      "Epoch 49, batch 114 D Loss: 1.4224586486816406, G Loss: 0.6676827073097229\n",
      "Epoch 49, batch 115 D Loss: 1.4090691804885864, G Loss: 0.678417444229126\n",
      "Epoch 49, batch 116 D Loss: 1.4043488502502441, G Loss: 0.6823357939720154\n",
      "Epoch 49, batch 117 D Loss: 1.3920024633407593, G Loss: 0.6772733330726624\n",
      "Epoch 49, batch 118 D Loss: 1.3933379650115967, G Loss: 0.6868463754653931\n",
      "Epoch 49, batch 119 D Loss: 1.3965786695480347, G Loss: 0.6952030062675476\n",
      "Epoch 49, batch 120 D Loss: 1.3924223184585571, G Loss: 0.6852554082870483\n",
      "Epoch 49, batch 121 D Loss: 1.414785385131836, G Loss: 0.6785123944282532\n",
      "Epoch 49, batch 122 D Loss: 1.4115703105926514, G Loss: 0.6877850890159607\n",
      "Epoch 49, batch 123 D Loss: 1.3974504470825195, G Loss: 0.6896646618843079\n",
      "Epoch 49, batch 124 D Loss: 1.393998622894287, G Loss: 0.6972463130950928\n",
      "Epoch 49, batch 125 D Loss: 1.3948516845703125, G Loss: 0.7001301646232605\n",
      "Epoch 49, batch 126 D Loss: 1.404737949371338, G Loss: 0.6953589916229248\n",
      "Epoch 49, batch 127 D Loss: 1.384865641593933, G Loss: 0.704062283039093\n",
      "Epoch 49, batch 128 D Loss: 1.4062068462371826, G Loss: 0.6971569657325745\n",
      "Epoch 49, batch 129 D Loss: 1.3969416618347168, G Loss: 0.7087369561195374\n",
      "Epoch 49, batch 130 D Loss: 1.3813166618347168, G Loss: 0.707582414150238\n",
      "Epoch 49, batch 131 D Loss: 1.3892815113067627, G Loss: 0.7047945261001587\n",
      "Epoch 49, batch 132 D Loss: 1.3721210956573486, G Loss: 0.7223765850067139\n",
      "Epoch 49, batch 133 D Loss: 1.3770108222961426, G Loss: 0.7103315591812134\n",
      "Epoch 49, batch 134 D Loss: 1.3880696296691895, G Loss: 0.7091414928436279\n",
      "Epoch 49, batch 135 D Loss: 1.3737953901290894, G Loss: 0.7262862920761108\n",
      "Epoch 49, batch 136 D Loss: 1.378010869026184, G Loss: 0.7161243557929993\n",
      "Epoch 49, batch 137 D Loss: 1.3832907676696777, G Loss: 0.7214886546134949\n",
      "Epoch 49, batch 138 D Loss: 1.3830910921096802, G Loss: 0.7186293005943298\n",
      "Epoch 49, batch 139 D Loss: 1.3803768157958984, G Loss: 0.714285135269165\n",
      "Epoch 49, batch 140 D Loss: 1.358839988708496, G Loss: 0.7440586090087891\n",
      "Epoch 49, batch 141 D Loss: 1.3769214153289795, G Loss: 0.7304091453552246\n",
      "Epoch 49, batch 142 D Loss: 1.3904495239257812, G Loss: 0.7168262600898743\n",
      "Epoch 49, batch 143 D Loss: 1.374584436416626, G Loss: 0.7310521006584167\n",
      "Epoch 49, batch 144 D Loss: 1.3773305416107178, G Loss: 0.7219869494438171\n",
      "Epoch 49, batch 145 D Loss: 1.4001150131225586, G Loss: 0.7104746103286743\n",
      "Epoch 49, batch 146 D Loss: 1.3480304479599, G Loss: 0.7688882946968079\n",
      "Epoch 49, batch 147 D Loss: 1.362358570098877, G Loss: 0.7430092692375183\n",
      "Epoch 49, batch 148 D Loss: 1.3755675554275513, G Loss: 0.7446379661560059\n",
      "Epoch 49, batch 149 D Loss: 1.3497602939605713, G Loss: 0.7516629099845886\n",
      "Epoch 49, batch 150 D Loss: 1.3950395584106445, G Loss: 0.7244968414306641\n",
      "Epoch 49, batch 151 D Loss: 1.3944231271743774, G Loss: 0.7129721641540527\n",
      "Epoch 49, batch 152 D Loss: 1.3692941665649414, G Loss: 0.7365396618843079\n",
      "Epoch 49, batch 153 D Loss: 1.3467077016830444, G Loss: 0.763969898223877\n",
      "Epoch 49, batch 154 D Loss: 1.372268795967102, G Loss: 0.7456239461898804\n",
      "Epoch 49, batch 155 D Loss: 1.3813347816467285, G Loss: 0.7285706996917725\n",
      "Epoch 49, batch 156 D Loss: 1.3787322044372559, G Loss: 0.7362688183784485\n",
      "Epoch 49, batch 157 D Loss: 1.3583472967147827, G Loss: 0.7354137897491455\n",
      "Epoch 49, batch 158 D Loss: 1.3682117462158203, G Loss: 0.7673388719558716\n",
      "Epoch 49, batch 159 D Loss: 1.3966610431671143, G Loss: 0.7216235399246216\n",
      "Epoch 49, batch 160 D Loss: 1.3589906692504883, G Loss: 0.7400854229927063\n",
      "Epoch 49, batch 161 D Loss: 1.3922055959701538, G Loss: 0.7139428853988647\n",
      "Epoch 49, batch 162 D Loss: 1.4039301872253418, G Loss: 0.7267038226127625\n",
      "Epoch 49, batch 163 D Loss: 1.3765299320220947, G Loss: 0.732322096824646\n",
      "Epoch 49, batch 164 D Loss: 1.3593087196350098, G Loss: 0.7369570136070251\n",
      "Epoch 49, batch 165 D Loss: 1.3947315216064453, G Loss: 0.7138987183570862\n",
      "Epoch 49, batch 166 D Loss: 1.3999719619750977, G Loss: 0.699934720993042\n",
      "Epoch 49, batch 167 D Loss: 1.399580955505371, G Loss: 0.7022117376327515\n",
      "Epoch 49, batch 168 D Loss: 1.3674147129058838, G Loss: 0.7166260480880737\n",
      "Epoch 49, batch 169 D Loss: 1.3918371200561523, G Loss: 0.7186380624771118\n",
      "Epoch 49, batch 170 D Loss: 1.3754668235778809, G Loss: 0.7195022106170654\n",
      "Epoch 49, batch 171 D Loss: 1.3939847946166992, G Loss: 0.7120494842529297\n",
      "Epoch 49, batch 172 D Loss: 1.380530595779419, G Loss: 0.7252094149589539\n",
      "Epoch 49, batch 173 D Loss: 1.381408929824829, G Loss: 0.7042781114578247\n",
      "Epoch 49, batch 174 D Loss: 1.3987407684326172, G Loss: 0.6942306756973267\n",
      "Epoch 49, batch 175 D Loss: 1.3885116577148438, G Loss: 0.7154041528701782\n",
      "Epoch 49, batch 176 D Loss: 1.3906323909759521, G Loss: 0.7150871157646179\n",
      "Epoch 49, batch 177 D Loss: 1.4178392887115479, G Loss: 0.6977880597114563\n",
      "Epoch 49, batch 178 D Loss: 1.4216303825378418, G Loss: 0.689748227596283\n",
      "Epoch 49, batch 179 D Loss: 1.3956104516983032, G Loss: 0.7079909443855286\n",
      "Epoch 49, batch 180 D Loss: 1.436638593673706, G Loss: 0.6843120455741882\n",
      "Epoch 49, batch 181 D Loss: 1.4298655986785889, G Loss: 0.6900858283042908\n",
      "Epoch 49, batch 182 D Loss: 1.371495008468628, G Loss: 0.7287594676017761\n",
      "Epoch 49, batch 183 D Loss: 1.4035379886627197, G Loss: 0.6991105675697327\n",
      "Epoch 49, batch 184 D Loss: 1.4086461067199707, G Loss: 0.6928030252456665\n",
      "Epoch 49, batch 185 D Loss: 1.424398422241211, G Loss: 0.6849387288093567\n",
      "Epoch 49, batch 186 D Loss: 1.3934589624404907, G Loss: 0.7009651064872742\n",
      "Epoch 49, batch 187 D Loss: 1.4068536758422852, G Loss: 0.6994847059249878\n",
      "Epoch 49, batch 188 D Loss: 1.4241955280303955, G Loss: 0.7012844085693359\n",
      "Epoch 49, batch 189 D Loss: 1.4095609188079834, G Loss: 0.7035720348358154\n",
      "Epoch 49, batch 190 D Loss: 1.4166913032531738, G Loss: 0.6980112195014954\n",
      "Epoch 49, batch 191 D Loss: 1.399315595626831, G Loss: 0.7059405446052551\n",
      "Epoch 49, batch 192 D Loss: 1.4090255498886108, G Loss: 0.7093791365623474\n",
      "Epoch 49, batch 193 D Loss: 1.4033093452453613, G Loss: 0.7139993906021118\n",
      "Epoch 49, batch 194 D Loss: 1.4066925048828125, G Loss: 0.7021837830543518\n",
      "Epoch 49, batch 195 D Loss: 1.3969919681549072, G Loss: 0.7183086276054382\n",
      "Epoch 49, batch 196 D Loss: 1.4075143337249756, G Loss: 0.7072800993919373\n",
      "Epoch 49, batch 197 D Loss: 1.4037076234817505, G Loss: 0.7195062041282654\n",
      "Epoch 49, batch 198 D Loss: 1.4025788307189941, G Loss: 0.7179515361785889\n",
      "Epoch 49, batch 199 D Loss: 1.3904227018356323, G Loss: 0.7326725125312805\n",
      "Epoch 49, batch 200 D Loss: 1.3899435997009277, G Loss: 0.7224166989326477\n",
      "Epoch 50, batch 1 D Loss: 1.3850938081741333, G Loss: 0.7323997020721436\n",
      "Epoch 50, batch 2 D Loss: 1.394075870513916, G Loss: 0.7391620874404907\n",
      "Epoch 50, batch 3 D Loss: 1.3878321647644043, G Loss: 0.7490531802177429\n",
      "Epoch 50, batch 4 D Loss: 1.401464819908142, G Loss: 0.7427771091461182\n",
      "Epoch 50, batch 5 D Loss: 1.3845410346984863, G Loss: 0.7366073727607727\n",
      "Epoch 50, batch 6 D Loss: 1.3861145973205566, G Loss: 0.7481329441070557\n",
      "Epoch 50, batch 7 D Loss: 1.3675239086151123, G Loss: 0.7510221600532532\n",
      "Epoch 50, batch 8 D Loss: 1.3951468467712402, G Loss: 0.7469059824943542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, batch 9 D Loss: 1.3869906663894653, G Loss: 0.7540233135223389\n",
      "Epoch 50, batch 10 D Loss: 1.38197660446167, G Loss: 0.7459155917167664\n",
      "Epoch 50, batch 11 D Loss: 1.3728615045547485, G Loss: 0.7557828426361084\n",
      "Epoch 50, batch 12 D Loss: 1.3816537857055664, G Loss: 0.7581430077552795\n",
      "Epoch 50, batch 13 D Loss: 1.382032871246338, G Loss: 0.752855658531189\n",
      "Epoch 50, batch 14 D Loss: 1.3855537176132202, G Loss: 0.7469563484191895\n",
      "Epoch 50, batch 15 D Loss: 1.3636882305145264, G Loss: 0.7726632952690125\n",
      "Epoch 50, batch 16 D Loss: 1.363953948020935, G Loss: 0.7615393996238708\n",
      "Epoch 50, batch 17 D Loss: 1.366319179534912, G Loss: 0.7695106267929077\n",
      "Epoch 50, batch 18 D Loss: 1.3849306106567383, G Loss: 0.7634661793708801\n",
      "Epoch 50, batch 19 D Loss: 1.370029330253601, G Loss: 0.7725153565406799\n",
      "Epoch 50, batch 20 D Loss: 1.3769770860671997, G Loss: 0.7755910754203796\n",
      "Epoch 50, batch 21 D Loss: 1.3708937168121338, G Loss: 0.7762252688407898\n",
      "Epoch 50, batch 22 D Loss: 1.3620271682739258, G Loss: 0.8044878244400024\n",
      "Epoch 50, batch 23 D Loss: 1.350961685180664, G Loss: 0.8024136424064636\n",
      "Epoch 50, batch 24 D Loss: 1.3822228908538818, G Loss: 0.8039256930351257\n",
      "Epoch 50, batch 25 D Loss: 1.3647844791412354, G Loss: 0.8013678789138794\n",
      "Epoch 50, batch 26 D Loss: 1.3673263788223267, G Loss: 0.7640399932861328\n",
      "Epoch 50, batch 27 D Loss: 1.3963696956634521, G Loss: 0.7676791548728943\n",
      "Epoch 50, batch 28 D Loss: 1.3735573291778564, G Loss: 0.7839112877845764\n",
      "Epoch 50, batch 29 D Loss: 1.4263662099838257, G Loss: 0.7684533596038818\n",
      "Epoch 50, batch 30 D Loss: 1.3659250736236572, G Loss: 0.7664636373519897\n",
      "Epoch 50, batch 31 D Loss: 1.389888048171997, G Loss: 0.7764219045639038\n",
      "Epoch 50, batch 32 D Loss: 1.3610894680023193, G Loss: 0.7744581699371338\n",
      "Epoch 50, batch 33 D Loss: 1.3867340087890625, G Loss: 0.7648321390151978\n",
      "Epoch 50, batch 34 D Loss: 1.4025819301605225, G Loss: 0.7317461967468262\n",
      "Epoch 50, batch 35 D Loss: 1.3875702619552612, G Loss: 0.7622501254081726\n",
      "Epoch 50, batch 36 D Loss: 1.3686702251434326, G Loss: 0.7380736470222473\n",
      "Epoch 50, batch 37 D Loss: 1.4144654273986816, G Loss: 0.7342948913574219\n",
      "Epoch 50, batch 38 D Loss: 1.4095109701156616, G Loss: 0.7466399669647217\n",
      "Epoch 50, batch 39 D Loss: 1.3853839635849, G Loss: 0.7291107177734375\n",
      "Epoch 50, batch 40 D Loss: 1.3935508728027344, G Loss: 0.7313930988311768\n",
      "Epoch 50, batch 41 D Loss: 1.3878272771835327, G Loss: 0.7302998304367065\n",
      "Epoch 50, batch 42 D Loss: 1.3914642333984375, G Loss: 0.7210285663604736\n",
      "Epoch 50, batch 43 D Loss: 1.3751027584075928, G Loss: 0.7498460412025452\n",
      "Epoch 50, batch 44 D Loss: 1.3941984176635742, G Loss: 0.7196016907691956\n",
      "Epoch 50, batch 45 D Loss: 1.3931925296783447, G Loss: 0.7102230787277222\n",
      "Epoch 50, batch 46 D Loss: 1.3959884643554688, G Loss: 0.7245824337005615\n",
      "Epoch 50, batch 47 D Loss: 1.4006754159927368, G Loss: 0.7148767709732056\n",
      "Epoch 50, batch 48 D Loss: 1.403462290763855, G Loss: 0.7102999091148376\n",
      "Epoch 50, batch 49 D Loss: 1.4335558414459229, G Loss: 0.6991058588027954\n",
      "Epoch 50, batch 50 D Loss: 1.4191746711730957, G Loss: 0.6974341869354248\n",
      "Epoch 50, batch 51 D Loss: 1.3992505073547363, G Loss: 0.7054694890975952\n",
      "Epoch 50, batch 52 D Loss: 1.3753063678741455, G Loss: 0.7143087983131409\n",
      "Epoch 50, batch 53 D Loss: 1.3960018157958984, G Loss: 0.6970115900039673\n",
      "Epoch 50, batch 54 D Loss: 1.4113894701004028, G Loss: 0.6850938200950623\n",
      "Epoch 50, batch 55 D Loss: 1.4117600917816162, G Loss: 0.682578980922699\n",
      "Epoch 50, batch 56 D Loss: 1.3909168243408203, G Loss: 0.7006921172142029\n",
      "Epoch 50, batch 57 D Loss: 1.4172389507293701, G Loss: 0.6809471249580383\n",
      "Epoch 50, batch 58 D Loss: 1.393688440322876, G Loss: 0.6877626776695251\n",
      "Epoch 50, batch 59 D Loss: 1.4057011604309082, G Loss: 0.6743459105491638\n",
      "Epoch 50, batch 60 D Loss: 1.4264814853668213, G Loss: 0.6716444492340088\n",
      "Epoch 50, batch 61 D Loss: 1.4389715194702148, G Loss: 0.6706222295761108\n",
      "Epoch 50, batch 62 D Loss: 1.4118032455444336, G Loss: 0.6783252954483032\n",
      "Epoch 50, batch 63 D Loss: 1.412021517753601, G Loss: 0.672924816608429\n",
      "Epoch 50, batch 64 D Loss: 1.403115153312683, G Loss: 0.6754344701766968\n",
      "Epoch 50, batch 65 D Loss: 1.4006034135818481, G Loss: 0.6755152344703674\n",
      "Epoch 50, batch 66 D Loss: 1.4069219827651978, G Loss: 0.6656002998352051\n",
      "Epoch 50, batch 67 D Loss: 1.4082506895065308, G Loss: 0.6615141034126282\n",
      "Epoch 50, batch 68 D Loss: 1.3969420194625854, G Loss: 0.6621026396751404\n",
      "Epoch 50, batch 69 D Loss: 1.3892273902893066, G Loss: 0.6727837920188904\n",
      "Epoch 50, batch 70 D Loss: 1.4041383266448975, G Loss: 0.6635023355484009\n",
      "Epoch 50, batch 71 D Loss: 1.3894314765930176, G Loss: 0.6701638102531433\n",
      "Epoch 50, batch 72 D Loss: 1.4119623899459839, G Loss: 0.662222146987915\n",
      "Epoch 50, batch 73 D Loss: 1.3969923257827759, G Loss: 0.661312460899353\n",
      "Epoch 50, batch 74 D Loss: 1.399226427078247, G Loss: 0.6592341065406799\n",
      "Epoch 50, batch 75 D Loss: 1.389148473739624, G Loss: 0.6622369885444641\n",
      "Epoch 50, batch 76 D Loss: 1.3897795677185059, G Loss: 0.6601158380508423\n",
      "Epoch 50, batch 77 D Loss: 1.395866870880127, G Loss: 0.6611241698265076\n",
      "Epoch 50, batch 78 D Loss: 1.3819313049316406, G Loss: 0.659371018409729\n",
      "Epoch 50, batch 79 D Loss: 1.3932712078094482, G Loss: 0.6568308472633362\n",
      "Epoch 50, batch 80 D Loss: 1.381272315979004, G Loss: 0.6623197197914124\n",
      "Epoch 50, batch 81 D Loss: 1.3876979351043701, G Loss: 0.6583025455474854\n",
      "Epoch 50, batch 82 D Loss: 1.3779339790344238, G Loss: 0.6588767170906067\n",
      "Epoch 50, batch 83 D Loss: 1.3880763053894043, G Loss: 0.6575453281402588\n",
      "Epoch 50, batch 84 D Loss: 1.3832370042800903, G Loss: 0.6575688123703003\n",
      "Epoch 50, batch 85 D Loss: 1.3863606452941895, G Loss: 0.6595996618270874\n",
      "Epoch 50, batch 86 D Loss: 1.3961927890777588, G Loss: 0.6537966728210449\n",
      "Epoch 50, batch 87 D Loss: 1.3896129131317139, G Loss: 0.6552934050559998\n",
      "Epoch 50, batch 88 D Loss: 1.3854174613952637, G Loss: 0.6564624905586243\n",
      "Epoch 50, batch 89 D Loss: 1.3774852752685547, G Loss: 0.656574010848999\n",
      "Epoch 50, batch 90 D Loss: 1.3695329427719116, G Loss: 0.6591793298721313\n",
      "Epoch 50, batch 91 D Loss: 1.385727882385254, G Loss: 0.6527579426765442\n",
      "Epoch 50, batch 92 D Loss: 1.380208134651184, G Loss: 0.6566792130470276\n",
      "Epoch 50, batch 93 D Loss: 1.3653535842895508, G Loss: 0.6551920175552368\n",
      "Epoch 50, batch 94 D Loss: 1.3837616443634033, G Loss: 0.6552230715751648\n",
      "Epoch 50, batch 95 D Loss: 1.383334755897522, G Loss: 0.6526589393615723\n",
      "Epoch 50, batch 96 D Loss: 1.383931279182434, G Loss: 0.6528483629226685\n",
      "Epoch 50, batch 97 D Loss: 1.3785724639892578, G Loss: 0.6542390584945679\n",
      "Epoch 50, batch 98 D Loss: 1.3829679489135742, G Loss: 0.6532080173492432\n",
      "Epoch 50, batch 99 D Loss: 1.374877691268921, G Loss: 0.6519615054130554\n",
      "Epoch 50, batch 100 D Loss: 1.378650426864624, G Loss: 0.6516854763031006\n",
      "Epoch 50, batch 101 D Loss: 1.371999740600586, G Loss: 0.6549730896949768\n",
      "Epoch 50, batch 102 D Loss: 1.3577802181243896, G Loss: 0.6552924513816833\n",
      "Epoch 50, batch 103 D Loss: 1.3698476552963257, G Loss: 0.647480309009552\n",
      "Epoch 50, batch 104 D Loss: 1.3706271648406982, G Loss: 0.654611349105835\n",
      "Epoch 50, batch 105 D Loss: 1.3738136291503906, G Loss: 0.6582244634628296\n",
      "Epoch 50, batch 106 D Loss: 1.3740334510803223, G Loss: 0.6534945964813232\n",
      "Epoch 50, batch 107 D Loss: 1.369513750076294, G Loss: 0.6538998484611511\n",
      "Epoch 50, batch 108 D Loss: 1.3761446475982666, G Loss: 0.6525312066078186\n",
      "Epoch 50, batch 109 D Loss: 1.3651305437088013, G Loss: 0.6542494297027588\n",
      "Epoch 50, batch 110 D Loss: 1.3572466373443604, G Loss: 0.6515452861785889\n",
      "Epoch 50, batch 111 D Loss: 1.3708996772766113, G Loss: 0.6519083976745605\n",
      "Epoch 50, batch 112 D Loss: 1.3596751689910889, G Loss: 0.6628290414810181\n",
      "Epoch 50, batch 113 D Loss: 1.3626983165740967, G Loss: 0.6539628505706787\n",
      "Epoch 50, batch 114 D Loss: 1.3799140453338623, G Loss: 0.6497161984443665\n",
      "Epoch 50, batch 115 D Loss: 1.3698711395263672, G Loss: 0.652026355266571\n",
      "Epoch 50, batch 116 D Loss: 1.3746881484985352, G Loss: 0.6509373188018799\n",
      "Epoch 50, batch 117 D Loss: 1.3698142766952515, G Loss: 0.652639627456665\n",
      "Epoch 50, batch 118 D Loss: 1.3888823986053467, G Loss: 0.6386253237724304\n",
      "Epoch 50, batch 119 D Loss: 1.3811240196228027, G Loss: 0.6567502021789551\n",
      "Epoch 50, batch 120 D Loss: 1.3903415203094482, G Loss: 0.6463756561279297\n",
      "Epoch 50, batch 121 D Loss: 1.3618756532669067, G Loss: 0.6523319482803345\n",
      "Epoch 50, batch 122 D Loss: 1.3729349374771118, G Loss: 0.6549219489097595\n",
      "Epoch 50, batch 123 D Loss: 1.3747202157974243, G Loss: 0.6517494320869446\n",
      "Epoch 50, batch 124 D Loss: 1.3826004266738892, G Loss: 0.649039626121521\n",
      "Epoch 50, batch 125 D Loss: 1.3768353462219238, G Loss: 0.6486501097679138\n",
      "Epoch 50, batch 126 D Loss: 1.3582611083984375, G Loss: 0.6546120643615723\n",
      "Epoch 50, batch 127 D Loss: 1.3866033554077148, G Loss: 0.6409852504730225\n",
      "Epoch 50, batch 128 D Loss: 1.3633193969726562, G Loss: 0.6506597995758057\n",
      "Epoch 50, batch 129 D Loss: 1.3884412050247192, G Loss: 0.6473081111907959\n",
      "Epoch 50, batch 130 D Loss: 1.3546870946884155, G Loss: 0.6550759673118591\n",
      "Epoch 50, batch 131 D Loss: 1.3817881345748901, G Loss: 0.6557832956314087\n",
      "Epoch 50, batch 132 D Loss: 1.3722381591796875, G Loss: 0.652268648147583\n",
      "Epoch 50, batch 133 D Loss: 1.3864774703979492, G Loss: 0.6510832905769348\n",
      "Epoch 50, batch 134 D Loss: 1.374572992324829, G Loss: 0.6527129411697388\n",
      "Epoch 50, batch 135 D Loss: 1.3933454751968384, G Loss: 0.6544987559318542\n",
      "Epoch 50, batch 136 D Loss: 1.3903110027313232, G Loss: 0.6458439826965332\n",
      "Epoch 50, batch 137 D Loss: 1.36289644241333, G Loss: 0.6613640785217285\n",
      "Epoch 50, batch 138 D Loss: 1.3432621955871582, G Loss: 0.67144376039505\n",
      "Epoch 50, batch 139 D Loss: 1.3544378280639648, G Loss: 0.6720210313796997\n",
      "Epoch 50, batch 140 D Loss: 1.401562213897705, G Loss: 0.6469825506210327\n",
      "Epoch 50, batch 141 D Loss: 1.3866312503814697, G Loss: 0.6617141962051392\n",
      "Epoch 50, batch 142 D Loss: 1.3557379245758057, G Loss: 0.6652652621269226\n",
      "Epoch 50, batch 143 D Loss: 1.3959057331085205, G Loss: 0.6555532217025757\n",
      "Epoch 50, batch 144 D Loss: 1.3803553581237793, G Loss: 0.6677847504615784\n",
      "Epoch 50, batch 145 D Loss: 1.3663556575775146, G Loss: 0.67487633228302\n",
      "Epoch 50, batch 146 D Loss: 1.363384485244751, G Loss: 0.6686529517173767\n",
      "Epoch 50, batch 147 D Loss: 1.384751796722412, G Loss: 0.6590399742126465\n",
      "Epoch 50, batch 148 D Loss: 1.378922462463379, G Loss: 0.6574598550796509\n",
      "Epoch 50, batch 149 D Loss: 1.3824536800384521, G Loss: 0.6694970726966858\n",
      "Epoch 50, batch 150 D Loss: 1.3791234493255615, G Loss: 0.6661167740821838\n",
      "Epoch 50, batch 151 D Loss: 1.386965274810791, G Loss: 0.667263925075531\n",
      "Epoch 50, batch 152 D Loss: 1.3939869403839111, G Loss: 0.6581276655197144\n",
      "Epoch 50, batch 153 D Loss: 1.377570390701294, G Loss: 0.6615927219390869\n",
      "Epoch 50, batch 154 D Loss: 1.3906804323196411, G Loss: 0.6613172292709351\n",
      "Epoch 50, batch 155 D Loss: 1.3988287448883057, G Loss: 0.6629272699356079\n",
      "Epoch 50, batch 156 D Loss: 1.37776780128479, G Loss: 0.6742600798606873\n",
      "Epoch 50, batch 157 D Loss: 1.3860900402069092, G Loss: 0.6621783971786499\n",
      "Epoch 50, batch 158 D Loss: 1.3690762519836426, G Loss: 0.6780809760093689\n",
      "Epoch 50, batch 159 D Loss: 1.369598388671875, G Loss: 0.6783956289291382\n",
      "Epoch 50, batch 160 D Loss: 1.3754050731658936, G Loss: 0.6808204054832458\n",
      "Epoch 50, batch 161 D Loss: 1.3737139701843262, G Loss: 0.6790337562561035\n",
      "Epoch 50, batch 162 D Loss: 1.3970742225646973, G Loss: 0.6720438599586487\n",
      "Epoch 50, batch 163 D Loss: 1.3961148262023926, G Loss: 0.6739163994789124\n",
      "Epoch 50, batch 164 D Loss: 1.389946460723877, G Loss: 0.6787508130073547\n",
      "Epoch 50, batch 165 D Loss: 1.3673300743103027, G Loss: 0.6749725341796875\n",
      "Epoch 50, batch 166 D Loss: 1.3774986267089844, G Loss: 0.6813680529594421\n",
      "Epoch 50, batch 167 D Loss: 1.373478651046753, G Loss: 0.6903409361839294\n",
      "Epoch 50, batch 168 D Loss: 1.3759779930114746, G Loss: 0.6767792701721191\n",
      "Epoch 50, batch 169 D Loss: 1.3615573644638062, G Loss: 0.6959722638130188\n",
      "Epoch 50, batch 170 D Loss: 1.374772310256958, G Loss: 0.6797610521316528\n",
      "Epoch 50, batch 171 D Loss: 1.4007041454315186, G Loss: 0.6913444399833679\n",
      "Epoch 50, batch 172 D Loss: 1.3929953575134277, G Loss: 0.6889474391937256\n",
      "Epoch 50, batch 173 D Loss: 1.3799431324005127, G Loss: 0.6962417364120483\n",
      "Epoch 50, batch 174 D Loss: 1.3766813278198242, G Loss: 0.6839210987091064\n",
      "Epoch 50, batch 175 D Loss: 1.372114896774292, G Loss: 0.7032274007797241\n",
      "Epoch 50, batch 176 D Loss: 1.3587955236434937, G Loss: 0.7063646912574768\n",
      "Epoch 50, batch 177 D Loss: 1.354278326034546, G Loss: 0.7058013677597046\n",
      "Epoch 50, batch 178 D Loss: 1.362342357635498, G Loss: 0.7094727158546448\n",
      "Epoch 50, batch 179 D Loss: 1.355025291442871, G Loss: 0.7021799683570862\n",
      "Epoch 50, batch 180 D Loss: 1.36616849899292, G Loss: 0.7021037936210632\n",
      "Epoch 50, batch 181 D Loss: 1.3614141941070557, G Loss: 0.7143809795379639\n",
      "Epoch 50, batch 182 D Loss: 1.3672823905944824, G Loss: 0.7271087765693665\n",
      "Epoch 50, batch 183 D Loss: 1.3616853952407837, G Loss: 0.7082719206809998\n",
      "Epoch 50, batch 184 D Loss: 1.3559975624084473, G Loss: 0.7048112750053406\n",
      "Epoch 50, batch 185 D Loss: 1.3575117588043213, G Loss: 0.7030659317970276\n",
      "Epoch 50, batch 186 D Loss: 1.3625327348709106, G Loss: 0.7183398604393005\n",
      "Epoch 50, batch 187 D Loss: 1.365004539489746, G Loss: 0.7301512956619263\n",
      "Epoch 50, batch 188 D Loss: 1.3679585456848145, G Loss: 0.725678563117981\n",
      "Epoch 50, batch 189 D Loss: 1.3645273447036743, G Loss: 0.723574697971344\n",
      "Epoch 50, batch 190 D Loss: 1.3564014434814453, G Loss: 0.7188825011253357\n",
      "Epoch 50, batch 191 D Loss: 1.3905360698699951, G Loss: 0.709723711013794\n",
      "Epoch 50, batch 192 D Loss: 1.3719946146011353, G Loss: 0.7178679704666138\n",
      "Epoch 50, batch 193 D Loss: 1.3393402099609375, G Loss: 0.739865779876709\n",
      "Epoch 50, batch 194 D Loss: 1.378403663635254, G Loss: 0.7216292023658752\n",
      "Epoch 50, batch 195 D Loss: 1.3692352771759033, G Loss: 0.7124627828598022\n",
      "Epoch 50, batch 196 D Loss: 1.4067796468734741, G Loss: 0.6976729035377502\n",
      "Epoch 50, batch 197 D Loss: 1.370101809501648, G Loss: 0.7107017040252686\n",
      "Epoch 50, batch 198 D Loss: 1.3284308910369873, G Loss: 0.7473957538604736\n",
      "Epoch 50, batch 199 D Loss: 1.346531867980957, G Loss: 0.757953405380249\n",
      "Epoch 50, batch 200 D Loss: 1.320085048675537, G Loss: 0.7653425335884094\n",
      "Epoch 51, batch 1 D Loss: 1.3586411476135254, G Loss: 0.725717306137085\n",
      "Epoch 51, batch 2 D Loss: 1.3567850589752197, G Loss: 0.733564019203186\n",
      "Epoch 51, batch 3 D Loss: 1.368147373199463, G Loss: 0.7523434162139893\n",
      "Epoch 51, batch 4 D Loss: 1.3964694738388062, G Loss: 0.7032468914985657\n",
      "Epoch 51, batch 5 D Loss: 1.351522445678711, G Loss: 0.7233834266662598\n",
      "Epoch 51, batch 6 D Loss: 1.405564308166504, G Loss: 0.7114285230636597\n",
      "Epoch 51, batch 7 D Loss: 1.426140546798706, G Loss: 0.7076902985572815\n",
      "Epoch 51, batch 8 D Loss: 1.3888280391693115, G Loss: 0.7102116346359253\n",
      "Epoch 51, batch 9 D Loss: 1.361769676208496, G Loss: 0.7197207808494568\n",
      "Epoch 51, batch 10 D Loss: 1.377192735671997, G Loss: 0.7106006145477295\n",
      "Epoch 51, batch 11 D Loss: 1.4064247608184814, G Loss: 0.7216731309890747\n",
      "Epoch 51, batch 12 D Loss: 1.3862528800964355, G Loss: 0.7034581899642944\n",
      "Epoch 51, batch 13 D Loss: 1.3918083906173706, G Loss: 0.7237979173660278\n",
      "Epoch 51, batch 14 D Loss: 1.4014376401901245, G Loss: 0.6912683248519897\n",
      "Epoch 51, batch 15 D Loss: 1.395475149154663, G Loss: 0.7281702160835266\n",
      "Epoch 51, batch 16 D Loss: 1.3525127172470093, G Loss: 0.7286556959152222\n",
      "Epoch 51, batch 17 D Loss: 1.4089332818984985, G Loss: 0.7131066918373108\n",
      "Epoch 51, batch 18 D Loss: 1.3911335468292236, G Loss: 0.7215568423271179\n",
      "Epoch 51, batch 19 D Loss: 1.3497788906097412, G Loss: 0.7118650674819946\n",
      "Epoch 51, batch 20 D Loss: 1.3723886013031006, G Loss: 0.7247534394264221\n",
      "Epoch 51, batch 21 D Loss: 1.3890044689178467, G Loss: 0.7194293141365051\n",
      "Epoch 51, batch 22 D Loss: 1.3955602645874023, G Loss: 0.7037626504898071\n",
      "Epoch 51, batch 23 D Loss: 1.3868579864501953, G Loss: 0.7199141979217529\n",
      "Epoch 51, batch 24 D Loss: 1.3792908191680908, G Loss: 0.7194593548774719\n",
      "Epoch 51, batch 25 D Loss: 1.372079849243164, G Loss: 0.7177392840385437\n",
      "Epoch 51, batch 26 D Loss: 1.4391629695892334, G Loss: 0.7034044861793518\n",
      "Epoch 51, batch 27 D Loss: 1.3938531875610352, G Loss: 0.704353392124176\n",
      "Epoch 51, batch 28 D Loss: 1.3711767196655273, G Loss: 0.700595498085022\n",
      "Epoch 51, batch 29 D Loss: 1.39224374294281, G Loss: 0.6998173594474792\n",
      "Epoch 51, batch 30 D Loss: 1.3983328342437744, G Loss: 0.7143244743347168\n",
      "Epoch 51, batch 31 D Loss: 1.392709493637085, G Loss: 0.6985697150230408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, batch 32 D Loss: 1.4099314212799072, G Loss: 0.7014381885528564\n",
      "Epoch 51, batch 33 D Loss: 1.4134602546691895, G Loss: 0.6844385266304016\n",
      "Epoch 51, batch 34 D Loss: 1.3992341756820679, G Loss: 0.7048149108886719\n",
      "Epoch 51, batch 35 D Loss: 1.377566933631897, G Loss: 0.7079753875732422\n",
      "Epoch 51, batch 36 D Loss: 1.395693063735962, G Loss: 0.7034122943878174\n",
      "Epoch 51, batch 37 D Loss: 1.3783705234527588, G Loss: 0.707004189491272\n",
      "Epoch 51, batch 38 D Loss: 1.4143235683441162, G Loss: 0.7061219215393066\n",
      "Epoch 51, batch 39 D Loss: 1.3868463039398193, G Loss: 0.718047559261322\n",
      "Epoch 51, batch 40 D Loss: 1.4044922590255737, G Loss: 0.6994901895523071\n",
      "Epoch 51, batch 41 D Loss: 1.3959424495697021, G Loss: 0.695137083530426\n",
      "Epoch 51, batch 42 D Loss: 1.3722926378250122, G Loss: 0.7060942649841309\n",
      "Epoch 51, batch 43 D Loss: 1.3803675174713135, G Loss: 0.701790988445282\n",
      "Epoch 51, batch 44 D Loss: 1.3625043630599976, G Loss: 0.7042522430419922\n",
      "Epoch 51, batch 45 D Loss: 1.3907415866851807, G Loss: 0.6937304735183716\n",
      "Epoch 51, batch 46 D Loss: 1.3466085195541382, G Loss: 0.7123871445655823\n",
      "Epoch 51, batch 47 D Loss: 1.3935916423797607, G Loss: 0.7020522952079773\n",
      "Epoch 51, batch 48 D Loss: 1.3660907745361328, G Loss: 0.706849217414856\n",
      "Epoch 51, batch 49 D Loss: 1.3763782978057861, G Loss: 0.7166352868080139\n",
      "Epoch 51, batch 50 D Loss: 1.3792707920074463, G Loss: 0.69793701171875\n",
      "Epoch 51, batch 51 D Loss: 1.3807843923568726, G Loss: 0.7157800197601318\n",
      "Epoch 51, batch 52 D Loss: 1.403830647468567, G Loss: 0.6944133639335632\n",
      "Epoch 51, batch 53 D Loss: 1.4081690311431885, G Loss: 0.6947997808456421\n",
      "Epoch 51, batch 54 D Loss: 1.3747738599777222, G Loss: 0.6994026899337769\n",
      "Epoch 51, batch 55 D Loss: 1.395946979522705, G Loss: 0.7071179151535034\n",
      "Epoch 51, batch 56 D Loss: 1.373695731163025, G Loss: 0.6933727264404297\n",
      "Epoch 51, batch 57 D Loss: 1.3909761905670166, G Loss: 0.7006331086158752\n",
      "Epoch 51, batch 58 D Loss: 1.3996331691741943, G Loss: 0.7037040591239929\n",
      "Epoch 51, batch 59 D Loss: 1.3843920230865479, G Loss: 0.7085725665092468\n",
      "Epoch 51, batch 60 D Loss: 1.3822911977767944, G Loss: 0.6990230083465576\n",
      "Epoch 51, batch 61 D Loss: 1.3629107475280762, G Loss: 0.7026485204696655\n",
      "Epoch 51, batch 62 D Loss: 1.3928582668304443, G Loss: 0.6986721158027649\n",
      "Epoch 51, batch 63 D Loss: 1.3896944522857666, G Loss: 0.6783270239830017\n",
      "Epoch 51, batch 64 D Loss: 1.3828794956207275, G Loss: 0.7008141875267029\n",
      "Epoch 51, batch 65 D Loss: 1.3679132461547852, G Loss: 0.6879438757896423\n",
      "Epoch 51, batch 66 D Loss: 1.4014620780944824, G Loss: 0.6974930763244629\n",
      "Epoch 51, batch 67 D Loss: 1.3450219631195068, G Loss: 0.701985776424408\n",
      "Epoch 51, batch 68 D Loss: 1.3816373348236084, G Loss: 0.7035002112388611\n",
      "Epoch 51, batch 69 D Loss: 1.3914960622787476, G Loss: 0.6890140771865845\n",
      "Epoch 51, batch 70 D Loss: 1.4048669338226318, G Loss: 0.676174521446228\n",
      "Epoch 51, batch 71 D Loss: 1.3918989896774292, G Loss: 0.6928713321685791\n",
      "Epoch 51, batch 72 D Loss: 1.343116283416748, G Loss: 0.7124482989311218\n",
      "Epoch 51, batch 73 D Loss: 1.3813660144805908, G Loss: 0.6844397783279419\n",
      "Epoch 51, batch 74 D Loss: 1.3949196338653564, G Loss: 0.6921963691711426\n",
      "Epoch 51, batch 75 D Loss: 1.3800373077392578, G Loss: 0.6971316337585449\n",
      "Epoch 51, batch 76 D Loss: 1.3835654258728027, G Loss: 0.6825348734855652\n",
      "Epoch 51, batch 77 D Loss: 1.3977737426757812, G Loss: 0.6742163300514221\n",
      "Epoch 51, batch 78 D Loss: 1.387127161026001, G Loss: 0.6949880719184875\n",
      "Epoch 51, batch 79 D Loss: 1.3750191926956177, G Loss: 0.7009922862052917\n",
      "Epoch 51, batch 80 D Loss: 1.3917304277420044, G Loss: 0.6869564652442932\n",
      "Epoch 51, batch 81 D Loss: 1.3911101818084717, G Loss: 0.6896318197250366\n",
      "Epoch 51, batch 82 D Loss: 1.4103341102600098, G Loss: 0.6930992007255554\n",
      "Epoch 51, batch 83 D Loss: 1.4037278890609741, G Loss: 0.6862597465515137\n",
      "Epoch 51, batch 84 D Loss: 1.4167956113815308, G Loss: 0.6833381652832031\n",
      "Epoch 51, batch 85 D Loss: 1.3911168575286865, G Loss: 0.7043039798736572\n",
      "Epoch 51, batch 86 D Loss: 1.3925938606262207, G Loss: 0.7026159763336182\n",
      "Epoch 51, batch 87 D Loss: 1.3850984573364258, G Loss: 0.7023866772651672\n",
      "Epoch 51, batch 88 D Loss: 1.4157838821411133, G Loss: 0.6868337988853455\n",
      "Epoch 51, batch 89 D Loss: 1.3999911546707153, G Loss: 0.7002090215682983\n",
      "Epoch 51, batch 90 D Loss: 1.418195128440857, G Loss: 0.6902324557304382\n",
      "Epoch 51, batch 91 D Loss: 1.3946046829223633, G Loss: 0.7041051387786865\n",
      "Epoch 51, batch 92 D Loss: 1.4064347743988037, G Loss: 0.6931310892105103\n",
      "Epoch 51, batch 93 D Loss: 1.4120917320251465, G Loss: 0.6832364797592163\n",
      "Epoch 51, batch 94 D Loss: 1.4379960298538208, G Loss: 0.6665897965431213\n",
      "Epoch 51, batch 95 D Loss: 1.4029897451400757, G Loss: 0.7061137557029724\n",
      "Epoch 51, batch 96 D Loss: 1.4065461158752441, G Loss: 0.7029642462730408\n",
      "Epoch 51, batch 97 D Loss: 1.3730876445770264, G Loss: 0.7238391041755676\n",
      "Epoch 51, batch 98 D Loss: 1.385874629020691, G Loss: 0.7079683542251587\n",
      "Epoch 51, batch 99 D Loss: 1.4195492267608643, G Loss: 0.6937174797058105\n",
      "Epoch 51, batch 100 D Loss: 1.3996355533599854, G Loss: 0.7068546414375305\n",
      "Epoch 51, batch 101 D Loss: 1.4049214124679565, G Loss: 0.7234890460968018\n",
      "Epoch 51, batch 102 D Loss: 1.3995964527130127, G Loss: 0.7274017333984375\n",
      "Epoch 51, batch 103 D Loss: 1.3919618129730225, G Loss: 0.7019870281219482\n",
      "Epoch 51, batch 104 D Loss: 1.4016202688217163, G Loss: 0.7222970724105835\n",
      "Epoch 51, batch 105 D Loss: 1.3982934951782227, G Loss: 0.7251135110855103\n",
      "Epoch 51, batch 106 D Loss: 1.4435971975326538, G Loss: 0.7171840071678162\n",
      "Epoch 51, batch 107 D Loss: 1.4059531688690186, G Loss: 0.7340714931488037\n",
      "Epoch 51, batch 108 D Loss: 1.364854097366333, G Loss: 0.7419663071632385\n",
      "Epoch 51, batch 109 D Loss: 1.4100600481033325, G Loss: 0.7338042259216309\n",
      "Epoch 51, batch 110 D Loss: 1.417285680770874, G Loss: 0.7194087505340576\n",
      "Epoch 51, batch 111 D Loss: 1.3886795043945312, G Loss: 0.732592761516571\n",
      "Epoch 51, batch 112 D Loss: 1.3891431093215942, G Loss: 0.7337629795074463\n",
      "Epoch 51, batch 113 D Loss: 1.39914870262146, G Loss: 0.7450856566429138\n",
      "Epoch 51, batch 114 D Loss: 1.4010521173477173, G Loss: 0.7344551682472229\n",
      "Epoch 51, batch 115 D Loss: 1.3883507251739502, G Loss: 0.7320407629013062\n",
      "Epoch 51, batch 116 D Loss: 1.371009111404419, G Loss: 0.7359224557876587\n",
      "Epoch 51, batch 117 D Loss: 1.380587100982666, G Loss: 0.7499179244041443\n",
      "Epoch 51, batch 118 D Loss: 1.3901746273040771, G Loss: 0.7422511577606201\n",
      "Epoch 51, batch 119 D Loss: 1.376389741897583, G Loss: 0.7697471380233765\n",
      "Epoch 51, batch 120 D Loss: 1.3625363111495972, G Loss: 0.7583353519439697\n",
      "Epoch 51, batch 121 D Loss: 1.4245444536209106, G Loss: 0.736307680606842\n",
      "Epoch 51, batch 122 D Loss: 1.387239694595337, G Loss: 0.7727055549621582\n",
      "Epoch 51, batch 123 D Loss: 1.4164162874221802, G Loss: 0.7371827960014343\n",
      "Epoch 51, batch 124 D Loss: 1.3982529640197754, G Loss: 0.7494206428527832\n",
      "Epoch 51, batch 125 D Loss: 1.396168828010559, G Loss: 0.7367967367172241\n",
      "Epoch 51, batch 126 D Loss: 1.392462968826294, G Loss: 0.7655899524688721\n",
      "Epoch 51, batch 127 D Loss: 1.3801453113555908, G Loss: 0.746931254863739\n",
      "Epoch 51, batch 128 D Loss: 1.3849520683288574, G Loss: 0.7300058007240295\n",
      "Epoch 51, batch 129 D Loss: 1.424839735031128, G Loss: 0.7355263233184814\n",
      "Epoch 51, batch 130 D Loss: 1.4349818229675293, G Loss: 0.7584542632102966\n",
      "Epoch 51, batch 131 D Loss: 1.380660057067871, G Loss: 0.7468702793121338\n",
      "Epoch 51, batch 132 D Loss: 1.4129999876022339, G Loss: 0.7286763191223145\n",
      "Epoch 51, batch 133 D Loss: 1.3744611740112305, G Loss: 0.7550869584083557\n",
      "Epoch 51, batch 134 D Loss: 1.3966680765151978, G Loss: 0.7529857754707336\n",
      "Epoch 51, batch 135 D Loss: 1.3772335052490234, G Loss: 0.7324331402778625\n",
      "Epoch 51, batch 136 D Loss: 1.393223524093628, G Loss: 0.7385498285293579\n",
      "Epoch 51, batch 137 D Loss: 1.3857438564300537, G Loss: 0.7206584215164185\n",
      "Epoch 51, batch 138 D Loss: 1.3883960247039795, G Loss: 0.73468017578125\n",
      "Epoch 51, batch 139 D Loss: 1.3952192068099976, G Loss: 0.719032883644104\n",
      "Epoch 51, batch 140 D Loss: 1.4025239944458008, G Loss: 0.732964038848877\n",
      "Epoch 51, batch 141 D Loss: 1.4080839157104492, G Loss: 0.746233344078064\n",
      "Epoch 51, batch 142 D Loss: 1.409597396850586, G Loss: 0.7641440629959106\n",
      "Epoch 51, batch 143 D Loss: 1.3894821405410767, G Loss: 0.7266507744789124\n",
      "Epoch 51, batch 144 D Loss: 1.4097933769226074, G Loss: 0.7086734175682068\n",
      "Epoch 51, batch 145 D Loss: 1.4136426448822021, G Loss: 0.7020921111106873\n",
      "Epoch 51, batch 146 D Loss: 1.4115736484527588, G Loss: 0.7119771838188171\n",
      "Epoch 51, batch 147 D Loss: 1.3888157606124878, G Loss: 0.7353234887123108\n",
      "Epoch 51, batch 148 D Loss: 1.3834450244903564, G Loss: 0.7399967312812805\n",
      "Epoch 51, batch 149 D Loss: 1.4015164375305176, G Loss: 0.7277991771697998\n",
      "Epoch 51, batch 150 D Loss: 1.4305537939071655, G Loss: 0.7118788957595825\n",
      "Epoch 51, batch 151 D Loss: 1.3905898332595825, G Loss: 0.7335653901100159\n",
      "Epoch 51, batch 152 D Loss: 1.393068552017212, G Loss: 0.7207417488098145\n",
      "Epoch 51, batch 153 D Loss: 1.4351065158843994, G Loss: 0.6989317536354065\n",
      "Epoch 51, batch 154 D Loss: 1.435955286026001, G Loss: 0.7139685153961182\n",
      "Epoch 51, batch 155 D Loss: 1.3747100830078125, G Loss: 0.7348515391349792\n",
      "Epoch 51, batch 156 D Loss: 1.3994293212890625, G Loss: 0.7242709398269653\n",
      "Epoch 51, batch 157 D Loss: 1.3726894855499268, G Loss: 0.7289380431175232\n",
      "Epoch 51, batch 158 D Loss: 1.436462163925171, G Loss: 0.6898994445800781\n",
      "Epoch 51, batch 159 D Loss: 1.3937792778015137, G Loss: 0.7273819446563721\n",
      "Epoch 51, batch 160 D Loss: 1.4300274848937988, G Loss: 0.7005214095115662\n",
      "Epoch 51, batch 161 D Loss: 1.4187467098236084, G Loss: 0.7246953845024109\n",
      "Epoch 51, batch 162 D Loss: 1.4066427946090698, G Loss: 0.7144628167152405\n",
      "Epoch 51, batch 163 D Loss: 1.3899688720703125, G Loss: 0.7259308099746704\n",
      "Epoch 51, batch 164 D Loss: 1.4091107845306396, G Loss: 0.7144799828529358\n",
      "Epoch 51, batch 165 D Loss: 1.389068603515625, G Loss: 0.7304763197898865\n",
      "Epoch 51, batch 166 D Loss: 1.4090428352355957, G Loss: 0.7044632434844971\n",
      "Epoch 51, batch 167 D Loss: 1.4711956977844238, G Loss: 0.699390709400177\n",
      "Epoch 51, batch 168 D Loss: 1.40989351272583, G Loss: 0.7072026133537292\n",
      "Epoch 51, batch 169 D Loss: 1.4091370105743408, G Loss: 0.7201478481292725\n",
      "Epoch 51, batch 170 D Loss: 1.399941325187683, G Loss: 0.7258206009864807\n",
      "Epoch 51, batch 171 D Loss: 1.4108376502990723, G Loss: 0.7277939319610596\n",
      "Epoch 51, batch 172 D Loss: 1.3929541110992432, G Loss: 0.7137300372123718\n",
      "Epoch 51, batch 173 D Loss: 1.4020540714263916, G Loss: 0.7118375301361084\n",
      "Epoch 51, batch 174 D Loss: 1.4181537628173828, G Loss: 0.7115650773048401\n",
      "Epoch 51, batch 175 D Loss: 1.4165763854980469, G Loss: 0.7116535902023315\n",
      "Epoch 51, batch 176 D Loss: 1.4026519060134888, G Loss: 0.7149043083190918\n",
      "Epoch 51, batch 177 D Loss: 1.4063971042633057, G Loss: 0.713828980922699\n",
      "Epoch 51, batch 178 D Loss: 1.4079487323760986, G Loss: 0.7170271873474121\n",
      "Epoch 51, batch 179 D Loss: 1.3994195461273193, G Loss: 0.7123532891273499\n",
      "Epoch 51, batch 180 D Loss: 1.3921782970428467, G Loss: 0.7166396379470825\n",
      "Epoch 51, batch 181 D Loss: 1.4022842645645142, G Loss: 0.7096011638641357\n",
      "Epoch 51, batch 182 D Loss: 1.4013535976409912, G Loss: 0.7126254439353943\n",
      "Epoch 51, batch 183 D Loss: 1.3890190124511719, G Loss: 0.7170946598052979\n",
      "Epoch 51, batch 184 D Loss: 1.3983116149902344, G Loss: 0.716483473777771\n",
      "Epoch 51, batch 185 D Loss: 1.3863627910614014, G Loss: 0.7125304937362671\n",
      "Epoch 51, batch 186 D Loss: 1.379604697227478, G Loss: 0.718407154083252\n",
      "Epoch 51, batch 187 D Loss: 1.3868460655212402, G Loss: 0.7198567986488342\n",
      "Epoch 51, batch 188 D Loss: 1.3893623352050781, G Loss: 0.7084572315216064\n",
      "Epoch 51, batch 189 D Loss: 1.3781936168670654, G Loss: 0.716361939907074\n",
      "Epoch 51, batch 190 D Loss: 1.378507375717163, G Loss: 0.7211816310882568\n",
      "Epoch 51, batch 191 D Loss: 1.3821847438812256, G Loss: 0.7148884534835815\n",
      "Epoch 51, batch 192 D Loss: 1.3773188591003418, G Loss: 0.7111836075782776\n",
      "Epoch 51, batch 193 D Loss: 1.3974007368087769, G Loss: 0.7160460948944092\n",
      "Epoch 51, batch 194 D Loss: 1.3843655586242676, G Loss: 0.7283226251602173\n",
      "Epoch 51, batch 195 D Loss: 1.3792531490325928, G Loss: 0.7231186628341675\n",
      "Epoch 51, batch 196 D Loss: 1.3800806999206543, G Loss: 0.7272014617919922\n",
      "Epoch 51, batch 197 D Loss: 1.381550669670105, G Loss: 0.7150898575782776\n",
      "Epoch 51, batch 198 D Loss: 1.3808362483978271, G Loss: 0.7217685580253601\n",
      "Epoch 51, batch 199 D Loss: 1.3883109092712402, G Loss: 0.712609052658081\n",
      "Epoch 51, batch 200 D Loss: 1.3957748413085938, G Loss: 0.7142737507820129\n",
      "Epoch 52, batch 1 D Loss: 1.375797986984253, G Loss: 0.7335841655731201\n",
      "Epoch 52, batch 2 D Loss: 1.3788690567016602, G Loss: 0.7157116532325745\n",
      "Epoch 52, batch 3 D Loss: 1.3642759323120117, G Loss: 0.7381205558776855\n",
      "Epoch 52, batch 4 D Loss: 1.3893651962280273, G Loss: 0.7243281006813049\n",
      "Epoch 52, batch 5 D Loss: 1.3979086875915527, G Loss: 0.7174775004386902\n",
      "Epoch 52, batch 6 D Loss: 1.3662705421447754, G Loss: 0.725090742111206\n",
      "Epoch 52, batch 7 D Loss: 1.3657617568969727, G Loss: 0.7322521805763245\n",
      "Epoch 52, batch 8 D Loss: 1.374682903289795, G Loss: 0.7257638573646545\n",
      "Epoch 52, batch 9 D Loss: 1.3914432525634766, G Loss: 0.719757080078125\n",
      "Epoch 52, batch 10 D Loss: 1.3717907667160034, G Loss: 0.728781521320343\n",
      "Epoch 52, batch 11 D Loss: 1.3662141561508179, G Loss: 0.720073401927948\n",
      "Epoch 52, batch 12 D Loss: 1.3912098407745361, G Loss: 0.7223120331764221\n",
      "Epoch 52, batch 13 D Loss: 1.3859056234359741, G Loss: 0.7231641411781311\n",
      "Epoch 52, batch 14 D Loss: 1.3633620738983154, G Loss: 0.7380275130271912\n",
      "Epoch 52, batch 15 D Loss: 1.3697986602783203, G Loss: 0.7280679941177368\n",
      "Epoch 52, batch 16 D Loss: 1.3433160781860352, G Loss: 0.7417815327644348\n",
      "Epoch 52, batch 17 D Loss: 1.389681339263916, G Loss: 0.722338855266571\n",
      "Epoch 52, batch 18 D Loss: 1.3607581853866577, G Loss: 0.7423898577690125\n",
      "Epoch 52, batch 19 D Loss: 1.3601651191711426, G Loss: 0.7329466342926025\n",
      "Epoch 52, batch 20 D Loss: 1.3733220100402832, G Loss: 0.7164056301116943\n",
      "Epoch 52, batch 21 D Loss: 1.3683271408081055, G Loss: 0.7233835458755493\n",
      "Epoch 52, batch 22 D Loss: 1.3727977275848389, G Loss: 0.728126049041748\n",
      "Epoch 52, batch 23 D Loss: 1.3624231815338135, G Loss: 0.7229045033454895\n",
      "Epoch 52, batch 24 D Loss: 1.3663465976715088, G Loss: 0.7167546153068542\n",
      "Epoch 52, batch 25 D Loss: 1.382007360458374, G Loss: 0.7144424319267273\n",
      "Epoch 52, batch 26 D Loss: 1.3537406921386719, G Loss: 0.7322236895561218\n",
      "Epoch 52, batch 27 D Loss: 1.3684189319610596, G Loss: 0.7199211120605469\n",
      "Epoch 52, batch 28 D Loss: 1.356126070022583, G Loss: 0.7302948832511902\n",
      "Epoch 52, batch 29 D Loss: 1.363039493560791, G Loss: 0.719207763671875\n",
      "Epoch 52, batch 30 D Loss: 1.36726713180542, G Loss: 0.725054144859314\n",
      "Epoch 52, batch 31 D Loss: 1.3662852048873901, G Loss: 0.7151892781257629\n",
      "Epoch 52, batch 32 D Loss: 1.3703596591949463, G Loss: 0.7222338914871216\n",
      "Epoch 52, batch 33 D Loss: 1.3589625358581543, G Loss: 0.7167666554450989\n",
      "Epoch 52, batch 34 D Loss: 1.3668454885482788, G Loss: 0.7157371044158936\n",
      "Epoch 52, batch 35 D Loss: 1.3656272888183594, G Loss: 0.7238319516181946\n",
      "Epoch 52, batch 36 D Loss: 1.38484525680542, G Loss: 0.7202458381652832\n",
      "Epoch 52, batch 37 D Loss: 1.3744664192199707, G Loss: 0.7151505947113037\n",
      "Epoch 52, batch 38 D Loss: 1.3796710968017578, G Loss: 0.7119249105453491\n",
      "Epoch 52, batch 39 D Loss: 1.371032476425171, G Loss: 0.7038065195083618\n",
      "Epoch 52, batch 40 D Loss: 1.3562288284301758, G Loss: 0.706022322177887\n",
      "Epoch 52, batch 41 D Loss: 1.395111322402954, G Loss: 0.7050259113311768\n",
      "Epoch 52, batch 42 D Loss: 1.3709111213684082, G Loss: 0.6989304423332214\n",
      "Epoch 52, batch 43 D Loss: 1.355355978012085, G Loss: 0.6996603608131409\n",
      "Epoch 52, batch 44 D Loss: 1.34352445602417, G Loss: 0.7260298132896423\n",
      "Epoch 52, batch 45 D Loss: 1.3837155103683472, G Loss: 0.6983383893966675\n",
      "Epoch 52, batch 46 D Loss: 1.3588266372680664, G Loss: 0.6855708360671997\n",
      "Epoch 52, batch 47 D Loss: 1.378410816192627, G Loss: 0.6915015578269958\n",
      "Epoch 52, batch 48 D Loss: 1.3628748655319214, G Loss: 0.6964349150657654\n",
      "Epoch 52, batch 49 D Loss: 1.3702397346496582, G Loss: 0.6855177879333496\n",
      "Epoch 52, batch 50 D Loss: 1.3681373596191406, G Loss: 0.6920498013496399\n",
      "Epoch 52, batch 51 D Loss: 1.3880033493041992, G Loss: 0.6728745102882385\n",
      "Epoch 52, batch 52 D Loss: 1.3784804344177246, G Loss: 0.6818313598632812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, batch 53 D Loss: 1.3474913835525513, G Loss: 0.6818181872367859\n",
      "Epoch 52, batch 54 D Loss: 1.3695125579833984, G Loss: 0.6849563717842102\n",
      "Epoch 52, batch 55 D Loss: 1.3825076818466187, G Loss: 0.671855628490448\n",
      "Epoch 52, batch 56 D Loss: 1.3622453212738037, G Loss: 0.6794427633285522\n",
      "Epoch 52, batch 57 D Loss: 1.3833115100860596, G Loss: 0.6625255346298218\n",
      "Epoch 52, batch 58 D Loss: 1.3771016597747803, G Loss: 0.660921037197113\n",
      "Epoch 52, batch 59 D Loss: 1.3791699409484863, G Loss: 0.670417308807373\n",
      "Epoch 52, batch 60 D Loss: 1.3498361110687256, G Loss: 0.665240466594696\n",
      "Epoch 52, batch 61 D Loss: 1.3766978979110718, G Loss: 0.6726164817810059\n",
      "Epoch 52, batch 62 D Loss: 1.3660502433776855, G Loss: 0.6731163263320923\n",
      "Epoch 52, batch 63 D Loss: 1.3696534633636475, G Loss: 0.6549707055091858\n",
      "Epoch 52, batch 64 D Loss: 1.3468492031097412, G Loss: 0.6773059964179993\n",
      "Epoch 52, batch 65 D Loss: 1.3905603885650635, G Loss: 0.653664767742157\n",
      "Epoch 52, batch 66 D Loss: 1.3814904689788818, G Loss: 0.6465563774108887\n",
      "Epoch 52, batch 67 D Loss: 1.3665838241577148, G Loss: 0.6712034344673157\n",
      "Epoch 52, batch 68 D Loss: 1.3933348655700684, G Loss: 0.6429961919784546\n",
      "Epoch 52, batch 69 D Loss: 1.408050775527954, G Loss: 0.6363916993141174\n",
      "Epoch 52, batch 70 D Loss: 1.3989770412445068, G Loss: 0.6469816565513611\n",
      "Epoch 52, batch 71 D Loss: 1.3531866073608398, G Loss: 0.6522965431213379\n",
      "Epoch 52, batch 72 D Loss: 1.394794225692749, G Loss: 0.6463263034820557\n",
      "Epoch 52, batch 73 D Loss: 1.3855500221252441, G Loss: 0.6467097401618958\n",
      "Epoch 52, batch 74 D Loss: 1.4079456329345703, G Loss: 0.6411148905754089\n",
      "Epoch 52, batch 75 D Loss: 1.3711974620819092, G Loss: 0.6502941250801086\n",
      "Epoch 52, batch 76 D Loss: 1.4031291007995605, G Loss: 0.6356276273727417\n",
      "Epoch 52, batch 77 D Loss: 1.398122787475586, G Loss: 0.6437220573425293\n",
      "Epoch 52, batch 78 D Loss: 1.3737127780914307, G Loss: 0.6498134732246399\n",
      "Epoch 52, batch 79 D Loss: 1.3865524530410767, G Loss: 0.652057409286499\n",
      "Epoch 52, batch 80 D Loss: 1.389939785003662, G Loss: 0.6459462642669678\n",
      "Epoch 52, batch 81 D Loss: 1.3836393356323242, G Loss: 0.6654704809188843\n",
      "Epoch 52, batch 82 D Loss: 1.3869470357894897, G Loss: 0.6501105427742004\n",
      "Epoch 52, batch 83 D Loss: 1.405463457107544, G Loss: 0.6349915862083435\n",
      "Epoch 52, batch 84 D Loss: 1.374988317489624, G Loss: 0.6544708013534546\n",
      "Epoch 52, batch 85 D Loss: 1.3933990001678467, G Loss: 0.6377795934677124\n",
      "Epoch 52, batch 86 D Loss: 1.3586695194244385, G Loss: 0.6428309679031372\n",
      "Epoch 52, batch 87 D Loss: 1.377830982208252, G Loss: 0.6595417857170105\n",
      "Epoch 52, batch 88 D Loss: 1.4085652828216553, G Loss: 0.6324421167373657\n",
      "Epoch 52, batch 89 D Loss: 1.406785249710083, G Loss: 0.633023738861084\n",
      "Epoch 52, batch 90 D Loss: 1.38787841796875, G Loss: 0.6376966238021851\n",
      "Epoch 52, batch 91 D Loss: 1.420323371887207, G Loss: 0.6231210231781006\n",
      "Epoch 52, batch 92 D Loss: 1.4240137338638306, G Loss: 0.6270638108253479\n",
      "Epoch 52, batch 93 D Loss: 1.4232196807861328, G Loss: 0.6251289248466492\n",
      "Epoch 52, batch 94 D Loss: 1.410719633102417, G Loss: 0.62841796875\n",
      "Epoch 52, batch 95 D Loss: 1.4203402996063232, G Loss: 0.6255189776420593\n",
      "Epoch 52, batch 96 D Loss: 1.3648279905319214, G Loss: 0.6633619666099548\n",
      "Epoch 52, batch 97 D Loss: 1.3853702545166016, G Loss: 0.6471318006515503\n",
      "Epoch 52, batch 98 D Loss: 1.3646621704101562, G Loss: 0.6567513346672058\n",
      "Epoch 52, batch 99 D Loss: 1.379284143447876, G Loss: 0.6386587023735046\n",
      "Epoch 52, batch 100 D Loss: 1.3678499460220337, G Loss: 0.6533388495445251\n",
      "Epoch 52, batch 101 D Loss: 1.3605908155441284, G Loss: 0.6611127257347107\n",
      "Epoch 52, batch 102 D Loss: 1.3874742984771729, G Loss: 0.6335158348083496\n",
      "Epoch 52, batch 103 D Loss: 1.376253604888916, G Loss: 0.655944287776947\n",
      "Epoch 52, batch 104 D Loss: 1.3610732555389404, G Loss: 0.6556538939476013\n",
      "Epoch 52, batch 105 D Loss: 1.3797940015792847, G Loss: 0.6506503820419312\n",
      "Epoch 52, batch 106 D Loss: 1.361307144165039, G Loss: 0.6686262488365173\n",
      "Epoch 52, batch 107 D Loss: 1.3433082103729248, G Loss: 0.6751951575279236\n",
      "Epoch 52, batch 108 D Loss: 1.3747066259384155, G Loss: 0.658223569393158\n",
      "Epoch 52, batch 109 D Loss: 1.364004373550415, G Loss: 0.66387540102005\n",
      "Epoch 52, batch 110 D Loss: 1.359834909439087, G Loss: 0.6770663261413574\n",
      "Epoch 52, batch 111 D Loss: 1.3721733093261719, G Loss: 0.6819359660148621\n",
      "Epoch 52, batch 112 D Loss: 1.3435943126678467, G Loss: 0.6676669120788574\n",
      "Epoch 52, batch 113 D Loss: 1.3296374082565308, G Loss: 0.6925575137138367\n",
      "Epoch 52, batch 114 D Loss: 1.3486859798431396, G Loss: 0.6943272948265076\n",
      "Epoch 52, batch 115 D Loss: 1.3294169902801514, G Loss: 0.6859280467033386\n",
      "Epoch 52, batch 116 D Loss: 1.2950491905212402, G Loss: 0.7146334648132324\n",
      "Epoch 52, batch 117 D Loss: 1.3283114433288574, G Loss: 0.6960419416427612\n",
      "Epoch 52, batch 118 D Loss: 1.3258110284805298, G Loss: 0.7062140107154846\n",
      "Epoch 52, batch 119 D Loss: 1.3246643543243408, G Loss: 0.716429591178894\n",
      "Epoch 52, batch 120 D Loss: 1.3234539031982422, G Loss: 0.7125436663627625\n",
      "Epoch 52, batch 121 D Loss: 1.2964651584625244, G Loss: 0.7164342403411865\n",
      "Epoch 52, batch 122 D Loss: 1.3749032020568848, G Loss: 0.7291982173919678\n",
      "Epoch 52, batch 123 D Loss: 1.3233506679534912, G Loss: 0.6873023509979248\n",
      "Epoch 52, batch 124 D Loss: 1.3133705854415894, G Loss: 0.7251811027526855\n",
      "Epoch 52, batch 125 D Loss: 1.287028193473816, G Loss: 0.7290349006652832\n",
      "Epoch 52, batch 126 D Loss: 1.335674524307251, G Loss: 0.7108592391014099\n",
      "Epoch 52, batch 127 D Loss: 1.3582909107208252, G Loss: 0.7148421406745911\n",
      "Epoch 52, batch 128 D Loss: 1.3344700336456299, G Loss: 0.7102581262588501\n",
      "Epoch 52, batch 129 D Loss: 1.3577430248260498, G Loss: 0.7256521582603455\n",
      "Epoch 52, batch 130 D Loss: 1.3168368339538574, G Loss: 0.7403480410575867\n",
      "Epoch 52, batch 131 D Loss: 1.2961056232452393, G Loss: 0.7433120012283325\n",
      "Epoch 52, batch 132 D Loss: 1.4453086853027344, G Loss: 0.7103822827339172\n",
      "Epoch 52, batch 133 D Loss: 1.422149658203125, G Loss: 0.7127363681793213\n",
      "Epoch 52, batch 134 D Loss: 1.3372917175292969, G Loss: 0.7231088280677795\n",
      "Epoch 52, batch 135 D Loss: 1.349232792854309, G Loss: 0.7529293298721313\n",
      "Epoch 52, batch 136 D Loss: 1.3817229270935059, G Loss: 0.7255221605300903\n",
      "Epoch 52, batch 137 D Loss: 1.4147896766662598, G Loss: 0.7230144739151001\n",
      "Epoch 52, batch 138 D Loss: 1.4128777980804443, G Loss: 0.7079439759254456\n",
      "Epoch 52, batch 139 D Loss: 1.2944068908691406, G Loss: 0.7795431613922119\n",
      "Epoch 52, batch 140 D Loss: 1.2758142948150635, G Loss: 0.7968806624412537\n",
      "Epoch 52, batch 141 D Loss: 1.3982994556427002, G Loss: 0.7685631513595581\n",
      "Epoch 52, batch 142 D Loss: 1.372916579246521, G Loss: 0.775674045085907\n",
      "Epoch 52, batch 143 D Loss: 1.4581692218780518, G Loss: 0.7285542488098145\n",
      "Epoch 52, batch 144 D Loss: 1.3581454753875732, G Loss: 0.7696928381919861\n",
      "Epoch 52, batch 145 D Loss: 1.4391694068908691, G Loss: 0.7617560029029846\n",
      "Epoch 52, batch 146 D Loss: 1.334887146949768, G Loss: 0.7569627165794373\n",
      "Epoch 52, batch 147 D Loss: 1.3945465087890625, G Loss: 0.7562997937202454\n",
      "Epoch 52, batch 148 D Loss: 1.3163490295410156, G Loss: 0.8110547065734863\n",
      "Epoch 52, batch 149 D Loss: 1.4458147287368774, G Loss: 0.6892811059951782\n",
      "Epoch 52, batch 150 D Loss: 1.4220542907714844, G Loss: 0.7376213669776917\n",
      "Epoch 52, batch 151 D Loss: 1.4182004928588867, G Loss: 0.7240922451019287\n",
      "Epoch 52, batch 152 D Loss: 1.4506962299346924, G Loss: 0.7149604558944702\n",
      "Epoch 52, batch 153 D Loss: 1.4057250022888184, G Loss: 0.7228437066078186\n",
      "Epoch 52, batch 154 D Loss: 1.4618933200836182, G Loss: 0.7093971967697144\n",
      "Epoch 52, batch 155 D Loss: 1.4638941287994385, G Loss: 0.6876416802406311\n",
      "Epoch 52, batch 156 D Loss: 1.4654195308685303, G Loss: 0.6973037719726562\n",
      "Epoch 52, batch 157 D Loss: 1.4434545040130615, G Loss: 0.7021042704582214\n",
      "Epoch 52, batch 158 D Loss: 1.4686777591705322, G Loss: 0.7113050222396851\n",
      "Epoch 52, batch 159 D Loss: 1.5150705575942993, G Loss: 0.6825894117355347\n",
      "Epoch 52, batch 160 D Loss: 1.4668374061584473, G Loss: 0.6827820539474487\n",
      "Epoch 52, batch 161 D Loss: 1.4822556972503662, G Loss: 0.6794956922531128\n",
      "Epoch 52, batch 162 D Loss: 1.410167932510376, G Loss: 0.7125783562660217\n",
      "Epoch 52, batch 163 D Loss: 1.4531712532043457, G Loss: 0.670576810836792\n",
      "Epoch 52, batch 164 D Loss: 1.5015995502471924, G Loss: 0.6887800097465515\n",
      "Epoch 52, batch 165 D Loss: 1.510239601135254, G Loss: 0.6803455352783203\n",
      "Epoch 52, batch 166 D Loss: 1.4528579711914062, G Loss: 0.7156105041503906\n",
      "Epoch 52, batch 167 D Loss: 1.5486042499542236, G Loss: 0.6798781752586365\n",
      "Epoch 52, batch 168 D Loss: 1.5086854696273804, G Loss: 0.6923516988754272\n",
      "Epoch 52, batch 169 D Loss: 1.4758878946304321, G Loss: 0.6984937787055969\n",
      "Epoch 52, batch 170 D Loss: 1.469984769821167, G Loss: 0.6882039904594421\n",
      "Epoch 52, batch 171 D Loss: 1.4714155197143555, G Loss: 0.7139136791229248\n",
      "Epoch 52, batch 172 D Loss: 1.5160688161849976, G Loss: 0.6899616718292236\n",
      "Epoch 52, batch 173 D Loss: 1.478840947151184, G Loss: 0.7007048726081848\n",
      "Epoch 52, batch 174 D Loss: 1.4413577318191528, G Loss: 0.704196572303772\n",
      "Epoch 52, batch 175 D Loss: 1.4108445644378662, G Loss: 0.7170625329017639\n",
      "Epoch 52, batch 176 D Loss: 1.4266929626464844, G Loss: 0.7337653636932373\n",
      "Epoch 52, batch 177 D Loss: 1.4228767156600952, G Loss: 0.7269022464752197\n",
      "Epoch 52, batch 178 D Loss: 1.4337774515151978, G Loss: 0.7198061347007751\n",
      "Epoch 52, batch 179 D Loss: 1.4173152446746826, G Loss: 0.7394733428955078\n",
      "Epoch 52, batch 180 D Loss: 1.405158519744873, G Loss: 0.7501564621925354\n",
      "Epoch 52, batch 181 D Loss: 1.3966530561447144, G Loss: 0.7670091986656189\n",
      "Epoch 52, batch 182 D Loss: 1.4065020084381104, G Loss: 0.7599261999130249\n",
      "Epoch 52, batch 183 D Loss: 1.4081151485443115, G Loss: 0.7701746225357056\n",
      "Epoch 52, batch 184 D Loss: 1.3904345035552979, G Loss: 0.7969698309898376\n",
      "Epoch 52, batch 185 D Loss: 1.3756499290466309, G Loss: 0.792931318283081\n",
      "Epoch 52, batch 186 D Loss: 1.3498713970184326, G Loss: 0.8596034049987793\n",
      "Epoch 52, batch 187 D Loss: 1.373779058456421, G Loss: 0.8040525913238525\n",
      "Epoch 52, batch 188 D Loss: 1.3552253246307373, G Loss: 0.8455715179443359\n",
      "Epoch 52, batch 189 D Loss: 1.3590826988220215, G Loss: 0.869871973991394\n",
      "Epoch 52, batch 190 D Loss: 1.3980356454849243, G Loss: 0.7909520864486694\n",
      "Epoch 52, batch 191 D Loss: 1.3271195888519287, G Loss: 0.8874057531356812\n",
      "Epoch 52, batch 192 D Loss: 1.3199636936187744, G Loss: 0.912476658821106\n",
      "Epoch 52, batch 193 D Loss: 1.325897455215454, G Loss: 0.8945882320404053\n",
      "Epoch 52, batch 194 D Loss: 1.3446259498596191, G Loss: 0.9087494611740112\n",
      "Epoch 52, batch 195 D Loss: 1.3635239601135254, G Loss: 0.8637771010398865\n",
      "Epoch 52, batch 196 D Loss: 1.403832197189331, G Loss: 0.8020398616790771\n",
      "Epoch 52, batch 197 D Loss: 1.3160037994384766, G Loss: 0.9335284233093262\n",
      "Epoch 52, batch 198 D Loss: 1.3560717105865479, G Loss: 0.8972463011741638\n",
      "Epoch 52, batch 199 D Loss: 1.406118392944336, G Loss: 0.8991656303405762\n",
      "Epoch 52, batch 200 D Loss: 1.3824354410171509, G Loss: 0.8598217964172363\n",
      "Epoch 53, batch 1 D Loss: 1.3599379062652588, G Loss: 0.8411173224449158\n",
      "Epoch 53, batch 2 D Loss: 1.3724322319030762, G Loss: 0.8171171545982361\n",
      "Epoch 53, batch 3 D Loss: 1.4318130016326904, G Loss: 0.813408374786377\n",
      "Epoch 53, batch 4 D Loss: 1.3244810104370117, G Loss: 0.8978246450424194\n",
      "Epoch 53, batch 5 D Loss: 1.4991624355316162, G Loss: 0.7665836811065674\n",
      "Epoch 53, batch 6 D Loss: 1.4490715265274048, G Loss: 0.7220951318740845\n",
      "Epoch 53, batch 7 D Loss: 1.366282343864441, G Loss: 0.7938908934593201\n",
      "Epoch 53, batch 8 D Loss: 1.3659307956695557, G Loss: 0.8254870772361755\n",
      "Epoch 53, batch 9 D Loss: 1.3507068157196045, G Loss: 0.7861515283584595\n",
      "Epoch 53, batch 10 D Loss: 1.390960693359375, G Loss: 0.7569253444671631\n",
      "Epoch 53, batch 11 D Loss: 1.3679766654968262, G Loss: 0.7585640549659729\n",
      "Epoch 53, batch 12 D Loss: 1.332897663116455, G Loss: 0.8001360893249512\n",
      "Epoch 53, batch 13 D Loss: 1.343921422958374, G Loss: 0.7552986741065979\n",
      "Epoch 53, batch 14 D Loss: 1.3925917148590088, G Loss: 0.7552846670150757\n",
      "Epoch 53, batch 15 D Loss: 1.3615543842315674, G Loss: 0.7763782739639282\n",
      "Epoch 53, batch 16 D Loss: 1.3504788875579834, G Loss: 0.7747540473937988\n",
      "Epoch 53, batch 17 D Loss: 1.342663049697876, G Loss: 0.7349006533622742\n",
      "Epoch 53, batch 18 D Loss: 1.3629465103149414, G Loss: 0.737770140171051\n",
      "Epoch 53, batch 19 D Loss: 1.3618017435073853, G Loss: 0.7308217883110046\n",
      "Epoch 53, batch 20 D Loss: 1.3748295307159424, G Loss: 0.7182772755622864\n",
      "Epoch 53, batch 21 D Loss: 1.3596361875534058, G Loss: 0.7363137006759644\n",
      "Epoch 53, batch 22 D Loss: 1.373626708984375, G Loss: 0.7378166317939758\n",
      "Epoch 53, batch 23 D Loss: 1.3449552059173584, G Loss: 0.7285686731338501\n",
      "Epoch 53, batch 24 D Loss: 1.329851746559143, G Loss: 0.7488315105438232\n",
      "Epoch 53, batch 25 D Loss: 1.331984519958496, G Loss: 0.7583463191986084\n",
      "Epoch 53, batch 26 D Loss: 1.3694947957992554, G Loss: 0.7165485620498657\n",
      "Epoch 53, batch 27 D Loss: 1.3321096897125244, G Loss: 0.7795462608337402\n",
      "Epoch 53, batch 28 D Loss: 1.3887183666229248, G Loss: 0.6832978129386902\n",
      "Epoch 53, batch 29 D Loss: 1.3596781492233276, G Loss: 0.747392475605011\n",
      "Epoch 53, batch 30 D Loss: 1.3160566091537476, G Loss: 0.7278246879577637\n",
      "Epoch 53, batch 31 D Loss: 1.3540029525756836, G Loss: 0.7217843532562256\n",
      "Epoch 53, batch 32 D Loss: 1.42496657371521, G Loss: 0.702280580997467\n",
      "Epoch 53, batch 33 D Loss: 1.336266279220581, G Loss: 0.7108873724937439\n",
      "Epoch 53, batch 34 D Loss: 1.3925098180770874, G Loss: 0.7202682495117188\n",
      "Epoch 53, batch 35 D Loss: 1.4298930168151855, G Loss: 0.6904206275939941\n",
      "Epoch 53, batch 36 D Loss: 1.388657808303833, G Loss: 0.6906185746192932\n",
      "Epoch 53, batch 37 D Loss: 1.3753023147583008, G Loss: 0.6910748481750488\n",
      "Epoch 53, batch 38 D Loss: 1.4128645658493042, G Loss: 0.6807786822319031\n",
      "Epoch 53, batch 39 D Loss: 1.3893160820007324, G Loss: 0.6791672706604004\n",
      "Epoch 53, batch 40 D Loss: 1.3342597484588623, G Loss: 0.6690065860748291\n",
      "Epoch 53, batch 41 D Loss: 1.3657149076461792, G Loss: 0.6822991371154785\n",
      "Epoch 53, batch 42 D Loss: 1.4095664024353027, G Loss: 0.6652188301086426\n",
      "Epoch 53, batch 43 D Loss: 1.412677526473999, G Loss: 0.6768695712089539\n",
      "Epoch 53, batch 44 D Loss: 1.3673195838928223, G Loss: 0.6736508011817932\n",
      "Epoch 53, batch 45 D Loss: 1.369375228881836, G Loss: 0.6808450818061829\n",
      "Epoch 53, batch 46 D Loss: 1.3933994770050049, G Loss: 0.6562353372573853\n",
      "Epoch 53, batch 47 D Loss: 1.354196310043335, G Loss: 0.6669989228248596\n",
      "Epoch 53, batch 48 D Loss: 1.3842482566833496, G Loss: 0.6511759757995605\n",
      "Epoch 53, batch 49 D Loss: 1.388023853302002, G Loss: 0.6485052704811096\n",
      "Epoch 53, batch 50 D Loss: 1.3638596534729004, G Loss: 0.6810241937637329\n",
      "Epoch 53, batch 51 D Loss: 1.356615662574768, G Loss: 0.6621010303497314\n",
      "Epoch 53, batch 52 D Loss: 1.3590812683105469, G Loss: 0.6551738977432251\n",
      "Epoch 53, batch 53 D Loss: 1.3964264392852783, G Loss: 0.641224205493927\n",
      "Epoch 53, batch 54 D Loss: 1.3906738758087158, G Loss: 0.6573797464370728\n",
      "Epoch 53, batch 55 D Loss: 1.3762054443359375, G Loss: 0.6572608351707458\n",
      "Epoch 53, batch 56 D Loss: 1.4341977834701538, G Loss: 0.6389428973197937\n",
      "Epoch 53, batch 57 D Loss: 1.398168683052063, G Loss: 0.6380606293678284\n",
      "Epoch 53, batch 58 D Loss: 1.3893589973449707, G Loss: 0.654856264591217\n",
      "Epoch 53, batch 59 D Loss: 1.3951283693313599, G Loss: 0.6399445533752441\n",
      "Epoch 53, batch 60 D Loss: 1.363919734954834, G Loss: 0.6418012976646423\n",
      "Epoch 53, batch 61 D Loss: 1.3644169569015503, G Loss: 0.6544532179832458\n",
      "Epoch 53, batch 62 D Loss: 1.355628252029419, G Loss: 0.6401904821395874\n",
      "Epoch 53, batch 63 D Loss: 1.3833353519439697, G Loss: 0.6384798288345337\n",
      "Epoch 53, batch 64 D Loss: 1.3877068758010864, G Loss: 0.6422078013420105\n",
      "Epoch 53, batch 65 D Loss: 1.4080127477645874, G Loss: 0.6430639028549194\n",
      "Epoch 53, batch 66 D Loss: 1.4177452325820923, G Loss: 0.637644350528717\n",
      "Epoch 53, batch 67 D Loss: 1.3915082216262817, G Loss: 0.6414080262184143\n",
      "Epoch 53, batch 68 D Loss: 1.417460322380066, G Loss: 0.6317787766456604\n",
      "Epoch 53, batch 69 D Loss: 1.4367635250091553, G Loss: 0.6169807314872742\n",
      "Epoch 53, batch 70 D Loss: 1.4092745780944824, G Loss: 0.6420385837554932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, batch 71 D Loss: 1.3961091041564941, G Loss: 0.6441396474838257\n",
      "Epoch 53, batch 72 D Loss: 1.387058973312378, G Loss: 0.6301763653755188\n",
      "Epoch 53, batch 73 D Loss: 1.4238860607147217, G Loss: 0.6284032464027405\n",
      "Epoch 53, batch 74 D Loss: 1.4724235534667969, G Loss: 0.6140762567520142\n",
      "Epoch 53, batch 75 D Loss: 1.4129748344421387, G Loss: 0.6378799676895142\n",
      "Epoch 53, batch 76 D Loss: 1.3905482292175293, G Loss: 0.6398240923881531\n",
      "Epoch 53, batch 77 D Loss: 1.4281203746795654, G Loss: 0.6319239139556885\n",
      "Epoch 53, batch 78 D Loss: 1.3925645351409912, G Loss: 0.6528522372245789\n",
      "Epoch 53, batch 79 D Loss: 1.4248179197311401, G Loss: 0.6406829953193665\n",
      "Epoch 53, batch 80 D Loss: 1.3750545978546143, G Loss: 0.654086172580719\n",
      "Epoch 53, batch 81 D Loss: 1.445712685585022, G Loss: 0.6235141158103943\n",
      "Epoch 53, batch 82 D Loss: 1.4100475311279297, G Loss: 0.640282154083252\n",
      "Epoch 53, batch 83 D Loss: 1.4452470541000366, G Loss: 0.6132198572158813\n",
      "Epoch 53, batch 84 D Loss: 1.4716097116470337, G Loss: 0.6177882552146912\n",
      "Epoch 53, batch 85 D Loss: 1.412447214126587, G Loss: 0.6427408456802368\n",
      "Epoch 53, batch 86 D Loss: 1.4434293508529663, G Loss: 0.6415892839431763\n",
      "Epoch 53, batch 87 D Loss: 1.4378347396850586, G Loss: 0.6412729024887085\n",
      "Epoch 53, batch 88 D Loss: 1.4319159984588623, G Loss: 0.6479585766792297\n",
      "Epoch 53, batch 89 D Loss: 1.4219954013824463, G Loss: 0.6599810719490051\n",
      "Epoch 53, batch 90 D Loss: 1.4604463577270508, G Loss: 0.6215704679489136\n",
      "Epoch 53, batch 91 D Loss: 1.4219558238983154, G Loss: 0.6556738018989563\n",
      "Epoch 53, batch 92 D Loss: 1.4199795722961426, G Loss: 0.6559333205223083\n",
      "Epoch 53, batch 93 D Loss: 1.421486735343933, G Loss: 0.6663903594017029\n",
      "Epoch 53, batch 94 D Loss: 1.4413621425628662, G Loss: 0.6479576230049133\n",
      "Epoch 53, batch 95 D Loss: 1.4001939296722412, G Loss: 0.6880348920822144\n",
      "Epoch 53, batch 96 D Loss: 1.4182305335998535, G Loss: 0.6755630970001221\n",
      "Epoch 53, batch 97 D Loss: 1.4245781898498535, G Loss: 0.6748188138008118\n",
      "Epoch 53, batch 98 D Loss: 1.4230657815933228, G Loss: 0.6799458265304565\n",
      "Epoch 53, batch 99 D Loss: 1.416349172592163, G Loss: 0.6877063512802124\n",
      "Epoch 53, batch 100 D Loss: 1.4009038209915161, G Loss: 0.702167809009552\n",
      "Epoch 53, batch 101 D Loss: 1.4029649496078491, G Loss: 0.6961991190910339\n",
      "Epoch 53, batch 102 D Loss: 1.3995237350463867, G Loss: 0.7065020203590393\n",
      "Epoch 53, batch 103 D Loss: 1.373877763748169, G Loss: 0.7203785181045532\n",
      "Epoch 53, batch 104 D Loss: 1.3939701318740845, G Loss: 0.7146443724632263\n",
      "Epoch 53, batch 105 D Loss: 1.3615319728851318, G Loss: 0.7395071387290955\n",
      "Epoch 53, batch 106 D Loss: 1.3622872829437256, G Loss: 0.7399555444717407\n",
      "Epoch 53, batch 107 D Loss: 1.3643018007278442, G Loss: 0.7493807077407837\n",
      "Epoch 53, batch 108 D Loss: 1.3554210662841797, G Loss: 0.7513865828514099\n",
      "Epoch 53, batch 109 D Loss: 1.3485801219940186, G Loss: 0.7632198333740234\n",
      "Epoch 53, batch 110 D Loss: 1.3437262773513794, G Loss: 0.7693095207214355\n",
      "Epoch 53, batch 111 D Loss: 1.3430113792419434, G Loss: 0.7758935689926147\n",
      "Epoch 53, batch 112 D Loss: 1.3419349193572998, G Loss: 0.7842671275138855\n",
      "Epoch 53, batch 113 D Loss: 1.3322598934173584, G Loss: 0.8009436130523682\n",
      "Epoch 53, batch 114 D Loss: 1.3298590183258057, G Loss: 0.8080543279647827\n",
      "Epoch 53, batch 115 D Loss: 1.29933500289917, G Loss: 0.8190410137176514\n",
      "Epoch 53, batch 116 D Loss: 1.30741286277771, G Loss: 0.844848096370697\n",
      "Epoch 53, batch 117 D Loss: 1.2952377796173096, G Loss: 0.8467161059379578\n",
      "Epoch 53, batch 118 D Loss: 1.3038339614868164, G Loss: 0.8264608979225159\n",
      "Epoch 53, batch 119 D Loss: 1.3226737976074219, G Loss: 0.8343100547790527\n",
      "Epoch 53, batch 120 D Loss: 1.2917672395706177, G Loss: 0.8609675765037537\n",
      "Epoch 53, batch 121 D Loss: 1.3026366233825684, G Loss: 0.8461277484893799\n",
      "Epoch 53, batch 122 D Loss: 1.287898063659668, G Loss: 0.8555155992507935\n",
      "Epoch 53, batch 123 D Loss: 1.2688205242156982, G Loss: 0.9026740193367004\n",
      "Epoch 53, batch 124 D Loss: 1.3173984289169312, G Loss: 0.8456601500511169\n",
      "Epoch 53, batch 125 D Loss: 1.267634391784668, G Loss: 0.8665044903755188\n",
      "Epoch 53, batch 126 D Loss: 1.3144631385803223, G Loss: 0.820353627204895\n",
      "Epoch 53, batch 127 D Loss: 1.2951726913452148, G Loss: 0.8527200222015381\n",
      "Epoch 53, batch 128 D Loss: 1.2356966733932495, G Loss: 0.9061650037765503\n",
      "Epoch 53, batch 129 D Loss: 1.3198885917663574, G Loss: 0.8225488066673279\n",
      "Epoch 53, batch 130 D Loss: 1.3091037273406982, G Loss: 0.832094132900238\n",
      "Epoch 53, batch 131 D Loss: 1.294177532196045, G Loss: 0.8964876532554626\n",
      "Epoch 53, batch 132 D Loss: 1.3104865550994873, G Loss: 0.8171998858451843\n",
      "Epoch 53, batch 133 D Loss: 1.2953130006790161, G Loss: 0.806094229221344\n",
      "Epoch 53, batch 134 D Loss: 1.2826359272003174, G Loss: 0.8504136800765991\n",
      "Epoch 53, batch 135 D Loss: 1.3136768341064453, G Loss: 0.748949408531189\n",
      "Epoch 53, batch 136 D Loss: 1.3234574794769287, G Loss: 0.7847036719322205\n",
      "Epoch 53, batch 137 D Loss: 1.3496534824371338, G Loss: 0.7892703413963318\n",
      "Epoch 53, batch 138 D Loss: 1.3569257259368896, G Loss: 0.7481797933578491\n",
      "Epoch 53, batch 139 D Loss: 1.3240737915039062, G Loss: 0.7708446979522705\n",
      "Epoch 53, batch 140 D Loss: 1.3178083896636963, G Loss: 0.7562253475189209\n",
      "Epoch 53, batch 141 D Loss: 1.3073651790618896, G Loss: 0.7424556016921997\n",
      "Epoch 53, batch 142 D Loss: 1.3531510829925537, G Loss: 0.7875716686248779\n",
      "Epoch 53, batch 143 D Loss: 1.3728306293487549, G Loss: 0.7403040528297424\n",
      "Epoch 53, batch 144 D Loss: 1.2938565015792847, G Loss: 0.7603051662445068\n",
      "Epoch 53, batch 145 D Loss: 1.3384945392608643, G Loss: 0.7357814908027649\n",
      "Epoch 53, batch 146 D Loss: 1.372855305671692, G Loss: 0.7239541411399841\n",
      "Epoch 53, batch 147 D Loss: 1.37668776512146, G Loss: 0.7272821068763733\n",
      "Epoch 53, batch 148 D Loss: 1.3950495719909668, G Loss: 0.7091857194900513\n",
      "Epoch 53, batch 149 D Loss: 1.3511197566986084, G Loss: 0.6919703483581543\n",
      "Epoch 53, batch 150 D Loss: 1.382594347000122, G Loss: 0.6745246648788452\n",
      "Epoch 53, batch 151 D Loss: 1.366919994354248, G Loss: 0.6878793239593506\n",
      "Epoch 53, batch 152 D Loss: 1.4175020456314087, G Loss: 0.6625445485115051\n",
      "Epoch 53, batch 153 D Loss: 1.4392733573913574, G Loss: 0.6623571515083313\n",
      "Epoch 53, batch 154 D Loss: 1.427220106124878, G Loss: 0.6352418661117554\n",
      "Epoch 53, batch 155 D Loss: 1.4762871265411377, G Loss: 0.6595489978790283\n",
      "Epoch 53, batch 156 D Loss: 1.452855110168457, G Loss: 0.6422062516212463\n",
      "Epoch 53, batch 157 D Loss: 1.4363961219787598, G Loss: 0.6440792679786682\n",
      "Epoch 53, batch 158 D Loss: 1.5085489749908447, G Loss: 0.6352280974388123\n",
      "Epoch 53, batch 159 D Loss: 1.438974142074585, G Loss: 0.649635374546051\n",
      "Epoch 53, batch 160 D Loss: 1.4641895294189453, G Loss: 0.6464619636535645\n",
      "Epoch 53, batch 161 D Loss: 1.4230238199234009, G Loss: 0.6495816111564636\n",
      "Epoch 53, batch 162 D Loss: 1.4201440811157227, G Loss: 0.6361088752746582\n",
      "Epoch 53, batch 163 D Loss: 1.3970167636871338, G Loss: 0.6391760110855103\n",
      "Epoch 53, batch 164 D Loss: 1.477112054824829, G Loss: 0.6041087508201599\n",
      "Epoch 53, batch 165 D Loss: 1.4622323513031006, G Loss: 0.6681295037269592\n",
      "Epoch 53, batch 166 D Loss: 1.4766430854797363, G Loss: 0.6370658278465271\n",
      "Epoch 53, batch 167 D Loss: 1.4399082660675049, G Loss: 0.6139951348304749\n",
      "Epoch 53, batch 168 D Loss: 1.465900182723999, G Loss: 0.6139440536499023\n",
      "Epoch 53, batch 169 D Loss: 1.444094181060791, G Loss: 0.6311111450195312\n",
      "Epoch 53, batch 170 D Loss: 1.5120552778244019, G Loss: 0.5715508460998535\n",
      "Epoch 53, batch 171 D Loss: 1.4633266925811768, G Loss: 0.610989511013031\n",
      "Epoch 53, batch 172 D Loss: 1.4791674613952637, G Loss: 0.6100605130195618\n",
      "Epoch 53, batch 173 D Loss: 1.497272253036499, G Loss: 0.6181501746177673\n",
      "Epoch 53, batch 174 D Loss: 1.4264194965362549, G Loss: 0.62278813123703\n",
      "Epoch 53, batch 175 D Loss: 1.4292278289794922, G Loss: 0.6406527757644653\n",
      "Epoch 53, batch 176 D Loss: 1.4319878816604614, G Loss: 0.6220400929450989\n",
      "Epoch 53, batch 177 D Loss: 1.457129955291748, G Loss: 0.6331622004508972\n",
      "Epoch 53, batch 178 D Loss: 1.4130412340164185, G Loss: 0.6430060863494873\n",
      "Epoch 53, batch 179 D Loss: 1.4327402114868164, G Loss: 0.639122486114502\n",
      "Epoch 53, batch 180 D Loss: 1.4905364513397217, G Loss: 0.6134795546531677\n",
      "Epoch 53, batch 181 D Loss: 1.4394071102142334, G Loss: 0.6366135478019714\n",
      "Epoch 53, batch 182 D Loss: 1.4386910200119019, G Loss: 0.6391627788543701\n",
      "Epoch 53, batch 183 D Loss: 1.4636640548706055, G Loss: 0.620088517665863\n",
      "Epoch 53, batch 184 D Loss: 1.4488757848739624, G Loss: 0.6302537322044373\n",
      "Epoch 53, batch 185 D Loss: 1.438673496246338, G Loss: 0.6377817392349243\n",
      "Epoch 53, batch 186 D Loss: 1.4402016401290894, G Loss: 0.6428964138031006\n",
      "Epoch 53, batch 187 D Loss: 1.444344162940979, G Loss: 0.6349061727523804\n",
      "Epoch 53, batch 188 D Loss: 1.4434894323349, G Loss: 0.6515279412269592\n",
      "Epoch 53, batch 189 D Loss: 1.445239782333374, G Loss: 0.6357883810997009\n",
      "Epoch 53, batch 190 D Loss: 1.4136936664581299, G Loss: 0.6498898267745972\n",
      "Epoch 53, batch 191 D Loss: 1.4275799989700317, G Loss: 0.6533777713775635\n",
      "Epoch 53, batch 192 D Loss: 1.4397637844085693, G Loss: 0.6504318118095398\n",
      "Epoch 53, batch 193 D Loss: 1.4398870468139648, G Loss: 0.6514495015144348\n",
      "Epoch 53, batch 194 D Loss: 1.4329110383987427, G Loss: 0.651795506477356\n",
      "Epoch 53, batch 195 D Loss: 1.413743019104004, G Loss: 0.6723790764808655\n",
      "Epoch 53, batch 196 D Loss: 1.4112699031829834, G Loss: 0.6679107546806335\n",
      "Epoch 53, batch 197 D Loss: 1.4353020191192627, G Loss: 0.6577761173248291\n",
      "Epoch 53, batch 198 D Loss: 1.3917040824890137, G Loss: 0.6743133664131165\n",
      "Epoch 53, batch 199 D Loss: 1.4114352464675903, G Loss: 0.6638531684875488\n",
      "Epoch 53, batch 200 D Loss: 1.411362648010254, G Loss: 0.6723654866218567\n",
      "Epoch 54, batch 1 D Loss: 1.3917591571807861, G Loss: 0.6778370141983032\n",
      "Epoch 54, batch 2 D Loss: 1.401511311531067, G Loss: 0.6784831881523132\n",
      "Epoch 54, batch 3 D Loss: 1.408219814300537, G Loss: 0.6769410967826843\n",
      "Epoch 54, batch 4 D Loss: 1.420510172843933, G Loss: 0.6716061234474182\n",
      "Epoch 54, batch 5 D Loss: 1.3781365156173706, G Loss: 0.6830570101737976\n",
      "Epoch 54, batch 6 D Loss: 1.3876100778579712, G Loss: 0.6864440441131592\n",
      "Epoch 54, batch 7 D Loss: 1.3928332328796387, G Loss: 0.6791506409645081\n",
      "Epoch 54, batch 8 D Loss: 1.3928859233856201, G Loss: 0.6782090663909912\n",
      "Epoch 54, batch 9 D Loss: 1.3756650686264038, G Loss: 0.6888679265975952\n",
      "Epoch 54, batch 10 D Loss: 1.3695108890533447, G Loss: 0.6921707391738892\n",
      "Epoch 54, batch 11 D Loss: 1.3590565919876099, G Loss: 0.698910653591156\n",
      "Epoch 54, batch 12 D Loss: 1.3718862533569336, G Loss: 0.6918318867683411\n",
      "Epoch 54, batch 13 D Loss: 1.344101905822754, G Loss: 0.6974289417266846\n",
      "Epoch 54, batch 14 D Loss: 1.3703787326812744, G Loss: 0.694581151008606\n",
      "Epoch 54, batch 15 D Loss: 1.3557113409042358, G Loss: 0.7054545879364014\n",
      "Epoch 54, batch 16 D Loss: 1.34701669216156, G Loss: 0.7037797570228577\n",
      "Epoch 54, batch 17 D Loss: 1.3542637825012207, G Loss: 0.69908607006073\n",
      "Epoch 54, batch 18 D Loss: 1.3258857727050781, G Loss: 0.7131399512290955\n",
      "Epoch 54, batch 19 D Loss: 1.3247640132904053, G Loss: 0.7158234119415283\n",
      "Epoch 54, batch 20 D Loss: 1.3543494939804077, G Loss: 0.7054688930511475\n",
      "Epoch 54, batch 21 D Loss: 1.3485023975372314, G Loss: 0.7144078016281128\n",
      "Epoch 54, batch 22 D Loss: 1.3331248760223389, G Loss: 0.7171846032142639\n",
      "Epoch 54, batch 23 D Loss: 1.321469783782959, G Loss: 0.7232627868652344\n",
      "Epoch 54, batch 24 D Loss: 1.3180820941925049, G Loss: 0.7197970747947693\n",
      "Epoch 54, batch 25 D Loss: 1.318845510482788, G Loss: 0.7105494141578674\n",
      "Epoch 54, batch 26 D Loss: 1.3395023345947266, G Loss: 0.7153295874595642\n",
      "Epoch 54, batch 27 D Loss: 1.3250858783721924, G Loss: 0.7094024419784546\n",
      "Epoch 54, batch 28 D Loss: 1.30917227268219, G Loss: 0.7322805523872375\n",
      "Epoch 54, batch 29 D Loss: 1.285882830619812, G Loss: 0.7490657567977905\n",
      "Epoch 54, batch 30 D Loss: 1.3232239484786987, G Loss: 0.7156521677970886\n",
      "Epoch 54, batch 31 D Loss: 1.2927916049957275, G Loss: 0.7324937582015991\n",
      "Epoch 54, batch 32 D Loss: 1.3637731075286865, G Loss: 0.6971380710601807\n",
      "Epoch 54, batch 33 D Loss: 1.3042163848876953, G Loss: 0.722908616065979\n",
      "Epoch 54, batch 34 D Loss: 1.3201210498809814, G Loss: 0.7179240584373474\n",
      "Epoch 54, batch 35 D Loss: 1.3037562370300293, G Loss: 0.7273865342140198\n",
      "Epoch 54, batch 36 D Loss: 1.286002278327942, G Loss: 0.7244584560394287\n",
      "Epoch 54, batch 37 D Loss: 1.3057012557983398, G Loss: 0.7233406901359558\n",
      "Epoch 54, batch 38 D Loss: 1.2796350717544556, G Loss: 0.7268662452697754\n",
      "Epoch 54, batch 39 D Loss: 1.3198256492614746, G Loss: 0.7277689576148987\n",
      "Epoch 54, batch 40 D Loss: 1.278782606124878, G Loss: 0.7229897975921631\n",
      "Epoch 54, batch 41 D Loss: 1.2773613929748535, G Loss: 0.724179208278656\n",
      "Epoch 54, batch 42 D Loss: 1.2677191495895386, G Loss: 0.7114605903625488\n",
      "Epoch 54, batch 43 D Loss: 1.3254890441894531, G Loss: 0.6998228430747986\n",
      "Epoch 54, batch 44 D Loss: 1.248298168182373, G Loss: 0.7534550428390503\n",
      "Epoch 54, batch 45 D Loss: 1.2752758264541626, G Loss: 0.7369228601455688\n",
      "Epoch 54, batch 46 D Loss: 1.2607481479644775, G Loss: 0.7496405839920044\n",
      "Epoch 54, batch 47 D Loss: 1.2863130569458008, G Loss: 0.7303014993667603\n",
      "Epoch 54, batch 48 D Loss: 1.2716546058654785, G Loss: 0.7318132519721985\n",
      "Epoch 54, batch 49 D Loss: 1.2978416681289673, G Loss: 0.7203550934791565\n",
      "Epoch 54, batch 50 D Loss: 1.2601648569107056, G Loss: 0.7513786554336548\n",
      "Epoch 54, batch 51 D Loss: 1.3248214721679688, G Loss: 0.7007784247398376\n",
      "Epoch 54, batch 52 D Loss: 1.2787959575653076, G Loss: 0.7286142706871033\n",
      "Epoch 54, batch 53 D Loss: 1.286811351776123, G Loss: 0.7269543409347534\n",
      "Epoch 54, batch 54 D Loss: 1.268120527267456, G Loss: 0.747506320476532\n",
      "Epoch 54, batch 55 D Loss: 1.282118320465088, G Loss: 0.7323041558265686\n",
      "Epoch 54, batch 56 D Loss: 1.3059700727462769, G Loss: 0.6804326772689819\n",
      "Epoch 54, batch 57 D Loss: 1.3112335205078125, G Loss: 0.7116813063621521\n",
      "Epoch 54, batch 58 D Loss: 1.3071143627166748, G Loss: 0.7099236249923706\n",
      "Epoch 54, batch 59 D Loss: 1.2958290576934814, G Loss: 0.728760838508606\n",
      "Epoch 54, batch 60 D Loss: 1.3052945137023926, G Loss: 0.6912521123886108\n",
      "Epoch 54, batch 61 D Loss: 1.2616459131240845, G Loss: 0.7072609663009644\n",
      "Epoch 54, batch 62 D Loss: 1.28078031539917, G Loss: 0.7003201842308044\n",
      "Epoch 54, batch 63 D Loss: 1.2324144840240479, G Loss: 0.6956443786621094\n",
      "Epoch 54, batch 64 D Loss: 1.3140811920166016, G Loss: 0.6958553791046143\n",
      "Epoch 54, batch 65 D Loss: 1.3365976810455322, G Loss: 0.6862874031066895\n",
      "Epoch 54, batch 66 D Loss: 1.3740119934082031, G Loss: 0.6929748058319092\n",
      "Epoch 54, batch 67 D Loss: 1.341604232788086, G Loss: 0.6897342205047607\n",
      "Epoch 54, batch 68 D Loss: 1.306091070175171, G Loss: 0.6833665370941162\n",
      "Epoch 54, batch 69 D Loss: 1.36421799659729, G Loss: 0.6692017912864685\n",
      "Epoch 54, batch 70 D Loss: 1.3949317932128906, G Loss: 0.661834180355072\n",
      "Epoch 54, batch 71 D Loss: 1.312345027923584, G Loss: 0.671383261680603\n",
      "Epoch 54, batch 72 D Loss: 1.327816128730774, G Loss: 0.670535147190094\n",
      "Epoch 54, batch 73 D Loss: 1.3448796272277832, G Loss: 0.6710943579673767\n",
      "Epoch 54, batch 74 D Loss: 1.3135981559753418, G Loss: 0.7147914171218872\n",
      "Epoch 54, batch 75 D Loss: 1.3812856674194336, G Loss: 0.6842509508132935\n",
      "Epoch 54, batch 76 D Loss: 1.343034029006958, G Loss: 0.6609900593757629\n",
      "Epoch 54, batch 77 D Loss: 1.331691861152649, G Loss: 0.690884530544281\n",
      "Epoch 54, batch 78 D Loss: 1.3682537078857422, G Loss: 0.6261999607086182\n",
      "Epoch 54, batch 79 D Loss: 1.3369364738464355, G Loss: 0.6883934736251831\n",
      "Epoch 54, batch 80 D Loss: 1.340484619140625, G Loss: 0.6652695536613464\n",
      "Epoch 54, batch 81 D Loss: 1.3439762592315674, G Loss: 0.680769145488739\n",
      "Epoch 54, batch 82 D Loss: 1.411759614944458, G Loss: 0.6422832608222961\n",
      "Epoch 54, batch 83 D Loss: 1.3659369945526123, G Loss: 0.6569305658340454\n",
      "Epoch 54, batch 84 D Loss: 1.3961095809936523, G Loss: 0.6472548842430115\n",
      "Epoch 54, batch 85 D Loss: 1.388222336769104, G Loss: 0.6541696190834045\n",
      "Epoch 54, batch 86 D Loss: 1.4122247695922852, G Loss: 0.6478127837181091\n",
      "Epoch 54, batch 87 D Loss: 1.3638255596160889, G Loss: 0.6853815317153931\n",
      "Epoch 54, batch 88 D Loss: 1.3824820518493652, G Loss: 0.638357400894165\n",
      "Epoch 54, batch 89 D Loss: 1.3844945430755615, G Loss: 0.6408849954605103\n",
      "Epoch 54, batch 90 D Loss: 1.4252235889434814, G Loss: 0.681206226348877\n",
      "Epoch 54, batch 91 D Loss: 1.3990368843078613, G Loss: 0.6795603036880493\n",
      "Epoch 54, batch 92 D Loss: 1.4546942710876465, G Loss: 0.6485509276390076\n",
      "Epoch 54, batch 93 D Loss: 1.4235732555389404, G Loss: 0.6628735065460205\n",
      "Epoch 54, batch 94 D Loss: 1.4789040088653564, G Loss: 0.6299592852592468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, batch 95 D Loss: 1.3925209045410156, G Loss: 0.6773850321769714\n",
      "Epoch 54, batch 96 D Loss: 1.5154834985733032, G Loss: 0.6455752849578857\n",
      "Epoch 54, batch 97 D Loss: 1.4359073638916016, G Loss: 0.6881365180015564\n",
      "Epoch 54, batch 98 D Loss: 1.5364333391189575, G Loss: 0.6010869145393372\n",
      "Epoch 54, batch 99 D Loss: 1.472313642501831, G Loss: 0.6574054956436157\n",
      "Epoch 54, batch 100 D Loss: 1.3851823806762695, G Loss: 0.6742427349090576\n",
      "Epoch 54, batch 101 D Loss: 1.4742019176483154, G Loss: 0.6401171684265137\n",
      "Epoch 54, batch 102 D Loss: 1.5504553318023682, G Loss: 0.6522617340087891\n",
      "Epoch 54, batch 103 D Loss: 1.6067171096801758, G Loss: 0.5829659700393677\n",
      "Epoch 54, batch 104 D Loss: 1.4701464176177979, G Loss: 0.6657803058624268\n",
      "Epoch 54, batch 105 D Loss: 1.5449602603912354, G Loss: 0.6339254975318909\n",
      "Epoch 54, batch 106 D Loss: 1.5190517902374268, G Loss: 0.6340091824531555\n",
      "Epoch 54, batch 107 D Loss: 1.4962663650512695, G Loss: 0.6669619679450989\n",
      "Epoch 54, batch 108 D Loss: 1.4786632061004639, G Loss: 0.7033873796463013\n",
      "Epoch 54, batch 109 D Loss: 1.463688850402832, G Loss: 0.7067028880119324\n",
      "Epoch 54, batch 110 D Loss: 1.4846112728118896, G Loss: 0.6710768342018127\n",
      "Epoch 54, batch 111 D Loss: 1.5734241008758545, G Loss: 0.6438170671463013\n",
      "Epoch 54, batch 112 D Loss: 1.6081955432891846, G Loss: 0.6374682784080505\n",
      "Epoch 54, batch 113 D Loss: 1.555975079536438, G Loss: 0.6438437700271606\n",
      "Epoch 54, batch 114 D Loss: 1.5275758504867554, G Loss: 0.6593670845031738\n",
      "Epoch 54, batch 115 D Loss: 1.495617151260376, G Loss: 0.6984519958496094\n",
      "Epoch 54, batch 116 D Loss: 1.5018818378448486, G Loss: 0.6933330297470093\n",
      "Epoch 54, batch 117 D Loss: 1.4905650615692139, G Loss: 0.698375940322876\n",
      "Epoch 54, batch 118 D Loss: 1.5159623622894287, G Loss: 0.6689722537994385\n",
      "Epoch 54, batch 119 D Loss: 1.5753026008605957, G Loss: 0.6448336243629456\n",
      "Epoch 54, batch 120 D Loss: 1.5944511890411377, G Loss: 0.6558931469917297\n",
      "Epoch 54, batch 121 D Loss: 1.4993016719818115, G Loss: 0.7046260237693787\n",
      "Epoch 54, batch 122 D Loss: 1.4806370735168457, G Loss: 0.7430344223976135\n",
      "Epoch 54, batch 123 D Loss: 1.4327614307403564, G Loss: 0.7513657212257385\n",
      "Epoch 54, batch 124 D Loss: 1.5267210006713867, G Loss: 0.7356509566307068\n",
      "Epoch 54, batch 125 D Loss: 1.4920411109924316, G Loss: 0.7269698977470398\n",
      "Epoch 54, batch 126 D Loss: 1.4217920303344727, G Loss: 0.7902314066886902\n",
      "Epoch 54, batch 127 D Loss: 1.4698781967163086, G Loss: 0.779416561126709\n",
      "Epoch 54, batch 128 D Loss: 1.4686781167984009, G Loss: 0.7486627697944641\n",
      "Epoch 54, batch 129 D Loss: 1.4208556413650513, G Loss: 0.8303459882736206\n",
      "Epoch 54, batch 130 D Loss: 1.4683929681777954, G Loss: 0.7926731109619141\n",
      "Epoch 54, batch 131 D Loss: 1.429241418838501, G Loss: 0.8214680552482605\n",
      "Epoch 54, batch 132 D Loss: 1.4149376153945923, G Loss: 0.8701236844062805\n",
      "Epoch 54, batch 133 D Loss: 1.3639981746673584, G Loss: 0.9417819976806641\n",
      "Epoch 54, batch 134 D Loss: 1.3838622570037842, G Loss: 0.8829881548881531\n",
      "Epoch 54, batch 135 D Loss: 1.3596341609954834, G Loss: 0.946946382522583\n",
      "Epoch 54, batch 136 D Loss: 1.3751866817474365, G Loss: 0.9953269362449646\n",
      "Epoch 54, batch 137 D Loss: 1.3468931913375854, G Loss: 0.9557849764823914\n",
      "Epoch 54, batch 138 D Loss: 1.3516359329223633, G Loss: 0.9788532853126526\n",
      "Epoch 54, batch 139 D Loss: 1.3868224620819092, G Loss: 0.9577851891517639\n",
      "Epoch 54, batch 140 D Loss: 1.3776289224624634, G Loss: 0.9765430688858032\n",
      "Epoch 54, batch 141 D Loss: 1.3114173412322998, G Loss: 1.078199863433838\n",
      "Epoch 54, batch 142 D Loss: 1.3235149383544922, G Loss: 1.0443952083587646\n",
      "Epoch 54, batch 143 D Loss: 1.3542311191558838, G Loss: 1.0482863187789917\n",
      "Epoch 54, batch 144 D Loss: 1.3356776237487793, G Loss: 0.9466583728790283\n",
      "Epoch 54, batch 145 D Loss: 1.313159465789795, G Loss: 1.048577070236206\n",
      "Epoch 54, batch 146 D Loss: 1.3443384170532227, G Loss: 0.9566648602485657\n",
      "Epoch 54, batch 147 D Loss: 1.3578062057495117, G Loss: 1.160263180732727\n",
      "Epoch 54, batch 148 D Loss: 1.302912950515747, G Loss: 1.1053059101104736\n",
      "Epoch 54, batch 149 D Loss: 1.362180471420288, G Loss: 0.9447937607765198\n",
      "Epoch 54, batch 150 D Loss: 1.35763680934906, G Loss: 0.9597129225730896\n",
      "Epoch 54, batch 151 D Loss: 1.337015986442566, G Loss: 1.0347172021865845\n",
      "Epoch 54, batch 152 D Loss: 1.3544445037841797, G Loss: 0.9545798301696777\n",
      "Epoch 54, batch 153 D Loss: 1.348130464553833, G Loss: 0.9596781134605408\n",
      "Epoch 54, batch 154 D Loss: 1.3558197021484375, G Loss: 0.9883499145507812\n",
      "Epoch 54, batch 155 D Loss: 1.3566690683364868, G Loss: 0.8820371031761169\n",
      "Epoch 54, batch 156 D Loss: 1.4040462970733643, G Loss: 0.8548421263694763\n",
      "Epoch 54, batch 157 D Loss: 1.5006436109542847, G Loss: 0.8333528637886047\n",
      "Epoch 54, batch 158 D Loss: 1.3957772254943848, G Loss: 0.8405542969703674\n",
      "Epoch 54, batch 159 D Loss: 1.4497911930084229, G Loss: 0.8401045203208923\n",
      "Epoch 54, batch 160 D Loss: 1.5241103172302246, G Loss: 0.8194061517715454\n",
      "Epoch 54, batch 161 D Loss: 1.4260179996490479, G Loss: 0.8090255856513977\n",
      "Epoch 54, batch 162 D Loss: 1.4128382205963135, G Loss: 0.8169418573379517\n",
      "Epoch 54, batch 163 D Loss: 1.414806842803955, G Loss: 0.785418689250946\n",
      "Epoch 54, batch 164 D Loss: 1.4172455072402954, G Loss: 0.7669735550880432\n",
      "Epoch 54, batch 165 D Loss: 1.4315478801727295, G Loss: 0.778912365436554\n",
      "Epoch 54, batch 166 D Loss: 1.4399683475494385, G Loss: 0.7786703705787659\n",
      "Epoch 54, batch 167 D Loss: 1.5066981315612793, G Loss: 0.7615429759025574\n",
      "Epoch 54, batch 168 D Loss: 1.459411859512329, G Loss: 0.7710566520690918\n",
      "Epoch 54, batch 169 D Loss: 1.4215354919433594, G Loss: 0.768278181552887\n",
      "Epoch 54, batch 170 D Loss: 1.4182562828063965, G Loss: 0.7653031945228577\n",
      "Epoch 54, batch 171 D Loss: 1.4411356449127197, G Loss: 0.7563839554786682\n",
      "Epoch 54, batch 172 D Loss: 1.4462388753890991, G Loss: 0.7497934699058533\n",
      "Epoch 54, batch 173 D Loss: 1.4380671977996826, G Loss: 0.7417086958885193\n",
      "Epoch 54, batch 174 D Loss: 1.4177623987197876, G Loss: 0.7458453178405762\n",
      "Epoch 54, batch 175 D Loss: 1.4396522045135498, G Loss: 0.7431252002716064\n",
      "Epoch 54, batch 176 D Loss: 1.4496455192565918, G Loss: 0.738433301448822\n",
      "Epoch 54, batch 177 D Loss: 1.4121356010437012, G Loss: 0.7387168407440186\n",
      "Epoch 54, batch 178 D Loss: 1.390105962753296, G Loss: 0.7374587059020996\n",
      "Epoch 54, batch 179 D Loss: 1.4446654319763184, G Loss: 0.7340161204338074\n",
      "Epoch 54, batch 180 D Loss: 1.4180364608764648, G Loss: 0.7327350378036499\n",
      "Epoch 54, batch 181 D Loss: 1.3956518173217773, G Loss: 0.7317689657211304\n",
      "Epoch 54, batch 182 D Loss: 1.4065039157867432, G Loss: 0.72550368309021\n",
      "Epoch 54, batch 183 D Loss: 1.3939096927642822, G Loss: 0.7310618758201599\n",
      "Epoch 54, batch 184 D Loss: 1.4317452907562256, G Loss: 0.7191066145896912\n",
      "Epoch 54, batch 185 D Loss: 1.4331893920898438, G Loss: 0.7262246608734131\n",
      "Epoch 54, batch 186 D Loss: 1.3948073387145996, G Loss: 0.7256442904472351\n",
      "Epoch 54, batch 187 D Loss: 1.3828835487365723, G Loss: 0.7222924828529358\n",
      "Epoch 54, batch 188 D Loss: 1.3970847129821777, G Loss: 0.7191025018692017\n",
      "Epoch 54, batch 189 D Loss: 1.4314175844192505, G Loss: 0.7247524857521057\n",
      "Epoch 54, batch 190 D Loss: 1.3956491947174072, G Loss: 0.7205499410629272\n",
      "Epoch 54, batch 191 D Loss: 1.4262831211090088, G Loss: 0.715911865234375\n",
      "Epoch 54, batch 192 D Loss: 1.418865442276001, G Loss: 0.714883029460907\n",
      "Epoch 54, batch 193 D Loss: 1.382434606552124, G Loss: 0.7080168128013611\n",
      "Epoch 54, batch 194 D Loss: 1.3934292793273926, G Loss: 0.7084127068519592\n",
      "Epoch 54, batch 195 D Loss: 1.3798593282699585, G Loss: 0.7102301120758057\n",
      "Epoch 54, batch 196 D Loss: 1.3876516819000244, G Loss: 0.7066854238510132\n",
      "Epoch 54, batch 197 D Loss: 1.3848315477371216, G Loss: 0.7087146043777466\n",
      "Epoch 54, batch 198 D Loss: 1.3778131008148193, G Loss: 0.7093157172203064\n",
      "Epoch 54, batch 199 D Loss: 1.3792469501495361, G Loss: 0.7072922587394714\n",
      "Epoch 54, batch 200 D Loss: 1.3829165697097778, G Loss: 0.7087792754173279\n",
      "Epoch 55, batch 1 D Loss: 1.3771231174468994, G Loss: 0.7063230872154236\n",
      "Epoch 55, batch 2 D Loss: 1.3823893070220947, G Loss: 0.7052776217460632\n",
      "Epoch 55, batch 3 D Loss: 1.3812103271484375, G Loss: 0.7015329003334045\n",
      "Epoch 55, batch 4 D Loss: 1.3646641969680786, G Loss: 0.7104532718658447\n",
      "Epoch 55, batch 5 D Loss: 1.3768408298492432, G Loss: 0.7006683945655823\n",
      "Epoch 55, batch 6 D Loss: 1.3821682929992676, G Loss: 0.699259877204895\n",
      "Epoch 55, batch 7 D Loss: 1.3799259662628174, G Loss: 0.7001453638076782\n",
      "Epoch 55, batch 8 D Loss: 1.3743417263031006, G Loss: 0.7025113105773926\n",
      "Epoch 55, batch 9 D Loss: 1.3859455585479736, G Loss: 0.690807580947876\n",
      "Epoch 55, batch 10 D Loss: 1.3801791667938232, G Loss: 0.6981669664382935\n",
      "Epoch 55, batch 11 D Loss: 1.3813252449035645, G Loss: 0.6944615840911865\n",
      "Epoch 55, batch 12 D Loss: 1.3747217655181885, G Loss: 0.6955050826072693\n",
      "Epoch 55, batch 13 D Loss: 1.3822999000549316, G Loss: 0.6940976977348328\n",
      "Epoch 55, batch 14 D Loss: 1.3813060522079468, G Loss: 0.6914376616477966\n",
      "Epoch 55, batch 15 D Loss: 1.3757390975952148, G Loss: 0.6973721981048584\n",
      "Epoch 55, batch 16 D Loss: 1.3701921701431274, G Loss: 0.6992723941802979\n",
      "Epoch 55, batch 17 D Loss: 1.3768972158432007, G Loss: 0.6962166428565979\n",
      "Epoch 55, batch 18 D Loss: 1.3666272163391113, G Loss: 0.6991444230079651\n",
      "Epoch 55, batch 19 D Loss: 1.3748811483383179, G Loss: 0.6940050721168518\n",
      "Epoch 55, batch 20 D Loss: 1.3641717433929443, G Loss: 0.6992328763008118\n",
      "Epoch 55, batch 21 D Loss: 1.3737623691558838, G Loss: 0.690779447555542\n",
      "Epoch 55, batch 22 D Loss: 1.378440022468567, G Loss: 0.6942880153656006\n",
      "Epoch 55, batch 23 D Loss: 1.3568249940872192, G Loss: 0.701953113079071\n",
      "Epoch 55, batch 24 D Loss: 1.3692216873168945, G Loss: 0.6899763345718384\n",
      "Epoch 55, batch 25 D Loss: 1.3694677352905273, G Loss: 0.6941007375717163\n",
      "Epoch 55, batch 26 D Loss: 1.3651700019836426, G Loss: 0.6941356062889099\n",
      "Epoch 55, batch 27 D Loss: 1.3588168621063232, G Loss: 0.688729465007782\n",
      "Epoch 55, batch 28 D Loss: 1.353585958480835, G Loss: 0.6947727799415588\n",
      "Epoch 55, batch 29 D Loss: 1.3664567470550537, G Loss: 0.6956157684326172\n",
      "Epoch 55, batch 30 D Loss: 1.3673315048217773, G Loss: 0.6918047070503235\n",
      "Epoch 55, batch 31 D Loss: 1.3545764684677124, G Loss: 0.6934002041816711\n",
      "Epoch 55, batch 32 D Loss: 1.3633084297180176, G Loss: 0.690359890460968\n",
      "Epoch 55, batch 33 D Loss: 1.3703433275222778, G Loss: 0.6954827308654785\n",
      "Epoch 55, batch 34 D Loss: 1.3613619804382324, G Loss: 0.6865555644035339\n",
      "Epoch 55, batch 35 D Loss: 1.3675594329833984, G Loss: 0.6916817426681519\n",
      "Epoch 55, batch 36 D Loss: 1.3560662269592285, G Loss: 0.6887598633766174\n",
      "Epoch 55, batch 37 D Loss: 1.3558564186096191, G Loss: 0.6990681290626526\n",
      "Epoch 55, batch 38 D Loss: 1.3602752685546875, G Loss: 0.6900270581245422\n",
      "Epoch 55, batch 39 D Loss: 1.3735711574554443, G Loss: 0.6882404088973999\n",
      "Epoch 55, batch 40 D Loss: 1.363018274307251, G Loss: 0.691846489906311\n",
      "Epoch 55, batch 41 D Loss: 1.3654301166534424, G Loss: 0.6946890950202942\n",
      "Epoch 55, batch 42 D Loss: 1.3528053760528564, G Loss: 0.6881905198097229\n",
      "Epoch 55, batch 43 D Loss: 1.360126256942749, G Loss: 0.6875045299530029\n",
      "Epoch 55, batch 44 D Loss: 1.3676302433013916, G Loss: 0.6929795742034912\n",
      "Epoch 55, batch 45 D Loss: 1.358086109161377, G Loss: 0.6897138953208923\n",
      "Epoch 55, batch 46 D Loss: 1.3841817378997803, G Loss: 0.6781796216964722\n",
      "Epoch 55, batch 47 D Loss: 1.3587768077850342, G Loss: 0.6921697854995728\n",
      "Epoch 55, batch 48 D Loss: 1.3511693477630615, G Loss: 0.6892549395561218\n",
      "Epoch 55, batch 49 D Loss: 1.3541641235351562, G Loss: 0.6897130012512207\n",
      "Epoch 55, batch 50 D Loss: 1.3306465148925781, G Loss: 0.6951904296875\n",
      "Epoch 55, batch 51 D Loss: 1.3735191822052002, G Loss: 0.6944694519042969\n",
      "Epoch 55, batch 52 D Loss: 1.3453481197357178, G Loss: 0.6941725015640259\n",
      "Epoch 55, batch 53 D Loss: 1.338460922241211, G Loss: 0.6965972781181335\n",
      "Epoch 55, batch 54 D Loss: 1.358445644378662, G Loss: 0.6846774220466614\n",
      "Epoch 55, batch 55 D Loss: 1.363495945930481, G Loss: 0.6887691020965576\n",
      "Epoch 55, batch 56 D Loss: 1.346171259880066, G Loss: 0.6988691687583923\n",
      "Epoch 55, batch 57 D Loss: 1.3423850536346436, G Loss: 0.696422278881073\n",
      "Epoch 55, batch 58 D Loss: 1.3464969396591187, G Loss: 0.6904173493385315\n",
      "Epoch 55, batch 59 D Loss: 1.358161449432373, G Loss: 0.6885673999786377\n",
      "Epoch 55, batch 60 D Loss: 1.3618102073669434, G Loss: 0.6884149312973022\n",
      "Epoch 55, batch 61 D Loss: 1.3847501277923584, G Loss: 0.6790860891342163\n",
      "Epoch 55, batch 62 D Loss: 1.3637561798095703, G Loss: 0.6835686564445496\n",
      "Epoch 55, batch 63 D Loss: 1.3354002237319946, G Loss: 0.6930376291275024\n",
      "Epoch 55, batch 64 D Loss: 1.3307061195373535, G Loss: 0.7093932628631592\n",
      "Epoch 55, batch 65 D Loss: 1.344270944595337, G Loss: 0.6937234997749329\n",
      "Epoch 55, batch 66 D Loss: 1.3511621952056885, G Loss: 0.6878684759140015\n",
      "Epoch 55, batch 67 D Loss: 1.3508291244506836, G Loss: 0.6898752450942993\n",
      "Epoch 55, batch 68 D Loss: 1.3748241662979126, G Loss: 0.6827685832977295\n",
      "Epoch 55, batch 69 D Loss: 1.3277606964111328, G Loss: 0.6993294358253479\n",
      "Epoch 55, batch 70 D Loss: 1.3411362171173096, G Loss: 0.6799779534339905\n",
      "Epoch 55, batch 71 D Loss: 1.3435014486312866, G Loss: 0.6916329860687256\n",
      "Epoch 55, batch 72 D Loss: 1.3427671194076538, G Loss: 0.6938179135322571\n",
      "Epoch 55, batch 73 D Loss: 1.3556349277496338, G Loss: 0.6926486492156982\n",
      "Epoch 55, batch 74 D Loss: 1.351630449295044, G Loss: 0.6825500726699829\n",
      "Epoch 55, batch 75 D Loss: 1.348584532737732, G Loss: 0.7074890732765198\n",
      "Epoch 55, batch 76 D Loss: 1.3685193061828613, G Loss: 0.7005718350410461\n",
      "Epoch 55, batch 77 D Loss: 1.3455513715744019, G Loss: 0.6977408528327942\n",
      "Epoch 55, batch 78 D Loss: 1.3395752906799316, G Loss: 0.6899805665016174\n",
      "Epoch 55, batch 79 D Loss: 1.3414638042449951, G Loss: 0.6991530656814575\n",
      "Epoch 55, batch 80 D Loss: 1.3771617412567139, G Loss: 0.7017344832420349\n",
      "Epoch 55, batch 81 D Loss: 1.3611927032470703, G Loss: 0.6924394965171814\n",
      "Epoch 55, batch 82 D Loss: 1.3587948083877563, G Loss: 0.7036068439483643\n",
      "Epoch 55, batch 83 D Loss: 1.3509154319763184, G Loss: 0.6977909803390503\n",
      "Epoch 55, batch 84 D Loss: 1.336120843887329, G Loss: 0.7081298232078552\n",
      "Epoch 55, batch 85 D Loss: 1.3596611022949219, G Loss: 0.6899561882019043\n",
      "Epoch 55, batch 86 D Loss: 1.377967119216919, G Loss: 0.682914137840271\n",
      "Epoch 55, batch 87 D Loss: 1.3946974277496338, G Loss: 0.6710250973701477\n",
      "Epoch 55, batch 88 D Loss: 1.343635082244873, G Loss: 0.6976549029350281\n",
      "Epoch 55, batch 89 D Loss: 1.416405439376831, G Loss: 0.671169102191925\n",
      "Epoch 55, batch 90 D Loss: 1.3845512866973877, G Loss: 0.6673586964607239\n",
      "Epoch 55, batch 91 D Loss: 1.3776133060455322, G Loss: 0.6755831837654114\n",
      "Epoch 55, batch 92 D Loss: 1.3741867542266846, G Loss: 0.6919727921485901\n",
      "Epoch 55, batch 93 D Loss: 1.3409714698791504, G Loss: 0.6894070506095886\n",
      "Epoch 55, batch 94 D Loss: 1.3581733703613281, G Loss: 0.6745739579200745\n",
      "Epoch 55, batch 95 D Loss: 1.4034709930419922, G Loss: 0.6700373291969299\n",
      "Epoch 55, batch 96 D Loss: 1.4005011320114136, G Loss: 0.6690042614936829\n",
      "Epoch 55, batch 97 D Loss: 1.4115054607391357, G Loss: 0.6803597211837769\n",
      "Epoch 55, batch 98 D Loss: 1.3716875314712524, G Loss: 0.6768761277198792\n",
      "Epoch 55, batch 99 D Loss: 1.342576026916504, G Loss: 0.6934673190116882\n",
      "Epoch 55, batch 100 D Loss: 1.3788634538650513, G Loss: 0.6806355118751526\n",
      "Epoch 55, batch 101 D Loss: 1.405383586883545, G Loss: 0.6595154404640198\n",
      "Epoch 55, batch 102 D Loss: 1.392326831817627, G Loss: 0.683239758014679\n",
      "Epoch 55, batch 103 D Loss: 1.4142804145812988, G Loss: 0.6640373468399048\n",
      "Epoch 55, batch 104 D Loss: 1.3587851524353027, G Loss: 0.6969329714775085\n",
      "Epoch 55, batch 105 D Loss: 1.3521450757980347, G Loss: 0.697528600692749\n",
      "Epoch 55, batch 106 D Loss: 1.4194953441619873, G Loss: 0.6711411476135254\n",
      "Epoch 55, batch 107 D Loss: 1.4018661975860596, G Loss: 0.6654068827629089\n",
      "Epoch 55, batch 108 D Loss: 1.380768060684204, G Loss: 0.6812125444412231\n",
      "Epoch 55, batch 109 D Loss: 1.4067540168762207, G Loss: 0.6633181571960449\n",
      "Epoch 55, batch 110 D Loss: 1.3765003681182861, G Loss: 0.6843678951263428\n",
      "Epoch 55, batch 111 D Loss: 1.4144257307052612, G Loss: 0.6519578695297241\n",
      "Epoch 55, batch 112 D Loss: 1.3672561645507812, G Loss: 0.6986730694770813\n",
      "Epoch 55, batch 113 D Loss: 1.4010202884674072, G Loss: 0.6862959861755371\n",
      "Epoch 55, batch 114 D Loss: 1.4088683128356934, G Loss: 0.668209969997406\n",
      "Epoch 55, batch 115 D Loss: 1.3841280937194824, G Loss: 0.6897403597831726\n",
      "Epoch 55, batch 116 D Loss: 1.3988680839538574, G Loss: 0.6666776537895203\n",
      "Epoch 55, batch 117 D Loss: 1.3904149532318115, G Loss: 0.6852786540985107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, batch 118 D Loss: 1.3730483055114746, G Loss: 0.6983662247657776\n",
      "Epoch 55, batch 119 D Loss: 1.3741220235824585, G Loss: 0.7014610767364502\n",
      "Epoch 55, batch 120 D Loss: 1.4072915315628052, G Loss: 0.6798632144927979\n",
      "Epoch 55, batch 121 D Loss: 1.4054075479507446, G Loss: 0.6678058505058289\n",
      "Epoch 55, batch 122 D Loss: 1.4007446765899658, G Loss: 0.6776102185249329\n",
      "Epoch 55, batch 123 D Loss: 1.4018054008483887, G Loss: 0.6896969676017761\n",
      "Epoch 55, batch 124 D Loss: 1.4056036472320557, G Loss: 0.686354398727417\n",
      "Epoch 55, batch 125 D Loss: 1.4055633544921875, G Loss: 0.6986657977104187\n",
      "Epoch 55, batch 126 D Loss: 1.4199929237365723, G Loss: 0.6843165755271912\n",
      "Epoch 55, batch 127 D Loss: 1.4167184829711914, G Loss: 0.6891596913337708\n",
      "Epoch 55, batch 128 D Loss: 1.4182305335998535, G Loss: 0.6819954514503479\n",
      "Epoch 55, batch 129 D Loss: 1.386587142944336, G Loss: 0.7006933093070984\n",
      "Epoch 55, batch 130 D Loss: 1.3959060907363892, G Loss: 0.7010914087295532\n",
      "Epoch 55, batch 131 D Loss: 1.4087921380996704, G Loss: 0.7103614211082458\n",
      "Epoch 55, batch 132 D Loss: 1.3909858465194702, G Loss: 0.6971549391746521\n",
      "Epoch 55, batch 133 D Loss: 1.3835207223892212, G Loss: 0.7184513807296753\n",
      "Epoch 55, batch 134 D Loss: 1.3836040496826172, G Loss: 0.7064846754074097\n",
      "Epoch 55, batch 135 D Loss: 1.37385892868042, G Loss: 0.7051910161972046\n",
      "Epoch 55, batch 136 D Loss: 1.3844022750854492, G Loss: 0.7161678075790405\n",
      "Epoch 55, batch 137 D Loss: 1.414146900177002, G Loss: 0.7028080821037292\n",
      "Epoch 55, batch 138 D Loss: 1.4027800559997559, G Loss: 0.7038565278053284\n",
      "Epoch 55, batch 139 D Loss: 1.4148950576782227, G Loss: 0.6876733303070068\n",
      "Epoch 55, batch 140 D Loss: 1.4119665622711182, G Loss: 0.7018476724624634\n",
      "Epoch 55, batch 141 D Loss: 1.4123201370239258, G Loss: 0.7059666514396667\n",
      "Epoch 55, batch 142 D Loss: 1.405726671218872, G Loss: 0.7078616619110107\n",
      "Epoch 55, batch 143 D Loss: 1.4095659255981445, G Loss: 0.6918808817863464\n",
      "Epoch 55, batch 144 D Loss: 1.402214527130127, G Loss: 0.717286229133606\n",
      "Epoch 55, batch 145 D Loss: 1.4092426300048828, G Loss: 0.7057579755783081\n",
      "Epoch 55, batch 146 D Loss: 1.3965466022491455, G Loss: 0.7070768475532532\n",
      "Epoch 55, batch 147 D Loss: 1.4079171419143677, G Loss: 0.7153173089027405\n",
      "Epoch 55, batch 148 D Loss: 1.4027652740478516, G Loss: 0.7156772613525391\n",
      "Epoch 55, batch 149 D Loss: 1.3974616527557373, G Loss: 0.7203658819198608\n",
      "Epoch 55, batch 150 D Loss: 1.3934104442596436, G Loss: 0.7384678721427917\n",
      "Epoch 55, batch 151 D Loss: 1.4092801809310913, G Loss: 0.712208092212677\n",
      "Epoch 55, batch 152 D Loss: 1.4135613441467285, G Loss: 0.7173902988433838\n",
      "Epoch 55, batch 153 D Loss: 1.4129695892333984, G Loss: 0.7241508364677429\n",
      "Epoch 55, batch 154 D Loss: 1.3837239742279053, G Loss: 0.737425684928894\n",
      "Epoch 55, batch 155 D Loss: 1.384359359741211, G Loss: 0.7294065952301025\n",
      "Epoch 55, batch 156 D Loss: 1.4185843467712402, G Loss: 0.7057380676269531\n",
      "Epoch 55, batch 157 D Loss: 1.4076883792877197, G Loss: 0.7141607403755188\n",
      "Epoch 55, batch 158 D Loss: 1.4271694421768188, G Loss: 0.7096462845802307\n",
      "Epoch 55, batch 159 D Loss: 1.404590129852295, G Loss: 0.7290668487548828\n",
      "Epoch 55, batch 160 D Loss: 1.4274394512176514, G Loss: 0.7108469605445862\n",
      "Epoch 55, batch 161 D Loss: 1.3901106119155884, G Loss: 0.7250469923019409\n",
      "Epoch 55, batch 162 D Loss: 1.4050928354263306, G Loss: 0.7217411994934082\n",
      "Epoch 55, batch 163 D Loss: 1.3895174264907837, G Loss: 0.7377633452415466\n",
      "Epoch 55, batch 164 D Loss: 1.3978724479675293, G Loss: 0.7268911004066467\n",
      "Epoch 55, batch 165 D Loss: 1.383313536643982, G Loss: 0.74660325050354\n",
      "Epoch 55, batch 166 D Loss: 1.394904375076294, G Loss: 0.7408337593078613\n",
      "Epoch 55, batch 167 D Loss: 1.3884634971618652, G Loss: 0.7280327677726746\n",
      "Epoch 55, batch 168 D Loss: 1.3762307167053223, G Loss: 0.74884033203125\n",
      "Epoch 55, batch 169 D Loss: 1.3844995498657227, G Loss: 0.7521308064460754\n",
      "Epoch 55, batch 170 D Loss: 1.4246857166290283, G Loss: 0.7095047831535339\n",
      "Epoch 55, batch 171 D Loss: 1.3646057844161987, G Loss: 0.7545658946037292\n",
      "Epoch 55, batch 172 D Loss: 1.3844187259674072, G Loss: 0.7541660070419312\n",
      "Epoch 55, batch 173 D Loss: 1.428640365600586, G Loss: 0.7196937799453735\n",
      "Epoch 55, batch 174 D Loss: 1.3886852264404297, G Loss: 0.7429904341697693\n",
      "Epoch 55, batch 175 D Loss: 1.4030601978302002, G Loss: 0.727653980255127\n",
      "Epoch 55, batch 176 D Loss: 1.3631672859191895, G Loss: 0.7844429612159729\n",
      "Epoch 55, batch 177 D Loss: 1.3877460956573486, G Loss: 0.7524145245552063\n",
      "Epoch 55, batch 178 D Loss: 1.384864330291748, G Loss: 0.732468843460083\n",
      "Epoch 55, batch 179 D Loss: 1.410387396812439, G Loss: 0.7317208051681519\n",
      "Epoch 55, batch 180 D Loss: 1.4042296409606934, G Loss: 0.733377993106842\n",
      "Epoch 55, batch 181 D Loss: 1.3677603006362915, G Loss: 0.7841598987579346\n",
      "Epoch 55, batch 182 D Loss: 1.413622260093689, G Loss: 0.728201150894165\n",
      "Epoch 55, batch 183 D Loss: 1.385871410369873, G Loss: 0.7494359016418457\n",
      "Epoch 55, batch 184 D Loss: 1.3639681339263916, G Loss: 0.7694193124771118\n",
      "Epoch 55, batch 185 D Loss: 1.4029825925827026, G Loss: 0.7254534959793091\n",
      "Epoch 55, batch 186 D Loss: 1.3976101875305176, G Loss: 0.735320508480072\n",
      "Epoch 55, batch 187 D Loss: 1.3872239589691162, G Loss: 0.73224937915802\n",
      "Epoch 55, batch 188 D Loss: 1.3759260177612305, G Loss: 0.7465025186538696\n",
      "Epoch 55, batch 189 D Loss: 1.394036054611206, G Loss: 0.7425444722175598\n",
      "Epoch 55, batch 190 D Loss: 1.418553113937378, G Loss: 0.7329765558242798\n",
      "Epoch 55, batch 191 D Loss: 1.3863110542297363, G Loss: 0.7590287923812866\n",
      "Epoch 55, batch 192 D Loss: 1.383147120475769, G Loss: 0.7595639824867249\n",
      "Epoch 55, batch 193 D Loss: 1.3838629722595215, G Loss: 0.7633699178695679\n",
      "Epoch 55, batch 194 D Loss: 1.396960973739624, G Loss: 0.7371112704277039\n",
      "Epoch 55, batch 195 D Loss: 1.3784997463226318, G Loss: 0.7621824741363525\n",
      "Epoch 55, batch 196 D Loss: 1.3776099681854248, G Loss: 0.7682203054428101\n",
      "Epoch 55, batch 197 D Loss: 1.3996751308441162, G Loss: 0.7495635747909546\n",
      "Epoch 55, batch 198 D Loss: 1.3854107856750488, G Loss: 0.7558549642562866\n",
      "Epoch 55, batch 199 D Loss: 1.374211311340332, G Loss: 0.7651441097259521\n",
      "Epoch 55, batch 200 D Loss: 1.380798101425171, G Loss: 0.7668171525001526\n",
      "Epoch 56, batch 1 D Loss: 1.3550727367401123, G Loss: 0.7785933017730713\n",
      "Epoch 56, batch 2 D Loss: 1.3519010543823242, G Loss: 0.7754315137863159\n",
      "Epoch 56, batch 3 D Loss: 1.3559249639511108, G Loss: 0.7783383727073669\n",
      "Epoch 56, batch 4 D Loss: 1.38229238986969, G Loss: 0.7663056254386902\n",
      "Epoch 56, batch 5 D Loss: 1.386181354522705, G Loss: 0.7716245055198669\n",
      "Epoch 56, batch 6 D Loss: 1.3718265295028687, G Loss: 0.759328305721283\n",
      "Epoch 56, batch 7 D Loss: 1.3812611103057861, G Loss: 0.7585040926933289\n",
      "Epoch 56, batch 8 D Loss: 1.369081974029541, G Loss: 0.769960343837738\n",
      "Epoch 56, batch 9 D Loss: 1.3665993213653564, G Loss: 0.7834367156028748\n",
      "Epoch 56, batch 10 D Loss: 1.3740525245666504, G Loss: 0.768354058265686\n",
      "Epoch 56, batch 11 D Loss: 1.3470778465270996, G Loss: 0.8008809089660645\n",
      "Epoch 56, batch 12 D Loss: 1.3952081203460693, G Loss: 0.749650239944458\n",
      "Epoch 56, batch 13 D Loss: 1.3695167303085327, G Loss: 0.790869951248169\n",
      "Epoch 56, batch 14 D Loss: 1.3964061737060547, G Loss: 0.7672209739685059\n",
      "Epoch 56, batch 15 D Loss: 1.4091272354125977, G Loss: 0.8031942248344421\n",
      "Epoch 56, batch 16 D Loss: 1.389777421951294, G Loss: 0.7794764637947083\n",
      "Epoch 56, batch 17 D Loss: 1.3992974758148193, G Loss: 0.7550008893013\n",
      "Epoch 56, batch 18 D Loss: 1.3766553401947021, G Loss: 0.7979089617729187\n",
      "Epoch 56, batch 19 D Loss: 1.360939860343933, G Loss: 0.7817386388778687\n",
      "Epoch 56, batch 20 D Loss: 1.4060254096984863, G Loss: 0.7655261158943176\n",
      "Epoch 56, batch 21 D Loss: 1.374143362045288, G Loss: 0.7617266178131104\n",
      "Epoch 56, batch 22 D Loss: 1.390059232711792, G Loss: 0.7668088674545288\n",
      "Epoch 56, batch 23 D Loss: 1.4248297214508057, G Loss: 0.7602360248565674\n",
      "Epoch 56, batch 24 D Loss: 1.3858888149261475, G Loss: 0.7541999816894531\n",
      "Epoch 56, batch 25 D Loss: 1.360044240951538, G Loss: 0.7803859114646912\n",
      "Epoch 56, batch 26 D Loss: 1.3453541994094849, G Loss: 0.788534939289093\n",
      "Epoch 56, batch 27 D Loss: 1.380368947982788, G Loss: 0.7679203152656555\n",
      "Epoch 56, batch 28 D Loss: 1.3714864253997803, G Loss: 0.7661611437797546\n",
      "Epoch 56, batch 29 D Loss: 1.3738325834274292, G Loss: 0.7909641861915588\n",
      "Epoch 56, batch 30 D Loss: 1.4006787538528442, G Loss: 0.732475221157074\n",
      "Epoch 56, batch 31 D Loss: 1.41023588180542, G Loss: 0.763522207736969\n",
      "Epoch 56, batch 32 D Loss: 1.3975497484207153, G Loss: 0.7748441100120544\n",
      "Epoch 56, batch 33 D Loss: 1.380354881286621, G Loss: 0.7574626207351685\n",
      "Epoch 56, batch 34 D Loss: 1.4005823135375977, G Loss: 0.7800554633140564\n",
      "Epoch 56, batch 35 D Loss: 1.4051333665847778, G Loss: 0.7528536915779114\n",
      "Epoch 56, batch 36 D Loss: 1.4127705097198486, G Loss: 0.7313745617866516\n",
      "Epoch 56, batch 37 D Loss: 1.3988749980926514, G Loss: 0.7292054891586304\n",
      "Epoch 56, batch 38 D Loss: 1.3861478567123413, G Loss: 0.7774415612220764\n",
      "Epoch 56, batch 39 D Loss: 1.4025356769561768, G Loss: 0.7448263764381409\n",
      "Epoch 56, batch 40 D Loss: 1.3975253105163574, G Loss: 0.7288011312484741\n",
      "Epoch 56, batch 41 D Loss: 1.4083173274993896, G Loss: 0.7200741767883301\n",
      "Epoch 56, batch 42 D Loss: 1.3878618478775024, G Loss: 0.7343626618385315\n",
      "Epoch 56, batch 43 D Loss: 1.3968100547790527, G Loss: 0.722499668598175\n",
      "Epoch 56, batch 44 D Loss: 1.3804221153259277, G Loss: 0.7395420074462891\n",
      "Epoch 56, batch 45 D Loss: 1.384628176689148, G Loss: 0.7205989360809326\n",
      "Epoch 56, batch 46 D Loss: 1.4133974313735962, G Loss: 0.7204463481903076\n",
      "Epoch 56, batch 47 D Loss: 1.4100035429000854, G Loss: 0.7321419715881348\n",
      "Epoch 56, batch 48 D Loss: 1.4059805870056152, G Loss: 0.7175318002700806\n",
      "Epoch 56, batch 49 D Loss: 1.3818159103393555, G Loss: 0.7240390777587891\n",
      "Epoch 56, batch 50 D Loss: 1.3733198642730713, G Loss: 0.7454973459243774\n",
      "Epoch 56, batch 51 D Loss: 1.3896387815475464, G Loss: 0.7249407768249512\n",
      "Epoch 56, batch 52 D Loss: 1.4026079177856445, G Loss: 0.7267720103263855\n",
      "Epoch 56, batch 53 D Loss: 1.4003406763076782, G Loss: 0.7281665802001953\n",
      "Epoch 56, batch 54 D Loss: 1.3745737075805664, G Loss: 0.7373507618904114\n",
      "Epoch 56, batch 55 D Loss: 1.4000117778778076, G Loss: 0.7237821817398071\n",
      "Epoch 56, batch 56 D Loss: 1.3945517539978027, G Loss: 0.7240134477615356\n",
      "Epoch 56, batch 57 D Loss: 1.3842716217041016, G Loss: 0.7230121493339539\n",
      "Epoch 56, batch 58 D Loss: 1.3888750076293945, G Loss: 0.7231568694114685\n",
      "Epoch 56, batch 59 D Loss: 1.4260361194610596, G Loss: 0.711780846118927\n",
      "Epoch 56, batch 60 D Loss: 1.3929173946380615, G Loss: 0.717022716999054\n",
      "Epoch 56, batch 61 D Loss: 1.3872045278549194, G Loss: 0.7313505411148071\n",
      "Epoch 56, batch 62 D Loss: 1.3775482177734375, G Loss: 0.7275159955024719\n",
      "Epoch 56, batch 63 D Loss: 1.3792402744293213, G Loss: 0.7333777546882629\n",
      "Epoch 56, batch 64 D Loss: 1.4015214443206787, G Loss: 0.7276073694229126\n",
      "Epoch 56, batch 65 D Loss: 1.3736391067504883, G Loss: 0.7586277723312378\n",
      "Epoch 56, batch 66 D Loss: 1.3773159980773926, G Loss: 0.7212434411048889\n",
      "Epoch 56, batch 67 D Loss: 1.3929365873336792, G Loss: 0.7277887463569641\n",
      "Epoch 56, batch 68 D Loss: 1.3804117441177368, G Loss: 0.739716649055481\n",
      "Epoch 56, batch 69 D Loss: 1.3861958980560303, G Loss: 0.7297647595405579\n",
      "Epoch 56, batch 70 D Loss: 1.3884916305541992, G Loss: 0.72438645362854\n",
      "Epoch 56, batch 71 D Loss: 1.3642791509628296, G Loss: 0.741016149520874\n",
      "Epoch 56, batch 72 D Loss: 1.3779029846191406, G Loss: 0.7317802309989929\n",
      "Epoch 56, batch 73 D Loss: 1.3652170896530151, G Loss: 0.7407631874084473\n",
      "Epoch 56, batch 74 D Loss: 1.3763363361358643, G Loss: 0.7286083102226257\n",
      "Epoch 56, batch 75 D Loss: 1.371893286705017, G Loss: 0.743366003036499\n",
      "Epoch 56, batch 76 D Loss: 1.3907183408737183, G Loss: 0.7328131198883057\n",
      "Epoch 56, batch 77 D Loss: 1.3948698043823242, G Loss: 0.7421653270721436\n",
      "Epoch 56, batch 78 D Loss: 1.377086877822876, G Loss: 0.7405321002006531\n",
      "Epoch 56, batch 79 D Loss: 1.3848931789398193, G Loss: 0.7485058307647705\n",
      "Epoch 56, batch 80 D Loss: 1.393240213394165, G Loss: 0.7296383380889893\n",
      "Epoch 56, batch 81 D Loss: 1.3768177032470703, G Loss: 0.7374009490013123\n",
      "Epoch 56, batch 82 D Loss: 1.36635160446167, G Loss: 0.7399871945381165\n",
      "Epoch 56, batch 83 D Loss: 1.3694384098052979, G Loss: 0.7503399848937988\n",
      "Epoch 56, batch 84 D Loss: 1.3888537883758545, G Loss: 0.7239505052566528\n",
      "Epoch 56, batch 85 D Loss: 1.3810006380081177, G Loss: 0.7238763570785522\n",
      "Epoch 56, batch 86 D Loss: 1.3886919021606445, G Loss: 0.7195761203765869\n",
      "Epoch 56, batch 87 D Loss: 1.3930513858795166, G Loss: 0.7269180417060852\n",
      "Epoch 56, batch 88 D Loss: 1.3817262649536133, G Loss: 0.7384619116783142\n",
      "Epoch 56, batch 89 D Loss: 1.3805348873138428, G Loss: 0.7074902057647705\n",
      "Epoch 56, batch 90 D Loss: 1.3895773887634277, G Loss: 0.7377191185951233\n",
      "Epoch 56, batch 91 D Loss: 1.3652591705322266, G Loss: 0.7277957201004028\n",
      "Epoch 56, batch 92 D Loss: 1.3779621124267578, G Loss: 0.7211300134658813\n",
      "Epoch 56, batch 93 D Loss: 1.3657722473144531, G Loss: 0.731550395488739\n",
      "Epoch 56, batch 94 D Loss: 1.3967766761779785, G Loss: 0.7099904417991638\n",
      "Epoch 56, batch 95 D Loss: 1.4093552827835083, G Loss: 0.7158201336860657\n",
      "Epoch 56, batch 96 D Loss: 1.3944979906082153, G Loss: 0.7085624933242798\n",
      "Epoch 56, batch 97 D Loss: 1.3984711170196533, G Loss: 0.6962751746177673\n",
      "Epoch 56, batch 98 D Loss: 1.3819547891616821, G Loss: 0.7133965492248535\n",
      "Epoch 56, batch 99 D Loss: 1.3931667804718018, G Loss: 0.7069587111473083\n",
      "Epoch 56, batch 100 D Loss: 1.405361294746399, G Loss: 0.7044457793235779\n",
      "Epoch 56, batch 101 D Loss: 1.3896660804748535, G Loss: 0.6897130012512207\n",
      "Epoch 56, batch 102 D Loss: 1.3997817039489746, G Loss: 0.6939639449119568\n",
      "Epoch 56, batch 103 D Loss: 1.3932796716690063, G Loss: 0.6858546733856201\n",
      "Epoch 56, batch 104 D Loss: 1.3880831003189087, G Loss: 0.687936007976532\n",
      "Epoch 56, batch 105 D Loss: 1.4183149337768555, G Loss: 0.6885659694671631\n",
      "Epoch 56, batch 106 D Loss: 1.396264910697937, G Loss: 0.6806337237358093\n",
      "Epoch 56, batch 107 D Loss: 1.3730454444885254, G Loss: 0.6826593279838562\n",
      "Epoch 56, batch 108 D Loss: 1.386380672454834, G Loss: 0.6786321401596069\n",
      "Epoch 56, batch 109 D Loss: 1.4054429531097412, G Loss: 0.6715896725654602\n",
      "Epoch 56, batch 110 D Loss: 1.4053173065185547, G Loss: 0.6734967231750488\n",
      "Epoch 56, batch 111 D Loss: 1.3946661949157715, G Loss: 0.6718248128890991\n",
      "Epoch 56, batch 112 D Loss: 1.4330854415893555, G Loss: 0.6620824337005615\n",
      "Epoch 56, batch 113 D Loss: 1.3991491794586182, G Loss: 0.6714836955070496\n",
      "Epoch 56, batch 114 D Loss: 1.410555124282837, G Loss: 0.6718736886978149\n",
      "Epoch 56, batch 115 D Loss: 1.4008278846740723, G Loss: 0.666932225227356\n",
      "Epoch 56, batch 116 D Loss: 1.40873122215271, G Loss: 0.6651222705841064\n",
      "Epoch 56, batch 117 D Loss: 1.3999258279800415, G Loss: 0.6583592295646667\n",
      "Epoch 56, batch 118 D Loss: 1.395349144935608, G Loss: 0.6664415597915649\n",
      "Epoch 56, batch 119 D Loss: 1.4017590284347534, G Loss: 0.6581360697746277\n",
      "Epoch 56, batch 120 D Loss: 1.4007039070129395, G Loss: 0.6637063026428223\n",
      "Epoch 56, batch 121 D Loss: 1.400094747543335, G Loss: 0.6580814123153687\n",
      "Epoch 56, batch 122 D Loss: 1.403545618057251, G Loss: 0.6613060235977173\n",
      "Epoch 56, batch 123 D Loss: 1.401499629020691, G Loss: 0.6524674892425537\n",
      "Epoch 56, batch 124 D Loss: 1.407529354095459, G Loss: 0.6508260369300842\n",
      "Epoch 56, batch 125 D Loss: 1.4068689346313477, G Loss: 0.6530153155326843\n",
      "Epoch 56, batch 126 D Loss: 1.3886771202087402, G Loss: 0.6494407653808594\n",
      "Epoch 56, batch 127 D Loss: 1.3910706043243408, G Loss: 0.6560778021812439\n",
      "Epoch 56, batch 128 D Loss: 1.3901762962341309, G Loss: 0.6545437574386597\n",
      "Epoch 56, batch 129 D Loss: 1.3877484798431396, G Loss: 0.6547796726226807\n",
      "Epoch 56, batch 130 D Loss: 1.39201819896698, G Loss: 0.6500644683837891\n",
      "Epoch 56, batch 131 D Loss: 1.384134292602539, G Loss: 0.6495950222015381\n",
      "Epoch 56, batch 132 D Loss: 1.394212245941162, G Loss: 0.6505810022354126\n",
      "Epoch 56, batch 133 D Loss: 1.385049819946289, G Loss: 0.6496617197990417\n",
      "Epoch 56, batch 134 D Loss: 1.3863558769226074, G Loss: 0.6485913991928101\n",
      "Epoch 56, batch 135 D Loss: 1.3744747638702393, G Loss: 0.6542210578918457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, batch 136 D Loss: 1.3842027187347412, G Loss: 0.6413871049880981\n",
      "Epoch 56, batch 137 D Loss: 1.3901149034500122, G Loss: 0.6517611145973206\n",
      "Epoch 56, batch 138 D Loss: 1.3810768127441406, G Loss: 0.6445252299308777\n",
      "Epoch 56, batch 139 D Loss: 1.379936695098877, G Loss: 0.6458162069320679\n",
      "Epoch 56, batch 140 D Loss: 1.385277509689331, G Loss: 0.6470257639884949\n",
      "Epoch 56, batch 141 D Loss: 1.3833612203598022, G Loss: 0.6499361991882324\n",
      "Epoch 56, batch 142 D Loss: 1.384985327720642, G Loss: 0.650160551071167\n",
      "Epoch 56, batch 143 D Loss: 1.3748998641967773, G Loss: 0.6501233577728271\n",
      "Epoch 56, batch 144 D Loss: 1.3754191398620605, G Loss: 0.6499085426330566\n",
      "Epoch 56, batch 145 D Loss: 1.3754780292510986, G Loss: 0.6504302024841309\n",
      "Epoch 56, batch 146 D Loss: 1.3660508394241333, G Loss: 0.6463977694511414\n",
      "Epoch 56, batch 147 D Loss: 1.372403621673584, G Loss: 0.6518976092338562\n",
      "Epoch 56, batch 148 D Loss: 1.3778820037841797, G Loss: 0.6467065215110779\n",
      "Epoch 56, batch 149 D Loss: 1.3805477619171143, G Loss: 0.6463175415992737\n",
      "Epoch 56, batch 150 D Loss: 1.3627337217330933, G Loss: 0.6526280045509338\n",
      "Epoch 56, batch 151 D Loss: 1.369023084640503, G Loss: 0.6477442979812622\n",
      "Epoch 56, batch 152 D Loss: 1.3675930500030518, G Loss: 0.6509914398193359\n",
      "Epoch 56, batch 153 D Loss: 1.377578616142273, G Loss: 0.6448023915290833\n",
      "Epoch 56, batch 154 D Loss: 1.3672349452972412, G Loss: 0.6535696983337402\n",
      "Epoch 56, batch 155 D Loss: 1.3681385517120361, G Loss: 0.6514461040496826\n",
      "Epoch 56, batch 156 D Loss: 1.350793719291687, G Loss: 0.6546055674552917\n",
      "Epoch 56, batch 157 D Loss: 1.3487634658813477, G Loss: 0.6565752625465393\n",
      "Epoch 56, batch 158 D Loss: 1.3773223161697388, G Loss: 0.6545597314834595\n",
      "Epoch 56, batch 159 D Loss: 1.3658318519592285, G Loss: 0.6519047021865845\n",
      "Epoch 56, batch 160 D Loss: 1.3386399745941162, G Loss: 0.6514188647270203\n",
      "Epoch 56, batch 161 D Loss: 1.3390247821807861, G Loss: 0.6540440917015076\n",
      "Epoch 56, batch 162 D Loss: 1.368930697441101, G Loss: 0.6475366353988647\n",
      "Epoch 56, batch 163 D Loss: 1.3614022731781006, G Loss: 0.6494210958480835\n",
      "Epoch 56, batch 164 D Loss: 1.3525347709655762, G Loss: 0.6415871381759644\n",
      "Epoch 56, batch 165 D Loss: 1.3489519357681274, G Loss: 0.6588462591171265\n",
      "Epoch 56, batch 166 D Loss: 1.3655319213867188, G Loss: 0.6524015665054321\n",
      "Epoch 56, batch 167 D Loss: 1.3691363334655762, G Loss: 0.6482555866241455\n",
      "Epoch 56, batch 168 D Loss: 1.359920620918274, G Loss: 0.6530015468597412\n",
      "Epoch 56, batch 169 D Loss: 1.3609552383422852, G Loss: 0.6531808376312256\n",
      "Epoch 56, batch 170 D Loss: 1.3371987342834473, G Loss: 0.6499597430229187\n",
      "Epoch 56, batch 171 D Loss: 1.3657793998718262, G Loss: 0.6463363170623779\n",
      "Epoch 56, batch 172 D Loss: 1.339803695678711, G Loss: 0.6513784527778625\n",
      "Epoch 56, batch 173 D Loss: 1.3480989933013916, G Loss: 0.6599486470222473\n",
      "Epoch 56, batch 174 D Loss: 1.3245222568511963, G Loss: 0.6568148136138916\n",
      "Epoch 56, batch 175 D Loss: 1.3709444999694824, G Loss: 0.6538264751434326\n",
      "Epoch 56, batch 176 D Loss: 1.3687644004821777, G Loss: 0.6493351459503174\n",
      "Epoch 56, batch 177 D Loss: 1.3474692106246948, G Loss: 0.6514379382133484\n",
      "Epoch 56, batch 178 D Loss: 1.3583390712738037, G Loss: 0.6603939533233643\n",
      "Epoch 56, batch 179 D Loss: 1.3724408149719238, G Loss: 0.6472995281219482\n",
      "Epoch 56, batch 180 D Loss: 1.36376953125, G Loss: 0.6480317711830139\n",
      "Epoch 56, batch 181 D Loss: 1.3416669368743896, G Loss: 0.6517173647880554\n",
      "Epoch 56, batch 182 D Loss: 1.3290464878082275, G Loss: 0.6554737091064453\n",
      "Epoch 56, batch 183 D Loss: 1.3319976329803467, G Loss: 0.6618377566337585\n",
      "Epoch 56, batch 184 D Loss: 1.3469674587249756, G Loss: 0.6461672186851501\n",
      "Epoch 56, batch 185 D Loss: 1.3285222053527832, G Loss: 0.6632643342018127\n",
      "Epoch 56, batch 186 D Loss: 1.3472716808319092, G Loss: 0.6485649347305298\n",
      "Epoch 56, batch 187 D Loss: 1.3185608386993408, G Loss: 0.6672170758247375\n",
      "Epoch 56, batch 188 D Loss: 1.3549504280090332, G Loss: 0.6334468126296997\n",
      "Epoch 56, batch 189 D Loss: 1.34719979763031, G Loss: 0.6523306369781494\n",
      "Epoch 56, batch 190 D Loss: 1.3864691257476807, G Loss: 0.6428270936012268\n",
      "Epoch 56, batch 191 D Loss: 1.3460984230041504, G Loss: 0.647562563419342\n",
      "Epoch 56, batch 192 D Loss: 1.3508520126342773, G Loss: 0.6535225510597229\n",
      "Epoch 56, batch 193 D Loss: 1.3610408306121826, G Loss: 0.6566141247749329\n",
      "Epoch 56, batch 194 D Loss: 1.3529932498931885, G Loss: 0.6491933465003967\n",
      "Epoch 56, batch 195 D Loss: 1.3530492782592773, G Loss: 0.6446846127510071\n",
      "Epoch 56, batch 196 D Loss: 1.3425049781799316, G Loss: 0.6538059115409851\n",
      "Epoch 56, batch 197 D Loss: 1.372847080230713, G Loss: 0.64310622215271\n",
      "Epoch 56, batch 198 D Loss: 1.3923842906951904, G Loss: 0.6412103176116943\n",
      "Epoch 56, batch 199 D Loss: 1.3795788288116455, G Loss: 0.6450920104980469\n",
      "Epoch 56, batch 200 D Loss: 1.3900223970413208, G Loss: 0.6292954087257385\n",
      "Epoch 57, batch 1 D Loss: 1.3334057331085205, G Loss: 0.6539846658706665\n",
      "Epoch 57, batch 2 D Loss: 1.3515141010284424, G Loss: 0.65101158618927\n",
      "Epoch 57, batch 3 D Loss: 1.3462755680084229, G Loss: 0.6446412801742554\n",
      "Epoch 57, batch 4 D Loss: 1.4008936882019043, G Loss: 0.6421361565589905\n",
      "Epoch 57, batch 5 D Loss: 1.368911623954773, G Loss: 0.6400569081306458\n",
      "Epoch 57, batch 6 D Loss: 1.3854080438613892, G Loss: 0.6417427062988281\n",
      "Epoch 57, batch 7 D Loss: 1.33616042137146, G Loss: 0.6636356115341187\n",
      "Epoch 57, batch 8 D Loss: 1.3922991752624512, G Loss: 0.6474415063858032\n",
      "Epoch 57, batch 9 D Loss: 1.3818387985229492, G Loss: 0.6446089148521423\n",
      "Epoch 57, batch 10 D Loss: 1.377260684967041, G Loss: 0.652127206325531\n",
      "Epoch 57, batch 11 D Loss: 1.3913626670837402, G Loss: 0.6461162567138672\n",
      "Epoch 57, batch 12 D Loss: 1.381434440612793, G Loss: 0.6493483185768127\n",
      "Epoch 57, batch 13 D Loss: 1.402845859527588, G Loss: 0.6326828598976135\n",
      "Epoch 57, batch 14 D Loss: 1.445704698562622, G Loss: 0.6113685369491577\n",
      "Epoch 57, batch 15 D Loss: 1.4017454385757446, G Loss: 0.6447686553001404\n",
      "Epoch 57, batch 16 D Loss: 1.3843944072723389, G Loss: 0.6454172730445862\n",
      "Epoch 57, batch 17 D Loss: 1.4515290260314941, G Loss: 0.6056103706359863\n",
      "Epoch 57, batch 18 D Loss: 1.4628500938415527, G Loss: 0.6181777715682983\n",
      "Epoch 57, batch 19 D Loss: 1.3557872772216797, G Loss: 0.6531673669815063\n",
      "Epoch 57, batch 20 D Loss: 1.3772743940353394, G Loss: 0.6594843864440918\n",
      "Epoch 57, batch 21 D Loss: 1.3797780275344849, G Loss: 0.6439680457115173\n",
      "Epoch 57, batch 22 D Loss: 1.4164977073669434, G Loss: 0.6227478981018066\n",
      "Epoch 57, batch 23 D Loss: 1.3689258098602295, G Loss: 0.6615779399871826\n",
      "Epoch 57, batch 24 D Loss: 1.4187815189361572, G Loss: 0.6300747990608215\n",
      "Epoch 57, batch 25 D Loss: 1.3994771242141724, G Loss: 0.6605849266052246\n",
      "Epoch 57, batch 26 D Loss: 1.4123958349227905, G Loss: 0.6455421447753906\n",
      "Epoch 57, batch 27 D Loss: 1.4613299369812012, G Loss: 0.629844605922699\n",
      "Epoch 57, batch 28 D Loss: 1.4529833793640137, G Loss: 0.6392634510993958\n",
      "Epoch 57, batch 29 D Loss: 1.4606854915618896, G Loss: 0.6408005356788635\n",
      "Epoch 57, batch 30 D Loss: 1.4660224914550781, G Loss: 0.6365106105804443\n",
      "Epoch 57, batch 31 D Loss: 1.5057746171951294, G Loss: 0.6059026122093201\n",
      "Epoch 57, batch 32 D Loss: 1.4173665046691895, G Loss: 0.6692473888397217\n",
      "Epoch 57, batch 33 D Loss: 1.4288221597671509, G Loss: 0.6418901085853577\n",
      "Epoch 57, batch 34 D Loss: 1.4556405544281006, G Loss: 0.6364091038703918\n",
      "Epoch 57, batch 35 D Loss: 1.479858160018921, G Loss: 0.6157341599464417\n",
      "Epoch 57, batch 36 D Loss: 1.4758739471435547, G Loss: 0.6306778788566589\n",
      "Epoch 57, batch 37 D Loss: 1.4013099670410156, G Loss: 0.6903663873672485\n",
      "Epoch 57, batch 38 D Loss: 1.443552017211914, G Loss: 0.6610348224639893\n",
      "Epoch 57, batch 39 D Loss: 1.4102153778076172, G Loss: 0.6851334571838379\n",
      "Epoch 57, batch 40 D Loss: 1.4369672536849976, G Loss: 0.6589328050613403\n",
      "Epoch 57, batch 41 D Loss: 1.4619874954223633, G Loss: 0.6573519706726074\n",
      "Epoch 57, batch 42 D Loss: 1.4346601963043213, G Loss: 0.6736578345298767\n",
      "Epoch 57, batch 43 D Loss: 1.4606735706329346, G Loss: 0.6648315191268921\n",
      "Epoch 57, batch 44 D Loss: 1.4284005165100098, G Loss: 0.6842626333236694\n",
      "Epoch 57, batch 45 D Loss: 1.4521124362945557, G Loss: 0.684502363204956\n",
      "Epoch 57, batch 46 D Loss: 1.4159352779388428, G Loss: 0.714622437953949\n",
      "Epoch 57, batch 47 D Loss: 1.4283006191253662, G Loss: 0.7006266713142395\n",
      "Epoch 57, batch 48 D Loss: 1.4098151922225952, G Loss: 0.7055671811103821\n",
      "Epoch 57, batch 49 D Loss: 1.445719599723816, G Loss: 0.7006576061248779\n",
      "Epoch 57, batch 50 D Loss: 1.457413673400879, G Loss: 0.6918737888336182\n",
      "Epoch 57, batch 51 D Loss: 1.4198639392852783, G Loss: 0.7108195424079895\n",
      "Epoch 57, batch 52 D Loss: 1.4326646327972412, G Loss: 0.7183412313461304\n",
      "Epoch 57, batch 53 D Loss: 1.4298605918884277, G Loss: 0.7162908315658569\n",
      "Epoch 57, batch 54 D Loss: 1.4494590759277344, G Loss: 0.7119009494781494\n",
      "Epoch 57, batch 55 D Loss: 1.4010076522827148, G Loss: 0.741187334060669\n",
      "Epoch 57, batch 56 D Loss: 1.3945624828338623, G Loss: 0.7526207566261292\n",
      "Epoch 57, batch 57 D Loss: 1.3959028720855713, G Loss: 0.748633623123169\n",
      "Epoch 57, batch 58 D Loss: 1.3859260082244873, G Loss: 0.7856971621513367\n",
      "Epoch 57, batch 59 D Loss: 1.3602913618087769, G Loss: 0.7904741168022156\n",
      "Epoch 57, batch 60 D Loss: 1.3857918977737427, G Loss: 0.7741252779960632\n",
      "Epoch 57, batch 61 D Loss: 1.3892860412597656, G Loss: 0.7756100296974182\n",
      "Epoch 57, batch 62 D Loss: 1.385408878326416, G Loss: 0.7671358585357666\n",
      "Epoch 57, batch 63 D Loss: 1.3858273029327393, G Loss: 0.7893556952476501\n",
      "Epoch 57, batch 64 D Loss: 1.345137596130371, G Loss: 0.832410454750061\n",
      "Epoch 57, batch 65 D Loss: 1.3570904731750488, G Loss: 0.8382965326309204\n",
      "Epoch 57, batch 66 D Loss: 1.3881553411483765, G Loss: 0.805968165397644\n",
      "Epoch 57, batch 67 D Loss: 1.384610652923584, G Loss: 0.7905462384223938\n",
      "Epoch 57, batch 68 D Loss: 1.3665730953216553, G Loss: 0.8270117044448853\n",
      "Epoch 57, batch 69 D Loss: 1.3507554531097412, G Loss: 0.8235854506492615\n",
      "Epoch 57, batch 70 D Loss: 1.3629014492034912, G Loss: 0.8308561444282532\n",
      "Epoch 57, batch 71 D Loss: 1.358661413192749, G Loss: 0.834754228591919\n",
      "Epoch 57, batch 72 D Loss: 1.3623945713043213, G Loss: 0.84934401512146\n",
      "Epoch 57, batch 73 D Loss: 1.3257888555526733, G Loss: 0.8806095719337463\n",
      "Epoch 57, batch 74 D Loss: 1.3737070560455322, G Loss: 0.8085670471191406\n",
      "Epoch 57, batch 75 D Loss: 1.3652002811431885, G Loss: 0.8504699468612671\n",
      "Epoch 57, batch 76 D Loss: 1.3613085746765137, G Loss: 0.8326084017753601\n",
      "Epoch 57, batch 77 D Loss: 1.3934133052825928, G Loss: 0.7921640872955322\n",
      "Epoch 57, batch 78 D Loss: 1.3365681171417236, G Loss: 0.878196120262146\n",
      "Epoch 57, batch 79 D Loss: 1.3402312994003296, G Loss: 0.8355690240859985\n",
      "Epoch 57, batch 80 D Loss: 1.3610658645629883, G Loss: 0.8642820715904236\n",
      "Epoch 57, batch 81 D Loss: 1.3274022340774536, G Loss: 0.8579295873641968\n",
      "Epoch 57, batch 82 D Loss: 1.3448402881622314, G Loss: 0.870806872844696\n",
      "Epoch 57, batch 83 D Loss: 1.3671274185180664, G Loss: 0.8179678320884705\n",
      "Epoch 57, batch 84 D Loss: 1.3700677156448364, G Loss: 0.8326542377471924\n",
      "Epoch 57, batch 85 D Loss: 1.3372395038604736, G Loss: 0.8738734722137451\n",
      "Epoch 57, batch 86 D Loss: 1.347665786743164, G Loss: 0.8649154901504517\n",
      "Epoch 57, batch 87 D Loss: 1.3507226705551147, G Loss: 0.835326611995697\n",
      "Epoch 57, batch 88 D Loss: 1.4029244184494019, G Loss: 0.8152110576629639\n",
      "Epoch 57, batch 89 D Loss: 1.3898441791534424, G Loss: 0.8515723943710327\n",
      "Epoch 57, batch 90 D Loss: 1.3826870918273926, G Loss: 0.8515365719795227\n",
      "Epoch 57, batch 91 D Loss: 1.3656516075134277, G Loss: 0.8216922283172607\n",
      "Epoch 57, batch 92 D Loss: 1.3668594360351562, G Loss: 0.8182931542396545\n",
      "Epoch 57, batch 93 D Loss: 1.3750618696212769, G Loss: 0.8195106983184814\n",
      "Epoch 57, batch 94 D Loss: 1.3597157001495361, G Loss: 0.8366249799728394\n",
      "Epoch 57, batch 95 D Loss: 1.4375072717666626, G Loss: 0.8042789697647095\n",
      "Epoch 57, batch 96 D Loss: 1.3571579456329346, G Loss: 0.7914153933525085\n",
      "Epoch 57, batch 97 D Loss: 1.3485313653945923, G Loss: 0.8417867422103882\n",
      "Epoch 57, batch 98 D Loss: 1.38869309425354, G Loss: 0.7942458391189575\n",
      "Epoch 57, batch 99 D Loss: 1.458701729774475, G Loss: 0.7712658047676086\n",
      "Epoch 57, batch 100 D Loss: 1.4090120792388916, G Loss: 0.7878120541572571\n",
      "Epoch 57, batch 101 D Loss: 1.4160631895065308, G Loss: 0.7551882266998291\n",
      "Epoch 57, batch 102 D Loss: 1.387148141860962, G Loss: 0.7604718208312988\n",
      "Epoch 57, batch 103 D Loss: 1.4116140604019165, G Loss: 0.7631111145019531\n",
      "Epoch 57, batch 104 D Loss: 1.3779182434082031, G Loss: 0.7620086669921875\n",
      "Epoch 57, batch 105 D Loss: 1.3982999324798584, G Loss: 0.7637872099876404\n",
      "Epoch 57, batch 106 D Loss: 1.381422519683838, G Loss: 0.7782161235809326\n",
      "Epoch 57, batch 107 D Loss: 1.4259872436523438, G Loss: 0.7649143934249878\n",
      "Epoch 57, batch 108 D Loss: 1.39505136013031, G Loss: 0.7432627081871033\n",
      "Epoch 57, batch 109 D Loss: 1.3929927349090576, G Loss: 0.7616339325904846\n",
      "Epoch 57, batch 110 D Loss: 1.3811986446380615, G Loss: 0.7646176815032959\n",
      "Epoch 57, batch 111 D Loss: 1.4098684787750244, G Loss: 0.7397357225418091\n",
      "Epoch 57, batch 112 D Loss: 1.3666329383850098, G Loss: 0.7651680707931519\n",
      "Epoch 57, batch 113 D Loss: 1.3793598413467407, G Loss: 0.7642217874526978\n",
      "Epoch 57, batch 114 D Loss: 1.4058674573898315, G Loss: 0.7417215704917908\n",
      "Epoch 57, batch 115 D Loss: 1.4323077201843262, G Loss: 0.7265382409095764\n",
      "Epoch 57, batch 116 D Loss: 1.398073434829712, G Loss: 0.7499925494194031\n",
      "Epoch 57, batch 117 D Loss: 1.3901820182800293, G Loss: 0.7425167560577393\n",
      "Epoch 57, batch 118 D Loss: 1.3940398693084717, G Loss: 0.7299185395240784\n",
      "Epoch 57, batch 119 D Loss: 1.3748855590820312, G Loss: 0.7417172193527222\n",
      "Epoch 57, batch 120 D Loss: 1.4278583526611328, G Loss: 0.7139891982078552\n",
      "Epoch 57, batch 121 D Loss: 1.3854918479919434, G Loss: 0.7146985530853271\n",
      "Epoch 57, batch 122 D Loss: 1.4183998107910156, G Loss: 0.7134315967559814\n",
      "Epoch 57, batch 123 D Loss: 1.4278182983398438, G Loss: 0.7244459390640259\n",
      "Epoch 57, batch 124 D Loss: 1.3903708457946777, G Loss: 0.7120715379714966\n",
      "Epoch 57, batch 125 D Loss: 1.385399580001831, G Loss: 0.7110397219657898\n",
      "Epoch 57, batch 126 D Loss: 1.3995323181152344, G Loss: 0.7134893536567688\n",
      "Epoch 57, batch 127 D Loss: 1.394470453262329, G Loss: 0.7082451581954956\n",
      "Epoch 57, batch 128 D Loss: 1.3895076513290405, G Loss: 0.7044818997383118\n",
      "Epoch 57, batch 129 D Loss: 1.4026647806167603, G Loss: 0.7123265266418457\n",
      "Epoch 57, batch 130 D Loss: 1.4233710765838623, G Loss: 0.6922914385795593\n",
      "Epoch 57, batch 131 D Loss: 1.3958313465118408, G Loss: 0.7205010056495667\n",
      "Epoch 57, batch 132 D Loss: 1.4228596687316895, G Loss: 0.7033828496932983\n",
      "Epoch 57, batch 133 D Loss: 1.3805272579193115, G Loss: 0.711611807346344\n",
      "Epoch 57, batch 134 D Loss: 1.4333797693252563, G Loss: 0.6923530697822571\n",
      "Epoch 57, batch 135 D Loss: 1.4073022603988647, G Loss: 0.707746148109436\n",
      "Epoch 57, batch 136 D Loss: 1.422424077987671, G Loss: 0.6938188672065735\n",
      "Epoch 57, batch 137 D Loss: 1.411745309829712, G Loss: 0.6964545249938965\n",
      "Epoch 57, batch 138 D Loss: 1.409608244895935, G Loss: 0.6932872533798218\n",
      "Epoch 57, batch 139 D Loss: 1.4064676761627197, G Loss: 0.6826066374778748\n",
      "Epoch 57, batch 140 D Loss: 1.4140117168426514, G Loss: 0.690799355506897\n",
      "Epoch 57, batch 141 D Loss: 1.4044713973999023, G Loss: 0.6792259812355042\n",
      "Epoch 57, batch 142 D Loss: 1.4071654081344604, G Loss: 0.677456796169281\n",
      "Epoch 57, batch 143 D Loss: 1.4201406240463257, G Loss: 0.6792112588882446\n",
      "Epoch 57, batch 144 D Loss: 1.4003794193267822, G Loss: 0.6773938536643982\n",
      "Epoch 57, batch 145 D Loss: 1.4120008945465088, G Loss: 0.6636888384819031\n",
      "Epoch 57, batch 146 D Loss: 1.406843900680542, G Loss: 0.6730899214744568\n",
      "Epoch 57, batch 147 D Loss: 1.4045555591583252, G Loss: 0.667269766330719\n",
      "Epoch 57, batch 148 D Loss: 1.3957711458206177, G Loss: 0.6678693890571594\n",
      "Epoch 57, batch 149 D Loss: 1.4051039218902588, G Loss: 0.6727027297019958\n",
      "Epoch 57, batch 150 D Loss: 1.3966929912567139, G Loss: 0.6715482473373413\n",
      "Epoch 57, batch 151 D Loss: 1.4023648500442505, G Loss: 0.6706483960151672\n",
      "Epoch 57, batch 152 D Loss: 1.4017002582550049, G Loss: 0.6576586365699768\n",
      "Epoch 57, batch 153 D Loss: 1.4034004211425781, G Loss: 0.6683771014213562\n",
      "Epoch 57, batch 154 D Loss: 1.3866801261901855, G Loss: 0.6742600202560425\n",
      "Epoch 57, batch 155 D Loss: 1.4006696939468384, G Loss: 0.6650573015213013\n",
      "Epoch 57, batch 156 D Loss: 1.4060378074645996, G Loss: 0.6535919308662415\n",
      "Epoch 57, batch 157 D Loss: 1.4102628231048584, G Loss: 0.6533985733985901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, batch 158 D Loss: 1.4013701677322388, G Loss: 0.6653568148612976\n",
      "Epoch 57, batch 159 D Loss: 1.395597219467163, G Loss: 0.6587763428688049\n",
      "Epoch 57, batch 160 D Loss: 1.4002809524536133, G Loss: 0.6609282493591309\n",
      "Epoch 57, batch 161 D Loss: 1.3970915079116821, G Loss: 0.6613920331001282\n",
      "Epoch 57, batch 162 D Loss: 1.3850305080413818, G Loss: 0.667186439037323\n",
      "Epoch 57, batch 163 D Loss: 1.3943369388580322, G Loss: 0.6633920073509216\n",
      "Epoch 57, batch 164 D Loss: 1.3953027725219727, G Loss: 0.6537582874298096\n",
      "Epoch 57, batch 165 D Loss: 1.4001219272613525, G Loss: 0.6553360223770142\n",
      "Epoch 57, batch 166 D Loss: 1.3845967054367065, G Loss: 0.660643458366394\n",
      "Epoch 57, batch 167 D Loss: 1.3866134881973267, G Loss: 0.6612628102302551\n",
      "Epoch 57, batch 168 D Loss: 1.387933373451233, G Loss: 0.6568666696548462\n",
      "Epoch 57, batch 169 D Loss: 1.3828253746032715, G Loss: 0.6622306704521179\n",
      "Epoch 57, batch 170 D Loss: 1.3835740089416504, G Loss: 0.659913957118988\n",
      "Epoch 57, batch 171 D Loss: 1.3802313804626465, G Loss: 0.660088062286377\n",
      "Epoch 57, batch 172 D Loss: 1.3902771472930908, G Loss: 0.6544138193130493\n",
      "Epoch 57, batch 173 D Loss: 1.3921626806259155, G Loss: 0.6568620800971985\n",
      "Epoch 57, batch 174 D Loss: 1.3778595924377441, G Loss: 0.6631624698638916\n",
      "Epoch 57, batch 175 D Loss: 1.389085292816162, G Loss: 0.6541142463684082\n",
      "Epoch 57, batch 176 D Loss: 1.3754619359970093, G Loss: 0.656973123550415\n",
      "Epoch 57, batch 177 D Loss: 1.3778024911880493, G Loss: 0.6558480262756348\n",
      "Epoch 57, batch 178 D Loss: 1.3756784200668335, G Loss: 0.6614978313446045\n",
      "Epoch 57, batch 179 D Loss: 1.3673899173736572, G Loss: 0.6616469621658325\n",
      "Epoch 57, batch 180 D Loss: 1.3670248985290527, G Loss: 0.6646102666854858\n",
      "Epoch 57, batch 181 D Loss: 1.369161605834961, G Loss: 0.667121410369873\n",
      "Epoch 57, batch 182 D Loss: 1.3747437000274658, G Loss: 0.6635212898254395\n",
      "Epoch 57, batch 183 D Loss: 1.3722753524780273, G Loss: 0.6642230153083801\n",
      "Epoch 57, batch 184 D Loss: 1.3876274824142456, G Loss: 0.6544483304023743\n",
      "Epoch 57, batch 185 D Loss: 1.3713384866714478, G Loss: 0.6591438055038452\n",
      "Epoch 57, batch 186 D Loss: 1.361055612564087, G Loss: 0.6687057614326477\n",
      "Epoch 57, batch 187 D Loss: 1.369939923286438, G Loss: 0.6622008681297302\n",
      "Epoch 57, batch 188 D Loss: 1.3691067695617676, G Loss: 0.6688260436058044\n",
      "Epoch 57, batch 189 D Loss: 1.3664109706878662, G Loss: 0.6647903323173523\n",
      "Epoch 57, batch 190 D Loss: 1.3689799308776855, G Loss: 0.6646997928619385\n",
      "Epoch 57, batch 191 D Loss: 1.3516721725463867, G Loss: 0.6689241528511047\n",
      "Epoch 57, batch 192 D Loss: 1.3654868602752686, G Loss: 0.6644945740699768\n",
      "Epoch 57, batch 193 D Loss: 1.3596073389053345, G Loss: 0.6606965065002441\n",
      "Epoch 57, batch 194 D Loss: 1.3585777282714844, G Loss: 0.6720631122589111\n",
      "Epoch 57, batch 195 D Loss: 1.3513513803482056, G Loss: 0.6710155606269836\n",
      "Epoch 57, batch 196 D Loss: 1.3478741645812988, G Loss: 0.6666794419288635\n",
      "Epoch 57, batch 197 D Loss: 1.3637375831604004, G Loss: 0.6625778079032898\n",
      "Epoch 57, batch 198 D Loss: 1.3509420156478882, G Loss: 0.6611068248748779\n",
      "Epoch 57, batch 199 D Loss: 1.357872486114502, G Loss: 0.668633222579956\n",
      "Epoch 57, batch 200 D Loss: 1.3757433891296387, G Loss: 0.6569663882255554\n",
      "Epoch 58, batch 1 D Loss: 1.350604772567749, G Loss: 0.6711066365242004\n",
      "Epoch 58, batch 2 D Loss: 1.3673961162567139, G Loss: 0.6621910333633423\n",
      "Epoch 58, batch 3 D Loss: 1.3624703884124756, G Loss: 0.6675102710723877\n",
      "Epoch 58, batch 4 D Loss: 1.3651225566864014, G Loss: 0.6666851043701172\n",
      "Epoch 58, batch 5 D Loss: 1.3615809679031372, G Loss: 0.6731265783309937\n",
      "Epoch 58, batch 6 D Loss: 1.3410390615463257, G Loss: 0.6719056963920593\n",
      "Epoch 58, batch 7 D Loss: 1.3721811771392822, G Loss: 0.6622021198272705\n",
      "Epoch 58, batch 8 D Loss: 1.3479878902435303, G Loss: 0.6633009314537048\n",
      "Epoch 58, batch 9 D Loss: 1.354352593421936, G Loss: 0.6682771444320679\n",
      "Epoch 58, batch 10 D Loss: 1.345418930053711, G Loss: 0.669403612613678\n",
      "Epoch 58, batch 11 D Loss: 1.3686233758926392, G Loss: 0.6581176519393921\n",
      "Epoch 58, batch 12 D Loss: 1.370970606803894, G Loss: 0.6588469743728638\n",
      "Epoch 58, batch 13 D Loss: 1.3485689163208008, G Loss: 0.6731731295585632\n",
      "Epoch 58, batch 14 D Loss: 1.3588855266571045, G Loss: 0.6683892011642456\n",
      "Epoch 58, batch 15 D Loss: 1.3623418807983398, G Loss: 0.6753995418548584\n",
      "Epoch 58, batch 16 D Loss: 1.3499186038970947, G Loss: 0.6666465997695923\n",
      "Epoch 58, batch 17 D Loss: 1.3507691621780396, G Loss: 0.674985408782959\n",
      "Epoch 58, batch 18 D Loss: 1.3548929691314697, G Loss: 0.6656726598739624\n",
      "Epoch 58, batch 19 D Loss: 1.3497203588485718, G Loss: 0.6720617413520813\n",
      "Epoch 58, batch 20 D Loss: 1.3792716264724731, G Loss: 0.6640671491622925\n",
      "Epoch 58, batch 21 D Loss: 1.3659284114837646, G Loss: 0.6721327304840088\n",
      "Epoch 58, batch 22 D Loss: 1.3306667804718018, G Loss: 0.6708810925483704\n",
      "Epoch 58, batch 23 D Loss: 1.348242163658142, G Loss: 0.6747597455978394\n",
      "Epoch 58, batch 24 D Loss: 1.3557426929473877, G Loss: 0.6797645092010498\n",
      "Epoch 58, batch 25 D Loss: 1.3426599502563477, G Loss: 0.6780515909194946\n",
      "Epoch 58, batch 26 D Loss: 1.3716615438461304, G Loss: 0.6623284220695496\n",
      "Epoch 58, batch 27 D Loss: 1.3593411445617676, G Loss: 0.6674151420593262\n",
      "Epoch 58, batch 28 D Loss: 1.3544933795928955, G Loss: 0.6691723465919495\n",
      "Epoch 58, batch 29 D Loss: 1.3595292568206787, G Loss: 0.675082802772522\n",
      "Epoch 58, batch 30 D Loss: 1.29660964012146, G Loss: 0.684374213218689\n",
      "Epoch 58, batch 31 D Loss: 1.3437001705169678, G Loss: 0.6660412549972534\n",
      "Epoch 58, batch 32 D Loss: 1.3709275722503662, G Loss: 0.6684592962265015\n",
      "Epoch 58, batch 33 D Loss: 1.370417833328247, G Loss: 0.6740286350250244\n",
      "Epoch 58, batch 34 D Loss: 1.3374137878417969, G Loss: 0.6754605770111084\n",
      "Epoch 58, batch 35 D Loss: 1.3417342901229858, G Loss: 0.6626550555229187\n",
      "Epoch 58, batch 36 D Loss: 1.3270748853683472, G Loss: 0.6792448163032532\n",
      "Epoch 58, batch 37 D Loss: 1.325197696685791, G Loss: 0.6722064018249512\n",
      "Epoch 58, batch 38 D Loss: 1.33449125289917, G Loss: 0.6666580438613892\n",
      "Epoch 58, batch 39 D Loss: 1.3564907312393188, G Loss: 0.6736411452293396\n",
      "Epoch 58, batch 40 D Loss: 1.3443629741668701, G Loss: 0.6606884598731995\n",
      "Epoch 58, batch 41 D Loss: 1.3685096502304077, G Loss: 0.6576787829399109\n",
      "Epoch 58, batch 42 D Loss: 1.3589417934417725, G Loss: 0.6631593108177185\n",
      "Epoch 58, batch 43 D Loss: 1.3427999019622803, G Loss: 0.6772531270980835\n",
      "Epoch 58, batch 44 D Loss: 1.3625130653381348, G Loss: 0.6628180146217346\n",
      "Epoch 58, batch 45 D Loss: 1.3464797735214233, G Loss: 0.6582221984863281\n",
      "Epoch 58, batch 46 D Loss: 1.3406226634979248, G Loss: 0.6856514811515808\n",
      "Epoch 58, batch 47 D Loss: 1.3872321844100952, G Loss: 0.6604245901107788\n",
      "Epoch 58, batch 48 D Loss: 1.3309299945831299, G Loss: 0.6683946251869202\n",
      "Epoch 58, batch 49 D Loss: 1.353777527809143, G Loss: 0.6790515780448914\n",
      "Epoch 58, batch 50 D Loss: 1.3495209217071533, G Loss: 0.6621736288070679\n",
      "Epoch 58, batch 51 D Loss: 1.3822994232177734, G Loss: 0.6542958617210388\n",
      "Epoch 58, batch 52 D Loss: 1.3761937618255615, G Loss: 0.6661379337310791\n",
      "Epoch 58, batch 53 D Loss: 1.3646433353424072, G Loss: 0.6796563863754272\n",
      "Epoch 58, batch 54 D Loss: 1.3516733646392822, G Loss: 0.6695303320884705\n",
      "Epoch 58, batch 55 D Loss: 1.3700876235961914, G Loss: 0.6699755787849426\n",
      "Epoch 58, batch 56 D Loss: 1.3774131536483765, G Loss: 0.6587886810302734\n",
      "Epoch 58, batch 57 D Loss: 1.3770891427993774, G Loss: 0.6702227592468262\n",
      "Epoch 58, batch 58 D Loss: 1.3532131910324097, G Loss: 0.6777766942977905\n",
      "Epoch 58, batch 59 D Loss: 1.3554645776748657, G Loss: 0.6553784012794495\n",
      "Epoch 58, batch 60 D Loss: 1.370516061782837, G Loss: 0.6551045179367065\n",
      "Epoch 58, batch 61 D Loss: 1.3762552738189697, G Loss: 0.6583263874053955\n",
      "Epoch 58, batch 62 D Loss: 1.3908579349517822, G Loss: 0.6529393196105957\n",
      "Epoch 58, batch 63 D Loss: 1.3969693183898926, G Loss: 0.6478313207626343\n",
      "Epoch 58, batch 64 D Loss: 1.404130220413208, G Loss: 0.6456487774848938\n",
      "Epoch 58, batch 65 D Loss: 1.3774795532226562, G Loss: 0.6591147780418396\n",
      "Epoch 58, batch 66 D Loss: 1.4099233150482178, G Loss: 0.6429290771484375\n",
      "Epoch 58, batch 67 D Loss: 1.3651556968688965, G Loss: 0.656609296798706\n",
      "Epoch 58, batch 68 D Loss: 1.3819084167480469, G Loss: 0.6496481895446777\n",
      "Epoch 58, batch 69 D Loss: 1.4018192291259766, G Loss: 0.6586109399795532\n",
      "Epoch 58, batch 70 D Loss: 1.3853683471679688, G Loss: 0.6571713089942932\n",
      "Epoch 58, batch 71 D Loss: 1.394470453262329, G Loss: 0.6591849327087402\n",
      "Epoch 58, batch 72 D Loss: 1.3718812465667725, G Loss: 0.6539492607116699\n",
      "Epoch 58, batch 73 D Loss: 1.350264310836792, G Loss: 0.6829114556312561\n",
      "Epoch 58, batch 74 D Loss: 1.4180939197540283, G Loss: 0.6356455087661743\n",
      "Epoch 58, batch 75 D Loss: 1.376814365386963, G Loss: 0.6574293375015259\n",
      "Epoch 58, batch 76 D Loss: 1.3990962505340576, G Loss: 0.650992214679718\n",
      "Epoch 58, batch 77 D Loss: 1.4169260263442993, G Loss: 0.6577088832855225\n",
      "Epoch 58, batch 78 D Loss: 1.4170360565185547, G Loss: 0.6399233341217041\n",
      "Epoch 58, batch 79 D Loss: 1.4106225967407227, G Loss: 0.6545475721359253\n",
      "Epoch 58, batch 80 D Loss: 1.4090453386306763, G Loss: 0.6573385000228882\n",
      "Epoch 58, batch 81 D Loss: 1.3729746341705322, G Loss: 0.6587584614753723\n",
      "Epoch 58, batch 82 D Loss: 1.4243199825286865, G Loss: 0.6357272863388062\n",
      "Epoch 58, batch 83 D Loss: 1.429014801979065, G Loss: 0.6529518365859985\n",
      "Epoch 58, batch 84 D Loss: 1.4313595294952393, G Loss: 0.643119752407074\n",
      "Epoch 58, batch 85 D Loss: 1.4050402641296387, G Loss: 0.6538091897964478\n",
      "Epoch 58, batch 86 D Loss: 1.4578899145126343, G Loss: 0.6371039152145386\n",
      "Epoch 58, batch 87 D Loss: 1.4176437854766846, G Loss: 0.6593480110168457\n",
      "Epoch 58, batch 88 D Loss: 1.4379420280456543, G Loss: 0.6413543820381165\n",
      "Epoch 58, batch 89 D Loss: 1.401336431503296, G Loss: 0.6664013862609863\n",
      "Epoch 58, batch 90 D Loss: 1.4194071292877197, G Loss: 0.6564010381698608\n",
      "Epoch 58, batch 91 D Loss: 1.427112102508545, G Loss: 0.6559516787528992\n",
      "Epoch 58, batch 92 D Loss: 1.412153720855713, G Loss: 0.6606599688529968\n",
      "Epoch 58, batch 93 D Loss: 1.4659669399261475, G Loss: 0.6316594481468201\n",
      "Epoch 58, batch 94 D Loss: 1.4526777267456055, G Loss: 0.6560714840888977\n",
      "Epoch 58, batch 95 D Loss: 1.4410392045974731, G Loss: 0.6640521287918091\n",
      "Epoch 58, batch 96 D Loss: 1.466773271560669, G Loss: 0.6348863840103149\n",
      "Epoch 58, batch 97 D Loss: 1.441451072692871, G Loss: 0.6435847282409668\n",
      "Epoch 58, batch 98 D Loss: 1.485690951347351, G Loss: 0.6249287724494934\n",
      "Epoch 58, batch 99 D Loss: 1.4724793434143066, G Loss: 0.6396724581718445\n",
      "Epoch 58, batch 100 D Loss: 1.4501307010650635, G Loss: 0.6568306088447571\n",
      "Epoch 58, batch 101 D Loss: 1.4453625679016113, G Loss: 0.6547514200210571\n",
      "Epoch 58, batch 102 D Loss: 1.4390199184417725, G Loss: 0.6664198040962219\n",
      "Epoch 58, batch 103 D Loss: 1.4339039325714111, G Loss: 0.6640889644622803\n",
      "Epoch 58, batch 104 D Loss: 1.4682079553604126, G Loss: 0.6422432065010071\n",
      "Epoch 58, batch 105 D Loss: 1.4388179779052734, G Loss: 0.6687564253807068\n",
      "Epoch 58, batch 106 D Loss: 1.4475512504577637, G Loss: 0.6651257276535034\n",
      "Epoch 58, batch 107 D Loss: 1.4372352361679077, G Loss: 0.6734808087348938\n",
      "Epoch 58, batch 108 D Loss: 1.4292734861373901, G Loss: 0.6785653829574585\n",
      "Epoch 58, batch 109 D Loss: 1.4442591667175293, G Loss: 0.6541423201560974\n",
      "Epoch 58, batch 110 D Loss: 1.4353793859481812, G Loss: 0.6843478679656982\n",
      "Epoch 58, batch 111 D Loss: 1.417567253112793, G Loss: 0.6783671379089355\n",
      "Epoch 58, batch 112 D Loss: 1.3992655277252197, G Loss: 0.7067938446998596\n",
      "Epoch 58, batch 113 D Loss: 1.3811590671539307, G Loss: 0.71517014503479\n",
      "Epoch 58, batch 114 D Loss: 1.4337928295135498, G Loss: 0.7017583250999451\n",
      "Epoch 58, batch 115 D Loss: 1.4295222759246826, G Loss: 0.6939405202865601\n",
      "Epoch 58, batch 116 D Loss: 1.4404910802841187, G Loss: 0.6793612837791443\n",
      "Epoch 58, batch 117 D Loss: 1.422287940979004, G Loss: 0.6936612129211426\n",
      "Epoch 58, batch 118 D Loss: 1.4324893951416016, G Loss: 0.6932083964347839\n",
      "Epoch 58, batch 119 D Loss: 1.4140042066574097, G Loss: 0.7000419497489929\n",
      "Epoch 58, batch 120 D Loss: 1.4166464805603027, G Loss: 0.7038960456848145\n",
      "Epoch 58, batch 121 D Loss: 1.4194262027740479, G Loss: 0.7006852030754089\n",
      "Epoch 58, batch 122 D Loss: 1.4188590049743652, G Loss: 0.7093040943145752\n",
      "Epoch 58, batch 123 D Loss: 1.4276301860809326, G Loss: 0.7040773034095764\n",
      "Epoch 58, batch 124 D Loss: 1.4035906791687012, G Loss: 0.7259050607681274\n",
      "Epoch 58, batch 125 D Loss: 1.399848222732544, G Loss: 0.7269898056983948\n",
      "Epoch 58, batch 126 D Loss: 1.4031877517700195, G Loss: 0.7365445494651794\n",
      "Epoch 58, batch 127 D Loss: 1.4212687015533447, G Loss: 0.7241012454032898\n",
      "Epoch 58, batch 128 D Loss: 1.4181256294250488, G Loss: 0.7235804200172424\n",
      "Epoch 58, batch 129 D Loss: 1.3954901695251465, G Loss: 0.728534996509552\n",
      "Epoch 58, batch 130 D Loss: 1.4082245826721191, G Loss: 0.7269347310066223\n",
      "Epoch 58, batch 131 D Loss: 1.3860862255096436, G Loss: 0.7540751099586487\n",
      "Epoch 58, batch 132 D Loss: 1.3880350589752197, G Loss: 0.7502931356430054\n",
      "Epoch 58, batch 133 D Loss: 1.389280915260315, G Loss: 0.7456893920898438\n",
      "Epoch 58, batch 134 D Loss: 1.388042688369751, G Loss: 0.7621663808822632\n",
      "Epoch 58, batch 135 D Loss: 1.3813828229904175, G Loss: 0.7615510821342468\n",
      "Epoch 58, batch 136 D Loss: 1.3788496255874634, G Loss: 0.7708722949028015\n",
      "Epoch 58, batch 137 D Loss: 1.3925282955169678, G Loss: 0.7521266341209412\n",
      "Epoch 58, batch 138 D Loss: 1.3823736906051636, G Loss: 0.7623690962791443\n",
      "Epoch 58, batch 139 D Loss: 1.386101484298706, G Loss: 0.7616773247718811\n",
      "Epoch 58, batch 140 D Loss: 1.366405725479126, G Loss: 0.77716064453125\n",
      "Epoch 58, batch 141 D Loss: 1.3671858310699463, G Loss: 0.7726499438285828\n",
      "Epoch 58, batch 142 D Loss: 1.3697898387908936, G Loss: 0.7779778838157654\n",
      "Epoch 58, batch 143 D Loss: 1.3693821430206299, G Loss: 0.797222375869751\n",
      "Epoch 58, batch 144 D Loss: 1.3603861331939697, G Loss: 0.8224881887435913\n",
      "Epoch 58, batch 145 D Loss: 1.38037109375, G Loss: 0.7853259444236755\n",
      "Epoch 58, batch 146 D Loss: 1.3740918636322021, G Loss: 0.7956058382987976\n",
      "Epoch 58, batch 147 D Loss: 1.3668875694274902, G Loss: 0.7879828810691833\n",
      "Epoch 58, batch 148 D Loss: 1.376824140548706, G Loss: 0.7899348735809326\n",
      "Epoch 58, batch 149 D Loss: 1.344924807548523, G Loss: 0.8194980025291443\n",
      "Epoch 58, batch 150 D Loss: 1.358958125114441, G Loss: 0.8036242723464966\n",
      "Epoch 58, batch 151 D Loss: 1.4039522409439087, G Loss: 0.7577673196792603\n",
      "Epoch 58, batch 152 D Loss: 1.367997407913208, G Loss: 0.794620156288147\n",
      "Epoch 58, batch 153 D Loss: 1.329769492149353, G Loss: 0.8647856712341309\n",
      "Epoch 58, batch 154 D Loss: 1.3633184432983398, G Loss: 0.8065090775489807\n",
      "Epoch 58, batch 155 D Loss: 1.3735363483428955, G Loss: 0.8098316788673401\n",
      "Epoch 58, batch 156 D Loss: 1.3636432886123657, G Loss: 0.8400291204452515\n",
      "Epoch 58, batch 157 D Loss: 1.3598146438598633, G Loss: 0.8264666795730591\n",
      "Epoch 58, batch 158 D Loss: 1.3578908443450928, G Loss: 0.8330392241477966\n",
      "Epoch 58, batch 159 D Loss: 1.3878893852233887, G Loss: 0.809226393699646\n",
      "Epoch 58, batch 160 D Loss: 1.371858835220337, G Loss: 0.7992846965789795\n",
      "Epoch 58, batch 161 D Loss: 1.373421311378479, G Loss: 0.8152890205383301\n",
      "Epoch 58, batch 162 D Loss: 1.4593536853790283, G Loss: 0.7618823051452637\n",
      "Epoch 58, batch 163 D Loss: 1.3947235345840454, G Loss: 0.7756265997886658\n",
      "Epoch 58, batch 164 D Loss: 1.3958356380462646, G Loss: 0.7864271402359009\n",
      "Epoch 58, batch 165 D Loss: 1.362825632095337, G Loss: 0.7817623019218445\n",
      "Epoch 58, batch 166 D Loss: 1.385582685470581, G Loss: 0.7541280388832092\n",
      "Epoch 58, batch 167 D Loss: 1.3719871044158936, G Loss: 0.7685035467147827\n",
      "Epoch 58, batch 168 D Loss: 1.360086441040039, G Loss: 0.7947481274604797\n",
      "Epoch 58, batch 169 D Loss: 1.382004976272583, G Loss: 0.7690916657447815\n",
      "Epoch 58, batch 170 D Loss: 1.3907113075256348, G Loss: 0.7538062334060669\n",
      "Epoch 58, batch 171 D Loss: 1.3723247051239014, G Loss: 0.7603105902671814\n",
      "Epoch 58, batch 172 D Loss: 1.4002059698104858, G Loss: 0.7597064971923828\n",
      "Epoch 58, batch 173 D Loss: 1.4072120189666748, G Loss: 0.7416357398033142\n",
      "Epoch 58, batch 174 D Loss: 1.4131349325180054, G Loss: 0.7567355632781982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, batch 175 D Loss: 1.4137272834777832, G Loss: 0.76250159740448\n",
      "Epoch 58, batch 176 D Loss: 1.4043316841125488, G Loss: 0.770875871181488\n",
      "Epoch 58, batch 177 D Loss: 1.4072015285491943, G Loss: 0.7391378879547119\n",
      "Epoch 58, batch 178 D Loss: 1.3883183002471924, G Loss: 0.7384887933731079\n",
      "Epoch 58, batch 179 D Loss: 1.4288853406906128, G Loss: 0.7336990833282471\n",
      "Epoch 58, batch 180 D Loss: 1.4061273336410522, G Loss: 0.7268660664558411\n",
      "Epoch 58, batch 181 D Loss: 1.3909432888031006, G Loss: 0.7403808832168579\n",
      "Epoch 58, batch 182 D Loss: 1.4274578094482422, G Loss: 0.7193455696105957\n",
      "Epoch 58, batch 183 D Loss: 1.4153869152069092, G Loss: 0.7119739651679993\n",
      "Epoch 58, batch 184 D Loss: 1.4365549087524414, G Loss: 0.718109667301178\n",
      "Epoch 58, batch 185 D Loss: 1.3933460712432861, G Loss: 0.7161358594894409\n",
      "Epoch 58, batch 186 D Loss: 1.388828992843628, G Loss: 0.7266061305999756\n",
      "Epoch 58, batch 187 D Loss: 1.424849033355713, G Loss: 0.7209837436676025\n",
      "Epoch 58, batch 188 D Loss: 1.4085826873779297, G Loss: 0.7121019959449768\n",
      "Epoch 58, batch 189 D Loss: 1.4308475255966187, G Loss: 0.7108935713768005\n",
      "Epoch 58, batch 190 D Loss: 1.401498556137085, G Loss: 0.7108750343322754\n",
      "Epoch 58, batch 191 D Loss: 1.423108458518982, G Loss: 0.7119540572166443\n",
      "Epoch 58, batch 192 D Loss: 1.3962889909744263, G Loss: 0.7096209526062012\n",
      "Epoch 58, batch 193 D Loss: 1.407558798789978, G Loss: 0.7069529891014099\n",
      "Epoch 58, batch 194 D Loss: 1.4130973815917969, G Loss: 0.7095732688903809\n",
      "Epoch 58, batch 195 D Loss: 1.4054442644119263, G Loss: 0.7052958011627197\n",
      "Epoch 58, batch 196 D Loss: 1.3982808589935303, G Loss: 0.7025070190429688\n",
      "Epoch 58, batch 197 D Loss: 1.3983488082885742, G Loss: 0.7038136124610901\n",
      "Epoch 58, batch 198 D Loss: 1.4015369415283203, G Loss: 0.7014943957328796\n",
      "Epoch 58, batch 199 D Loss: 1.398369312286377, G Loss: 0.7022364735603333\n",
      "Epoch 58, batch 200 D Loss: 1.400998592376709, G Loss: 0.703639805316925\n",
      "Epoch 59, batch 1 D Loss: 1.3873302936553955, G Loss: 0.7031247019767761\n",
      "Epoch 59, batch 2 D Loss: 1.3937742710113525, G Loss: 0.7038821578025818\n",
      "Epoch 59, batch 3 D Loss: 1.399904727935791, G Loss: 0.7028285264968872\n",
      "Epoch 59, batch 4 D Loss: 1.3946025371551514, G Loss: 0.6994494795799255\n",
      "Epoch 59, batch 5 D Loss: 1.3893564939498901, G Loss: 0.6962376236915588\n",
      "Epoch 59, batch 6 D Loss: 1.3869731426239014, G Loss: 0.7008603811264038\n",
      "Epoch 59, batch 7 D Loss: 1.4122264385223389, G Loss: 0.6980431079864502\n",
      "Epoch 59, batch 8 D Loss: 1.4015958309173584, G Loss: 0.6960955262184143\n",
      "Epoch 59, batch 9 D Loss: 1.399820327758789, G Loss: 0.697934091091156\n",
      "Epoch 59, batch 10 D Loss: 1.398151159286499, G Loss: 0.6962546706199646\n",
      "Epoch 59, batch 11 D Loss: 1.3966798782348633, G Loss: 0.6949307918548584\n",
      "Epoch 59, batch 12 D Loss: 1.3978134393692017, G Loss: 0.6939154863357544\n",
      "Epoch 59, batch 13 D Loss: 1.384407639503479, G Loss: 0.6939920783042908\n",
      "Epoch 59, batch 14 D Loss: 1.3939893245697021, G Loss: 0.6934681534767151\n",
      "Epoch 59, batch 15 D Loss: 1.3842463493347168, G Loss: 0.6942917704582214\n",
      "Epoch 59, batch 16 D Loss: 1.384749412536621, G Loss: 0.6945698261260986\n",
      "Epoch 59, batch 17 D Loss: 1.3848519325256348, G Loss: 0.6949896216392517\n",
      "Epoch 59, batch 18 D Loss: 1.3895587921142578, G Loss: 0.6913613080978394\n",
      "Epoch 59, batch 19 D Loss: 1.3946800231933594, G Loss: 0.6921612620353699\n",
      "Epoch 59, batch 20 D Loss: 1.3832550048828125, G Loss: 0.6934526562690735\n",
      "Epoch 59, batch 21 D Loss: 1.3863639831542969, G Loss: 0.6916614770889282\n",
      "Epoch 59, batch 22 D Loss: 1.387263298034668, G Loss: 0.692776620388031\n",
      "Epoch 59, batch 23 D Loss: 1.3951334953308105, G Loss: 0.6916228532791138\n",
      "Epoch 59, batch 24 D Loss: 1.391662836074829, G Loss: 0.6888582706451416\n",
      "Epoch 59, batch 25 D Loss: 1.3880691528320312, G Loss: 0.6909323930740356\n",
      "Epoch 59, batch 26 D Loss: 1.391826868057251, G Loss: 0.6894640922546387\n",
      "Epoch 59, batch 27 D Loss: 1.3809616565704346, G Loss: 0.6904864311218262\n",
      "Epoch 59, batch 28 D Loss: 1.3849003314971924, G Loss: 0.6912512183189392\n",
      "Epoch 59, batch 29 D Loss: 1.3841607570648193, G Loss: 0.6874534487724304\n",
      "Epoch 59, batch 30 D Loss: 1.3834106922149658, G Loss: 0.6902284026145935\n",
      "Epoch 59, batch 31 D Loss: 1.3795714378356934, G Loss: 0.6889694929122925\n",
      "Epoch 59, batch 32 D Loss: 1.385698676109314, G Loss: 0.6882016062736511\n",
      "Epoch 59, batch 33 D Loss: 1.3826966285705566, G Loss: 0.6904444694519043\n",
      "Epoch 59, batch 34 D Loss: 1.385498285293579, G Loss: 0.6869769096374512\n",
      "Epoch 59, batch 35 D Loss: 1.3863892555236816, G Loss: 0.6876518130302429\n",
      "Epoch 59, batch 36 D Loss: 1.385772705078125, G Loss: 0.686124324798584\n",
      "Epoch 59, batch 37 D Loss: 1.3814129829406738, G Loss: 0.6868370771408081\n",
      "Epoch 59, batch 38 D Loss: 1.3774662017822266, G Loss: 0.6916148662567139\n",
      "Epoch 59, batch 39 D Loss: 1.3834691047668457, G Loss: 0.6869652271270752\n",
      "Epoch 59, batch 40 D Loss: 1.3874797821044922, G Loss: 0.6849921941757202\n",
      "Epoch 59, batch 41 D Loss: 1.3889448642730713, G Loss: 0.6853284239768982\n",
      "Epoch 59, batch 42 D Loss: 1.37949538230896, G Loss: 0.6920081973075867\n",
      "Epoch 59, batch 43 D Loss: 1.3827295303344727, G Loss: 0.6865743398666382\n",
      "Epoch 59, batch 44 D Loss: 1.3803343772888184, G Loss: 0.6830263733863831\n",
      "Epoch 59, batch 45 D Loss: 1.3879002332687378, G Loss: 0.6848440766334534\n",
      "Epoch 59, batch 46 D Loss: 1.3861968517303467, G Loss: 0.6859875321388245\n",
      "Epoch 59, batch 47 D Loss: 1.382375717163086, G Loss: 0.6883931159973145\n",
      "Epoch 59, batch 48 D Loss: 1.3807671070098877, G Loss: 0.686173677444458\n",
      "Epoch 59, batch 49 D Loss: 1.383796215057373, G Loss: 0.6849237680435181\n",
      "Epoch 59, batch 50 D Loss: 1.3795833587646484, G Loss: 0.686985194683075\n",
      "Epoch 59, batch 51 D Loss: 1.3849760293960571, G Loss: 0.6825844049453735\n",
      "Epoch 59, batch 52 D Loss: 1.3849945068359375, G Loss: 0.6834673881530762\n",
      "Epoch 59, batch 53 D Loss: 1.3758904933929443, G Loss: 0.6856387257575989\n",
      "Epoch 59, batch 54 D Loss: 1.377518653869629, G Loss: 0.6872390508651733\n",
      "Epoch 59, batch 55 D Loss: 1.384945034980774, G Loss: 0.6827264428138733\n",
      "Epoch 59, batch 56 D Loss: 1.3789294958114624, G Loss: 0.6859155297279358\n",
      "Epoch 59, batch 57 D Loss: 1.3818180561065674, G Loss: 0.686268150806427\n",
      "Epoch 59, batch 58 D Loss: 1.3730686902999878, G Loss: 0.6866748332977295\n",
      "Epoch 59, batch 59 D Loss: 1.377645492553711, G Loss: 0.6861580014228821\n",
      "Epoch 59, batch 60 D Loss: 1.3738420009613037, G Loss: 0.6867868900299072\n",
      "Epoch 59, batch 61 D Loss: 1.3809388875961304, G Loss: 0.6843827962875366\n",
      "Epoch 59, batch 62 D Loss: 1.3813893795013428, G Loss: 0.6869217157363892\n",
      "Epoch 59, batch 63 D Loss: 1.379776954650879, G Loss: 0.6842021942138672\n",
      "Epoch 59, batch 64 D Loss: 1.3862321376800537, G Loss: 0.6862234473228455\n",
      "Epoch 59, batch 65 D Loss: 1.376695990562439, G Loss: 0.6849751472473145\n",
      "Epoch 59, batch 66 D Loss: 1.3838980197906494, G Loss: 0.6813014149665833\n",
      "Epoch 59, batch 67 D Loss: 1.3845738172531128, G Loss: 0.6805891990661621\n",
      "Epoch 59, batch 68 D Loss: 1.383371114730835, G Loss: 0.6834147572517395\n",
      "Epoch 59, batch 69 D Loss: 1.3882157802581787, G Loss: 0.6831597685813904\n",
      "Epoch 59, batch 70 D Loss: 1.3770661354064941, G Loss: 0.6861583590507507\n",
      "Epoch 59, batch 71 D Loss: 1.3775577545166016, G Loss: 0.688373863697052\n",
      "Epoch 59, batch 72 D Loss: 1.375154733657837, G Loss: 0.6823889017105103\n",
      "Epoch 59, batch 73 D Loss: 1.3809635639190674, G Loss: 0.6845575571060181\n",
      "Epoch 59, batch 74 D Loss: 1.3820037841796875, G Loss: 0.6817665696144104\n",
      "Epoch 59, batch 75 D Loss: 1.3799681663513184, G Loss: 0.682634174823761\n",
      "Epoch 59, batch 76 D Loss: 1.375807762145996, G Loss: 0.6850289702415466\n",
      "Epoch 59, batch 77 D Loss: 1.373974323272705, G Loss: 0.6871380805969238\n",
      "Epoch 59, batch 78 D Loss: 1.3839831352233887, G Loss: 0.6851555109024048\n",
      "Epoch 59, batch 79 D Loss: 1.3834521770477295, G Loss: 0.681600034236908\n",
      "Epoch 59, batch 80 D Loss: 1.386730432510376, G Loss: 0.6826714277267456\n",
      "Epoch 59, batch 81 D Loss: 1.3869099617004395, G Loss: 0.68440842628479\n",
      "Epoch 59, batch 82 D Loss: 1.3826427459716797, G Loss: 0.682513952255249\n",
      "Epoch 59, batch 83 D Loss: 1.37681245803833, G Loss: 0.6791076064109802\n",
      "Epoch 59, batch 84 D Loss: 1.3866313695907593, G Loss: 0.6798450946807861\n",
      "Epoch 59, batch 85 D Loss: 1.3777456283569336, G Loss: 0.6817371845245361\n",
      "Epoch 59, batch 86 D Loss: 1.3829184770584106, G Loss: 0.6821596026420593\n",
      "Epoch 59, batch 87 D Loss: 1.3742704391479492, G Loss: 0.680471658706665\n",
      "Epoch 59, batch 88 D Loss: 1.3782840967178345, G Loss: 0.6838814616203308\n",
      "Epoch 59, batch 89 D Loss: 1.3774938583374023, G Loss: 0.6766160726547241\n",
      "Epoch 59, batch 90 D Loss: 1.3765729665756226, G Loss: 0.6856594085693359\n",
      "Epoch 59, batch 91 D Loss: 1.3808462619781494, G Loss: 0.68038010597229\n",
      "Epoch 59, batch 92 D Loss: 1.3834781646728516, G Loss: 0.6776115894317627\n",
      "Epoch 59, batch 93 D Loss: 1.3780333995819092, G Loss: 0.6852209568023682\n",
      "Epoch 59, batch 94 D Loss: 1.3842267990112305, G Loss: 0.6806322336196899\n",
      "Epoch 59, batch 95 D Loss: 1.375131368637085, G Loss: 0.6833673715591431\n",
      "Epoch 59, batch 96 D Loss: 1.378279685974121, G Loss: 0.6783066391944885\n",
      "Epoch 59, batch 97 D Loss: 1.381762146949768, G Loss: 0.68424391746521\n",
      "Epoch 59, batch 98 D Loss: 1.3794116973876953, G Loss: 0.6829343438148499\n",
      "Epoch 59, batch 99 D Loss: 1.3922091722488403, G Loss: 0.6769535541534424\n",
      "Epoch 59, batch 100 D Loss: 1.373941421508789, G Loss: 0.6848942637443542\n",
      "Epoch 59, batch 101 D Loss: 1.3798928260803223, G Loss: 0.6839207410812378\n",
      "Epoch 59, batch 102 D Loss: 1.3833222389221191, G Loss: 0.6752210259437561\n",
      "Epoch 59, batch 103 D Loss: 1.382585048675537, G Loss: 0.6715840101242065\n",
      "Epoch 59, batch 104 D Loss: 1.3825178146362305, G Loss: 0.6754414439201355\n",
      "Epoch 59, batch 105 D Loss: 1.3784918785095215, G Loss: 0.6790639758110046\n",
      "Epoch 59, batch 106 D Loss: 1.372583031654358, G Loss: 0.6847460269927979\n",
      "Epoch 59, batch 107 D Loss: 1.3751449584960938, G Loss: 0.6830950975418091\n",
      "Epoch 59, batch 108 D Loss: 1.3885624408721924, G Loss: 0.6779164671897888\n",
      "Epoch 59, batch 109 D Loss: 1.3787505626678467, G Loss: 0.6845433115959167\n",
      "Epoch 59, batch 110 D Loss: 1.3855745792388916, G Loss: 0.6811779141426086\n",
      "Epoch 59, batch 111 D Loss: 1.3902767896652222, G Loss: 0.6781960129737854\n",
      "Epoch 59, batch 112 D Loss: 1.380910873413086, G Loss: 0.6774356961250305\n",
      "Epoch 59, batch 113 D Loss: 1.387440800666809, G Loss: 0.6746629476547241\n",
      "Epoch 59, batch 114 D Loss: 1.388638973236084, G Loss: 0.6811359524726868\n",
      "Epoch 59, batch 115 D Loss: 1.383418321609497, G Loss: 0.6812520027160645\n",
      "Epoch 59, batch 116 D Loss: 1.3799700736999512, G Loss: 0.6869076490402222\n",
      "Epoch 59, batch 117 D Loss: 1.384964942932129, G Loss: 0.6839249134063721\n",
      "Epoch 59, batch 118 D Loss: 1.3859245777130127, G Loss: 0.6799378395080566\n",
      "Epoch 59, batch 119 D Loss: 1.3882496356964111, G Loss: 0.6754081845283508\n",
      "Epoch 59, batch 120 D Loss: 1.3964366912841797, G Loss: 0.6733253598213196\n",
      "Epoch 59, batch 121 D Loss: 1.3842195272445679, G Loss: 0.6811953186988831\n",
      "Epoch 59, batch 122 D Loss: 1.3814456462860107, G Loss: 0.6810016632080078\n",
      "Epoch 59, batch 123 D Loss: 1.383122444152832, G Loss: 0.6768051385879517\n",
      "Epoch 59, batch 124 D Loss: 1.3816035985946655, G Loss: 0.6863400340080261\n",
      "Epoch 59, batch 125 D Loss: 1.3761389255523682, G Loss: 0.6880573034286499\n",
      "Epoch 59, batch 126 D Loss: 1.3810458183288574, G Loss: 0.6810694932937622\n",
      "Epoch 59, batch 127 D Loss: 1.3909657001495361, G Loss: 0.6747528910636902\n",
      "Epoch 59, batch 128 D Loss: 1.3826143741607666, G Loss: 0.6840672492980957\n",
      "Epoch 59, batch 129 D Loss: 1.380295753479004, G Loss: 0.6779047250747681\n",
      "Epoch 59, batch 130 D Loss: 1.3957371711730957, G Loss: 0.6793254017829895\n",
      "Epoch 59, batch 131 D Loss: 1.39217209815979, G Loss: 0.676749587059021\n",
      "Epoch 59, batch 132 D Loss: 1.3909227848052979, G Loss: 0.6783753037452698\n",
      "Epoch 59, batch 133 D Loss: 1.3779981136322021, G Loss: 0.6799718737602234\n",
      "Epoch 59, batch 134 D Loss: 1.3789836168289185, G Loss: 0.6908924579620361\n",
      "Epoch 59, batch 135 D Loss: 1.3874073028564453, G Loss: 0.6798979043960571\n",
      "Epoch 59, batch 136 D Loss: 1.383139729499817, G Loss: 0.6881757974624634\n",
      "Epoch 59, batch 137 D Loss: 1.3853726387023926, G Loss: 0.6868008375167847\n",
      "Epoch 59, batch 138 D Loss: 1.3946218490600586, G Loss: 0.6807820200920105\n",
      "Epoch 59, batch 139 D Loss: 1.3887691497802734, G Loss: 0.6836981773376465\n",
      "Epoch 59, batch 140 D Loss: 1.3801507949829102, G Loss: 0.6834747195243835\n",
      "Epoch 59, batch 141 D Loss: 1.3904836177825928, G Loss: 0.6810434460639954\n",
      "Epoch 59, batch 142 D Loss: 1.3890358209609985, G Loss: 0.6791875958442688\n",
      "Epoch 59, batch 143 D Loss: 1.3879506587982178, G Loss: 0.6840633153915405\n",
      "Epoch 59, batch 144 D Loss: 1.3904428482055664, G Loss: 0.6819645762443542\n",
      "Epoch 59, batch 145 D Loss: 1.3828332424163818, G Loss: 0.6886918544769287\n",
      "Epoch 59, batch 146 D Loss: 1.3851854801177979, G Loss: 0.6888841986656189\n",
      "Epoch 59, batch 147 D Loss: 1.3863556385040283, G Loss: 0.6847283840179443\n",
      "Epoch 59, batch 148 D Loss: 1.389462947845459, G Loss: 0.6837356686592102\n",
      "Epoch 59, batch 149 D Loss: 1.3918449878692627, G Loss: 0.674724280834198\n",
      "Epoch 59, batch 150 D Loss: 1.3853826522827148, G Loss: 0.6861428618431091\n",
      "Epoch 59, batch 151 D Loss: 1.396558165550232, G Loss: 0.6761799454689026\n",
      "Epoch 59, batch 152 D Loss: 1.3859710693359375, G Loss: 0.6865294575691223\n",
      "Epoch 59, batch 153 D Loss: 1.3799147605895996, G Loss: 0.689330518245697\n",
      "Epoch 59, batch 154 D Loss: 1.3816990852355957, G Loss: 0.686656653881073\n",
      "Epoch 59, batch 155 D Loss: 1.3865350484848022, G Loss: 0.6834425926208496\n",
      "Epoch 59, batch 156 D Loss: 1.3835723400115967, G Loss: 0.6849338412284851\n",
      "Epoch 59, batch 157 D Loss: 1.3994897603988647, G Loss: 0.6761110424995422\n",
      "Epoch 59, batch 158 D Loss: 1.3759880065917969, G Loss: 0.6881253123283386\n",
      "Epoch 59, batch 159 D Loss: 1.3905940055847168, G Loss: 0.6856785416603088\n",
      "Epoch 59, batch 160 D Loss: 1.3708246946334839, G Loss: 0.6940574049949646\n",
      "Epoch 59, batch 161 D Loss: 1.401648759841919, G Loss: 0.6777541637420654\n",
      "Epoch 59, batch 162 D Loss: 1.4079281091690063, G Loss: 0.675685703754425\n",
      "Epoch 59, batch 163 D Loss: 1.3887677192687988, G Loss: 0.6886193156242371\n",
      "Epoch 59, batch 164 D Loss: 1.3865501880645752, G Loss: 0.6914794445037842\n",
      "Epoch 59, batch 165 D Loss: 1.4036178588867188, G Loss: 0.6859989166259766\n",
      "Epoch 59, batch 166 D Loss: 1.3939064741134644, G Loss: 0.6912767291069031\n",
      "Epoch 59, batch 167 D Loss: 1.3915308713912964, G Loss: 0.6847561001777649\n",
      "Epoch 59, batch 168 D Loss: 1.3895361423492432, G Loss: 0.6901395320892334\n",
      "Epoch 59, batch 169 D Loss: 1.3900913000106812, G Loss: 0.6880133152008057\n",
      "Epoch 59, batch 170 D Loss: 1.3923230171203613, G Loss: 0.6938969492912292\n",
      "Epoch 59, batch 171 D Loss: 1.3899078369140625, G Loss: 0.6876397728919983\n",
      "Epoch 59, batch 172 D Loss: 1.3804773092269897, G Loss: 0.6932422518730164\n",
      "Epoch 59, batch 173 D Loss: 1.3957469463348389, G Loss: 0.6876550912857056\n",
      "Epoch 59, batch 174 D Loss: 1.392126202583313, G Loss: 0.6876318454742432\n",
      "Epoch 59, batch 175 D Loss: 1.4065353870391846, G Loss: 0.6843496561050415\n",
      "Epoch 59, batch 176 D Loss: 1.396026611328125, G Loss: 0.6842026710510254\n",
      "Epoch 59, batch 177 D Loss: 1.4047420024871826, G Loss: 0.6804351806640625\n",
      "Epoch 59, batch 178 D Loss: 1.3973814249038696, G Loss: 0.6938016414642334\n",
      "Epoch 59, batch 179 D Loss: 1.3925787210464478, G Loss: 0.6868024468421936\n",
      "Epoch 59, batch 180 D Loss: 1.3902387619018555, G Loss: 0.6906359195709229\n",
      "Epoch 59, batch 181 D Loss: 1.3885412216186523, G Loss: 0.6957833170890808\n",
      "Epoch 59, batch 182 D Loss: 1.3847144842147827, G Loss: 0.6946141123771667\n",
      "Epoch 59, batch 183 D Loss: 1.3929386138916016, G Loss: 0.691724419593811\n",
      "Epoch 59, batch 184 D Loss: 1.3921222686767578, G Loss: 0.6907029747962952\n",
      "Epoch 59, batch 185 D Loss: 1.392223834991455, G Loss: 0.6977995038032532\n",
      "Epoch 59, batch 186 D Loss: 1.3918108940124512, G Loss: 0.6948762536048889\n",
      "Epoch 59, batch 187 D Loss: 1.400406837463379, G Loss: 0.6867035031318665\n",
      "Epoch 59, batch 188 D Loss: 1.393489122390747, G Loss: 0.6985664963722229\n",
      "Epoch 59, batch 189 D Loss: 1.401885747909546, G Loss: 0.6885294318199158\n",
      "Epoch 59, batch 190 D Loss: 1.3936281204223633, G Loss: 0.6876626014709473\n",
      "Epoch 59, batch 191 D Loss: 1.4002528190612793, G Loss: 0.6862297058105469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, batch 192 D Loss: 1.3941528797149658, G Loss: 0.6971475481987\n",
      "Epoch 59, batch 193 D Loss: 1.3904738426208496, G Loss: 0.6962652802467346\n",
      "Epoch 59, batch 194 D Loss: 1.3908498287200928, G Loss: 0.7021927833557129\n",
      "Epoch 59, batch 195 D Loss: 1.385550856590271, G Loss: 0.6998216509819031\n",
      "Epoch 59, batch 196 D Loss: 1.3923158645629883, G Loss: 0.6946994662284851\n",
      "Epoch 59, batch 197 D Loss: 1.3982155323028564, G Loss: 0.6951435208320618\n",
      "Epoch 59, batch 198 D Loss: 1.3902103900909424, G Loss: 0.6870203614234924\n",
      "Epoch 59, batch 199 D Loss: 1.388725996017456, G Loss: 0.7002294063568115\n",
      "Epoch 59, batch 200 D Loss: 1.3853846788406372, G Loss: 0.70624178647995\n",
      "Epoch 60, batch 1 D Loss: 1.393191933631897, G Loss: 0.6931970119476318\n",
      "Epoch 60, batch 2 D Loss: 1.3943150043487549, G Loss: 0.6942928433418274\n",
      "Epoch 60, batch 3 D Loss: 1.3943501710891724, G Loss: 0.6939147710800171\n",
      "Epoch 60, batch 4 D Loss: 1.4003794193267822, G Loss: 0.6998053193092346\n",
      "Epoch 60, batch 5 D Loss: 1.3926923274993896, G Loss: 0.6929306983947754\n",
      "Epoch 60, batch 6 D Loss: 1.3979709148406982, G Loss: 0.6901887655258179\n",
      "Epoch 60, batch 7 D Loss: 1.4016907215118408, G Loss: 0.694455087184906\n",
      "Epoch 60, batch 8 D Loss: 1.396734595298767, G Loss: 0.694668173789978\n",
      "Epoch 60, batch 9 D Loss: 1.3965895175933838, G Loss: 0.6875869631767273\n",
      "Epoch 60, batch 10 D Loss: 1.3965680599212646, G Loss: 0.688998818397522\n",
      "Epoch 60, batch 11 D Loss: 1.3892371654510498, G Loss: 0.695996105670929\n",
      "Epoch 60, batch 12 D Loss: 1.3894643783569336, G Loss: 0.6952918767929077\n",
      "Epoch 60, batch 13 D Loss: 1.3989856243133545, G Loss: 0.6949784755706787\n",
      "Epoch 60, batch 14 D Loss: 1.4007375240325928, G Loss: 0.6901809573173523\n",
      "Epoch 60, batch 15 D Loss: 1.3869001865386963, G Loss: 0.6967015862464905\n",
      "Epoch 60, batch 16 D Loss: 1.385852575302124, G Loss: 0.7018600702285767\n",
      "Epoch 60, batch 17 D Loss: 1.3874387741088867, G Loss: 0.6969698071479797\n",
      "Epoch 60, batch 18 D Loss: 1.3937277793884277, G Loss: 0.6971086263656616\n",
      "Epoch 60, batch 19 D Loss: 1.3962182998657227, G Loss: 0.6895579695701599\n",
      "Epoch 60, batch 20 D Loss: 1.392472505569458, G Loss: 0.6944621205329895\n",
      "Epoch 60, batch 21 D Loss: 1.3837409019470215, G Loss: 0.7019977569580078\n",
      "Epoch 60, batch 22 D Loss: 1.3906354904174805, G Loss: 0.692851722240448\n",
      "Epoch 60, batch 23 D Loss: 1.3935928344726562, G Loss: 0.6914913058280945\n",
      "Epoch 60, batch 24 D Loss: 1.3837376832962036, G Loss: 0.6933814287185669\n",
      "Epoch 60, batch 25 D Loss: 1.3853694200515747, G Loss: 0.7084325551986694\n",
      "Epoch 60, batch 26 D Loss: 1.384117603302002, G Loss: 0.6985579133033752\n",
      "Epoch 60, batch 27 D Loss: 1.3956917524337769, G Loss: 0.6943217515945435\n",
      "Epoch 60, batch 28 D Loss: 1.3829220533370972, G Loss: 0.6992717981338501\n",
      "Epoch 60, batch 29 D Loss: 1.385772943496704, G Loss: 0.6982587575912476\n",
      "Epoch 60, batch 30 D Loss: 1.394419550895691, G Loss: 0.6903423070907593\n",
      "Epoch 60, batch 31 D Loss: 1.3799586296081543, G Loss: 0.7021234631538391\n",
      "Epoch 60, batch 32 D Loss: 1.3860777616500854, G Loss: 0.6958901286125183\n",
      "Epoch 60, batch 33 D Loss: 1.378812551498413, G Loss: 0.696816086769104\n",
      "Epoch 60, batch 34 D Loss: 1.394269585609436, G Loss: 0.7006151676177979\n",
      "Epoch 60, batch 35 D Loss: 1.3880627155303955, G Loss: 0.6893208026885986\n",
      "Epoch 60, batch 36 D Loss: 1.3782531023025513, G Loss: 0.69669508934021\n",
      "Epoch 60, batch 37 D Loss: 1.382106065750122, G Loss: 0.6932280659675598\n",
      "Epoch 60, batch 38 D Loss: 1.389890193939209, G Loss: 0.688311755657196\n",
      "Epoch 60, batch 39 D Loss: 1.390932559967041, G Loss: 0.6883985996246338\n",
      "Epoch 60, batch 40 D Loss: 1.378010630607605, G Loss: 0.7039486765861511\n",
      "Epoch 60, batch 41 D Loss: 1.3696218729019165, G Loss: 0.705331027507782\n",
      "Epoch 60, batch 42 D Loss: 1.381441354751587, G Loss: 0.6960482001304626\n",
      "Epoch 60, batch 43 D Loss: 1.391601800918579, G Loss: 0.6890806555747986\n",
      "Epoch 60, batch 44 D Loss: 1.372622013092041, G Loss: 0.7055132985115051\n",
      "Epoch 60, batch 45 D Loss: 1.3876112699508667, G Loss: 0.6926420331001282\n",
      "Epoch 60, batch 46 D Loss: 1.3830156326293945, G Loss: 0.7068789601325989\n",
      "Epoch 60, batch 47 D Loss: 1.3799529075622559, G Loss: 0.6974965929985046\n",
      "Epoch 60, batch 48 D Loss: 1.3764859437942505, G Loss: 0.6951926946640015\n",
      "Epoch 60, batch 49 D Loss: 1.3824442625045776, G Loss: 0.6947000026702881\n",
      "Epoch 60, batch 50 D Loss: 1.3818984031677246, G Loss: 0.7013177275657654\n",
      "Epoch 60, batch 51 D Loss: 1.3704407215118408, G Loss: 0.6928417086601257\n",
      "Epoch 60, batch 52 D Loss: 1.3837480545043945, G Loss: 0.6956195831298828\n",
      "Epoch 60, batch 53 D Loss: 1.3818378448486328, G Loss: 0.694374144077301\n",
      "Epoch 60, batch 54 D Loss: 1.3851475715637207, G Loss: 0.6910906434059143\n",
      "Epoch 60, batch 55 D Loss: 1.3696047067642212, G Loss: 0.7056878805160522\n",
      "Epoch 60, batch 56 D Loss: 1.3824090957641602, G Loss: 0.6942726969718933\n",
      "Epoch 60, batch 57 D Loss: 1.3782329559326172, G Loss: 0.6869075298309326\n",
      "Epoch 60, batch 58 D Loss: 1.3673251867294312, G Loss: 0.697813093662262\n",
      "Epoch 60, batch 59 D Loss: 1.3679288625717163, G Loss: 0.6949684023857117\n",
      "Epoch 60, batch 60 D Loss: 1.3768285512924194, G Loss: 0.6981697678565979\n",
      "Epoch 60, batch 61 D Loss: 1.3882391452789307, G Loss: 0.6945825815200806\n",
      "Epoch 60, batch 62 D Loss: 1.3529667854309082, G Loss: 0.714937686920166\n",
      "Epoch 60, batch 63 D Loss: 1.3753490447998047, G Loss: 0.7064242362976074\n",
      "Epoch 60, batch 64 D Loss: 1.3914546966552734, G Loss: 0.6850801110267639\n",
      "Epoch 60, batch 65 D Loss: 1.379441261291504, G Loss: 0.6803641319274902\n",
      "Epoch 60, batch 66 D Loss: 1.3666657209396362, G Loss: 0.699393630027771\n",
      "Epoch 60, batch 67 D Loss: 1.3853862285614014, G Loss: 0.6891205310821533\n",
      "Epoch 60, batch 68 D Loss: 1.3707855939865112, G Loss: 0.693970799446106\n",
      "Epoch 60, batch 69 D Loss: 1.3767998218536377, G Loss: 0.7056925892829895\n",
      "Epoch 60, batch 70 D Loss: 1.3639395236968994, G Loss: 0.7012922167778015\n",
      "Epoch 60, batch 71 D Loss: 1.3893946409225464, G Loss: 0.6858616471290588\n",
      "Epoch 60, batch 72 D Loss: 1.379166841506958, G Loss: 0.6936911940574646\n",
      "Epoch 60, batch 73 D Loss: 1.3830665349960327, G Loss: 0.6945081353187561\n",
      "Epoch 60, batch 74 D Loss: 1.3826472759246826, G Loss: 0.6807716488838196\n",
      "Epoch 60, batch 75 D Loss: 1.3780938386917114, G Loss: 0.7029650211334229\n",
      "Epoch 60, batch 76 D Loss: 1.384575366973877, G Loss: 0.6871654987335205\n",
      "Epoch 60, batch 77 D Loss: 1.3821368217468262, G Loss: 0.6949114203453064\n",
      "Epoch 60, batch 78 D Loss: 1.3781758546829224, G Loss: 0.703441321849823\n",
      "Epoch 60, batch 79 D Loss: 1.3708432912826538, G Loss: 0.6997193098068237\n",
      "Epoch 60, batch 80 D Loss: 1.3699710369110107, G Loss: 0.707899272441864\n",
      "Epoch 60, batch 81 D Loss: 1.371415615081787, G Loss: 0.7017884254455566\n",
      "Epoch 60, batch 82 D Loss: 1.3767037391662598, G Loss: 0.6947383284568787\n",
      "Epoch 60, batch 83 D Loss: 1.3940277099609375, G Loss: 0.6849001049995422\n",
      "Epoch 60, batch 84 D Loss: 1.3842653036117554, G Loss: 0.6881960034370422\n",
      "Epoch 60, batch 85 D Loss: 1.375701665878296, G Loss: 0.7004885673522949\n",
      "Epoch 60, batch 86 D Loss: 1.3832167387008667, G Loss: 0.6963117122650146\n",
      "Epoch 60, batch 87 D Loss: 1.380439043045044, G Loss: 0.6916964650154114\n",
      "Epoch 60, batch 88 D Loss: 1.3932881355285645, G Loss: 0.6850314140319824\n",
      "Epoch 60, batch 89 D Loss: 1.3600382804870605, G Loss: 0.708503782749176\n",
      "Epoch 60, batch 90 D Loss: 1.3756576776504517, G Loss: 0.6946335434913635\n",
      "Epoch 60, batch 91 D Loss: 1.3617587089538574, G Loss: 0.7323132157325745\n",
      "Epoch 60, batch 92 D Loss: 1.3876044750213623, G Loss: 0.696689784526825\n",
      "Epoch 60, batch 93 D Loss: 1.3899126052856445, G Loss: 0.7041571140289307\n",
      "Epoch 60, batch 94 D Loss: 1.3922696113586426, G Loss: 0.6998451352119446\n",
      "Epoch 60, batch 95 D Loss: 1.3779232501983643, G Loss: 0.7061861157417297\n",
      "Epoch 60, batch 96 D Loss: 1.3640378713607788, G Loss: 0.7135083675384521\n",
      "Epoch 60, batch 97 D Loss: 1.3974254131317139, G Loss: 0.6991273760795593\n",
      "Epoch 60, batch 98 D Loss: 1.3725425004959106, G Loss: 0.7081701159477234\n",
      "Epoch 60, batch 99 D Loss: 1.374515175819397, G Loss: 0.7065578699111938\n",
      "Epoch 60, batch 100 D Loss: 1.374016523361206, G Loss: 0.7309338450431824\n",
      "Epoch 60, batch 101 D Loss: 1.3764625787734985, G Loss: 0.7124654650688171\n",
      "Epoch 60, batch 102 D Loss: 1.3677581548690796, G Loss: 0.7293321490287781\n",
      "Epoch 60, batch 103 D Loss: 1.3672016859054565, G Loss: 0.7142153978347778\n",
      "Epoch 60, batch 104 D Loss: 1.3975005149841309, G Loss: 0.7107406854629517\n",
      "Epoch 60, batch 105 D Loss: 1.3971819877624512, G Loss: 0.705517053604126\n",
      "Epoch 60, batch 106 D Loss: 1.3872194290161133, G Loss: 0.7082595825195312\n",
      "Epoch 60, batch 107 D Loss: 1.379366397857666, G Loss: 0.7167462706565857\n",
      "Epoch 60, batch 108 D Loss: 1.3858896493911743, G Loss: 0.7083642482757568\n",
      "Epoch 60, batch 109 D Loss: 1.3817511796951294, G Loss: 0.7291821241378784\n",
      "Epoch 60, batch 110 D Loss: 1.3828380107879639, G Loss: 0.7083001136779785\n",
      "Epoch 60, batch 111 D Loss: 1.3748921155929565, G Loss: 0.7480130195617676\n",
      "Epoch 60, batch 112 D Loss: 1.3765192031860352, G Loss: 0.7289493680000305\n",
      "Epoch 60, batch 113 D Loss: 1.3824241161346436, G Loss: 0.7209271192550659\n",
      "Epoch 60, batch 114 D Loss: 1.3706929683685303, G Loss: 0.7236970663070679\n",
      "Epoch 60, batch 115 D Loss: 1.3716537952423096, G Loss: 0.7415971159934998\n",
      "Epoch 60, batch 116 D Loss: 1.3815714120864868, G Loss: 0.7202219367027283\n",
      "Epoch 60, batch 117 D Loss: 1.394500732421875, G Loss: 0.7024862766265869\n",
      "Epoch 60, batch 118 D Loss: 1.387744426727295, G Loss: 0.7272066473960876\n",
      "Epoch 60, batch 119 D Loss: 1.398809790611267, G Loss: 0.7066845893859863\n",
      "Epoch 60, batch 120 D Loss: 1.392029047012329, G Loss: 0.7405186295509338\n",
      "Epoch 60, batch 121 D Loss: 1.387623906135559, G Loss: 0.7212282419204712\n",
      "Epoch 60, batch 122 D Loss: 1.374901533126831, G Loss: 0.7203264832496643\n",
      "Epoch 60, batch 123 D Loss: 1.3863797187805176, G Loss: 0.7019719481468201\n",
      "Epoch 60, batch 124 D Loss: 1.3769586086273193, G Loss: 0.7046996355056763\n",
      "Epoch 60, batch 125 D Loss: 1.390343189239502, G Loss: 0.6968392729759216\n",
      "Epoch 60, batch 126 D Loss: 1.40327787399292, G Loss: 0.6923890709877014\n",
      "Epoch 60, batch 127 D Loss: 1.4124423265457153, G Loss: 0.6931710243225098\n",
      "Epoch 60, batch 128 D Loss: 1.4004828929901123, G Loss: 0.6998760104179382\n",
      "Epoch 60, batch 129 D Loss: 1.410783290863037, G Loss: 0.6901704668998718\n",
      "Epoch 60, batch 130 D Loss: 1.4031128883361816, G Loss: 0.6926618218421936\n",
      "Epoch 60, batch 131 D Loss: 1.37130868434906, G Loss: 0.6957197785377502\n",
      "Epoch 60, batch 132 D Loss: 1.3804937601089478, G Loss: 0.6949276924133301\n",
      "Epoch 60, batch 133 D Loss: 1.3961267471313477, G Loss: 0.6805368661880493\n",
      "Epoch 60, batch 134 D Loss: 1.4014201164245605, G Loss: 0.6842958331108093\n",
      "Epoch 60, batch 135 D Loss: 1.3817298412322998, G Loss: 0.6843459606170654\n",
      "Epoch 60, batch 136 D Loss: 1.3784105777740479, G Loss: 0.6912750005722046\n",
      "Epoch 60, batch 137 D Loss: 1.3991507291793823, G Loss: 0.6781689524650574\n",
      "Epoch 60, batch 138 D Loss: 1.3923537731170654, G Loss: 0.6806013584136963\n",
      "Epoch 60, batch 139 D Loss: 1.4076809883117676, G Loss: 0.684360146522522\n",
      "Epoch 60, batch 140 D Loss: 1.4012856483459473, G Loss: 0.6815815567970276\n",
      "Epoch 60, batch 141 D Loss: 1.401336669921875, G Loss: 0.6825602650642395\n",
      "Epoch 60, batch 142 D Loss: 1.3910619020462036, G Loss: 0.681606650352478\n",
      "Epoch 60, batch 143 D Loss: 1.3881430625915527, G Loss: 0.675926685333252\n",
      "Epoch 60, batch 144 D Loss: 1.380179762840271, G Loss: 0.6865659356117249\n",
      "Epoch 60, batch 145 D Loss: 1.3986010551452637, G Loss: 0.6735539436340332\n",
      "Epoch 60, batch 146 D Loss: 1.3898980617523193, G Loss: 0.6737709045410156\n",
      "Epoch 60, batch 147 D Loss: 1.404818058013916, G Loss: 0.6716271042823792\n",
      "Epoch 60, batch 148 D Loss: 1.3952016830444336, G Loss: 0.6716681122779846\n",
      "Epoch 60, batch 149 D Loss: 1.4040385484695435, G Loss: 0.6677200198173523\n",
      "Epoch 60, batch 150 D Loss: 1.3779312372207642, G Loss: 0.6743597388267517\n",
      "Epoch 60, batch 151 D Loss: 1.3823306560516357, G Loss: 0.6741254925727844\n",
      "Epoch 60, batch 152 D Loss: 1.3827388286590576, G Loss: 0.676588237285614\n",
      "Epoch 60, batch 153 D Loss: 1.3881287574768066, G Loss: 0.673707902431488\n",
      "Epoch 60, batch 154 D Loss: 1.3875939846038818, G Loss: 0.6672714352607727\n",
      "Epoch 60, batch 155 D Loss: 1.3740915060043335, G Loss: 0.6780653595924377\n",
      "Epoch 60, batch 156 D Loss: 1.3742504119873047, G Loss: 0.6751880049705505\n",
      "Epoch 60, batch 157 D Loss: 1.3722472190856934, G Loss: 0.6772264242172241\n",
      "Epoch 60, batch 158 D Loss: 1.376344084739685, G Loss: 0.6750949025154114\n",
      "Epoch 60, batch 159 D Loss: 1.3725653886795044, G Loss: 0.672939658164978\n",
      "Epoch 60, batch 160 D Loss: 1.3726046085357666, G Loss: 0.672681987285614\n",
      "Epoch 60, batch 161 D Loss: 1.3798000812530518, G Loss: 0.6717362403869629\n",
      "Epoch 60, batch 162 D Loss: 1.3777161836624146, G Loss: 0.6662930250167847\n",
      "Epoch 60, batch 163 D Loss: 1.3786451816558838, G Loss: 0.6734703183174133\n",
      "Epoch 60, batch 164 D Loss: 1.3829607963562012, G Loss: 0.672009289264679\n",
      "Epoch 60, batch 165 D Loss: 1.3808646202087402, G Loss: 0.6746295690536499\n",
      "Epoch 60, batch 166 D Loss: 1.391475796699524, G Loss: 0.6703963279724121\n",
      "Epoch 60, batch 167 D Loss: 1.3707058429718018, G Loss: 0.6736290454864502\n",
      "Epoch 60, batch 168 D Loss: 1.3904292583465576, G Loss: 0.6673200726509094\n",
      "Epoch 60, batch 169 D Loss: 1.3571439981460571, G Loss: 0.6751207113265991\n",
      "Epoch 60, batch 170 D Loss: 1.3745670318603516, G Loss: 0.6741155982017517\n",
      "Epoch 60, batch 171 D Loss: 1.3761272430419922, G Loss: 0.6717692613601685\n",
      "Epoch 60, batch 172 D Loss: 1.37080717086792, G Loss: 0.6747934818267822\n",
      "Epoch 60, batch 173 D Loss: 1.3721158504486084, G Loss: 0.6788201928138733\n",
      "Epoch 60, batch 174 D Loss: 1.3770458698272705, G Loss: 0.6735225915908813\n",
      "Epoch 60, batch 175 D Loss: 1.3975825309753418, G Loss: 0.6584405303001404\n",
      "Epoch 60, batch 176 D Loss: 1.3789232969284058, G Loss: 0.6783627271652222\n",
      "Epoch 60, batch 177 D Loss: 1.3900210857391357, G Loss: 0.6637551784515381\n",
      "Epoch 60, batch 178 D Loss: 1.3967375755310059, G Loss: 0.669140636920929\n",
      "Epoch 60, batch 179 D Loss: 1.3789012432098389, G Loss: 0.6787987351417542\n",
      "Epoch 60, batch 180 D Loss: 1.3860864639282227, G Loss: 0.6736657619476318\n",
      "Epoch 60, batch 181 D Loss: 1.3819141387939453, G Loss: 0.6755435466766357\n",
      "Epoch 60, batch 182 D Loss: 1.34806489944458, G Loss: 0.6904272437095642\n",
      "Epoch 60, batch 183 D Loss: 1.4093883037567139, G Loss: 0.6639712452888489\n",
      "Epoch 60, batch 184 D Loss: 1.3900716304779053, G Loss: 0.6681823134422302\n",
      "Epoch 60, batch 185 D Loss: 1.4045565128326416, G Loss: 0.6722954511642456\n",
      "Epoch 60, batch 186 D Loss: 1.3925285339355469, G Loss: 0.6722182631492615\n",
      "Epoch 60, batch 187 D Loss: 1.3899261951446533, G Loss: 0.6749626994132996\n",
      "Epoch 60, batch 188 D Loss: 1.3838849067687988, G Loss: 0.6788229942321777\n",
      "Epoch 60, batch 189 D Loss: 1.394358515739441, G Loss: 0.666401743888855\n",
      "Epoch 60, batch 190 D Loss: 1.3806838989257812, G Loss: 0.685676097869873\n",
      "Epoch 60, batch 191 D Loss: 1.3844845294952393, G Loss: 0.6751390695571899\n",
      "Epoch 60, batch 192 D Loss: 1.3851492404937744, G Loss: 0.6801959872245789\n",
      "Epoch 60, batch 193 D Loss: 1.3923225402832031, G Loss: 0.6749598979949951\n",
      "Epoch 60, batch 194 D Loss: 1.3868736028671265, G Loss: 0.6801069378852844\n",
      "Epoch 60, batch 195 D Loss: 1.397686243057251, G Loss: 0.6719915866851807\n",
      "Epoch 60, batch 196 D Loss: 1.3891658782958984, G Loss: 0.6816191673278809\n",
      "Epoch 60, batch 197 D Loss: 1.4027835130691528, G Loss: 0.6657113432884216\n",
      "Epoch 60, batch 198 D Loss: 1.3985079526901245, G Loss: 0.6777397990226746\n",
      "Epoch 60, batch 199 D Loss: 1.3859567642211914, G Loss: 0.6837841868400574\n",
      "Epoch 60, batch 200 D Loss: 1.4050559997558594, G Loss: 0.6796373724937439\n",
      "Epoch 61, batch 1 D Loss: 1.3790005445480347, G Loss: 0.6870758533477783\n",
      "Epoch 61, batch 2 D Loss: 1.4094494581222534, G Loss: 0.677797794342041\n",
      "Epoch 61, batch 3 D Loss: 1.3883051872253418, G Loss: 0.6838181018829346\n",
      "Epoch 61, batch 4 D Loss: 1.3873648643493652, G Loss: 0.6885032653808594\n",
      "Epoch 61, batch 5 D Loss: 1.3831510543823242, G Loss: 0.6856313347816467\n",
      "Epoch 61, batch 6 D Loss: 1.3864994049072266, G Loss: 0.6813416481018066\n",
      "Epoch 61, batch 7 D Loss: 1.3966820240020752, G Loss: 0.6776988506317139\n",
      "Epoch 61, batch 8 D Loss: 1.375577688217163, G Loss: 0.687537431716919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, batch 9 D Loss: 1.3947489261627197, G Loss: 0.68477463722229\n",
      "Epoch 61, batch 10 D Loss: 1.388521671295166, G Loss: 0.6872716546058655\n",
      "Epoch 61, batch 11 D Loss: 1.3917908668518066, G Loss: 0.6933280229568481\n",
      "Epoch 61, batch 12 D Loss: 1.3966429233551025, G Loss: 0.6797812581062317\n",
      "Epoch 61, batch 13 D Loss: 1.4017760753631592, G Loss: 0.6863732933998108\n",
      "Epoch 61, batch 14 D Loss: 1.409698247909546, G Loss: 0.6903203725814819\n",
      "Epoch 61, batch 15 D Loss: 1.3961403369903564, G Loss: 0.6849732398986816\n",
      "Epoch 61, batch 16 D Loss: 1.3925089836120605, G Loss: 0.6881200671195984\n",
      "Epoch 61, batch 17 D Loss: 1.3998650312423706, G Loss: 0.6877301931381226\n",
      "Epoch 61, batch 18 D Loss: 1.3909512758255005, G Loss: 0.6929894089698792\n",
      "Epoch 61, batch 19 D Loss: 1.3735382556915283, G Loss: 0.6996713876724243\n",
      "Epoch 61, batch 20 D Loss: 1.3858165740966797, G Loss: 0.6941595673561096\n",
      "Epoch 61, batch 21 D Loss: 1.395938754081726, G Loss: 0.6896086931228638\n",
      "Epoch 61, batch 22 D Loss: 1.3895312547683716, G Loss: 0.6891645193099976\n",
      "Epoch 61, batch 23 D Loss: 1.3812748193740845, G Loss: 0.6998741626739502\n",
      "Epoch 61, batch 24 D Loss: 1.391505241394043, G Loss: 0.6956943273544312\n",
      "Epoch 61, batch 25 D Loss: 1.385289192199707, G Loss: 0.6985254883766174\n",
      "Epoch 61, batch 26 D Loss: 1.378586769104004, G Loss: 0.6994307041168213\n",
      "Epoch 61, batch 27 D Loss: 1.382917881011963, G Loss: 0.6986575126647949\n",
      "Epoch 61, batch 28 D Loss: 1.3694498538970947, G Loss: 0.7058089375495911\n",
      "Epoch 61, batch 29 D Loss: 1.376415491104126, G Loss: 0.7008667588233948\n",
      "Epoch 61, batch 30 D Loss: 1.3671369552612305, G Loss: 0.704048216342926\n",
      "Epoch 61, batch 31 D Loss: 1.3873060941696167, G Loss: 0.6936612129211426\n",
      "Epoch 61, batch 32 D Loss: 1.3808109760284424, G Loss: 0.7004292011260986\n",
      "Epoch 61, batch 33 D Loss: 1.3999104499816895, G Loss: 0.6926974654197693\n",
      "Epoch 61, batch 34 D Loss: 1.3761847019195557, G Loss: 0.7071402072906494\n",
      "Epoch 61, batch 35 D Loss: 1.375275731086731, G Loss: 0.698707103729248\n",
      "Epoch 61, batch 36 D Loss: 1.3663115501403809, G Loss: 0.7115505933761597\n",
      "Epoch 61, batch 37 D Loss: 1.3777329921722412, G Loss: 0.7068726420402527\n",
      "Epoch 61, batch 38 D Loss: 1.380772352218628, G Loss: 0.7000097036361694\n",
      "Epoch 61, batch 39 D Loss: 1.373023509979248, G Loss: 0.7057381272315979\n",
      "Epoch 61, batch 40 D Loss: 1.3729581832885742, G Loss: 0.7118712067604065\n",
      "Epoch 61, batch 41 D Loss: 1.3848798274993896, G Loss: 0.7030163407325745\n",
      "Epoch 61, batch 42 D Loss: 1.3711546659469604, G Loss: 0.7123454809188843\n",
      "Epoch 61, batch 43 D Loss: 1.381628155708313, G Loss: 0.70367032289505\n",
      "Epoch 61, batch 44 D Loss: 1.3764698505401611, G Loss: 0.7081640362739563\n",
      "Epoch 61, batch 45 D Loss: 1.3744181394577026, G Loss: 0.7109937071800232\n",
      "Epoch 61, batch 46 D Loss: 1.3771328926086426, G Loss: 0.7160018682479858\n",
      "Epoch 61, batch 47 D Loss: 1.3832032680511475, G Loss: 0.7082713842391968\n",
      "Epoch 61, batch 48 D Loss: 1.374058485031128, G Loss: 0.7116056680679321\n",
      "Epoch 61, batch 49 D Loss: 1.3565376996994019, G Loss: 0.7120510935783386\n",
      "Epoch 61, batch 50 D Loss: 1.3604451417922974, G Loss: 0.7150601148605347\n",
      "Epoch 61, batch 51 D Loss: 1.3537940979003906, G Loss: 0.7287797331809998\n",
      "Epoch 61, batch 52 D Loss: 1.36127507686615, G Loss: 0.7219053506851196\n",
      "Epoch 61, batch 53 D Loss: 1.3636345863342285, G Loss: 0.7275229692459106\n",
      "Epoch 61, batch 54 D Loss: 1.3668978214263916, G Loss: 0.718249499797821\n",
      "Epoch 61, batch 55 D Loss: 1.3670985698699951, G Loss: 0.7265394330024719\n",
      "Epoch 61, batch 56 D Loss: 1.3671886920928955, G Loss: 0.7241377830505371\n",
      "Epoch 61, batch 57 D Loss: 1.3616108894348145, G Loss: 0.7311599254608154\n",
      "Epoch 61, batch 58 D Loss: 1.3638360500335693, G Loss: 0.7224987149238586\n",
      "Epoch 61, batch 59 D Loss: 1.352107048034668, G Loss: 0.7290185689926147\n",
      "Epoch 61, batch 60 D Loss: 1.3502423763275146, G Loss: 0.7235565781593323\n",
      "Epoch 61, batch 61 D Loss: 1.356454849243164, G Loss: 0.728376030921936\n",
      "Epoch 61, batch 62 D Loss: 1.3687140941619873, G Loss: 0.7221065759658813\n",
      "Epoch 61, batch 63 D Loss: 1.3754432201385498, G Loss: 0.7203861474990845\n",
      "Epoch 61, batch 64 D Loss: 1.3563717603683472, G Loss: 0.7327889800071716\n",
      "Epoch 61, batch 65 D Loss: 1.3541007041931152, G Loss: 0.7212551832199097\n",
      "Epoch 61, batch 66 D Loss: 1.3589816093444824, G Loss: 0.7125145196914673\n",
      "Epoch 61, batch 67 D Loss: 1.366281509399414, G Loss: 0.7188158631324768\n",
      "Epoch 61, batch 68 D Loss: 1.3473849296569824, G Loss: 0.7316608428955078\n",
      "Epoch 61, batch 69 D Loss: 1.3379170894622803, G Loss: 0.7305452227592468\n",
      "Epoch 61, batch 70 D Loss: 1.3379549980163574, G Loss: 0.726601243019104\n",
      "Epoch 61, batch 71 D Loss: 1.3516905307769775, G Loss: 0.7264111042022705\n",
      "Epoch 61, batch 72 D Loss: 1.356539011001587, G Loss: 0.7243521213531494\n",
      "Epoch 61, batch 73 D Loss: 1.342402696609497, G Loss: 0.7229409217834473\n",
      "Epoch 61, batch 74 D Loss: 1.368708610534668, G Loss: 0.7142741680145264\n",
      "Epoch 61, batch 75 D Loss: 1.3661221265792847, G Loss: 0.7201635837554932\n",
      "Epoch 61, batch 76 D Loss: 1.3637595176696777, G Loss: 0.7194213271141052\n",
      "Epoch 61, batch 77 D Loss: 1.3502328395843506, G Loss: 0.7170323133468628\n",
      "Epoch 61, batch 78 D Loss: 1.376672387123108, G Loss: 0.7165651917457581\n",
      "Epoch 61, batch 79 D Loss: 1.353355884552002, G Loss: 0.7274749875068665\n",
      "Epoch 61, batch 80 D Loss: 1.3512797355651855, G Loss: 0.7246044874191284\n",
      "Epoch 61, batch 81 D Loss: 1.3805139064788818, G Loss: 0.698456883430481\n",
      "Epoch 61, batch 82 D Loss: 1.3908717632293701, G Loss: 0.6999791860580444\n",
      "Epoch 61, batch 83 D Loss: 1.359421968460083, G Loss: 0.7154840230941772\n",
      "Epoch 61, batch 84 D Loss: 1.3578379154205322, G Loss: 0.7120422124862671\n",
      "Epoch 61, batch 85 D Loss: 1.3783499002456665, G Loss: 0.6922259330749512\n",
      "Epoch 61, batch 86 D Loss: 1.357296347618103, G Loss: 0.6929777264595032\n",
      "Epoch 61, batch 87 D Loss: 1.3470115661621094, G Loss: 0.7003024220466614\n",
      "Epoch 61, batch 88 D Loss: 1.3713631629943848, G Loss: 0.7179628014564514\n",
      "Epoch 61, batch 89 D Loss: 1.39191734790802, G Loss: 0.6923558712005615\n",
      "Epoch 61, batch 90 D Loss: 1.3804088830947876, G Loss: 0.7058385610580444\n",
      "Epoch 61, batch 91 D Loss: 1.3601285219192505, G Loss: 0.698926568031311\n",
      "Epoch 61, batch 92 D Loss: 1.3829822540283203, G Loss: 0.697886049747467\n",
      "Epoch 61, batch 93 D Loss: 1.4052773714065552, G Loss: 0.6840220093727112\n",
      "Epoch 61, batch 94 D Loss: 1.4007413387298584, G Loss: 0.6813796162605286\n",
      "Epoch 61, batch 95 D Loss: 1.3897027969360352, G Loss: 0.6805139780044556\n",
      "Epoch 61, batch 96 D Loss: 1.4063701629638672, G Loss: 0.6905523538589478\n",
      "Epoch 61, batch 97 D Loss: 1.3651713132858276, G Loss: 0.6984301805496216\n",
      "Epoch 61, batch 98 D Loss: 1.3891745805740356, G Loss: 0.6859617829322815\n",
      "Epoch 61, batch 99 D Loss: 1.4223729372024536, G Loss: 0.6675031185150146\n",
      "Epoch 61, batch 100 D Loss: 1.4164092540740967, G Loss: 0.6562141180038452\n",
      "Epoch 61, batch 101 D Loss: 1.4154622554779053, G Loss: 0.6840194463729858\n",
      "Epoch 61, batch 102 D Loss: 1.442997694015503, G Loss: 0.6353481411933899\n",
      "Epoch 61, batch 103 D Loss: 1.392806887626648, G Loss: 0.6737164258956909\n",
      "Epoch 61, batch 104 D Loss: 1.412067174911499, G Loss: 0.6664767265319824\n",
      "Epoch 61, batch 105 D Loss: 1.4536324739456177, G Loss: 0.6590865850448608\n",
      "Epoch 61, batch 106 D Loss: 1.4412751197814941, G Loss: 0.670962393283844\n",
      "Epoch 61, batch 107 D Loss: 1.4251062870025635, G Loss: 0.673108696937561\n",
      "Epoch 61, batch 108 D Loss: 1.4491628408432007, G Loss: 0.6641061902046204\n",
      "Epoch 61, batch 109 D Loss: 1.4000279903411865, G Loss: 0.6643579006195068\n",
      "Epoch 61, batch 110 D Loss: 1.4325785636901855, G Loss: 0.6567728519439697\n",
      "Epoch 61, batch 111 D Loss: 1.439787745475769, G Loss: 0.6600650548934937\n",
      "Epoch 61, batch 112 D Loss: 1.4411793947219849, G Loss: 0.6815742254257202\n",
      "Epoch 61, batch 113 D Loss: 1.4210678339004517, G Loss: 0.675110936164856\n",
      "Epoch 61, batch 114 D Loss: 1.4073586463928223, G Loss: 0.6897803544998169\n",
      "Epoch 61, batch 115 D Loss: 1.4123406410217285, G Loss: 0.6914899349212646\n",
      "Epoch 61, batch 116 D Loss: 1.4387285709381104, G Loss: 0.6727434396743774\n",
      "Epoch 61, batch 117 D Loss: 1.414084792137146, G Loss: 0.6852012872695923\n",
      "Epoch 61, batch 118 D Loss: 1.4220731258392334, G Loss: 0.6809210181236267\n",
      "Epoch 61, batch 119 D Loss: 1.4226689338684082, G Loss: 0.6777346730232239\n",
      "Epoch 61, batch 120 D Loss: 1.4271018505096436, G Loss: 0.6704403162002563\n",
      "Epoch 61, batch 121 D Loss: 1.4403789043426514, G Loss: 0.6681256294250488\n",
      "Epoch 61, batch 122 D Loss: 1.4025394916534424, G Loss: 0.6937519907951355\n",
      "Epoch 61, batch 123 D Loss: 1.4477481842041016, G Loss: 0.6631940603256226\n",
      "Epoch 61, batch 124 D Loss: 1.4313504695892334, G Loss: 0.6804935932159424\n",
      "Epoch 61, batch 125 D Loss: 1.403131127357483, G Loss: 0.6888242959976196\n",
      "Epoch 61, batch 126 D Loss: 1.4230430126190186, G Loss: 0.6897591948509216\n",
      "Epoch 61, batch 127 D Loss: 1.432707667350769, G Loss: 0.6783275604248047\n",
      "Epoch 61, batch 128 D Loss: 1.4249577522277832, G Loss: 0.6935176253318787\n",
      "Epoch 61, batch 129 D Loss: 1.3973084688186646, G Loss: 0.7009727954864502\n",
      "Epoch 61, batch 130 D Loss: 1.4213829040527344, G Loss: 0.7017146348953247\n",
      "Epoch 61, batch 131 D Loss: 1.4041204452514648, G Loss: 0.7052825689315796\n",
      "Epoch 61, batch 132 D Loss: 1.4125902652740479, G Loss: 0.7095740437507629\n",
      "Epoch 61, batch 133 D Loss: 1.4059127569198608, G Loss: 0.7168535590171814\n",
      "Epoch 61, batch 134 D Loss: 1.4009361267089844, G Loss: 0.7225205898284912\n",
      "Epoch 61, batch 135 D Loss: 1.395385503768921, G Loss: 0.7215063571929932\n",
      "Epoch 61, batch 136 D Loss: 1.3851158618927002, G Loss: 0.7308619022369385\n",
      "Epoch 61, batch 137 D Loss: 1.3962452411651611, G Loss: 0.7252297401428223\n",
      "Epoch 61, batch 138 D Loss: 1.3975123167037964, G Loss: 0.7239445447921753\n",
      "Epoch 61, batch 139 D Loss: 1.3699520826339722, G Loss: 0.7498033046722412\n",
      "Epoch 61, batch 140 D Loss: 1.38361656665802, G Loss: 0.7430567145347595\n",
      "Epoch 61, batch 141 D Loss: 1.3591728210449219, G Loss: 0.7727077007293701\n",
      "Epoch 61, batch 142 D Loss: 1.4000706672668457, G Loss: 0.7329661846160889\n",
      "Epoch 61, batch 143 D Loss: 1.3754627704620361, G Loss: 0.7590165734291077\n",
      "Epoch 61, batch 144 D Loss: 1.3689827919006348, G Loss: 0.7643547058105469\n",
      "Epoch 61, batch 145 D Loss: 1.3449580669403076, G Loss: 0.7750895619392395\n",
      "Epoch 61, batch 146 D Loss: 1.3439912796020508, G Loss: 0.7838588953018188\n",
      "Epoch 61, batch 147 D Loss: 1.3469810485839844, G Loss: 0.7670844197273254\n",
      "Epoch 61, batch 148 D Loss: 1.3341498374938965, G Loss: 0.821136474609375\n",
      "Epoch 61, batch 149 D Loss: 1.3555535078048706, G Loss: 0.7839698195457458\n",
      "Epoch 61, batch 150 D Loss: 1.363154411315918, G Loss: 0.7774601578712463\n",
      "Epoch 61, batch 151 D Loss: 1.3272312879562378, G Loss: 0.7830696702003479\n",
      "Epoch 61, batch 152 D Loss: 1.3450459241867065, G Loss: 0.7884758114814758\n",
      "Epoch 61, batch 153 D Loss: 1.3390164375305176, G Loss: 0.7919989228248596\n",
      "Epoch 61, batch 154 D Loss: 1.3743865489959717, G Loss: 0.7928183674812317\n",
      "Epoch 61, batch 155 D Loss: 1.35963773727417, G Loss: 0.7630318403244019\n",
      "Epoch 61, batch 156 D Loss: 1.345423698425293, G Loss: 0.7663984894752502\n",
      "Epoch 61, batch 157 D Loss: 1.3403699398040771, G Loss: 0.791707456111908\n",
      "Epoch 61, batch 158 D Loss: 1.3702373504638672, G Loss: 0.7804320454597473\n",
      "Epoch 61, batch 159 D Loss: 1.3424252271652222, G Loss: 0.7870028018951416\n",
      "Epoch 61, batch 160 D Loss: 1.3307228088378906, G Loss: 0.7858763933181763\n",
      "Epoch 61, batch 161 D Loss: 1.3281526565551758, G Loss: 0.7854284644126892\n",
      "Epoch 61, batch 162 D Loss: 1.3452775478363037, G Loss: 0.7595391869544983\n",
      "Epoch 61, batch 163 D Loss: 1.3924880027770996, G Loss: 0.7413084506988525\n",
      "Epoch 61, batch 164 D Loss: 1.3484805822372437, G Loss: 0.7778667211532593\n",
      "Epoch 61, batch 165 D Loss: 1.357436180114746, G Loss: 0.7160441875457764\n",
      "Epoch 61, batch 166 D Loss: 1.3737741708755493, G Loss: 0.7526543140411377\n",
      "Epoch 61, batch 167 D Loss: 1.4079113006591797, G Loss: 0.7153322100639343\n",
      "Epoch 61, batch 168 D Loss: 1.3575674295425415, G Loss: 0.7165918946266174\n",
      "Epoch 61, batch 169 D Loss: 1.3600891828536987, G Loss: 0.7494447827339172\n",
      "Epoch 61, batch 170 D Loss: 1.3751065731048584, G Loss: 0.7237980365753174\n",
      "Epoch 61, batch 171 D Loss: 1.4259092807769775, G Loss: 0.7041963934898376\n",
      "Epoch 61, batch 172 D Loss: 1.3620457649230957, G Loss: 0.7172561883926392\n",
      "Epoch 61, batch 173 D Loss: 1.4049642086029053, G Loss: 0.7080947756767273\n",
      "Epoch 61, batch 174 D Loss: 1.351574182510376, G Loss: 0.7023589611053467\n",
      "Epoch 61, batch 175 D Loss: 1.3919868469238281, G Loss: 0.7000184655189514\n",
      "Epoch 61, batch 176 D Loss: 1.3884589672088623, G Loss: 0.7217113971710205\n",
      "Epoch 61, batch 177 D Loss: 1.3981640338897705, G Loss: 0.6884691715240479\n",
      "Epoch 61, batch 178 D Loss: 1.4281269311904907, G Loss: 0.6676257848739624\n",
      "Epoch 61, batch 179 D Loss: 1.4260902404785156, G Loss: 0.657328724861145\n",
      "Epoch 61, batch 180 D Loss: 1.4139385223388672, G Loss: 0.6840834617614746\n",
      "Epoch 61, batch 181 D Loss: 1.3452197313308716, G Loss: 0.711083173751831\n",
      "Epoch 61, batch 182 D Loss: 1.4227430820465088, G Loss: 0.6622750163078308\n",
      "Epoch 61, batch 183 D Loss: 1.4053518772125244, G Loss: 0.6709449887275696\n",
      "Epoch 61, batch 184 D Loss: 1.4346275329589844, G Loss: 0.6457015872001648\n",
      "Epoch 61, batch 185 D Loss: 1.4162814617156982, G Loss: 0.6569036841392517\n",
      "Epoch 61, batch 186 D Loss: 1.4140809774398804, G Loss: 0.6503869891166687\n",
      "Epoch 61, batch 187 D Loss: 1.4111260175704956, G Loss: 0.631051242351532\n",
      "Epoch 61, batch 188 D Loss: 1.4343442916870117, G Loss: 0.6508232355117798\n",
      "Epoch 61, batch 189 D Loss: 1.414294719696045, G Loss: 0.6483696103096008\n",
      "Epoch 61, batch 190 D Loss: 1.4067535400390625, G Loss: 0.6551064848899841\n",
      "Epoch 61, batch 191 D Loss: 1.4222736358642578, G Loss: 0.6680802702903748\n",
      "Epoch 61, batch 192 D Loss: 1.4460539817810059, G Loss: 0.6471098065376282\n",
      "Epoch 61, batch 193 D Loss: 1.4707403182983398, G Loss: 0.6317853927612305\n",
      "Epoch 61, batch 194 D Loss: 1.4742166996002197, G Loss: 0.6218217015266418\n",
      "Epoch 61, batch 195 D Loss: 1.4369258880615234, G Loss: 0.6310313940048218\n",
      "Epoch 61, batch 196 D Loss: 1.4773430824279785, G Loss: 0.6244401335716248\n",
      "Epoch 61, batch 197 D Loss: 1.4691941738128662, G Loss: 0.6331018209457397\n",
      "Epoch 61, batch 198 D Loss: 1.4568030834197998, G Loss: 0.6363798379898071\n",
      "Epoch 61, batch 199 D Loss: 1.4473035335540771, G Loss: 0.6323902010917664\n",
      "Epoch 61, batch 200 D Loss: 1.4395627975463867, G Loss: 0.6424858570098877\n",
      "Epoch 62, batch 1 D Loss: 1.4157885313034058, G Loss: 0.6451570391654968\n",
      "Epoch 62, batch 2 D Loss: 1.4835671186447144, G Loss: 0.6133826971054077\n",
      "Epoch 62, batch 3 D Loss: 1.4056546688079834, G Loss: 0.6402628421783447\n",
      "Epoch 62, batch 4 D Loss: 1.4356231689453125, G Loss: 0.6383448839187622\n",
      "Epoch 62, batch 5 D Loss: 1.446909785270691, G Loss: 0.6504153609275818\n",
      "Epoch 62, batch 6 D Loss: 1.436753511428833, G Loss: 0.6361984014511108\n",
      "Epoch 62, batch 7 D Loss: 1.4183506965637207, G Loss: 0.6504254341125488\n",
      "Epoch 62, batch 8 D Loss: 1.4309816360473633, G Loss: 0.6356867551803589\n",
      "Epoch 62, batch 9 D Loss: 1.4211264848709106, G Loss: 0.6639938354492188\n",
      "Epoch 62, batch 10 D Loss: 1.4512654542922974, G Loss: 0.6396992802619934\n",
      "Epoch 62, batch 11 D Loss: 1.4175045490264893, G Loss: 0.6471740007400513\n",
      "Epoch 62, batch 12 D Loss: 1.441537857055664, G Loss: 0.6405767798423767\n",
      "Epoch 62, batch 13 D Loss: 1.4338825941085815, G Loss: 0.6602391004562378\n",
      "Epoch 62, batch 14 D Loss: 1.4095690250396729, G Loss: 0.6615427136421204\n",
      "Epoch 62, batch 15 D Loss: 1.4035511016845703, G Loss: 0.6647642254829407\n",
      "Epoch 62, batch 16 D Loss: 1.412581205368042, G Loss: 0.6638987064361572\n",
      "Epoch 62, batch 17 D Loss: 1.4099233150482178, G Loss: 0.6628881692886353\n",
      "Epoch 62, batch 18 D Loss: 1.4125924110412598, G Loss: 0.6592082977294922\n",
      "Epoch 62, batch 19 D Loss: 1.4123296737670898, G Loss: 0.6668521761894226\n",
      "Epoch 62, batch 20 D Loss: 1.4302258491516113, G Loss: 0.657798171043396\n",
      "Epoch 62, batch 21 D Loss: 1.404962420463562, G Loss: 0.6591328382492065\n",
      "Epoch 62, batch 22 D Loss: 1.3985450267791748, G Loss: 0.6760586500167847\n",
      "Epoch 62, batch 23 D Loss: 1.3966233730316162, G Loss: 0.6694485545158386\n",
      "Epoch 62, batch 24 D Loss: 1.40981125831604, G Loss: 0.6671302318572998\n",
      "Epoch 62, batch 25 D Loss: 1.4047045707702637, G Loss: 0.6748126149177551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, batch 26 D Loss: 1.3901715278625488, G Loss: 0.6796923279762268\n",
      "Epoch 62, batch 27 D Loss: 1.3857576847076416, G Loss: 0.6892329454421997\n",
      "Epoch 62, batch 28 D Loss: 1.3903275728225708, G Loss: 0.6816918849945068\n",
      "Epoch 62, batch 29 D Loss: 1.3993353843688965, G Loss: 0.6791027784347534\n",
      "Epoch 62, batch 30 D Loss: 1.3981475830078125, G Loss: 0.6843863129615784\n",
      "Epoch 62, batch 31 D Loss: 1.3867268562316895, G Loss: 0.6876744031906128\n",
      "Epoch 62, batch 32 D Loss: 1.390538215637207, G Loss: 0.6892637610435486\n",
      "Epoch 62, batch 33 D Loss: 1.3850047588348389, G Loss: 0.6874632239341736\n",
      "Epoch 62, batch 34 D Loss: 1.3842023611068726, G Loss: 0.6909410357475281\n",
      "Epoch 62, batch 35 D Loss: 1.3827928304672241, G Loss: 0.6944846510887146\n",
      "Epoch 62, batch 36 D Loss: 1.381677269935608, G Loss: 0.6979528665542603\n",
      "Epoch 62, batch 37 D Loss: 1.3697023391723633, G Loss: 0.6971569061279297\n",
      "Epoch 62, batch 38 D Loss: 1.3742752075195312, G Loss: 0.6970544457435608\n",
      "Epoch 62, batch 39 D Loss: 1.3708722591400146, G Loss: 0.6987561583518982\n",
      "Epoch 62, batch 40 D Loss: 1.3698439598083496, G Loss: 0.7038568258285522\n",
      "Epoch 62, batch 41 D Loss: 1.3692467212677002, G Loss: 0.7048002481460571\n",
      "Epoch 62, batch 42 D Loss: 1.3679227828979492, G Loss: 0.7038140892982483\n",
      "Epoch 62, batch 43 D Loss: 1.352240800857544, G Loss: 0.7058802247047424\n",
      "Epoch 62, batch 44 D Loss: 1.3601863384246826, G Loss: 0.7106026411056519\n",
      "Epoch 62, batch 45 D Loss: 1.3501561880111694, G Loss: 0.7078185081481934\n",
      "Epoch 62, batch 46 D Loss: 1.3623945713043213, G Loss: 0.7117486000061035\n",
      "Epoch 62, batch 47 D Loss: 1.3569552898406982, G Loss: 0.7077563405036926\n",
      "Epoch 62, batch 48 D Loss: 1.3510730266571045, G Loss: 0.7172821164131165\n",
      "Epoch 62, batch 49 D Loss: 1.3595850467681885, G Loss: 0.7113608717918396\n",
      "Epoch 62, batch 50 D Loss: 1.355961561203003, G Loss: 0.7196237444877625\n",
      "Epoch 62, batch 51 D Loss: 1.360198736190796, G Loss: 0.7146897912025452\n",
      "Epoch 62, batch 52 D Loss: 1.338172197341919, G Loss: 0.7167666554450989\n",
      "Epoch 62, batch 53 D Loss: 1.3510301113128662, G Loss: 0.7207844257354736\n",
      "Epoch 62, batch 54 D Loss: 1.3317766189575195, G Loss: 0.7167219519615173\n",
      "Epoch 62, batch 55 D Loss: 1.3065119981765747, G Loss: 0.7450665831565857\n",
      "Epoch 62, batch 56 D Loss: 1.3262219429016113, G Loss: 0.7224873900413513\n",
      "Epoch 62, batch 57 D Loss: 1.3069419860839844, G Loss: 0.7376899719238281\n",
      "Epoch 62, batch 58 D Loss: 1.3262748718261719, G Loss: 0.7244879007339478\n",
      "Epoch 62, batch 59 D Loss: 1.3123555183410645, G Loss: 0.7441046237945557\n",
      "Epoch 62, batch 60 D Loss: 1.2996820211410522, G Loss: 0.7492100596427917\n",
      "Epoch 62, batch 61 D Loss: 1.3285391330718994, G Loss: 0.7380112409591675\n",
      "Epoch 62, batch 62 D Loss: 1.3299998044967651, G Loss: 0.7314013838768005\n",
      "Epoch 62, batch 63 D Loss: 1.3095812797546387, G Loss: 0.7154539227485657\n",
      "Epoch 62, batch 64 D Loss: 1.3210417032241821, G Loss: 0.7198829650878906\n",
      "Epoch 62, batch 65 D Loss: 1.3237268924713135, G Loss: 0.7428216338157654\n",
      "Epoch 62, batch 66 D Loss: 1.3017851114273071, G Loss: 0.7484182119369507\n",
      "Epoch 62, batch 67 D Loss: 1.3053488731384277, G Loss: 0.7543980479240417\n",
      "Epoch 62, batch 68 D Loss: 1.3201606273651123, G Loss: 0.731925368309021\n",
      "Epoch 62, batch 69 D Loss: 1.3091535568237305, G Loss: 0.7475686073303223\n",
      "Epoch 62, batch 70 D Loss: 1.331794261932373, G Loss: 0.7435572743415833\n",
      "Epoch 62, batch 71 D Loss: 1.2976958751678467, G Loss: 0.74688720703125\n",
      "Epoch 62, batch 72 D Loss: 1.337343454360962, G Loss: 0.7313104271888733\n",
      "Epoch 62, batch 73 D Loss: 1.3362905979156494, G Loss: 0.7034144401550293\n",
      "Epoch 62, batch 74 D Loss: 1.333100438117981, G Loss: 0.7137840390205383\n",
      "Epoch 62, batch 75 D Loss: 1.290144443511963, G Loss: 0.7477543354034424\n",
      "Epoch 62, batch 76 D Loss: 1.298034906387329, G Loss: 0.7519341111183167\n",
      "Epoch 62, batch 77 D Loss: 1.3620120286941528, G Loss: 0.7200377583503723\n",
      "Epoch 62, batch 78 D Loss: 1.3565748929977417, G Loss: 0.733009934425354\n",
      "Epoch 62, batch 79 D Loss: 1.2605290412902832, G Loss: 0.7937941551208496\n",
      "Epoch 62, batch 80 D Loss: 1.2872772216796875, G Loss: 0.7228260636329651\n",
      "Epoch 62, batch 81 D Loss: 1.3416714668273926, G Loss: 0.7408298254013062\n",
      "Epoch 62, batch 82 D Loss: 1.3293228149414062, G Loss: 0.7215374112129211\n",
      "Epoch 62, batch 83 D Loss: 1.3205814361572266, G Loss: 0.7407459020614624\n",
      "Epoch 62, batch 84 D Loss: 1.313234806060791, G Loss: 0.7661175727844238\n",
      "Epoch 62, batch 85 D Loss: 1.3061139583587646, G Loss: 0.7351337671279907\n",
      "Epoch 62, batch 86 D Loss: 1.3584383726119995, G Loss: 0.7059235572814941\n",
      "Epoch 62, batch 87 D Loss: 1.3961999416351318, G Loss: 0.6867428421974182\n",
      "Epoch 62, batch 88 D Loss: 1.4021201133728027, G Loss: 0.6673583984375\n",
      "Epoch 62, batch 89 D Loss: 1.4038481712341309, G Loss: 0.6650230884552002\n",
      "Epoch 62, batch 90 D Loss: 1.3519293069839478, G Loss: 0.6728690266609192\n",
      "Epoch 62, batch 91 D Loss: 1.3715858459472656, G Loss: 0.6638287305831909\n",
      "Epoch 62, batch 92 D Loss: 1.34224271774292, G Loss: 0.6863597631454468\n",
      "Epoch 62, batch 93 D Loss: 1.4035842418670654, G Loss: 0.658449113368988\n",
      "Epoch 62, batch 94 D Loss: 1.3694567680358887, G Loss: 0.6491415500640869\n",
      "Epoch 62, batch 95 D Loss: 1.3484299182891846, G Loss: 0.6774096488952637\n",
      "Epoch 62, batch 96 D Loss: 1.4530714750289917, G Loss: 0.643092930316925\n",
      "Epoch 62, batch 97 D Loss: 1.3381609916687012, G Loss: 0.6754537224769592\n",
      "Epoch 62, batch 98 D Loss: 1.3910727500915527, G Loss: 0.6336386203765869\n",
      "Epoch 62, batch 99 D Loss: 1.4142731428146362, G Loss: 0.6430285573005676\n",
      "Epoch 62, batch 100 D Loss: 1.3510255813598633, G Loss: 0.6569960117340088\n",
      "Epoch 62, batch 101 D Loss: 1.4461488723754883, G Loss: 0.6302483081817627\n",
      "Epoch 62, batch 102 D Loss: 1.4231832027435303, G Loss: 0.6449055671691895\n",
      "Epoch 62, batch 103 D Loss: 1.4439959526062012, G Loss: 0.6419427990913391\n",
      "Epoch 62, batch 104 D Loss: 1.5563559532165527, G Loss: 0.5994936227798462\n",
      "Epoch 62, batch 105 D Loss: 1.461756944656372, G Loss: 0.6237989068031311\n",
      "Epoch 62, batch 106 D Loss: 1.4577311277389526, G Loss: 0.6291289329528809\n",
      "Epoch 62, batch 107 D Loss: 1.5550373792648315, G Loss: 0.5879371166229248\n",
      "Epoch 62, batch 108 D Loss: 1.400958776473999, G Loss: 0.6424348950386047\n",
      "Epoch 62, batch 109 D Loss: 1.4912803173065186, G Loss: 0.6052524447441101\n",
      "Epoch 62, batch 110 D Loss: 1.5540809631347656, G Loss: 0.5892177820205688\n",
      "Epoch 62, batch 111 D Loss: 1.4312171936035156, G Loss: 0.6304030418395996\n",
      "Epoch 62, batch 112 D Loss: 1.3981046676635742, G Loss: 0.6559678912162781\n",
      "Epoch 62, batch 113 D Loss: 1.4118068218231201, G Loss: 0.660185694694519\n",
      "Epoch 62, batch 114 D Loss: 1.4521939754486084, G Loss: 0.6359620690345764\n",
      "Epoch 62, batch 115 D Loss: 1.536417007446289, G Loss: 0.6002849340438843\n",
      "Epoch 62, batch 116 D Loss: 1.5038957595825195, G Loss: 0.6164387464523315\n",
      "Epoch 62, batch 117 D Loss: 1.4961340427398682, G Loss: 0.5994186401367188\n",
      "Epoch 62, batch 118 D Loss: 1.4712624549865723, G Loss: 0.6249924302101135\n",
      "Epoch 62, batch 119 D Loss: 1.4088746309280396, G Loss: 0.6509557962417603\n",
      "Epoch 62, batch 120 D Loss: 1.4428141117095947, G Loss: 0.6395869255065918\n",
      "Epoch 62, batch 121 D Loss: 1.4308393001556396, G Loss: 0.6655952334403992\n",
      "Epoch 62, batch 122 D Loss: 1.4416453838348389, G Loss: 0.6611202955245972\n",
      "Epoch 62, batch 123 D Loss: 1.4385051727294922, G Loss: 0.6557063460350037\n",
      "Epoch 62, batch 124 D Loss: 1.4103093147277832, G Loss: 0.6629412174224854\n",
      "Epoch 62, batch 125 D Loss: 1.446605920791626, G Loss: 0.6623879075050354\n",
      "Epoch 62, batch 126 D Loss: 1.4485423564910889, G Loss: 0.6598247289657593\n",
      "Epoch 62, batch 127 D Loss: 1.4028114080429077, G Loss: 0.6767218708992004\n",
      "Epoch 62, batch 128 D Loss: 1.4159388542175293, G Loss: 0.6769047379493713\n",
      "Epoch 62, batch 129 D Loss: 1.40427565574646, G Loss: 0.6963632702827454\n",
      "Epoch 62, batch 130 D Loss: 1.3939428329467773, G Loss: 0.6995468139648438\n",
      "Epoch 62, batch 131 D Loss: 1.429741621017456, G Loss: 0.6904203295707703\n",
      "Epoch 62, batch 132 D Loss: 1.3875752687454224, G Loss: 0.7206793427467346\n",
      "Epoch 62, batch 133 D Loss: 1.3719637393951416, G Loss: 0.7133530378341675\n",
      "Epoch 62, batch 134 D Loss: 1.396075963973999, G Loss: 0.7177892327308655\n",
      "Epoch 62, batch 135 D Loss: 1.3580918312072754, G Loss: 0.7518068552017212\n",
      "Epoch 62, batch 136 D Loss: 1.3763209581375122, G Loss: 0.7325177192687988\n",
      "Epoch 62, batch 137 D Loss: 1.3826918601989746, G Loss: 0.7247956991195679\n",
      "Epoch 62, batch 138 D Loss: 1.3600035905838013, G Loss: 0.7435653209686279\n",
      "Epoch 62, batch 139 D Loss: 1.3533096313476562, G Loss: 0.7766006588935852\n",
      "Epoch 62, batch 140 D Loss: 1.366938829421997, G Loss: 0.7471588253974915\n",
      "Epoch 62, batch 141 D Loss: 1.339909553527832, G Loss: 0.8178034424781799\n",
      "Epoch 62, batch 142 D Loss: 1.3582309484481812, G Loss: 0.7724496722221375\n",
      "Epoch 62, batch 143 D Loss: 1.354104995727539, G Loss: 0.7820798754692078\n",
      "Epoch 62, batch 144 D Loss: 1.3360294103622437, G Loss: 0.779339611530304\n",
      "Epoch 62, batch 145 D Loss: 1.3564932346343994, G Loss: 0.7768336534500122\n",
      "Epoch 62, batch 146 D Loss: 1.3134765625, G Loss: 0.8641555905342102\n",
      "Epoch 62, batch 147 D Loss: 1.3117363452911377, G Loss: 0.8448387384414673\n",
      "Epoch 62, batch 148 D Loss: 1.3023624420166016, G Loss: 0.8661754727363586\n",
      "Epoch 62, batch 149 D Loss: 1.3449599742889404, G Loss: 0.8589006662368774\n",
      "Epoch 62, batch 150 D Loss: 1.3384790420532227, G Loss: 0.8241745233535767\n",
      "Epoch 62, batch 151 D Loss: 1.3186907768249512, G Loss: 0.8705304861068726\n",
      "Epoch 62, batch 152 D Loss: 1.346947193145752, G Loss: 0.839169979095459\n",
      "Epoch 62, batch 153 D Loss: 1.3648762702941895, G Loss: 0.789975106716156\n",
      "Epoch 62, batch 154 D Loss: 1.3625099658966064, G Loss: 0.8172898888587952\n",
      "Epoch 62, batch 155 D Loss: 1.3512904644012451, G Loss: 0.790202260017395\n",
      "Epoch 62, batch 156 D Loss: 1.3526983261108398, G Loss: 0.8069650530815125\n",
      "Epoch 62, batch 157 D Loss: 1.3753999471664429, G Loss: 0.7707686424255371\n",
      "Epoch 62, batch 158 D Loss: 1.314117193222046, G Loss: 0.8491546511650085\n",
      "Epoch 62, batch 159 D Loss: 1.3526577949523926, G Loss: 0.8447760939598083\n",
      "Epoch 62, batch 160 D Loss: 1.350173830986023, G Loss: 0.8409019708633423\n",
      "Epoch 62, batch 161 D Loss: 1.459827184677124, G Loss: 0.733122706413269\n",
      "Epoch 62, batch 162 D Loss: 1.330249547958374, G Loss: 0.8603879809379578\n",
      "Epoch 62, batch 163 D Loss: 1.4124133586883545, G Loss: 0.7465044260025024\n",
      "Epoch 62, batch 164 D Loss: 1.4039448499679565, G Loss: 0.7635163068771362\n",
      "Epoch 62, batch 165 D Loss: 1.396821141242981, G Loss: 0.755484938621521\n",
      "Epoch 62, batch 166 D Loss: 1.421478509902954, G Loss: 0.7952545881271362\n",
      "Epoch 62, batch 167 D Loss: 1.4421751499176025, G Loss: 0.7113999128341675\n",
      "Epoch 62, batch 168 D Loss: 1.3779420852661133, G Loss: 0.7498510479927063\n",
      "Epoch 62, batch 169 D Loss: 1.3767049312591553, G Loss: 0.7674941420555115\n",
      "Epoch 62, batch 170 D Loss: 1.434919834136963, G Loss: 0.718342125415802\n",
      "Epoch 62, batch 171 D Loss: 1.4332084655761719, G Loss: 0.7084267139434814\n",
      "Epoch 62, batch 172 D Loss: 1.4342858791351318, G Loss: 0.7181532382965088\n",
      "Epoch 62, batch 173 D Loss: 1.4245071411132812, G Loss: 0.6963838934898376\n",
      "Epoch 62, batch 174 D Loss: 1.4558825492858887, G Loss: 0.6914689540863037\n",
      "Epoch 62, batch 175 D Loss: 1.414930820465088, G Loss: 0.686278223991394\n",
      "Epoch 62, batch 176 D Loss: 1.376659870147705, G Loss: 0.7402441501617432\n",
      "Epoch 62, batch 177 D Loss: 1.3898975849151611, G Loss: 0.7242050170898438\n",
      "Epoch 62, batch 178 D Loss: 1.3949801921844482, G Loss: 0.7258468866348267\n",
      "Epoch 62, batch 179 D Loss: 1.4471397399902344, G Loss: 0.7007283568382263\n",
      "Epoch 62, batch 180 D Loss: 1.4357941150665283, G Loss: 0.7084745764732361\n",
      "Epoch 62, batch 181 D Loss: 1.4296729564666748, G Loss: 0.6953754425048828\n",
      "Epoch 62, batch 182 D Loss: 1.4221985340118408, G Loss: 0.7034347057342529\n",
      "Epoch 62, batch 183 D Loss: 1.3878958225250244, G Loss: 0.6967530250549316\n",
      "Epoch 62, batch 184 D Loss: 1.4490478038787842, G Loss: 0.7069483995437622\n",
      "Epoch 62, batch 185 D Loss: 1.4514002799987793, G Loss: 0.6957278251647949\n",
      "Epoch 62, batch 186 D Loss: 1.422777771949768, G Loss: 0.6776909828186035\n",
      "Epoch 62, batch 187 D Loss: 1.4097561836242676, G Loss: 0.7190492153167725\n",
      "Epoch 62, batch 188 D Loss: 1.4457168579101562, G Loss: 0.6931558847427368\n",
      "Epoch 62, batch 189 D Loss: 1.4082725048065186, G Loss: 0.7063090801239014\n",
      "Epoch 62, batch 190 D Loss: 1.422257423400879, G Loss: 0.6906567215919495\n",
      "Epoch 62, batch 191 D Loss: 1.401984691619873, G Loss: 0.6992528438568115\n",
      "Epoch 62, batch 192 D Loss: 1.423018217086792, G Loss: 0.701834499835968\n",
      "Epoch 62, batch 193 D Loss: 1.3793113231658936, G Loss: 0.7061207294464111\n",
      "Epoch 62, batch 194 D Loss: 1.3783081769943237, G Loss: 0.7084612846374512\n",
      "Epoch 62, batch 195 D Loss: 1.3890130519866943, G Loss: 0.7018234729766846\n",
      "Epoch 62, batch 196 D Loss: 1.413501262664795, G Loss: 0.7013792991638184\n",
      "Epoch 62, batch 197 D Loss: 1.4119064807891846, G Loss: 0.7083698511123657\n",
      "Epoch 62, batch 198 D Loss: 1.4522461891174316, G Loss: 0.6935175061225891\n",
      "Epoch 62, batch 199 D Loss: 1.393495798110962, G Loss: 0.7022073864936829\n",
      "Epoch 62, batch 200 D Loss: 1.3847346305847168, G Loss: 0.7057029604911804\n",
      "Epoch 63, batch 1 D Loss: 1.3717610836029053, G Loss: 0.7071813344955444\n",
      "Epoch 63, batch 2 D Loss: 1.384106159210205, G Loss: 0.7124637365341187\n",
      "Epoch 63, batch 3 D Loss: 1.3768770694732666, G Loss: 0.7085517048835754\n",
      "Epoch 63, batch 4 D Loss: 1.378321886062622, G Loss: 0.7127836346626282\n",
      "Epoch 63, batch 5 D Loss: 1.407434105873108, G Loss: 0.7068546414375305\n",
      "Epoch 63, batch 6 D Loss: 1.3887784481048584, G Loss: 0.7142446041107178\n",
      "Epoch 63, batch 7 D Loss: 1.379438877105713, G Loss: 0.7136048674583435\n",
      "Epoch 63, batch 8 D Loss: 1.3797885179519653, G Loss: 0.7192878723144531\n",
      "Epoch 63, batch 9 D Loss: 1.3453655242919922, G Loss: 0.7167933583259583\n",
      "Epoch 63, batch 10 D Loss: 1.358203411102295, G Loss: 0.729106605052948\n",
      "Epoch 63, batch 11 D Loss: 1.3553650379180908, G Loss: 0.697407603263855\n",
      "Epoch 63, batch 12 D Loss: 1.381451964378357, G Loss: 0.7379960417747498\n",
      "Epoch 63, batch 13 D Loss: 1.3906809091567993, G Loss: 0.7065963745117188\n",
      "Epoch 63, batch 14 D Loss: 1.3330001831054688, G Loss: 0.73627108335495\n",
      "Epoch 63, batch 15 D Loss: 1.3670755624771118, G Loss: 0.7243902683258057\n",
      "Epoch 63, batch 16 D Loss: 1.3660986423492432, G Loss: 0.732434868812561\n",
      "Epoch 63, batch 17 D Loss: 1.3150660991668701, G Loss: 0.728080153465271\n",
      "Epoch 63, batch 18 D Loss: 1.3382340669631958, G Loss: 0.7176325917243958\n",
      "Epoch 63, batch 19 D Loss: 1.3681352138519287, G Loss: 0.7151685357093811\n",
      "Epoch 63, batch 20 D Loss: 1.3332072496414185, G Loss: 0.752712607383728\n",
      "Epoch 63, batch 21 D Loss: 1.3123891353607178, G Loss: 0.7193735241889954\n",
      "Epoch 63, batch 22 D Loss: 1.3497748374938965, G Loss: 0.7125002145767212\n",
      "Epoch 63, batch 23 D Loss: 1.3510725498199463, G Loss: 0.7402899265289307\n",
      "Epoch 63, batch 24 D Loss: 1.3406070470809937, G Loss: 0.7214856147766113\n",
      "Epoch 63, batch 25 D Loss: 1.3509457111358643, G Loss: 0.6907616257667542\n",
      "Epoch 63, batch 26 D Loss: 1.2959920167922974, G Loss: 0.7541165351867676\n",
      "Epoch 63, batch 27 D Loss: 1.3033734560012817, G Loss: 0.7181108593940735\n",
      "Epoch 63, batch 28 D Loss: 1.3687704801559448, G Loss: 0.723958432674408\n",
      "Epoch 63, batch 29 D Loss: 1.352696418762207, G Loss: 0.7065738439559937\n",
      "Epoch 63, batch 30 D Loss: 1.3427410125732422, G Loss: 0.7072822451591492\n",
      "Epoch 63, batch 31 D Loss: 1.336066484451294, G Loss: 0.7392535209655762\n",
      "Epoch 63, batch 32 D Loss: 1.287912368774414, G Loss: 0.7728279232978821\n",
      "Epoch 63, batch 33 D Loss: 1.2905352115631104, G Loss: 0.7573612928390503\n",
      "Epoch 63, batch 34 D Loss: 1.3258142471313477, G Loss: 0.7503955364227295\n",
      "Epoch 63, batch 35 D Loss: 1.3560956716537476, G Loss: 0.7201857566833496\n",
      "Epoch 63, batch 36 D Loss: 1.2698907852172852, G Loss: 0.7296237349510193\n",
      "Epoch 63, batch 37 D Loss: 1.3959832191467285, G Loss: 0.7143459320068359\n",
      "Epoch 63, batch 38 D Loss: 1.351952075958252, G Loss: 0.7010746002197266\n",
      "Epoch 63, batch 39 D Loss: 1.3653745651245117, G Loss: 0.7110075354576111\n",
      "Epoch 63, batch 40 D Loss: 1.3390979766845703, G Loss: 0.7528733611106873\n",
      "Epoch 63, batch 41 D Loss: 1.3519742488861084, G Loss: 0.685290515422821\n",
      "Epoch 63, batch 42 D Loss: 1.362200379371643, G Loss: 0.6959492564201355\n",
      "Epoch 63, batch 43 D Loss: 1.3813364505767822, G Loss: 0.7208704352378845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, batch 44 D Loss: 1.4236122369766235, G Loss: 0.6828228831291199\n",
      "Epoch 63, batch 45 D Loss: 1.4148123264312744, G Loss: 0.6975986957550049\n",
      "Epoch 63, batch 46 D Loss: 1.3369193077087402, G Loss: 0.7018278241157532\n",
      "Epoch 63, batch 47 D Loss: 1.3340140581130981, G Loss: 0.7702614665031433\n",
      "Epoch 63, batch 48 D Loss: 1.3511147499084473, G Loss: 0.7144443988800049\n",
      "Epoch 63, batch 49 D Loss: 1.3588964939117432, G Loss: 0.6973550319671631\n",
      "Epoch 63, batch 50 D Loss: 1.33872389793396, G Loss: 0.7035585641860962\n",
      "Epoch 63, batch 51 D Loss: 1.4077290296554565, G Loss: 0.7000839114189148\n",
      "Epoch 63, batch 52 D Loss: 1.3808984756469727, G Loss: 0.71424400806427\n",
      "Epoch 63, batch 53 D Loss: 1.401149034500122, G Loss: 0.6704817414283752\n",
      "Epoch 63, batch 54 D Loss: 1.4401322603225708, G Loss: 0.6499879360198975\n",
      "Epoch 63, batch 55 D Loss: 1.3748252391815186, G Loss: 0.7185688614845276\n",
      "Epoch 63, batch 56 D Loss: 1.475873589515686, G Loss: 0.6430124044418335\n",
      "Epoch 63, batch 57 D Loss: 1.4457995891571045, G Loss: 0.6413031220436096\n",
      "Epoch 63, batch 58 D Loss: 1.3606998920440674, G Loss: 0.6935847401618958\n",
      "Epoch 63, batch 59 D Loss: 1.4022449254989624, G Loss: 0.6237108111381531\n",
      "Epoch 63, batch 60 D Loss: 1.3848743438720703, G Loss: 0.6742831468582153\n",
      "Epoch 63, batch 61 D Loss: 1.468254804611206, G Loss: 0.6295096278190613\n",
      "Epoch 63, batch 62 D Loss: 1.391895055770874, G Loss: 0.6623640656471252\n",
      "Epoch 63, batch 63 D Loss: 1.4042258262634277, G Loss: 0.6379966139793396\n",
      "Epoch 63, batch 64 D Loss: 1.4609267711639404, G Loss: 0.6642802953720093\n",
      "Epoch 63, batch 65 D Loss: 1.4989655017852783, G Loss: 0.6238396167755127\n",
      "Epoch 63, batch 66 D Loss: 1.4888595342636108, G Loss: 0.6410127282142639\n",
      "Epoch 63, batch 67 D Loss: 1.4281764030456543, G Loss: 0.652588963508606\n",
      "Epoch 63, batch 68 D Loss: 1.3659138679504395, G Loss: 0.6738948225975037\n",
      "Epoch 63, batch 69 D Loss: 1.388146162033081, G Loss: 0.6824615001678467\n",
      "Epoch 63, batch 70 D Loss: 1.4353368282318115, G Loss: 0.6757925152778625\n",
      "Epoch 63, batch 71 D Loss: 1.4508376121520996, G Loss: 0.6651039719581604\n",
      "Epoch 63, batch 72 D Loss: 1.4330642223358154, G Loss: 0.6436797976493835\n",
      "Epoch 63, batch 73 D Loss: 1.4388628005981445, G Loss: 0.6643943786621094\n",
      "Epoch 63, batch 74 D Loss: 1.4194176197052002, G Loss: 0.6665742993354797\n",
      "Epoch 63, batch 75 D Loss: 1.4800846576690674, G Loss: 0.6352782249450684\n",
      "Epoch 63, batch 76 D Loss: 1.4252939224243164, G Loss: 0.6901491284370422\n",
      "Epoch 63, batch 77 D Loss: 1.4542797803878784, G Loss: 0.6390693187713623\n",
      "Epoch 63, batch 78 D Loss: 1.4286212921142578, G Loss: 0.6714738607406616\n",
      "Epoch 63, batch 79 D Loss: 1.4440207481384277, G Loss: 0.6519418954849243\n",
      "Epoch 63, batch 80 D Loss: 1.463664174079895, G Loss: 0.6475263237953186\n",
      "Epoch 63, batch 81 D Loss: 1.4341721534729004, G Loss: 0.6744899749755859\n",
      "Epoch 63, batch 82 D Loss: 1.3922193050384521, G Loss: 0.6738293170928955\n",
      "Epoch 63, batch 83 D Loss: 1.4214752912521362, G Loss: 0.6619837284088135\n",
      "Epoch 63, batch 84 D Loss: 1.388854742050171, G Loss: 0.7022753953933716\n",
      "Epoch 63, batch 85 D Loss: 1.4297068119049072, G Loss: 0.6562559604644775\n",
      "Epoch 63, batch 86 D Loss: 1.470173954963684, G Loss: 0.6553094983100891\n",
      "Epoch 63, batch 87 D Loss: 1.4379432201385498, G Loss: 0.6781324148178101\n",
      "Epoch 63, batch 88 D Loss: 1.4413585662841797, G Loss: 0.6763581037521362\n",
      "Epoch 63, batch 89 D Loss: 1.4093639850616455, G Loss: 0.6885694265365601\n",
      "Epoch 63, batch 90 D Loss: 1.40580153465271, G Loss: 0.7000468373298645\n",
      "Epoch 63, batch 91 D Loss: 1.4278117418289185, G Loss: 0.6778826117515564\n",
      "Epoch 63, batch 92 D Loss: 1.4239736795425415, G Loss: 0.6939301490783691\n",
      "Epoch 63, batch 93 D Loss: 1.4011319875717163, G Loss: 0.7002353072166443\n",
      "Epoch 63, batch 94 D Loss: 1.3952772617340088, G Loss: 0.7025741338729858\n",
      "Epoch 63, batch 95 D Loss: 1.4162123203277588, G Loss: 0.6764043569564819\n",
      "Epoch 63, batch 96 D Loss: 1.408433437347412, G Loss: 0.6913702487945557\n",
      "Epoch 63, batch 97 D Loss: 1.4296793937683105, G Loss: 0.6918699145317078\n",
      "Epoch 63, batch 98 D Loss: 1.3819572925567627, G Loss: 0.7083017230033875\n",
      "Epoch 63, batch 99 D Loss: 1.396801233291626, G Loss: 0.694131076335907\n",
      "Epoch 63, batch 100 D Loss: 1.3921971321105957, G Loss: 0.7190344333648682\n",
      "Epoch 63, batch 101 D Loss: 1.4052624702453613, G Loss: 0.6946163177490234\n",
      "Epoch 63, batch 102 D Loss: 1.4137239456176758, G Loss: 0.6999173760414124\n",
      "Epoch 63, batch 103 D Loss: 1.3754737377166748, G Loss: 0.7159592509269714\n",
      "Epoch 63, batch 104 D Loss: 1.3997910022735596, G Loss: 0.7103315591812134\n",
      "Epoch 63, batch 105 D Loss: 1.3790926933288574, G Loss: 0.7216489315032959\n",
      "Epoch 63, batch 106 D Loss: 1.3883538246154785, G Loss: 0.7107862234115601\n",
      "Epoch 63, batch 107 D Loss: 1.3924777507781982, G Loss: 0.710705578327179\n",
      "Epoch 63, batch 108 D Loss: 1.40086030960083, G Loss: 0.7187432646751404\n",
      "Epoch 63, batch 109 D Loss: 1.3886940479278564, G Loss: 0.7163663506507874\n",
      "Epoch 63, batch 110 D Loss: 1.4000639915466309, G Loss: 0.7267982363700867\n",
      "Epoch 63, batch 111 D Loss: 1.3648921251296997, G Loss: 0.7361741662025452\n",
      "Epoch 63, batch 112 D Loss: 1.3689229488372803, G Loss: 0.7271673679351807\n",
      "Epoch 63, batch 113 D Loss: 1.380078673362732, G Loss: 0.7225271463394165\n",
      "Epoch 63, batch 114 D Loss: 1.3833584785461426, G Loss: 0.7293421030044556\n",
      "Epoch 63, batch 115 D Loss: 1.3809428215026855, G Loss: 0.7382627725601196\n",
      "Epoch 63, batch 116 D Loss: 1.3604260683059692, G Loss: 0.7481728196144104\n",
      "Epoch 63, batch 117 D Loss: 1.3490287065505981, G Loss: 0.7438502311706543\n",
      "Epoch 63, batch 118 D Loss: 1.3685615062713623, G Loss: 0.7412800788879395\n",
      "Epoch 63, batch 119 D Loss: 1.3603256940841675, G Loss: 0.7426019310951233\n",
      "Epoch 63, batch 120 D Loss: 1.341901421546936, G Loss: 0.7561741471290588\n",
      "Epoch 63, batch 121 D Loss: 1.315263271331787, G Loss: 0.778236448764801\n",
      "Epoch 63, batch 122 D Loss: 1.3552987575531006, G Loss: 0.7613885402679443\n",
      "Epoch 63, batch 123 D Loss: 1.347787857055664, G Loss: 0.7838503122329712\n",
      "Epoch 63, batch 124 D Loss: 1.3322832584381104, G Loss: 0.7879886627197266\n",
      "Epoch 63, batch 125 D Loss: 1.3246617317199707, G Loss: 0.790779709815979\n",
      "Epoch 63, batch 126 D Loss: 1.3257899284362793, G Loss: 0.8355312943458557\n",
      "Epoch 63, batch 127 D Loss: 1.3439700603485107, G Loss: 0.7902194261550903\n",
      "Epoch 63, batch 128 D Loss: 1.2983810901641846, G Loss: 0.8095362186431885\n",
      "Epoch 63, batch 129 D Loss: 1.29579758644104, G Loss: 0.8118217587471008\n",
      "Epoch 63, batch 130 D Loss: 1.3293859958648682, G Loss: 0.8217010498046875\n",
      "Epoch 63, batch 131 D Loss: 1.3656755685806274, G Loss: 0.7900646924972534\n",
      "Epoch 63, batch 132 D Loss: 1.3095803260803223, G Loss: 0.8066444396972656\n",
      "Epoch 63, batch 133 D Loss: 1.3140629529953003, G Loss: 0.8358232378959656\n",
      "Epoch 63, batch 134 D Loss: 1.2679669857025146, G Loss: 0.8653907179832458\n",
      "Epoch 63, batch 135 D Loss: 1.3251625299453735, G Loss: 0.7957904934883118\n",
      "Epoch 63, batch 136 D Loss: 1.3230037689208984, G Loss: 0.84410560131073\n",
      "Epoch 63, batch 137 D Loss: 1.3225445747375488, G Loss: 0.8871310353279114\n",
      "Epoch 63, batch 138 D Loss: 1.3127202987670898, G Loss: 0.830474317073822\n",
      "Epoch 63, batch 139 D Loss: 1.3351044654846191, G Loss: 0.7922836542129517\n",
      "Epoch 63, batch 140 D Loss: 1.419682264328003, G Loss: 0.7555845379829407\n",
      "Epoch 63, batch 141 D Loss: 1.3645026683807373, G Loss: 0.8090495467185974\n",
      "Epoch 63, batch 142 D Loss: 1.4047026634216309, G Loss: 0.788618803024292\n",
      "Epoch 63, batch 143 D Loss: 1.2967008352279663, G Loss: 0.8384265303611755\n",
      "Epoch 63, batch 144 D Loss: 1.3179165124893188, G Loss: 0.7753774523735046\n",
      "Epoch 63, batch 145 D Loss: 1.4033881425857544, G Loss: 0.7950780987739563\n",
      "Epoch 63, batch 146 D Loss: 1.295140266418457, G Loss: 0.7919294238090515\n",
      "Epoch 63, batch 147 D Loss: 1.434330940246582, G Loss: 0.762204110622406\n",
      "Epoch 63, batch 148 D Loss: 1.3687291145324707, G Loss: 0.8069477677345276\n",
      "Epoch 63, batch 149 D Loss: 1.3604780435562134, G Loss: 0.7408277988433838\n",
      "Epoch 63, batch 150 D Loss: 1.3213746547698975, G Loss: 0.7908973097801208\n",
      "Epoch 63, batch 151 D Loss: 1.34068763256073, G Loss: 0.7541855573654175\n",
      "Epoch 63, batch 152 D Loss: 1.3204379081726074, G Loss: 0.7763940691947937\n",
      "Epoch 63, batch 153 D Loss: 1.3388299942016602, G Loss: 0.7877233624458313\n",
      "Epoch 63, batch 154 D Loss: 1.355191707611084, G Loss: 0.7342128753662109\n",
      "Epoch 63, batch 155 D Loss: 1.4558649063110352, G Loss: 0.7324577569961548\n",
      "Epoch 63, batch 156 D Loss: 1.3657736778259277, G Loss: 0.7333711385726929\n",
      "Epoch 63, batch 157 D Loss: 1.3761625289916992, G Loss: 0.7586419582366943\n",
      "Epoch 63, batch 158 D Loss: 1.3762032985687256, G Loss: 0.728193998336792\n",
      "Epoch 63, batch 159 D Loss: 1.3701817989349365, G Loss: 0.7417535185813904\n",
      "Epoch 63, batch 160 D Loss: 1.3976647853851318, G Loss: 0.7176374793052673\n",
      "Epoch 63, batch 161 D Loss: 1.3367626667022705, G Loss: 0.742439329624176\n",
      "Epoch 63, batch 162 D Loss: 1.3836524486541748, G Loss: 0.6881042718887329\n",
      "Epoch 63, batch 163 D Loss: 1.3572124242782593, G Loss: 0.732841432094574\n",
      "Epoch 63, batch 164 D Loss: 1.3917624950408936, G Loss: 0.6870058178901672\n",
      "Epoch 63, batch 165 D Loss: 1.4261534214019775, G Loss: 0.6944956183433533\n",
      "Epoch 63, batch 166 D Loss: 1.443372130393982, G Loss: 0.691270649433136\n",
      "Epoch 63, batch 167 D Loss: 1.397782802581787, G Loss: 0.680854320526123\n",
      "Epoch 63, batch 168 D Loss: 1.3611278533935547, G Loss: 0.7123153209686279\n",
      "Epoch 63, batch 169 D Loss: 1.3984241485595703, G Loss: 0.6835311651229858\n",
      "Epoch 63, batch 170 D Loss: 1.4122240543365479, G Loss: 0.6787919402122498\n",
      "Epoch 63, batch 171 D Loss: 1.351590633392334, G Loss: 0.7101737260818481\n",
      "Epoch 63, batch 172 D Loss: 1.4167804718017578, G Loss: 0.6693671941757202\n",
      "Epoch 63, batch 173 D Loss: 1.4439622163772583, G Loss: 0.6845533847808838\n",
      "Epoch 63, batch 174 D Loss: 1.4289988279342651, G Loss: 0.674928605556488\n",
      "Epoch 63, batch 175 D Loss: 1.3630435466766357, G Loss: 0.690019965171814\n",
      "Epoch 63, batch 176 D Loss: 1.4176807403564453, G Loss: 0.6791066527366638\n",
      "Epoch 63, batch 177 D Loss: 1.3826444149017334, G Loss: 0.6780005693435669\n",
      "Epoch 63, batch 178 D Loss: 1.452533483505249, G Loss: 0.6461212038993835\n",
      "Epoch 63, batch 179 D Loss: 1.4690265655517578, G Loss: 0.6524970531463623\n",
      "Epoch 63, batch 180 D Loss: 1.4656848907470703, G Loss: 0.6701658368110657\n",
      "Epoch 63, batch 181 D Loss: 1.4203720092773438, G Loss: 0.666638970375061\n",
      "Epoch 63, batch 182 D Loss: 1.387110948562622, G Loss: 0.6654841899871826\n",
      "Epoch 63, batch 183 D Loss: 1.4394885301589966, G Loss: 0.6681901812553406\n",
      "Epoch 63, batch 184 D Loss: 1.3990399837493896, G Loss: 0.6756523251533508\n",
      "Epoch 63, batch 185 D Loss: 1.4044238328933716, G Loss: 0.6652620434761047\n",
      "Epoch 63, batch 186 D Loss: 1.4294052124023438, G Loss: 0.6724735498428345\n",
      "Epoch 63, batch 187 D Loss: 1.4631567001342773, G Loss: 0.6639850735664368\n",
      "Epoch 63, batch 188 D Loss: 1.4327176809310913, G Loss: 0.6836206316947937\n",
      "Epoch 63, batch 189 D Loss: 1.404606819152832, G Loss: 0.6923312544822693\n",
      "Epoch 63, batch 190 D Loss: 1.4313058853149414, G Loss: 0.6577289700508118\n",
      "Epoch 63, batch 191 D Loss: 1.470174789428711, G Loss: 0.6528719067573547\n",
      "Epoch 63, batch 192 D Loss: 1.4448975324630737, G Loss: 0.6728248596191406\n",
      "Epoch 63, batch 193 D Loss: 1.4254170656204224, G Loss: 0.6544415354728699\n",
      "Epoch 63, batch 194 D Loss: 1.4080393314361572, G Loss: 0.6740525960922241\n",
      "Epoch 63, batch 195 D Loss: 1.4256348609924316, G Loss: 0.6656712889671326\n",
      "Epoch 63, batch 196 D Loss: 1.4014813899993896, G Loss: 0.678139865398407\n",
      "Epoch 63, batch 197 D Loss: 1.442032814025879, G Loss: 0.6917513012886047\n",
      "Epoch 63, batch 198 D Loss: 1.428424596786499, G Loss: 0.6573739051818848\n",
      "Epoch 63, batch 199 D Loss: 1.454113245010376, G Loss: 0.6500791311264038\n",
      "Epoch 63, batch 200 D Loss: 1.4236164093017578, G Loss: 0.6710803508758545\n",
      "Epoch 64, batch 1 D Loss: 1.4645545482635498, G Loss: 0.6550672054290771\n",
      "Epoch 64, batch 2 D Loss: 1.451182246208191, G Loss: 0.6546105146408081\n",
      "Epoch 64, batch 3 D Loss: 1.4198521375656128, G Loss: 0.6952985525131226\n",
      "Epoch 64, batch 4 D Loss: 1.4264347553253174, G Loss: 0.6888012886047363\n",
      "Epoch 64, batch 5 D Loss: 1.412837028503418, G Loss: 0.6903833150863647\n",
      "Epoch 64, batch 6 D Loss: 1.451292872428894, G Loss: 0.674553394317627\n",
      "Epoch 64, batch 7 D Loss: 1.4424793720245361, G Loss: 0.6775785088539124\n",
      "Epoch 64, batch 8 D Loss: 1.3993712663650513, G Loss: 0.7100513577461243\n",
      "Epoch 64, batch 9 D Loss: 1.4336621761322021, G Loss: 0.6904637813568115\n",
      "Epoch 64, batch 10 D Loss: 1.415313482284546, G Loss: 0.6920414566993713\n",
      "Epoch 64, batch 11 D Loss: 1.387930989265442, G Loss: 0.72123122215271\n",
      "Epoch 64, batch 12 D Loss: 1.399877905845642, G Loss: 0.7141985893249512\n",
      "Epoch 64, batch 13 D Loss: 1.3874220848083496, G Loss: 0.7102072834968567\n",
      "Epoch 64, batch 14 D Loss: 1.404287338256836, G Loss: 0.6993643045425415\n",
      "Epoch 64, batch 15 D Loss: 1.391366720199585, G Loss: 0.7065949440002441\n",
      "Epoch 64, batch 16 D Loss: 1.411901593208313, G Loss: 0.7107285857200623\n",
      "Epoch 64, batch 17 D Loss: 1.3849960565567017, G Loss: 0.7115797996520996\n",
      "Epoch 64, batch 18 D Loss: 1.3875234127044678, G Loss: 0.7317476868629456\n",
      "Epoch 64, batch 19 D Loss: 1.3687195777893066, G Loss: 0.7338814735412598\n",
      "Epoch 64, batch 20 D Loss: 1.3373017311096191, G Loss: 0.7415792942047119\n",
      "Epoch 64, batch 21 D Loss: 1.396246314048767, G Loss: 0.7158674597740173\n",
      "Epoch 64, batch 22 D Loss: 1.3945366144180298, G Loss: 0.7191080451011658\n",
      "Epoch 64, batch 23 D Loss: 1.3809680938720703, G Loss: 0.7228999137878418\n",
      "Epoch 64, batch 24 D Loss: 1.359764575958252, G Loss: 0.7237377166748047\n",
      "Epoch 64, batch 25 D Loss: 1.3628439903259277, G Loss: 0.7157429456710815\n",
      "Epoch 64, batch 26 D Loss: 1.390036940574646, G Loss: 0.7177214026451111\n",
      "Epoch 64, batch 27 D Loss: 1.3387227058410645, G Loss: 0.7283574938774109\n",
      "Epoch 64, batch 28 D Loss: 1.3398873805999756, G Loss: 0.7414298057556152\n",
      "Epoch 64, batch 29 D Loss: 1.356926441192627, G Loss: 0.7391542196273804\n",
      "Epoch 64, batch 30 D Loss: 1.3658536672592163, G Loss: 0.7287035584449768\n",
      "Epoch 64, batch 31 D Loss: 1.3499341011047363, G Loss: 0.7438956499099731\n",
      "Epoch 64, batch 32 D Loss: 1.3537877798080444, G Loss: 0.7373548746109009\n",
      "Epoch 64, batch 33 D Loss: 1.3426029682159424, G Loss: 0.7474332451820374\n",
      "Epoch 64, batch 34 D Loss: 1.358104944229126, G Loss: 0.7661007642745972\n",
      "Epoch 64, batch 35 D Loss: 1.3539612293243408, G Loss: 0.7434354424476624\n",
      "Epoch 64, batch 36 D Loss: 1.3476665019989014, G Loss: 0.7489497661590576\n",
      "Epoch 64, batch 37 D Loss: 1.315522313117981, G Loss: 0.7691380977630615\n",
      "Epoch 64, batch 38 D Loss: 1.35272216796875, G Loss: 0.7395988702774048\n",
      "Epoch 64, batch 39 D Loss: 1.355360984802246, G Loss: 0.7366861701011658\n",
      "Epoch 64, batch 40 D Loss: 1.376542329788208, G Loss: 0.7037219405174255\n",
      "Epoch 64, batch 41 D Loss: 1.3558589220046997, G Loss: 0.7450951933860779\n",
      "Epoch 64, batch 42 D Loss: 1.3242826461791992, G Loss: 0.7494043707847595\n",
      "Epoch 64, batch 43 D Loss: 1.3363392353057861, G Loss: 0.7443768382072449\n",
      "Epoch 64, batch 44 D Loss: 1.3358854055404663, G Loss: 0.7564007639884949\n",
      "Epoch 64, batch 45 D Loss: 1.322141170501709, G Loss: 0.7488136291503906\n",
      "Epoch 64, batch 46 D Loss: 1.3301206827163696, G Loss: 0.7351171374320984\n",
      "Epoch 64, batch 47 D Loss: 1.3191956281661987, G Loss: 0.745433509349823\n",
      "Epoch 64, batch 48 D Loss: 1.329627275466919, G Loss: 0.7293927073478699\n",
      "Epoch 64, batch 49 D Loss: 1.376793384552002, G Loss: 0.7003844976425171\n",
      "Epoch 64, batch 50 D Loss: 1.3737266063690186, G Loss: 0.7311365604400635\n",
      "Epoch 64, batch 51 D Loss: 1.3227777481079102, G Loss: 0.7527419328689575\n",
      "Epoch 64, batch 52 D Loss: 1.3477473258972168, G Loss: 0.7072497606277466\n",
      "Epoch 64, batch 53 D Loss: 1.40815269947052, G Loss: 0.6776161193847656\n",
      "Epoch 64, batch 54 D Loss: 1.342539668083191, G Loss: 0.709088921546936\n",
      "Epoch 64, batch 55 D Loss: 1.3195321559906006, G Loss: 0.7053490281105042\n",
      "Epoch 64, batch 56 D Loss: 1.3658027648925781, G Loss: 0.7124047875404358\n",
      "Epoch 64, batch 57 D Loss: 1.3632148504257202, G Loss: 0.6910483837127686\n",
      "Epoch 64, batch 58 D Loss: 1.3735222816467285, G Loss: 0.7007383108139038\n",
      "Epoch 64, batch 59 D Loss: 1.3852999210357666, G Loss: 0.674526035785675\n",
      "Epoch 64, batch 60 D Loss: 1.344803810119629, G Loss: 0.702608585357666\n",
      "Epoch 64, batch 61 D Loss: 1.3640031814575195, G Loss: 0.7051975727081299\n",
      "Epoch 64, batch 62 D Loss: 1.3716861009597778, G Loss: 0.6927587985992432\n",
      "Epoch 64, batch 63 D Loss: 1.3715362548828125, G Loss: 0.677096426486969\n",
      "Epoch 64, batch 64 D Loss: 1.3482747077941895, G Loss: 0.7114413976669312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, batch 65 D Loss: 1.3504142761230469, G Loss: 0.6831661462783813\n",
      "Epoch 64, batch 66 D Loss: 1.3754007816314697, G Loss: 0.6875400543212891\n",
      "Epoch 64, batch 67 D Loss: 1.3213486671447754, G Loss: 0.7171497941017151\n",
      "Epoch 64, batch 68 D Loss: 1.3245453834533691, G Loss: 0.6831823587417603\n",
      "Epoch 64, batch 69 D Loss: 1.3200170993804932, G Loss: 0.7169742584228516\n",
      "Epoch 64, batch 70 D Loss: 1.3269577026367188, G Loss: 0.676278829574585\n",
      "Epoch 64, batch 71 D Loss: 1.3330910205841064, G Loss: 0.6682234406471252\n",
      "Epoch 64, batch 72 D Loss: 1.3595744371414185, G Loss: 0.6702835559844971\n",
      "Epoch 64, batch 73 D Loss: 1.4129148721694946, G Loss: 0.6817173957824707\n",
      "Epoch 64, batch 74 D Loss: 1.367504596710205, G Loss: 0.664957582950592\n",
      "Epoch 64, batch 75 D Loss: 1.3560235500335693, G Loss: 0.6904623508453369\n",
      "Epoch 64, batch 76 D Loss: 1.3420917987823486, G Loss: 0.6864492297172546\n",
      "Epoch 64, batch 77 D Loss: 1.3912177085876465, G Loss: 0.6799015998840332\n",
      "Epoch 64, batch 78 D Loss: 1.382768988609314, G Loss: 0.6706802248954773\n",
      "Epoch 64, batch 79 D Loss: 1.4736076593399048, G Loss: 0.6302893161773682\n",
      "Epoch 64, batch 80 D Loss: 1.3834093809127808, G Loss: 0.6697667837142944\n",
      "Epoch 64, batch 81 D Loss: 1.40664803981781, G Loss: 0.6656262874603271\n",
      "Epoch 64, batch 82 D Loss: 1.3746974468231201, G Loss: 0.6574069857597351\n",
      "Epoch 64, batch 83 D Loss: 1.4431169033050537, G Loss: 0.6545413732528687\n",
      "Epoch 64, batch 84 D Loss: 1.3306589126586914, G Loss: 0.6964913010597229\n",
      "Epoch 64, batch 85 D Loss: 1.4277474880218506, G Loss: 0.6421170830726624\n",
      "Epoch 64, batch 86 D Loss: 1.4215011596679688, G Loss: 0.6570084095001221\n",
      "Epoch 64, batch 87 D Loss: 1.4067578315734863, G Loss: 0.6552376747131348\n",
      "Epoch 64, batch 88 D Loss: 1.3976027965545654, G Loss: 0.6768944263458252\n",
      "Epoch 64, batch 89 D Loss: 1.4142296314239502, G Loss: 0.6306120157241821\n",
      "Epoch 64, batch 90 D Loss: 1.3338075876235962, G Loss: 0.7024827599525452\n",
      "Epoch 64, batch 91 D Loss: 1.4347586631774902, G Loss: 0.6479209065437317\n",
      "Epoch 64, batch 92 D Loss: 1.3985388278961182, G Loss: 0.6936765909194946\n",
      "Epoch 64, batch 93 D Loss: 1.4570019245147705, G Loss: 0.6458143591880798\n",
      "Epoch 64, batch 94 D Loss: 1.3251919746398926, G Loss: 0.691877007484436\n",
      "Epoch 64, batch 95 D Loss: 1.4259202480316162, G Loss: 0.6646957397460938\n",
      "Epoch 64, batch 96 D Loss: 1.4666379690170288, G Loss: 0.6538514494895935\n",
      "Epoch 64, batch 97 D Loss: 1.4668962955474854, G Loss: 0.6321994066238403\n",
      "Epoch 64, batch 98 D Loss: 1.4762520790100098, G Loss: 0.6597091555595398\n",
      "Epoch 64, batch 99 D Loss: 1.4089208841323853, G Loss: 0.6701452732086182\n",
      "Epoch 64, batch 100 D Loss: 1.457432746887207, G Loss: 0.6531221270561218\n",
      "Epoch 64, batch 101 D Loss: 1.4968783855438232, G Loss: 0.6501532196998596\n",
      "Epoch 64, batch 102 D Loss: 1.4772380590438843, G Loss: 0.6803018450737\n",
      "Epoch 64, batch 103 D Loss: 1.4498714208602905, G Loss: 0.6653204560279846\n",
      "Epoch 64, batch 104 D Loss: 1.427290678024292, G Loss: 0.6745566725730896\n",
      "Epoch 64, batch 105 D Loss: 1.418438196182251, G Loss: 0.679496169090271\n",
      "Epoch 64, batch 106 D Loss: 1.5249851942062378, G Loss: 0.615394651889801\n",
      "Epoch 64, batch 107 D Loss: 1.4565348625183105, G Loss: 0.6557843089103699\n",
      "Epoch 64, batch 108 D Loss: 1.444830060005188, G Loss: 0.6721550226211548\n",
      "Epoch 64, batch 109 D Loss: 1.379575252532959, G Loss: 0.7146250009536743\n",
      "Epoch 64, batch 110 D Loss: 1.4512839317321777, G Loss: 0.6788896322250366\n",
      "Epoch 64, batch 111 D Loss: 1.4131476879119873, G Loss: 0.694029688835144\n",
      "Epoch 64, batch 112 D Loss: 1.4553580284118652, G Loss: 0.6933609843254089\n",
      "Epoch 64, batch 113 D Loss: 1.4899886846542358, G Loss: 0.6367173790931702\n",
      "Epoch 64, batch 114 D Loss: 1.4106117486953735, G Loss: 0.696139395236969\n",
      "Epoch 64, batch 115 D Loss: 1.4432971477508545, G Loss: 0.6961435079574585\n",
      "Epoch 64, batch 116 D Loss: 1.3853936195373535, G Loss: 0.7012035250663757\n",
      "Epoch 64, batch 117 D Loss: 1.4203810691833496, G Loss: 0.6948120594024658\n",
      "Epoch 64, batch 118 D Loss: 1.48798406124115, G Loss: 0.6551408171653748\n",
      "Epoch 64, batch 119 D Loss: 1.4965628385543823, G Loss: 0.6472935676574707\n",
      "Epoch 64, batch 120 D Loss: 1.3968231678009033, G Loss: 0.6998245120048523\n",
      "Epoch 64, batch 121 D Loss: 1.4037171602249146, G Loss: 0.7008937001228333\n",
      "Epoch 64, batch 122 D Loss: 1.438852310180664, G Loss: 0.6912006139755249\n",
      "Epoch 64, batch 123 D Loss: 1.4350007772445679, G Loss: 0.680399477481842\n",
      "Epoch 64, batch 124 D Loss: 1.4495431184768677, G Loss: 0.6916501522064209\n",
      "Epoch 64, batch 125 D Loss: 1.442880392074585, G Loss: 0.6736943125724792\n",
      "Epoch 64, batch 126 D Loss: 1.4435362815856934, G Loss: 0.6945970058441162\n",
      "Epoch 64, batch 127 D Loss: 1.3902385234832764, G Loss: 0.7067966461181641\n",
      "Epoch 64, batch 128 D Loss: 1.4155123233795166, G Loss: 0.7031549215316772\n",
      "Epoch 64, batch 129 D Loss: 1.414571762084961, G Loss: 0.7034198641777039\n",
      "Epoch 64, batch 130 D Loss: 1.3892810344696045, G Loss: 0.7269488573074341\n",
      "Epoch 64, batch 131 D Loss: 1.4135111570358276, G Loss: 0.697231650352478\n",
      "Epoch 64, batch 132 D Loss: 1.475060224533081, G Loss: 0.6693553924560547\n",
      "Epoch 64, batch 133 D Loss: 1.422891616821289, G Loss: 0.7008005380630493\n",
      "Epoch 64, batch 134 D Loss: 1.3973660469055176, G Loss: 0.7280694842338562\n",
      "Epoch 64, batch 135 D Loss: 1.4309886693954468, G Loss: 0.7097492218017578\n",
      "Epoch 64, batch 136 D Loss: 1.4004368782043457, G Loss: 0.7221410274505615\n",
      "Epoch 64, batch 137 D Loss: 1.4458688497543335, G Loss: 0.6881656646728516\n",
      "Epoch 64, batch 138 D Loss: 1.4077318906784058, G Loss: 0.7104780077934265\n",
      "Epoch 64, batch 139 D Loss: 1.4118163585662842, G Loss: 0.7156360745429993\n",
      "Epoch 64, batch 140 D Loss: 1.4264273643493652, G Loss: 0.6994985938072205\n",
      "Epoch 64, batch 141 D Loss: 1.3967688083648682, G Loss: 0.7236766219139099\n",
      "Epoch 64, batch 142 D Loss: 1.4140704870224, G Loss: 0.7265139818191528\n",
      "Epoch 64, batch 143 D Loss: 1.439997911453247, G Loss: 0.7120878100395203\n",
      "Epoch 64, batch 144 D Loss: 1.4104496240615845, G Loss: 0.7212827205657959\n",
      "Epoch 64, batch 145 D Loss: 1.4229531288146973, G Loss: 0.7126837372779846\n",
      "Epoch 64, batch 146 D Loss: 1.4251909255981445, G Loss: 0.7240925431251526\n",
      "Epoch 64, batch 147 D Loss: 1.4093937873840332, G Loss: 0.731228768825531\n",
      "Epoch 64, batch 148 D Loss: 1.4063866138458252, G Loss: 0.7449374198913574\n",
      "Epoch 64, batch 149 D Loss: 1.3930935859680176, G Loss: 0.744422197341919\n",
      "Epoch 64, batch 150 D Loss: 1.4125282764434814, G Loss: 0.725675106048584\n",
      "Epoch 64, batch 151 D Loss: 1.401038646697998, G Loss: 0.7550427913665771\n",
      "Epoch 64, batch 152 D Loss: 1.3963135480880737, G Loss: 0.742377758026123\n",
      "Epoch 64, batch 153 D Loss: 1.389845609664917, G Loss: 0.7474014759063721\n",
      "Epoch 64, batch 154 D Loss: 1.3822388648986816, G Loss: 0.7664219737052917\n",
      "Epoch 64, batch 155 D Loss: 1.3875658512115479, G Loss: 0.7566283345222473\n",
      "Epoch 64, batch 156 D Loss: 1.377529263496399, G Loss: 0.7693599462509155\n",
      "Epoch 64, batch 157 D Loss: 1.3686156272888184, G Loss: 0.7765191793441772\n",
      "Epoch 64, batch 158 D Loss: 1.371431827545166, G Loss: 0.7891235947608948\n",
      "Epoch 64, batch 159 D Loss: 1.3826549053192139, G Loss: 0.7695751786231995\n",
      "Epoch 64, batch 160 D Loss: 1.3607633113861084, G Loss: 0.7956142425537109\n",
      "Epoch 64, batch 161 D Loss: 1.3610954284667969, G Loss: 0.7786991596221924\n",
      "Epoch 64, batch 162 D Loss: 1.358667016029358, G Loss: 0.8077581524848938\n",
      "Epoch 64, batch 163 D Loss: 1.357633113861084, G Loss: 0.8059675097465515\n",
      "Epoch 64, batch 164 D Loss: 1.344531774520874, G Loss: 0.8215912580490112\n",
      "Epoch 64, batch 165 D Loss: 1.3449782133102417, G Loss: 0.8220094442367554\n",
      "Epoch 64, batch 166 D Loss: 1.343443751335144, G Loss: 0.8267062306404114\n",
      "Epoch 64, batch 167 D Loss: 1.351953387260437, G Loss: 0.8317306041717529\n",
      "Epoch 64, batch 168 D Loss: 1.3404433727264404, G Loss: 0.8463588953018188\n",
      "Epoch 64, batch 169 D Loss: 1.3417969942092896, G Loss: 0.8482669591903687\n",
      "Epoch 64, batch 170 D Loss: 1.347672700881958, G Loss: 0.8617643713951111\n",
      "Epoch 64, batch 171 D Loss: 1.3259263038635254, G Loss: 0.8787885904312134\n",
      "Epoch 64, batch 172 D Loss: 1.3313648700714111, G Loss: 0.8100698590278625\n",
      "Epoch 64, batch 173 D Loss: 1.3255109786987305, G Loss: 0.8525999188423157\n",
      "Epoch 64, batch 174 D Loss: 1.3157076835632324, G Loss: 0.8701698184013367\n",
      "Epoch 64, batch 175 D Loss: 1.3303868770599365, G Loss: 0.8742735981941223\n",
      "Epoch 64, batch 176 D Loss: 1.373255729675293, G Loss: 0.8408808708190918\n",
      "Epoch 64, batch 177 D Loss: 1.410171627998352, G Loss: 0.8212869763374329\n",
      "Epoch 64, batch 178 D Loss: 1.3562747240066528, G Loss: 0.8351358771324158\n",
      "Epoch 64, batch 179 D Loss: 1.3537935018539429, G Loss: 0.8063262701034546\n",
      "Epoch 64, batch 180 D Loss: 1.3866015672683716, G Loss: 0.8692910671234131\n",
      "Epoch 64, batch 181 D Loss: 1.328370213508606, G Loss: 0.8257623910903931\n",
      "Epoch 64, batch 182 D Loss: 1.375496506690979, G Loss: 0.8245859742164612\n",
      "Epoch 64, batch 183 D Loss: 1.4027831554412842, G Loss: 0.7626545429229736\n",
      "Epoch 64, batch 184 D Loss: 1.4447835683822632, G Loss: 0.769059419631958\n",
      "Epoch 64, batch 185 D Loss: 1.3895416259765625, G Loss: 0.7388120889663696\n",
      "Epoch 64, batch 186 D Loss: 1.4229459762573242, G Loss: 0.7852596044540405\n",
      "Epoch 64, batch 187 D Loss: 1.4267749786376953, G Loss: 0.7438378930091858\n",
      "Epoch 64, batch 188 D Loss: 1.4142844676971436, G Loss: 0.7798892855644226\n",
      "Epoch 64, batch 189 D Loss: 1.4424959421157837, G Loss: 0.7526618242263794\n",
      "Epoch 64, batch 190 D Loss: 1.4051107168197632, G Loss: 0.7413256168365479\n",
      "Epoch 64, batch 191 D Loss: 1.440361499786377, G Loss: 0.7463403344154358\n",
      "Epoch 64, batch 192 D Loss: 1.3731803894042969, G Loss: 0.7263631224632263\n",
      "Epoch 64, batch 193 D Loss: 1.379895806312561, G Loss: 0.7265089154243469\n",
      "Epoch 64, batch 194 D Loss: 1.4431495666503906, G Loss: 0.721656322479248\n",
      "Epoch 64, batch 195 D Loss: 1.3983283042907715, G Loss: 0.7028078436851501\n",
      "Epoch 64, batch 196 D Loss: 1.3717337846755981, G Loss: 0.7305632829666138\n",
      "Epoch 64, batch 197 D Loss: 1.425052523612976, G Loss: 0.7106035351753235\n",
      "Epoch 64, batch 198 D Loss: 1.4137566089630127, G Loss: 0.7060984969139099\n",
      "Epoch 64, batch 199 D Loss: 1.4118516445159912, G Loss: 0.7130354046821594\n",
      "Epoch 64, batch 200 D Loss: 1.4401509761810303, G Loss: 0.7023916840553284\n",
      "Epoch 65, batch 1 D Loss: 1.3986692428588867, G Loss: 0.701712429523468\n",
      "Epoch 65, batch 2 D Loss: 1.3940248489379883, G Loss: 0.70836341381073\n",
      "Epoch 65, batch 3 D Loss: 1.3874874114990234, G Loss: 0.6918955445289612\n",
      "Epoch 65, batch 4 D Loss: 1.4041763544082642, G Loss: 0.6884068250656128\n",
      "Epoch 65, batch 5 D Loss: 1.40664803981781, G Loss: 0.6908715963363647\n",
      "Epoch 65, batch 6 D Loss: 1.3804956674575806, G Loss: 0.6933807134628296\n",
      "Epoch 65, batch 7 D Loss: 1.3802998065948486, G Loss: 0.686038613319397\n",
      "Epoch 65, batch 8 D Loss: 1.393808126449585, G Loss: 0.6887871623039246\n",
      "Epoch 65, batch 9 D Loss: 1.3918089866638184, G Loss: 0.6853393316268921\n",
      "Epoch 65, batch 10 D Loss: 1.3922216892242432, G Loss: 0.687671959400177\n",
      "Epoch 65, batch 11 D Loss: 1.412135362625122, G Loss: 0.6905350685119629\n",
      "Epoch 65, batch 12 D Loss: 1.3714079856872559, G Loss: 0.6824337244033813\n",
      "Epoch 65, batch 13 D Loss: 1.3861587047576904, G Loss: 0.6820318102836609\n",
      "Epoch 65, batch 14 D Loss: 1.3921477794647217, G Loss: 0.6865739226341248\n",
      "Epoch 65, batch 15 D Loss: 1.3863205909729004, G Loss: 0.6880919933319092\n",
      "Epoch 65, batch 16 D Loss: 1.3635709285736084, G Loss: 0.6826322674751282\n",
      "Epoch 65, batch 17 D Loss: 1.3892821073532104, G Loss: 0.6808322072029114\n",
      "Epoch 65, batch 18 D Loss: 1.3955533504486084, G Loss: 0.6794412136077881\n",
      "Epoch 65, batch 19 D Loss: 1.3850786685943604, G Loss: 0.6882940530776978\n",
      "Epoch 65, batch 20 D Loss: 1.387033224105835, G Loss: 0.6761171817779541\n",
      "Epoch 65, batch 21 D Loss: 1.397462248802185, G Loss: 0.6713027954101562\n",
      "Epoch 65, batch 22 D Loss: 1.3815667629241943, G Loss: 0.6845331788063049\n",
      "Epoch 65, batch 23 D Loss: 1.4148244857788086, G Loss: 0.6790224313735962\n",
      "Epoch 65, batch 24 D Loss: 1.4040662050247192, G Loss: 0.6808473467826843\n",
      "Epoch 65, batch 25 D Loss: 1.3926670551300049, G Loss: 0.6701205968856812\n",
      "Epoch 65, batch 26 D Loss: 1.3783878087997437, G Loss: 0.6727031469345093\n",
      "Epoch 65, batch 27 D Loss: 1.3912241458892822, G Loss: 0.686160147190094\n",
      "Epoch 65, batch 28 D Loss: 1.3990676403045654, G Loss: 0.6710437536239624\n",
      "Epoch 65, batch 29 D Loss: 1.4050511121749878, G Loss: 0.6720513105392456\n",
      "Epoch 65, batch 30 D Loss: 1.4149240255355835, G Loss: 0.6699388027191162\n",
      "Epoch 65, batch 31 D Loss: 1.384418249130249, G Loss: 0.6829466223716736\n",
      "Epoch 65, batch 32 D Loss: 1.4198102951049805, G Loss: 0.6587236523628235\n",
      "Epoch 65, batch 33 D Loss: 1.3958964347839355, G Loss: 0.6709531545639038\n",
      "Epoch 65, batch 34 D Loss: 1.388763427734375, G Loss: 0.676419734954834\n",
      "Epoch 65, batch 35 D Loss: 1.400963544845581, G Loss: 0.6675662994384766\n",
      "Epoch 65, batch 36 D Loss: 1.3817636966705322, G Loss: 0.6816651821136475\n",
      "Epoch 65, batch 37 D Loss: 1.3917969465255737, G Loss: 0.6792116761207581\n",
      "Epoch 65, batch 38 D Loss: 1.3773235082626343, G Loss: 0.6814031004905701\n",
      "Epoch 65, batch 39 D Loss: 1.419506549835205, G Loss: 0.6538564562797546\n",
      "Epoch 65, batch 40 D Loss: 1.3897569179534912, G Loss: 0.6642423272132874\n",
      "Epoch 65, batch 41 D Loss: 1.4095864295959473, G Loss: 0.6811305284500122\n",
      "Epoch 65, batch 42 D Loss: 1.382822036743164, G Loss: 0.6746649742126465\n",
      "Epoch 65, batch 43 D Loss: 1.397317886352539, G Loss: 0.6690308451652527\n",
      "Epoch 65, batch 44 D Loss: 1.3938210010528564, G Loss: 0.675517737865448\n",
      "Epoch 65, batch 45 D Loss: 1.418168544769287, G Loss: 0.6615076661109924\n",
      "Epoch 65, batch 46 D Loss: 1.3827778100967407, G Loss: 0.6828916668891907\n",
      "Epoch 65, batch 47 D Loss: 1.4069972038269043, G Loss: 0.666831374168396\n",
      "Epoch 65, batch 48 D Loss: 1.412254810333252, G Loss: 0.6646432280540466\n",
      "Epoch 65, batch 49 D Loss: 1.396483063697815, G Loss: 0.6762101650238037\n",
      "Epoch 65, batch 50 D Loss: 1.392404556274414, G Loss: 0.6796395778656006\n",
      "Epoch 65, batch 51 D Loss: 1.4001259803771973, G Loss: 0.6659774780273438\n",
      "Epoch 65, batch 52 D Loss: 1.4034497737884521, G Loss: 0.6775943040847778\n",
      "Epoch 65, batch 53 D Loss: 1.3985553979873657, G Loss: 0.6670179963111877\n",
      "Epoch 65, batch 54 D Loss: 1.4032318592071533, G Loss: 0.6742016077041626\n",
      "Epoch 65, batch 55 D Loss: 1.4047207832336426, G Loss: 0.6773698329925537\n",
      "Epoch 65, batch 56 D Loss: 1.400756597518921, G Loss: 0.6754385232925415\n",
      "Epoch 65, batch 57 D Loss: 1.3928961753845215, G Loss: 0.687987744808197\n",
      "Epoch 65, batch 58 D Loss: 1.3939660787582397, G Loss: 0.6769931316375732\n",
      "Epoch 65, batch 59 D Loss: 1.397334098815918, G Loss: 0.6836755871772766\n",
      "Epoch 65, batch 60 D Loss: 1.3939647674560547, G Loss: 0.6846180558204651\n",
      "Epoch 65, batch 61 D Loss: 1.3927390575408936, G Loss: 0.6831857562065125\n",
      "Epoch 65, batch 62 D Loss: 1.4053049087524414, G Loss: 0.6726980805397034\n",
      "Epoch 65, batch 63 D Loss: 1.3805196285247803, G Loss: 0.6957266330718994\n",
      "Epoch 65, batch 64 D Loss: 1.4018466472625732, G Loss: 0.6746709942817688\n",
      "Epoch 65, batch 65 D Loss: 1.406229853630066, G Loss: 0.6720720529556274\n",
      "Epoch 65, batch 66 D Loss: 1.3853387832641602, G Loss: 0.6882477402687073\n",
      "Epoch 65, batch 67 D Loss: 1.3988678455352783, G Loss: 0.6807446479797363\n",
      "Epoch 65, batch 68 D Loss: 1.392474889755249, G Loss: 0.6798367500305176\n",
      "Epoch 65, batch 69 D Loss: 1.3922736644744873, G Loss: 0.6863643527030945\n",
      "Epoch 65, batch 70 D Loss: 1.3882757425308228, G Loss: 0.6877485513687134\n",
      "Epoch 65, batch 71 D Loss: 1.3918251991271973, G Loss: 0.6833293437957764\n",
      "Epoch 65, batch 72 D Loss: 1.390584945678711, G Loss: 0.6874002814292908\n",
      "Epoch 65, batch 73 D Loss: 1.3749667406082153, G Loss: 0.6960405111312866\n",
      "Epoch 65, batch 74 D Loss: 1.3832905292510986, G Loss: 0.6904823184013367\n",
      "Epoch 65, batch 75 D Loss: 1.38726806640625, G Loss: 0.6955928802490234\n",
      "Epoch 65, batch 76 D Loss: 1.3895460367202759, G Loss: 0.6890498399734497\n",
      "Epoch 65, batch 77 D Loss: 1.3832392692565918, G Loss: 0.6879278421401978\n",
      "Epoch 65, batch 78 D Loss: 1.3766694068908691, G Loss: 0.6998775601387024\n",
      "Epoch 65, batch 79 D Loss: 1.389209270477295, G Loss: 0.6937426924705505\n",
      "Epoch 65, batch 80 D Loss: 1.362410306930542, G Loss: 0.6984122395515442\n",
      "Epoch 65, batch 81 D Loss: 1.366706132888794, G Loss: 0.6955283284187317\n",
      "Epoch 65, batch 82 D Loss: 1.348675012588501, G Loss: 0.7024898529052734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, batch 83 D Loss: 1.3427011966705322, G Loss: 0.7051948308944702\n",
      "Epoch 65, batch 84 D Loss: 1.361730933189392, G Loss: 0.6944332122802734\n",
      "Epoch 65, batch 85 D Loss: 1.3560128211975098, G Loss: 0.7194831967353821\n",
      "Epoch 65, batch 86 D Loss: 1.3568568229675293, G Loss: 0.7017806768417358\n",
      "Epoch 65, batch 87 D Loss: 1.3483011722564697, G Loss: 0.7187621593475342\n",
      "Epoch 65, batch 88 D Loss: 1.3434100151062012, G Loss: 0.7288095951080322\n",
      "Epoch 65, batch 89 D Loss: 1.3382325172424316, G Loss: 0.7335078716278076\n",
      "Epoch 65, batch 90 D Loss: 1.3280882835388184, G Loss: 0.7284460663795471\n",
      "Epoch 65, batch 91 D Loss: 1.339810848236084, G Loss: 0.7221590280532837\n",
      "Epoch 65, batch 92 D Loss: 1.3594155311584473, G Loss: 0.7134059071540833\n",
      "Epoch 65, batch 93 D Loss: 1.3545589447021484, G Loss: 0.702781617641449\n",
      "Epoch 65, batch 94 D Loss: 1.3750693798065186, G Loss: 0.705521821975708\n",
      "Epoch 65, batch 95 D Loss: 1.3827481269836426, G Loss: 0.6894687414169312\n",
      "Epoch 65, batch 96 D Loss: 1.3584349155426025, G Loss: 0.7051418423652649\n",
      "Epoch 65, batch 97 D Loss: 1.326242208480835, G Loss: 0.7365972399711609\n",
      "Epoch 65, batch 98 D Loss: 1.3279595375061035, G Loss: 0.7567858695983887\n",
      "Epoch 65, batch 99 D Loss: 1.349690556526184, G Loss: 0.7201516032218933\n",
      "Epoch 65, batch 100 D Loss: 1.3749850988388062, G Loss: 0.709245502948761\n",
      "Epoch 65, batch 101 D Loss: 1.3490804433822632, G Loss: 0.7101803421974182\n",
      "Epoch 65, batch 102 D Loss: 1.3655614852905273, G Loss: 0.6975829601287842\n",
      "Epoch 65, batch 103 D Loss: 1.3462622165679932, G Loss: 0.7209428548812866\n",
      "Epoch 65, batch 104 D Loss: 1.3123912811279297, G Loss: 0.7491174936294556\n",
      "Epoch 65, batch 105 D Loss: 1.37677001953125, G Loss: 0.6926230788230896\n",
      "Epoch 65, batch 106 D Loss: 1.355458378791809, G Loss: 0.6885504722595215\n",
      "Epoch 65, batch 107 D Loss: 1.4022040367126465, G Loss: 0.6993985176086426\n",
      "Epoch 65, batch 108 D Loss: 1.3623898029327393, G Loss: 0.6999058723449707\n",
      "Epoch 65, batch 109 D Loss: 1.4254647493362427, G Loss: 0.6631935238838196\n",
      "Epoch 65, batch 110 D Loss: 1.3965615034103394, G Loss: 0.6779979467391968\n",
      "Epoch 65, batch 111 D Loss: 1.4305129051208496, G Loss: 0.658094048500061\n",
      "Epoch 65, batch 112 D Loss: 1.3981049060821533, G Loss: 0.6875471472740173\n",
      "Epoch 65, batch 113 D Loss: 1.42218816280365, G Loss: 0.6875396966934204\n",
      "Epoch 65, batch 114 D Loss: 1.4189304113388062, G Loss: 0.6924067139625549\n",
      "Epoch 65, batch 115 D Loss: 1.388551115989685, G Loss: 0.6926229000091553\n",
      "Epoch 65, batch 116 D Loss: 1.3620400428771973, G Loss: 0.6952034831047058\n",
      "Epoch 65, batch 117 D Loss: 1.3903377056121826, G Loss: 0.6997155547142029\n",
      "Epoch 65, batch 118 D Loss: 1.4109256267547607, G Loss: 0.6913665533065796\n",
      "Epoch 65, batch 119 D Loss: 1.4111905097961426, G Loss: 0.6691086292266846\n",
      "Epoch 65, batch 120 D Loss: 1.4074904918670654, G Loss: 0.6886918544769287\n",
      "Epoch 65, batch 121 D Loss: 1.362419843673706, G Loss: 0.7004488110542297\n",
      "Epoch 65, batch 122 D Loss: 1.4225687980651855, G Loss: 0.7064037919044495\n",
      "Epoch 65, batch 123 D Loss: 1.3821911811828613, G Loss: 0.6972297430038452\n",
      "Epoch 65, batch 124 D Loss: 1.3784070014953613, G Loss: 0.703421950340271\n",
      "Epoch 65, batch 125 D Loss: 1.4422156810760498, G Loss: 0.6788411140441895\n",
      "Epoch 65, batch 126 D Loss: 1.4271807670593262, G Loss: 0.6758107542991638\n",
      "Epoch 65, batch 127 D Loss: 1.3878600597381592, G Loss: 0.6988847255706787\n",
      "Epoch 65, batch 128 D Loss: 1.4027762413024902, G Loss: 0.7119296193122864\n",
      "Epoch 65, batch 129 D Loss: 1.4188811779022217, G Loss: 0.7034223675727844\n",
      "Epoch 65, batch 130 D Loss: 1.4062905311584473, G Loss: 0.678834855556488\n",
      "Epoch 65, batch 131 D Loss: 1.498072862625122, G Loss: 0.6680387854576111\n",
      "Epoch 65, batch 132 D Loss: 1.429629921913147, G Loss: 0.6844017505645752\n",
      "Epoch 65, batch 133 D Loss: 1.4177831411361694, G Loss: 0.7035536170005798\n",
      "Epoch 65, batch 134 D Loss: 1.4221789836883545, G Loss: 0.7090312242507935\n",
      "Epoch 65, batch 135 D Loss: 1.3998183012008667, G Loss: 0.6997987627983093\n",
      "Epoch 65, batch 136 D Loss: 1.3904118537902832, G Loss: 0.7055091261863708\n",
      "Epoch 65, batch 137 D Loss: 1.4098458290100098, G Loss: 0.6991788744926453\n",
      "Epoch 65, batch 138 D Loss: 1.4029638767242432, G Loss: 0.6957240104675293\n",
      "Epoch 65, batch 139 D Loss: 1.4232177734375, G Loss: 0.6887809634208679\n",
      "Epoch 65, batch 140 D Loss: 1.3937230110168457, G Loss: 0.7079723477363586\n",
      "Epoch 65, batch 141 D Loss: 1.415797233581543, G Loss: 0.7036247253417969\n",
      "Epoch 65, batch 142 D Loss: 1.393827199935913, G Loss: 0.702894389629364\n",
      "Epoch 65, batch 143 D Loss: 1.408837080001831, G Loss: 0.7005659341812134\n",
      "Epoch 65, batch 144 D Loss: 1.418163537979126, G Loss: 0.6956511735916138\n",
      "Epoch 65, batch 145 D Loss: 1.4070050716400146, G Loss: 0.7140328288078308\n",
      "Epoch 65, batch 146 D Loss: 1.432291030883789, G Loss: 0.6896193027496338\n",
      "Epoch 65, batch 147 D Loss: 1.4101471900939941, G Loss: 0.6964574456214905\n",
      "Epoch 65, batch 148 D Loss: 1.3974697589874268, G Loss: 0.7093191742897034\n",
      "Epoch 65, batch 149 D Loss: 1.416248083114624, G Loss: 0.7088894844055176\n",
      "Epoch 65, batch 150 D Loss: 1.418877124786377, G Loss: 0.7057914733886719\n",
      "Epoch 65, batch 151 D Loss: 1.4167497158050537, G Loss: 0.7071971893310547\n",
      "Epoch 65, batch 152 D Loss: 1.41312837600708, G Loss: 0.7033113241195679\n",
      "Epoch 65, batch 153 D Loss: 1.4015604257583618, G Loss: 0.7112519145011902\n",
      "Epoch 65, batch 154 D Loss: 1.4176883697509766, G Loss: 0.7064634561538696\n",
      "Epoch 65, batch 155 D Loss: 1.391065239906311, G Loss: 0.7167030572891235\n",
      "Epoch 65, batch 156 D Loss: 1.4136371612548828, G Loss: 0.7216467261314392\n",
      "Epoch 65, batch 157 D Loss: 1.3981554508209229, G Loss: 0.7204654216766357\n",
      "Epoch 65, batch 158 D Loss: 1.3939831256866455, G Loss: 0.717021107673645\n",
      "Epoch 65, batch 159 D Loss: 1.402273178100586, G Loss: 0.7172897458076477\n",
      "Epoch 65, batch 160 D Loss: 1.4108473062515259, G Loss: 0.7280166149139404\n",
      "Epoch 65, batch 161 D Loss: 1.3951411247253418, G Loss: 0.7259224653244019\n",
      "Epoch 65, batch 162 D Loss: 1.4046602249145508, G Loss: 0.7139767408370972\n",
      "Epoch 65, batch 163 D Loss: 1.3953747749328613, G Loss: 0.7194676399230957\n",
      "Epoch 65, batch 164 D Loss: 1.3995118141174316, G Loss: 0.7129470109939575\n",
      "Epoch 65, batch 165 D Loss: 1.3825297355651855, G Loss: 0.7336075305938721\n",
      "Epoch 65, batch 166 D Loss: 1.3978447914123535, G Loss: 0.7220816612243652\n",
      "Epoch 65, batch 167 D Loss: 1.3921127319335938, G Loss: 0.7238250970840454\n",
      "Epoch 65, batch 168 D Loss: 1.3911938667297363, G Loss: 0.7321039438247681\n",
      "Epoch 65, batch 169 D Loss: 1.405452847480774, G Loss: 0.7144483923912048\n",
      "Epoch 65, batch 170 D Loss: 1.391037940979004, G Loss: 0.731169581413269\n",
      "Epoch 65, batch 171 D Loss: 1.3962364196777344, G Loss: 0.7211306095123291\n",
      "Epoch 65, batch 172 D Loss: 1.3865172863006592, G Loss: 0.721412181854248\n",
      "Epoch 65, batch 173 D Loss: 1.395071029663086, G Loss: 0.7200884819030762\n",
      "Epoch 65, batch 174 D Loss: 1.3911848068237305, G Loss: 0.7303866744041443\n",
      "Epoch 65, batch 175 D Loss: 1.3836636543273926, G Loss: 0.7321234941482544\n",
      "Epoch 65, batch 176 D Loss: 1.386070966720581, G Loss: 0.7237670421600342\n",
      "Epoch 65, batch 177 D Loss: 1.386113166809082, G Loss: 0.7346929907798767\n",
      "Epoch 65, batch 178 D Loss: 1.4000518321990967, G Loss: 0.7253028750419617\n",
      "Epoch 65, batch 179 D Loss: 1.3965039253234863, G Loss: 0.7204965353012085\n",
      "Epoch 65, batch 180 D Loss: 1.384199857711792, G Loss: 0.7326975464820862\n",
      "Epoch 65, batch 181 D Loss: 1.3939874172210693, G Loss: 0.7207053899765015\n",
      "Epoch 65, batch 182 D Loss: 1.3982598781585693, G Loss: 0.7213304042816162\n",
      "Epoch 65, batch 183 D Loss: 1.3842281103134155, G Loss: 0.7327377796173096\n",
      "Epoch 65, batch 184 D Loss: 1.390502691268921, G Loss: 0.7262922525405884\n",
      "Epoch 65, batch 185 D Loss: 1.377370834350586, G Loss: 0.7349507212638855\n",
      "Epoch 65, batch 186 D Loss: 1.3798896074295044, G Loss: 0.7342510223388672\n",
      "Epoch 65, batch 187 D Loss: 1.4014561176300049, G Loss: 0.7186336517333984\n",
      "Epoch 65, batch 188 D Loss: 1.3818514347076416, G Loss: 0.7358689308166504\n",
      "Epoch 65, batch 189 D Loss: 1.36722993850708, G Loss: 0.7443779706954956\n",
      "Epoch 65, batch 190 D Loss: 1.3820704221725464, G Loss: 0.7441232204437256\n",
      "Epoch 65, batch 191 D Loss: 1.3811779022216797, G Loss: 0.7326749563217163\n",
      "Epoch 65, batch 192 D Loss: 1.3875043392181396, G Loss: 0.7316653728485107\n",
      "Epoch 65, batch 193 D Loss: 1.368577003479004, G Loss: 0.7584750652313232\n",
      "Epoch 65, batch 194 D Loss: 1.3708734512329102, G Loss: 0.7346096634864807\n",
      "Epoch 65, batch 195 D Loss: 1.3876492977142334, G Loss: 0.7310830950737\n",
      "Epoch 65, batch 196 D Loss: 1.3868328332901, G Loss: 0.7314825654029846\n",
      "Epoch 65, batch 197 D Loss: 1.3787862062454224, G Loss: 0.7358066439628601\n",
      "Epoch 65, batch 198 D Loss: 1.389742136001587, G Loss: 0.733955442905426\n",
      "Epoch 65, batch 199 D Loss: 1.3657028675079346, G Loss: 0.7642549276351929\n",
      "Epoch 65, batch 200 D Loss: 1.3796896934509277, G Loss: 0.7447433471679688\n",
      "Epoch 66, batch 1 D Loss: 1.3907673358917236, G Loss: 0.7286229133605957\n",
      "Epoch 66, batch 2 D Loss: 1.3932199478149414, G Loss: 0.7349265217781067\n",
      "Epoch 66, batch 3 D Loss: 1.3770747184753418, G Loss: 0.7407689094543457\n",
      "Epoch 66, batch 4 D Loss: 1.3860740661621094, G Loss: 0.732848584651947\n",
      "Epoch 66, batch 5 D Loss: 1.392181158065796, G Loss: 0.7268097400665283\n",
      "Epoch 66, batch 6 D Loss: 1.3837015628814697, G Loss: 0.7285709381103516\n",
      "Epoch 66, batch 7 D Loss: 1.3795206546783447, G Loss: 0.7355685234069824\n",
      "Epoch 66, batch 8 D Loss: 1.3913452625274658, G Loss: 0.7319363355636597\n",
      "Epoch 66, batch 9 D Loss: 1.3836241960525513, G Loss: 0.7234017252922058\n",
      "Epoch 66, batch 10 D Loss: 1.3694745302200317, G Loss: 0.728438138961792\n",
      "Epoch 66, batch 11 D Loss: 1.367169976234436, G Loss: 0.7258670330047607\n",
      "Epoch 66, batch 12 D Loss: 1.3798105716705322, G Loss: 0.7288469672203064\n",
      "Epoch 66, batch 13 D Loss: 1.3713771104812622, G Loss: 0.7392774820327759\n",
      "Epoch 66, batch 14 D Loss: 1.366396188735962, G Loss: 0.7384166121482849\n",
      "Epoch 66, batch 15 D Loss: 1.380950927734375, G Loss: 0.7324877381324768\n",
      "Epoch 66, batch 16 D Loss: 1.3754888772964478, G Loss: 0.7250059247016907\n",
      "Epoch 66, batch 17 D Loss: 1.3850533962249756, G Loss: 0.7314009666442871\n",
      "Epoch 66, batch 18 D Loss: 1.3765292167663574, G Loss: 0.7298944592475891\n",
      "Epoch 66, batch 19 D Loss: 1.381290078163147, G Loss: 0.7142226696014404\n",
      "Epoch 66, batch 20 D Loss: 1.3844987154006958, G Loss: 0.7203240990638733\n",
      "Epoch 66, batch 21 D Loss: 1.3874907493591309, G Loss: 0.7170437574386597\n",
      "Epoch 66, batch 22 D Loss: 1.3722004890441895, G Loss: 0.7336065769195557\n",
      "Epoch 66, batch 23 D Loss: 1.3644707202911377, G Loss: 0.7398359775543213\n",
      "Epoch 66, batch 24 D Loss: 1.3754000663757324, G Loss: 0.7237069606781006\n",
      "Epoch 66, batch 25 D Loss: 1.3861489295959473, G Loss: 0.726519763469696\n",
      "Epoch 66, batch 26 D Loss: 1.384145975112915, G Loss: 0.7189823389053345\n",
      "Epoch 66, batch 27 D Loss: 1.3899272680282593, G Loss: 0.7284620404243469\n",
      "Epoch 66, batch 28 D Loss: 1.391476035118103, G Loss: 0.7202913761138916\n",
      "Epoch 66, batch 29 D Loss: 1.3689565658569336, G Loss: 0.730987548828125\n",
      "Epoch 66, batch 30 D Loss: 1.381721019744873, G Loss: 0.7146570086479187\n",
      "Epoch 66, batch 31 D Loss: 1.388246774673462, G Loss: 0.7145224809646606\n",
      "Epoch 66, batch 32 D Loss: 1.3761041164398193, G Loss: 0.7173775434494019\n",
      "Epoch 66, batch 33 D Loss: 1.3955366611480713, G Loss: 0.7199012041091919\n",
      "Epoch 66, batch 34 D Loss: 1.3995788097381592, G Loss: 0.705761730670929\n",
      "Epoch 66, batch 35 D Loss: 1.3919689655303955, G Loss: 0.7166710495948792\n",
      "Epoch 66, batch 36 D Loss: 1.3939476013183594, G Loss: 0.7105575799942017\n",
      "Epoch 66, batch 37 D Loss: 1.3907840251922607, G Loss: 0.7170941233634949\n",
      "Epoch 66, batch 38 D Loss: 1.3779680728912354, G Loss: 0.7316846251487732\n",
      "Epoch 66, batch 39 D Loss: 1.3888758420944214, G Loss: 0.7142960429191589\n",
      "Epoch 66, batch 40 D Loss: 1.3923299312591553, G Loss: 0.7081478238105774\n",
      "Epoch 66, batch 41 D Loss: 1.3918070793151855, G Loss: 0.7153379321098328\n",
      "Epoch 66, batch 42 D Loss: 1.400447130203247, G Loss: 0.7052575945854187\n",
      "Epoch 66, batch 43 D Loss: 1.3811112642288208, G Loss: 0.7163320183753967\n",
      "Epoch 66, batch 44 D Loss: 1.3963267803192139, G Loss: 0.7034276723861694\n",
      "Epoch 66, batch 45 D Loss: 1.3827532529830933, G Loss: 0.7065525054931641\n",
      "Epoch 66, batch 46 D Loss: 1.3835828304290771, G Loss: 0.7174193859100342\n",
      "Epoch 66, batch 47 D Loss: 1.4024875164031982, G Loss: 0.7029091715812683\n",
      "Epoch 66, batch 48 D Loss: 1.3823225498199463, G Loss: 0.7058291435241699\n",
      "Epoch 66, batch 49 D Loss: 1.3856741189956665, G Loss: 0.7063761353492737\n",
      "Epoch 66, batch 50 D Loss: 1.3895175457000732, G Loss: 0.7127600312232971\n",
      "Epoch 66, batch 51 D Loss: 1.37351655960083, G Loss: 0.7114630341529846\n",
      "Epoch 66, batch 52 D Loss: 1.3966503143310547, G Loss: 0.7167033553123474\n",
      "Epoch 66, batch 53 D Loss: 1.3807730674743652, G Loss: 0.7098121643066406\n",
      "Epoch 66, batch 54 D Loss: 1.3901922702789307, G Loss: 0.7080159783363342\n",
      "Epoch 66, batch 55 D Loss: 1.379143238067627, G Loss: 0.7055993676185608\n",
      "Epoch 66, batch 56 D Loss: 1.3715424537658691, G Loss: 0.7168451547622681\n",
      "Epoch 66, batch 57 D Loss: 1.3613038063049316, G Loss: 0.7212740182876587\n",
      "Epoch 66, batch 58 D Loss: 1.3801441192626953, G Loss: 0.7121918201446533\n",
      "Epoch 66, batch 59 D Loss: 1.3857431411743164, G Loss: 0.700849711894989\n",
      "Epoch 66, batch 60 D Loss: 1.3873291015625, G Loss: 0.7176063656806946\n",
      "Epoch 66, batch 61 D Loss: 1.3811135292053223, G Loss: 0.710442066192627\n",
      "Epoch 66, batch 62 D Loss: 1.3797645568847656, G Loss: 0.7138223052024841\n",
      "Epoch 66, batch 63 D Loss: 1.3958704471588135, G Loss: 0.7125561237335205\n",
      "Epoch 66, batch 64 D Loss: 1.3832414150238037, G Loss: 0.705839991569519\n",
      "Epoch 66, batch 65 D Loss: 1.3779685497283936, G Loss: 0.7078579068183899\n",
      "Epoch 66, batch 66 D Loss: 1.3866057395935059, G Loss: 0.7010226249694824\n",
      "Epoch 66, batch 67 D Loss: 1.4082577228546143, G Loss: 0.7005246877670288\n",
      "Epoch 66, batch 68 D Loss: 1.376244306564331, G Loss: 0.7025797367095947\n",
      "Epoch 66, batch 69 D Loss: 1.3942816257476807, G Loss: 0.7023764252662659\n",
      "Epoch 66, batch 70 D Loss: 1.4006067514419556, G Loss: 0.695260226726532\n",
      "Epoch 66, batch 71 D Loss: 1.3900136947631836, G Loss: 0.6898991465568542\n",
      "Epoch 66, batch 72 D Loss: 1.3969486951828003, G Loss: 0.696645975112915\n",
      "Epoch 66, batch 73 D Loss: 1.389758586883545, G Loss: 0.7142342329025269\n",
      "Epoch 66, batch 74 D Loss: 1.385364294052124, G Loss: 0.6998965740203857\n",
      "Epoch 66, batch 75 D Loss: 1.3855746984481812, G Loss: 0.702279806137085\n",
      "Epoch 66, batch 76 D Loss: 1.383206844329834, G Loss: 0.7126449346542358\n",
      "Epoch 66, batch 77 D Loss: 1.3872649669647217, G Loss: 0.7096701860427856\n",
      "Epoch 66, batch 78 D Loss: 1.3641154766082764, G Loss: 0.7131848335266113\n",
      "Epoch 66, batch 79 D Loss: 1.3782092332839966, G Loss: 0.6953337788581848\n",
      "Epoch 66, batch 80 D Loss: 1.38450026512146, G Loss: 0.7023775577545166\n",
      "Epoch 66, batch 81 D Loss: 1.3920189142227173, G Loss: 0.7026588916778564\n",
      "Epoch 66, batch 82 D Loss: 1.4002175331115723, G Loss: 0.6843997240066528\n",
      "Epoch 66, batch 83 D Loss: 1.3816004991531372, G Loss: 0.6982374787330627\n",
      "Epoch 66, batch 84 D Loss: 1.4150493144989014, G Loss: 0.6793294548988342\n",
      "Epoch 66, batch 85 D Loss: 1.4005634784698486, G Loss: 0.6897562146186829\n",
      "Epoch 66, batch 86 D Loss: 1.3875467777252197, G Loss: 0.6903621554374695\n",
      "Epoch 66, batch 87 D Loss: 1.3806109428405762, G Loss: 0.6970062851905823\n",
      "Epoch 66, batch 88 D Loss: 1.3975753784179688, G Loss: 0.681476891040802\n",
      "Epoch 66, batch 89 D Loss: 1.3907334804534912, G Loss: 0.695850670337677\n",
      "Epoch 66, batch 90 D Loss: 1.3924617767333984, G Loss: 0.6803326606750488\n",
      "Epoch 66, batch 91 D Loss: 1.3738043308258057, G Loss: 0.6937688589096069\n",
      "Epoch 66, batch 92 D Loss: 1.38775634765625, G Loss: 0.6971541047096252\n",
      "Epoch 66, batch 93 D Loss: 1.402043342590332, G Loss: 0.6848511695861816\n",
      "Epoch 66, batch 94 D Loss: 1.3816883563995361, G Loss: 0.696952760219574\n",
      "Epoch 66, batch 95 D Loss: 1.3830152750015259, G Loss: 0.689802885055542\n",
      "Epoch 66, batch 96 D Loss: 1.388171911239624, G Loss: 0.683742344379425\n",
      "Epoch 66, batch 97 D Loss: 1.4010820388793945, G Loss: 0.6859579682350159\n",
      "Epoch 66, batch 98 D Loss: 1.3828840255737305, G Loss: 0.6913482546806335\n",
      "Epoch 66, batch 99 D Loss: 1.4099369049072266, G Loss: 0.6690685749053955\n",
      "Epoch 66, batch 100 D Loss: 1.3867557048797607, G Loss: 0.6892141699790955\n",
      "Epoch 66, batch 101 D Loss: 1.403505802154541, G Loss: 0.6784982085227966\n",
      "Epoch 66, batch 102 D Loss: 1.3956573009490967, G Loss: 0.6838504076004028\n",
      "Epoch 66, batch 103 D Loss: 1.4245579242706299, G Loss: 0.6760344505310059\n",
      "Epoch 66, batch 104 D Loss: 1.386721134185791, G Loss: 0.6845263838768005\n",
      "Epoch 66, batch 105 D Loss: 1.382228136062622, G Loss: 0.6911134123802185\n",
      "Epoch 66, batch 106 D Loss: 1.3891949653625488, G Loss: 0.6929219961166382\n",
      "Epoch 66, batch 107 D Loss: 1.410982608795166, G Loss: 0.6781767010688782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, batch 108 D Loss: 1.3854100704193115, G Loss: 0.6851522922515869\n",
      "Epoch 66, batch 109 D Loss: 1.382552146911621, G Loss: 0.6784298419952393\n",
      "Epoch 66, batch 110 D Loss: 1.3889621496200562, G Loss: 0.6758292317390442\n",
      "Epoch 66, batch 111 D Loss: 1.3935483694076538, G Loss: 0.6862959861755371\n",
      "Epoch 66, batch 112 D Loss: 1.3915324211120605, G Loss: 0.683643639087677\n",
      "Epoch 66, batch 113 D Loss: 1.3827221393585205, G Loss: 0.6831643581390381\n",
      "Epoch 66, batch 114 D Loss: 1.3950002193450928, G Loss: 0.6788479685783386\n",
      "Epoch 66, batch 115 D Loss: 1.3885374069213867, G Loss: 0.6724899411201477\n",
      "Epoch 66, batch 116 D Loss: 1.399791955947876, G Loss: 0.6778237819671631\n",
      "Epoch 66, batch 117 D Loss: 1.3863415718078613, G Loss: 0.687895655632019\n",
      "Epoch 66, batch 118 D Loss: 1.3922059535980225, G Loss: 0.6856400370597839\n",
      "Epoch 66, batch 119 D Loss: 1.3686307668685913, G Loss: 0.6905621886253357\n",
      "Epoch 66, batch 120 D Loss: 1.3897854089736938, G Loss: 0.6797841787338257\n",
      "Epoch 66, batch 121 D Loss: 1.3930487632751465, G Loss: 0.6818984746932983\n",
      "Epoch 66, batch 122 D Loss: 1.393384575843811, G Loss: 0.6760745048522949\n",
      "Epoch 66, batch 123 D Loss: 1.4043118953704834, G Loss: 0.6756718158721924\n",
      "Epoch 66, batch 124 D Loss: 1.3808355331420898, G Loss: 0.6927677989006042\n",
      "Epoch 66, batch 125 D Loss: 1.3857276439666748, G Loss: 0.6795933246612549\n",
      "Epoch 66, batch 126 D Loss: 1.3815207481384277, G Loss: 0.6783713698387146\n",
      "Epoch 66, batch 127 D Loss: 1.3852894306182861, G Loss: 0.6959725022315979\n",
      "Epoch 66, batch 128 D Loss: 1.396411657333374, G Loss: 0.684374213218689\n",
      "Epoch 66, batch 129 D Loss: 1.38731050491333, G Loss: 0.6878986358642578\n",
      "Epoch 66, batch 130 D Loss: 1.373443603515625, G Loss: 0.6930680871009827\n",
      "Epoch 66, batch 131 D Loss: 1.4000121355056763, G Loss: 0.6747883558273315\n",
      "Epoch 66, batch 132 D Loss: 1.3819152116775513, G Loss: 0.6848663091659546\n",
      "Epoch 66, batch 133 D Loss: 1.3836969137191772, G Loss: 0.6748512387275696\n",
      "Epoch 66, batch 134 D Loss: 1.382101058959961, G Loss: 0.6800035834312439\n",
      "Epoch 66, batch 135 D Loss: 1.3810296058654785, G Loss: 0.6892539858818054\n",
      "Epoch 66, batch 136 D Loss: 1.3780591487884521, G Loss: 0.6793677806854248\n",
      "Epoch 66, batch 137 D Loss: 1.3855862617492676, G Loss: 0.6828624606132507\n",
      "Epoch 66, batch 138 D Loss: 1.3894377946853638, G Loss: 0.6765763759613037\n",
      "Epoch 66, batch 139 D Loss: 1.3943591117858887, G Loss: 0.6697232723236084\n",
      "Epoch 66, batch 140 D Loss: 1.3779959678649902, G Loss: 0.68772292137146\n",
      "Epoch 66, batch 141 D Loss: 1.3903725147247314, G Loss: 0.6905209422111511\n",
      "Epoch 66, batch 142 D Loss: 1.391628384590149, G Loss: 0.6789003014564514\n",
      "Epoch 66, batch 143 D Loss: 1.381530523300171, G Loss: 0.6851677894592285\n",
      "Epoch 66, batch 144 D Loss: 1.3745315074920654, G Loss: 0.6835598945617676\n",
      "Epoch 66, batch 145 D Loss: 1.383604645729065, G Loss: 0.6855911016464233\n",
      "Epoch 66, batch 146 D Loss: 1.3927602767944336, G Loss: 0.6753733158111572\n",
      "Epoch 66, batch 147 D Loss: 1.4055564403533936, G Loss: 0.6765348315238953\n",
      "Epoch 66, batch 148 D Loss: 1.4001045227050781, G Loss: 0.6769118309020996\n",
      "Epoch 66, batch 149 D Loss: 1.4049479961395264, G Loss: 0.6757400631904602\n",
      "Epoch 66, batch 150 D Loss: 1.4033892154693604, G Loss: 0.6776688098907471\n",
      "Epoch 66, batch 151 D Loss: 1.4066050052642822, G Loss: 0.6798791289329529\n",
      "Epoch 66, batch 152 D Loss: 1.387074589729309, G Loss: 0.6758193373680115\n",
      "Epoch 66, batch 153 D Loss: 1.3913211822509766, G Loss: 0.6757670044898987\n",
      "Epoch 66, batch 154 D Loss: 1.393082618713379, G Loss: 0.6812847852706909\n",
      "Epoch 66, batch 155 D Loss: 1.3952407836914062, G Loss: 0.6751081347465515\n",
      "Epoch 66, batch 156 D Loss: 1.3991613388061523, G Loss: 0.6795700192451477\n",
      "Epoch 66, batch 157 D Loss: 1.3969738483428955, G Loss: 0.6749693155288696\n",
      "Epoch 66, batch 158 D Loss: 1.400963544845581, G Loss: 0.6771280169487\n",
      "Epoch 66, batch 159 D Loss: 1.3997070789337158, G Loss: 0.6727761030197144\n",
      "Epoch 66, batch 160 D Loss: 1.4103732109069824, G Loss: 0.6731895208358765\n",
      "Epoch 66, batch 161 D Loss: 1.3848419189453125, G Loss: 0.6827114820480347\n",
      "Epoch 66, batch 162 D Loss: 1.386316180229187, G Loss: 0.6840121746063232\n",
      "Epoch 66, batch 163 D Loss: 1.3919854164123535, G Loss: 0.6851560473442078\n",
      "Epoch 66, batch 164 D Loss: 1.3830440044403076, G Loss: 0.6800458431243896\n",
      "Epoch 66, batch 165 D Loss: 1.3878644704818726, G Loss: 0.6782676577568054\n",
      "Epoch 66, batch 166 D Loss: 1.3959883451461792, G Loss: 0.6777523756027222\n",
      "Epoch 66, batch 167 D Loss: 1.387096881866455, G Loss: 0.6844326853752136\n",
      "Epoch 66, batch 168 D Loss: 1.3758282661437988, G Loss: 0.6935604810714722\n",
      "Epoch 66, batch 169 D Loss: 1.3972415924072266, G Loss: 0.674013078212738\n",
      "Epoch 66, batch 170 D Loss: 1.408949375152588, G Loss: 0.6827342510223389\n",
      "Epoch 66, batch 171 D Loss: 1.380469560623169, G Loss: 0.6933472156524658\n",
      "Epoch 66, batch 172 D Loss: 1.3942283391952515, G Loss: 0.6860569715499878\n",
      "Epoch 66, batch 173 D Loss: 1.3978005647659302, G Loss: 0.6746436953544617\n",
      "Epoch 66, batch 174 D Loss: 1.392362117767334, G Loss: 0.6825975775718689\n",
      "Epoch 66, batch 175 D Loss: 1.3939167261123657, G Loss: 0.6869713068008423\n",
      "Epoch 66, batch 176 D Loss: 1.3777320384979248, G Loss: 0.6867379546165466\n",
      "Epoch 66, batch 177 D Loss: 1.393843412399292, G Loss: 0.6825299263000488\n",
      "Epoch 66, batch 178 D Loss: 1.3921468257904053, G Loss: 0.6762057542800903\n",
      "Epoch 66, batch 179 D Loss: 1.3868931531906128, G Loss: 0.6869756579399109\n",
      "Epoch 66, batch 180 D Loss: 1.3963991403579712, G Loss: 0.6865959167480469\n",
      "Epoch 66, batch 181 D Loss: 1.3947397470474243, G Loss: 0.6810098886489868\n",
      "Epoch 66, batch 182 D Loss: 1.398271083831787, G Loss: 0.686570405960083\n",
      "Epoch 66, batch 183 D Loss: 1.387772560119629, G Loss: 0.6788331866264343\n",
      "Epoch 66, batch 184 D Loss: 1.386817455291748, G Loss: 0.685036301612854\n",
      "Epoch 66, batch 185 D Loss: 1.3962762355804443, G Loss: 0.6893000602722168\n",
      "Epoch 66, batch 186 D Loss: 1.3831290006637573, G Loss: 0.6882607936859131\n",
      "Epoch 66, batch 187 D Loss: 1.383842945098877, G Loss: 0.6918858289718628\n",
      "Epoch 66, batch 188 D Loss: 1.3987454175949097, G Loss: 0.6843695044517517\n",
      "Epoch 66, batch 189 D Loss: 1.3899712562561035, G Loss: 0.6928601861000061\n",
      "Epoch 66, batch 190 D Loss: 1.3916677236557007, G Loss: 0.6916301846504211\n",
      "Epoch 66, batch 191 D Loss: 1.38053560256958, G Loss: 0.6913221478462219\n",
      "Epoch 66, batch 192 D Loss: 1.4028346538543701, G Loss: 0.6826398968696594\n",
      "Epoch 66, batch 193 D Loss: 1.400770664215088, G Loss: 0.6918196678161621\n",
      "Epoch 66, batch 194 D Loss: 1.384856939315796, G Loss: 0.695131242275238\n",
      "Epoch 66, batch 195 D Loss: 1.3837872743606567, G Loss: 0.6971608996391296\n",
      "Epoch 66, batch 196 D Loss: 1.3863996267318726, G Loss: 0.6902221441268921\n",
      "Epoch 66, batch 197 D Loss: 1.38067626953125, G Loss: 0.6887650489807129\n",
      "Epoch 66, batch 198 D Loss: 1.3759407997131348, G Loss: 0.6930208802223206\n",
      "Epoch 66, batch 199 D Loss: 1.3673768043518066, G Loss: 0.6990257501602173\n",
      "Epoch 66, batch 200 D Loss: 1.372889518737793, G Loss: 0.6939042806625366\n",
      "Epoch 67, batch 1 D Loss: 1.3919789791107178, G Loss: 0.6905391216278076\n",
      "Epoch 67, batch 2 D Loss: 1.3772656917572021, G Loss: 0.7024677991867065\n",
      "Epoch 67, batch 3 D Loss: 1.384103536605835, G Loss: 0.6928094625473022\n",
      "Epoch 67, batch 4 D Loss: 1.3831030130386353, G Loss: 0.7056974768638611\n",
      "Epoch 67, batch 5 D Loss: 1.3704230785369873, G Loss: 0.6953078508377075\n",
      "Epoch 67, batch 6 D Loss: 1.367436170578003, G Loss: 0.701984703540802\n",
      "Epoch 67, batch 7 D Loss: 1.3714382648468018, G Loss: 0.7023813128471375\n",
      "Epoch 67, batch 8 D Loss: 1.3832859992980957, G Loss: 0.6941056847572327\n",
      "Epoch 67, batch 9 D Loss: 1.3723039627075195, G Loss: 0.6995225548744202\n",
      "Epoch 67, batch 10 D Loss: 1.3644413948059082, G Loss: 0.7060533761978149\n",
      "Epoch 67, batch 11 D Loss: 1.3504091501235962, G Loss: 0.7072353959083557\n",
      "Epoch 67, batch 12 D Loss: 1.3642773628234863, G Loss: 0.7090017199516296\n",
      "Epoch 67, batch 13 D Loss: 1.360435962677002, G Loss: 0.6995990872383118\n",
      "Epoch 67, batch 14 D Loss: 1.3704147338867188, G Loss: 0.7022868394851685\n",
      "Epoch 67, batch 15 D Loss: 1.387162208557129, G Loss: 0.694777250289917\n",
      "Epoch 67, batch 16 D Loss: 1.36704683303833, G Loss: 0.7066630721092224\n",
      "Epoch 67, batch 17 D Loss: 1.3551459312438965, G Loss: 0.7157729268074036\n",
      "Epoch 67, batch 18 D Loss: 1.3694452047348022, G Loss: 0.710716962814331\n",
      "Epoch 67, batch 19 D Loss: 1.3771917819976807, G Loss: 0.7043423652648926\n",
      "Epoch 67, batch 20 D Loss: 1.362074375152588, G Loss: 0.7105135917663574\n",
      "Epoch 67, batch 21 D Loss: 1.3666784763336182, G Loss: 0.7031928300857544\n",
      "Epoch 67, batch 22 D Loss: 1.35453462600708, G Loss: 0.7154423594474792\n",
      "Epoch 67, batch 23 D Loss: 1.364088773727417, G Loss: 0.7068833112716675\n",
      "Epoch 67, batch 24 D Loss: 1.3665635585784912, G Loss: 0.7046387195587158\n",
      "Epoch 67, batch 25 D Loss: 1.3714784383773804, G Loss: 0.7015438079833984\n",
      "Epoch 67, batch 26 D Loss: 1.3683743476867676, G Loss: 0.7118808031082153\n",
      "Epoch 67, batch 27 D Loss: 1.3694615364074707, G Loss: 0.7095792889595032\n",
      "Epoch 67, batch 28 D Loss: 1.3895070552825928, G Loss: 0.679843008518219\n",
      "Epoch 67, batch 29 D Loss: 1.322709560394287, G Loss: 0.7335941791534424\n",
      "Epoch 67, batch 30 D Loss: 1.3788189888000488, G Loss: 0.7030684947967529\n",
      "Epoch 67, batch 31 D Loss: 1.3731415271759033, G Loss: 0.7028859853744507\n",
      "Epoch 67, batch 32 D Loss: 1.3616628646850586, G Loss: 0.713291347026825\n",
      "Epoch 67, batch 33 D Loss: 1.3688840866088867, G Loss: 0.7085112929344177\n",
      "Epoch 67, batch 34 D Loss: 1.352008581161499, G Loss: 0.6973779797554016\n",
      "Epoch 67, batch 35 D Loss: 1.363303303718567, G Loss: 0.7089012861251831\n",
      "Epoch 67, batch 36 D Loss: 1.3591532707214355, G Loss: 0.7124456167221069\n",
      "Epoch 67, batch 37 D Loss: 1.3584036827087402, G Loss: 0.7031749486923218\n",
      "Epoch 67, batch 38 D Loss: 1.3728028535842896, G Loss: 0.7113668918609619\n",
      "Epoch 67, batch 39 D Loss: 1.374031662940979, G Loss: 0.7036656141281128\n",
      "Epoch 67, batch 40 D Loss: 1.3665075302124023, G Loss: 0.7072166204452515\n",
      "Epoch 67, batch 41 D Loss: 1.3648490905761719, G Loss: 0.7069196105003357\n",
      "Epoch 67, batch 42 D Loss: 1.3624825477600098, G Loss: 0.7078953385353088\n",
      "Epoch 67, batch 43 D Loss: 1.3727495670318604, G Loss: 0.7133578658103943\n",
      "Epoch 67, batch 44 D Loss: 1.3403737545013428, G Loss: 0.7062221765518188\n",
      "Epoch 67, batch 45 D Loss: 1.385145664215088, G Loss: 0.6976413130760193\n",
      "Epoch 67, batch 46 D Loss: 1.3383947610855103, G Loss: 0.7286266088485718\n",
      "Epoch 67, batch 47 D Loss: 1.3298578262329102, G Loss: 0.7242892384529114\n",
      "Epoch 67, batch 48 D Loss: 1.3280549049377441, G Loss: 0.7207949161529541\n",
      "Epoch 67, batch 49 D Loss: 1.3698253631591797, G Loss: 0.7084787487983704\n",
      "Epoch 67, batch 50 D Loss: 1.343996286392212, G Loss: 0.7165417671203613\n",
      "Epoch 67, batch 51 D Loss: 1.3514790534973145, G Loss: 0.695270836353302\n",
      "Epoch 67, batch 52 D Loss: 1.3989959955215454, G Loss: 0.6900364756584167\n",
      "Epoch 67, batch 53 D Loss: 1.3506085872650146, G Loss: 0.7093803882598877\n",
      "Epoch 67, batch 54 D Loss: 1.392190933227539, G Loss: 0.6750810742378235\n",
      "Epoch 67, batch 55 D Loss: 1.3510278463363647, G Loss: 0.710975170135498\n",
      "Epoch 67, batch 56 D Loss: 1.380672812461853, G Loss: 0.6982879042625427\n",
      "Epoch 67, batch 57 D Loss: 1.4053740501403809, G Loss: 0.6788555383682251\n",
      "Epoch 67, batch 58 D Loss: 1.3816566467285156, G Loss: 0.7029858231544495\n",
      "Epoch 67, batch 59 D Loss: 1.3846195936203003, G Loss: 0.6957061290740967\n",
      "Epoch 67, batch 60 D Loss: 1.393938660621643, G Loss: 0.7034665942192078\n",
      "Epoch 67, batch 61 D Loss: 1.3421489000320435, G Loss: 0.7157611846923828\n",
      "Epoch 67, batch 62 D Loss: 1.393670916557312, G Loss: 0.683409571647644\n",
      "Epoch 67, batch 63 D Loss: 1.3799490928649902, G Loss: 0.6968174576759338\n",
      "Epoch 67, batch 64 D Loss: 1.3884105682373047, G Loss: 0.6920216083526611\n",
      "Epoch 67, batch 65 D Loss: 1.3887417316436768, G Loss: 0.6800277829170227\n",
      "Epoch 67, batch 66 D Loss: 1.4066095352172852, G Loss: 0.677947461605072\n",
      "Epoch 67, batch 67 D Loss: 1.3733630180358887, G Loss: 0.7140582799911499\n",
      "Epoch 67, batch 68 D Loss: 1.3692398071289062, G Loss: 0.7046692371368408\n",
      "Epoch 67, batch 69 D Loss: 1.3652098178863525, G Loss: 0.6943662166595459\n",
      "Epoch 67, batch 70 D Loss: 1.3815058469772339, G Loss: 0.6856619119644165\n",
      "Epoch 67, batch 71 D Loss: 1.3823941946029663, G Loss: 0.6902034282684326\n",
      "Epoch 67, batch 72 D Loss: 1.4076621532440186, G Loss: 0.6705731153488159\n",
      "Epoch 67, batch 73 D Loss: 1.4029457569122314, G Loss: 0.6899826526641846\n",
      "Epoch 67, batch 74 D Loss: 1.4280850887298584, G Loss: 0.6686275005340576\n",
      "Epoch 67, batch 75 D Loss: 1.3852412700653076, G Loss: 0.7029005289077759\n",
      "Epoch 67, batch 76 D Loss: 1.393958568572998, G Loss: 0.6782968640327454\n",
      "Epoch 67, batch 77 D Loss: 1.4308403730392456, G Loss: 0.6678997278213501\n",
      "Epoch 67, batch 78 D Loss: 1.4187510013580322, G Loss: 0.6549805402755737\n",
      "Epoch 67, batch 79 D Loss: 1.398643970489502, G Loss: 0.6972672939300537\n",
      "Epoch 67, batch 80 D Loss: 1.4021015167236328, G Loss: 0.6886921525001526\n",
      "Epoch 67, batch 81 D Loss: 1.4406577348709106, G Loss: 0.6608137488365173\n",
      "Epoch 67, batch 82 D Loss: 1.3863976001739502, G Loss: 0.6891198754310608\n",
      "Epoch 67, batch 83 D Loss: 1.3997552394866943, G Loss: 0.6758005023002625\n",
      "Epoch 67, batch 84 D Loss: 1.4743045568466187, G Loss: 0.6393791437149048\n",
      "Epoch 67, batch 85 D Loss: 1.420825481414795, G Loss: 0.6706650257110596\n",
      "Epoch 67, batch 86 D Loss: 1.4330620765686035, G Loss: 0.6682404279708862\n",
      "Epoch 67, batch 87 D Loss: 1.4040734767913818, G Loss: 0.6798657178878784\n",
      "Epoch 67, batch 88 D Loss: 1.4244292974472046, G Loss: 0.6758904457092285\n",
      "Epoch 67, batch 89 D Loss: 1.3997278213500977, G Loss: 0.6826907992362976\n",
      "Epoch 67, batch 90 D Loss: 1.4419361352920532, G Loss: 0.651350200176239\n",
      "Epoch 67, batch 91 D Loss: 1.3934791088104248, G Loss: 0.693775475025177\n",
      "Epoch 67, batch 92 D Loss: 1.3821830749511719, G Loss: 0.706092119216919\n",
      "Epoch 67, batch 93 D Loss: 1.3913135528564453, G Loss: 0.6911489963531494\n",
      "Epoch 67, batch 94 D Loss: 1.3961633443832397, G Loss: 0.6932291984558105\n",
      "Epoch 67, batch 95 D Loss: 1.420148253440857, G Loss: 0.6975740194320679\n",
      "Epoch 67, batch 96 D Loss: 1.4148046970367432, G Loss: 0.682478666305542\n",
      "Epoch 67, batch 97 D Loss: 1.3848752975463867, G Loss: 0.7201511859893799\n",
      "Epoch 67, batch 98 D Loss: 1.4153602123260498, G Loss: 0.6883047223091125\n",
      "Epoch 67, batch 99 D Loss: 1.401374340057373, G Loss: 0.7075374722480774\n",
      "Epoch 67, batch 100 D Loss: 1.3691892623901367, G Loss: 0.7447430491447449\n",
      "Epoch 67, batch 101 D Loss: 1.3918789625167847, G Loss: 0.737059473991394\n",
      "Epoch 67, batch 102 D Loss: 1.409616231918335, G Loss: 0.7081007361412048\n",
      "Epoch 67, batch 103 D Loss: 1.3823628425598145, G Loss: 0.7478792667388916\n",
      "Epoch 67, batch 104 D Loss: 1.40474534034729, G Loss: 0.7320740222930908\n",
      "Epoch 67, batch 105 D Loss: 1.360381841659546, G Loss: 0.7418520450592041\n",
      "Epoch 67, batch 106 D Loss: 1.389894962310791, G Loss: 0.7414059638977051\n",
      "Epoch 67, batch 107 D Loss: 1.3854950666427612, G Loss: 0.7579566240310669\n",
      "Epoch 67, batch 108 D Loss: 1.3998783826828003, G Loss: 0.7499392032623291\n",
      "Epoch 67, batch 109 D Loss: 1.3908616304397583, G Loss: 0.732635498046875\n",
      "Epoch 67, batch 110 D Loss: 1.4269909858703613, G Loss: 0.7422853708267212\n",
      "Epoch 67, batch 111 D Loss: 1.3857728242874146, G Loss: 0.7954257130622864\n",
      "Epoch 67, batch 112 D Loss: 1.367404580116272, G Loss: 0.7724728584289551\n",
      "Epoch 67, batch 113 D Loss: 1.3886096477508545, G Loss: 0.8091014623641968\n",
      "Epoch 67, batch 114 D Loss: 1.4139609336853027, G Loss: 0.7530710101127625\n",
      "Epoch 67, batch 115 D Loss: 1.3841876983642578, G Loss: 0.7699372172355652\n",
      "Epoch 67, batch 116 D Loss: 1.3579745292663574, G Loss: 0.8391130566596985\n",
      "Epoch 67, batch 117 D Loss: 1.4117473363876343, G Loss: 0.7644097805023193\n",
      "Epoch 67, batch 118 D Loss: 1.3538203239440918, G Loss: 0.8474481701850891\n",
      "Epoch 67, batch 119 D Loss: 1.3736870288848877, G Loss: 0.8019933104515076\n",
      "Epoch 67, batch 120 D Loss: 1.397340178489685, G Loss: 0.7874516844749451\n",
      "Epoch 67, batch 121 D Loss: 1.4258344173431396, G Loss: 0.7684733867645264\n",
      "Epoch 67, batch 122 D Loss: 1.3806474208831787, G Loss: 0.7910971641540527\n",
      "Epoch 67, batch 123 D Loss: 1.4070634841918945, G Loss: 0.7722947001457214\n",
      "Epoch 67, batch 124 D Loss: 1.4037175178527832, G Loss: 0.7902801036834717\n",
      "Epoch 67, batch 125 D Loss: 1.3922128677368164, G Loss: 0.811267077922821\n",
      "Epoch 67, batch 126 D Loss: 1.3525134325027466, G Loss: 0.8491547107696533\n",
      "Epoch 67, batch 127 D Loss: 1.361720323562622, G Loss: 0.7868404984474182\n",
      "Epoch 67, batch 128 D Loss: 1.4044725894927979, G Loss: 0.7906829714775085\n",
      "Epoch 67, batch 129 D Loss: 1.3539866209030151, G Loss: 0.8150587677955627\n",
      "Epoch 67, batch 130 D Loss: 1.3833116292953491, G Loss: 0.7768915295600891\n",
      "Epoch 67, batch 131 D Loss: 1.3624064922332764, G Loss: 0.8003643155097961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, batch 132 D Loss: 1.4084861278533936, G Loss: 0.7651405930519104\n",
      "Epoch 67, batch 133 D Loss: 1.4562196731567383, G Loss: 0.7723222970962524\n",
      "Epoch 67, batch 134 D Loss: 1.3691456317901611, G Loss: 0.8145871162414551\n",
      "Epoch 67, batch 135 D Loss: 1.4125885963439941, G Loss: 0.7416015863418579\n",
      "Epoch 67, batch 136 D Loss: 1.4119987487792969, G Loss: 0.7762413024902344\n",
      "Epoch 67, batch 137 D Loss: 1.4182367324829102, G Loss: 0.7242937684059143\n",
      "Epoch 67, batch 138 D Loss: 1.4154611825942993, G Loss: 0.7413559556007385\n",
      "Epoch 67, batch 139 D Loss: 1.4354935884475708, G Loss: 0.7448611259460449\n",
      "Epoch 67, batch 140 D Loss: 1.4020090103149414, G Loss: 0.7280540466308594\n",
      "Epoch 67, batch 141 D Loss: 1.439608097076416, G Loss: 0.7324682474136353\n",
      "Epoch 67, batch 142 D Loss: 1.3908395767211914, G Loss: 0.7391627430915833\n",
      "Epoch 67, batch 143 D Loss: 1.4256080389022827, G Loss: 0.7153522968292236\n",
      "Epoch 67, batch 144 D Loss: 1.4175496101379395, G Loss: 0.7069001793861389\n",
      "Epoch 67, batch 145 D Loss: 1.421665906906128, G Loss: 0.718670666217804\n",
      "Epoch 67, batch 146 D Loss: 1.3953137397766113, G Loss: 0.7226884365081787\n",
      "Epoch 67, batch 147 D Loss: 1.4023675918579102, G Loss: 0.7172116041183472\n",
      "Epoch 67, batch 148 D Loss: 1.3993748426437378, G Loss: 0.7227027416229248\n",
      "Epoch 67, batch 149 D Loss: 1.4090945720672607, G Loss: 0.7102764844894409\n",
      "Epoch 67, batch 150 D Loss: 1.4178433418273926, G Loss: 0.7058594226837158\n",
      "Epoch 67, batch 151 D Loss: 1.4204227924346924, G Loss: 0.7058963775634766\n",
      "Epoch 67, batch 152 D Loss: 1.406628966331482, G Loss: 0.699501633644104\n",
      "Epoch 67, batch 153 D Loss: 1.4220486879348755, G Loss: 0.6912180185317993\n",
      "Epoch 67, batch 154 D Loss: 1.4424898624420166, G Loss: 0.6965065598487854\n",
      "Epoch 67, batch 155 D Loss: 1.398261308670044, G Loss: 0.6963084936141968\n",
      "Epoch 67, batch 156 D Loss: 1.375206470489502, G Loss: 0.7231237292289734\n",
      "Epoch 67, batch 157 D Loss: 1.4320282936096191, G Loss: 0.6904560327529907\n",
      "Epoch 67, batch 158 D Loss: 1.4138526916503906, G Loss: 0.6870732307434082\n",
      "Epoch 67, batch 159 D Loss: 1.4017870426177979, G Loss: 0.6870881915092468\n",
      "Epoch 67, batch 160 D Loss: 1.4292722940444946, G Loss: 0.6891793012619019\n",
      "Epoch 67, batch 161 D Loss: 1.407799243927002, G Loss: 0.6866994500160217\n",
      "Epoch 67, batch 162 D Loss: 1.4041969776153564, G Loss: 0.6846608519554138\n",
      "Epoch 67, batch 163 D Loss: 1.3908157348632812, G Loss: 0.6844016909599304\n",
      "Epoch 67, batch 164 D Loss: 1.403012990951538, G Loss: 0.6867086887359619\n",
      "Epoch 67, batch 165 D Loss: 1.3917994499206543, G Loss: 0.6788294911384583\n",
      "Epoch 67, batch 166 D Loss: 1.3913464546203613, G Loss: 0.6794893741607666\n",
      "Epoch 67, batch 167 D Loss: 1.4054746627807617, G Loss: 0.6832431554794312\n",
      "Epoch 67, batch 168 D Loss: 1.400245189666748, G Loss: 0.6794581413269043\n",
      "Epoch 67, batch 169 D Loss: 1.3915114402770996, G Loss: 0.6804051995277405\n",
      "Epoch 67, batch 170 D Loss: 1.3917107582092285, G Loss: 0.6774387955665588\n",
      "Epoch 67, batch 171 D Loss: 1.3867779970169067, G Loss: 0.6786311268806458\n",
      "Epoch 67, batch 172 D Loss: 1.3876893520355225, G Loss: 0.678524911403656\n",
      "Epoch 67, batch 173 D Loss: 1.3904225826263428, G Loss: 0.6838597059249878\n",
      "Epoch 67, batch 174 D Loss: 1.3803973197937012, G Loss: 0.680764377117157\n",
      "Epoch 67, batch 175 D Loss: 1.3806517124176025, G Loss: 0.6792629361152649\n",
      "Epoch 67, batch 176 D Loss: 1.381766438484192, G Loss: 0.6816672682762146\n",
      "Epoch 67, batch 177 D Loss: 1.3842782974243164, G Loss: 0.6799765229225159\n",
      "Epoch 67, batch 178 D Loss: 1.3920156955718994, G Loss: 0.6791316270828247\n",
      "Epoch 67, batch 179 D Loss: 1.3894963264465332, G Loss: 0.6815342903137207\n",
      "Epoch 67, batch 180 D Loss: 1.3842195272445679, G Loss: 0.6797547340393066\n",
      "Epoch 67, batch 181 D Loss: 1.3789446353912354, G Loss: 0.6799321174621582\n",
      "Epoch 67, batch 182 D Loss: 1.3766930103302002, G Loss: 0.6806635856628418\n",
      "Epoch 67, batch 183 D Loss: 1.3776168823242188, G Loss: 0.6814130544662476\n",
      "Epoch 67, batch 184 D Loss: 1.3793333768844604, G Loss: 0.6792905926704407\n",
      "Epoch 67, batch 185 D Loss: 1.3781042098999023, G Loss: 0.6795058250427246\n",
      "Epoch 67, batch 186 D Loss: 1.363415002822876, G Loss: 0.6803446412086487\n",
      "Epoch 67, batch 187 D Loss: 1.3655874729156494, G Loss: 0.6794188022613525\n",
      "Epoch 67, batch 188 D Loss: 1.37550950050354, G Loss: 0.6792665123939514\n",
      "Epoch 67, batch 189 D Loss: 1.3768823146820068, G Loss: 0.6765698790550232\n",
      "Epoch 67, batch 190 D Loss: 1.3681973218917847, G Loss: 0.6794376969337463\n",
      "Epoch 67, batch 191 D Loss: 1.3738412857055664, G Loss: 0.6828131675720215\n",
      "Epoch 67, batch 192 D Loss: 1.3692522048950195, G Loss: 0.678973913192749\n",
      "Epoch 67, batch 193 D Loss: 1.370535135269165, G Loss: 0.676500141620636\n",
      "Epoch 67, batch 194 D Loss: 1.376027226448059, G Loss: 0.6733104586601257\n",
      "Epoch 67, batch 195 D Loss: 1.3684165477752686, G Loss: 0.6765825152397156\n",
      "Epoch 67, batch 196 D Loss: 1.340226650238037, G Loss: 0.6820414662361145\n",
      "Epoch 67, batch 197 D Loss: 1.3618838787078857, G Loss: 0.6814212799072266\n",
      "Epoch 67, batch 198 D Loss: 1.3515117168426514, G Loss: 0.6785888075828552\n",
      "Epoch 67, batch 199 D Loss: 1.3688101768493652, G Loss: 0.6769107580184937\n",
      "Epoch 67, batch 200 D Loss: 1.3665558099746704, G Loss: 0.670799970626831\n",
      "Epoch 68, batch 1 D Loss: 1.364893913269043, G Loss: 0.6755275130271912\n",
      "Epoch 68, batch 2 D Loss: 1.3523802757263184, G Loss: 0.6743539571762085\n",
      "Epoch 68, batch 3 D Loss: 1.3299134969711304, G Loss: 0.6734541058540344\n",
      "Epoch 68, batch 4 D Loss: 1.3633439540863037, G Loss: 0.6739799976348877\n",
      "Epoch 68, batch 5 D Loss: 1.3494588136672974, G Loss: 0.6729838848114014\n",
      "Epoch 68, batch 6 D Loss: 1.3719416856765747, G Loss: 0.6634038686752319\n",
      "Epoch 68, batch 7 D Loss: 1.358023762702942, G Loss: 0.6631561517715454\n",
      "Epoch 68, batch 8 D Loss: 1.3734105825424194, G Loss: 0.6633884310722351\n",
      "Epoch 68, batch 9 D Loss: 1.3369450569152832, G Loss: 0.6794565320014954\n",
      "Epoch 68, batch 10 D Loss: 1.3336522579193115, G Loss: 0.6647112965583801\n",
      "Epoch 68, batch 11 D Loss: 1.351539134979248, G Loss: 0.6610221862792969\n",
      "Epoch 68, batch 12 D Loss: 1.3749630451202393, G Loss: 0.6532360315322876\n",
      "Epoch 68, batch 13 D Loss: 1.3754853010177612, G Loss: 0.6535807847976685\n",
      "Epoch 68, batch 14 D Loss: 1.3448858261108398, G Loss: 0.657470703125\n",
      "Epoch 68, batch 15 D Loss: 1.344435453414917, G Loss: 0.6520286798477173\n",
      "Epoch 68, batch 16 D Loss: 1.3217499256134033, G Loss: 0.6666473150253296\n",
      "Epoch 68, batch 17 D Loss: 1.3602358102798462, G Loss: 0.656019926071167\n",
      "Epoch 68, batch 18 D Loss: 1.353524923324585, G Loss: 0.6406301259994507\n",
      "Epoch 68, batch 19 D Loss: 1.3708796501159668, G Loss: 0.6238389015197754\n",
      "Epoch 68, batch 20 D Loss: 1.3663718700408936, G Loss: 0.649488627910614\n",
      "Epoch 68, batch 21 D Loss: 1.386722207069397, G Loss: 0.6368269920349121\n",
      "Epoch 68, batch 22 D Loss: 1.383920669555664, G Loss: 0.6356440186500549\n",
      "Epoch 68, batch 23 D Loss: 1.4214985370635986, G Loss: 0.6218661665916443\n",
      "Epoch 68, batch 24 D Loss: 1.3485532999038696, G Loss: 0.6497867107391357\n",
      "Epoch 68, batch 25 D Loss: 1.385565996170044, G Loss: 0.6334724426269531\n",
      "Epoch 68, batch 26 D Loss: 1.3537209033966064, G Loss: 0.6434522867202759\n",
      "Epoch 68, batch 27 D Loss: 1.4394135475158691, G Loss: 0.6175193786621094\n",
      "Epoch 68, batch 28 D Loss: 1.412950038909912, G Loss: 0.6355128288269043\n",
      "Epoch 68, batch 29 D Loss: 1.4397728443145752, G Loss: 0.6227462887763977\n",
      "Epoch 68, batch 30 D Loss: 1.3643875122070312, G Loss: 0.6573441028594971\n",
      "Epoch 68, batch 31 D Loss: 1.4208641052246094, G Loss: 0.6379776000976562\n",
      "Epoch 68, batch 32 D Loss: 1.3883851766586304, G Loss: 0.6639401912689209\n",
      "Epoch 68, batch 33 D Loss: 1.4094014167785645, G Loss: 0.6308138370513916\n",
      "Epoch 68, batch 34 D Loss: 1.4254971742630005, G Loss: 0.6564382314682007\n",
      "Epoch 68, batch 35 D Loss: 1.426023244857788, G Loss: 0.6396097540855408\n",
      "Epoch 68, batch 36 D Loss: 1.3730227947235107, G Loss: 0.6604243516921997\n",
      "Epoch 68, batch 37 D Loss: 1.4725009202957153, G Loss: 0.6203463077545166\n",
      "Epoch 68, batch 38 D Loss: 1.3861628770828247, G Loss: 0.6569674015045166\n",
      "Epoch 68, batch 39 D Loss: 1.3817155361175537, G Loss: 0.6795645952224731\n",
      "Epoch 68, batch 40 D Loss: 1.430220603942871, G Loss: 0.6464868187904358\n",
      "Epoch 68, batch 41 D Loss: 1.4297279119491577, G Loss: 0.6596685647964478\n",
      "Epoch 68, batch 42 D Loss: 1.4351308345794678, G Loss: 0.6363133192062378\n",
      "Epoch 68, batch 43 D Loss: 1.424683928489685, G Loss: 0.662139892578125\n",
      "Epoch 68, batch 44 D Loss: 1.4421935081481934, G Loss: 0.6534483432769775\n",
      "Epoch 68, batch 45 D Loss: 1.3936059474945068, G Loss: 0.6890209913253784\n",
      "Epoch 68, batch 46 D Loss: 1.4025098085403442, G Loss: 0.699968159198761\n",
      "Epoch 68, batch 47 D Loss: 1.3828449249267578, G Loss: 0.6798133850097656\n",
      "Epoch 68, batch 48 D Loss: 1.4157640933990479, G Loss: 0.6775134205818176\n",
      "Epoch 68, batch 49 D Loss: 1.403237223625183, G Loss: 0.6847031116485596\n",
      "Epoch 68, batch 50 D Loss: 1.4207544326782227, G Loss: 0.6863536834716797\n",
      "Epoch 68, batch 51 D Loss: 1.3866230249404907, G Loss: 0.7054327130317688\n",
      "Epoch 68, batch 52 D Loss: 1.3991332054138184, G Loss: 0.7006962299346924\n",
      "Epoch 68, batch 53 D Loss: 1.4160552024841309, G Loss: 0.7136526703834534\n",
      "Epoch 68, batch 54 D Loss: 1.4066252708435059, G Loss: 0.7088959217071533\n",
      "Epoch 68, batch 55 D Loss: 1.395219326019287, G Loss: 0.7105209231376648\n",
      "Epoch 68, batch 56 D Loss: 1.4287874698638916, G Loss: 0.7035952806472778\n",
      "Epoch 68, batch 57 D Loss: 1.3956468105316162, G Loss: 0.7051053643226624\n",
      "Epoch 68, batch 58 D Loss: 1.4086871147155762, G Loss: 0.7164592742919922\n",
      "Epoch 68, batch 59 D Loss: 1.374170184135437, G Loss: 0.7359554171562195\n",
      "Epoch 68, batch 60 D Loss: 1.3968220949172974, G Loss: 0.7214050889015198\n",
      "Epoch 68, batch 61 D Loss: 1.3918278217315674, G Loss: 0.7233904004096985\n",
      "Epoch 68, batch 62 D Loss: 1.3962528705596924, G Loss: 0.7302231788635254\n",
      "Epoch 68, batch 63 D Loss: 1.3936026096343994, G Loss: 0.7360842227935791\n",
      "Epoch 68, batch 64 D Loss: 1.370408535003662, G Loss: 0.7500951290130615\n",
      "Epoch 68, batch 65 D Loss: 1.376461148262024, G Loss: 0.7500065565109253\n",
      "Epoch 68, batch 66 D Loss: 1.36320161819458, G Loss: 0.7599582076072693\n",
      "Epoch 68, batch 67 D Loss: 1.3779820203781128, G Loss: 0.7825881242752075\n",
      "Epoch 68, batch 68 D Loss: 1.3773689270019531, G Loss: 0.7659296989440918\n",
      "Epoch 68, batch 69 D Loss: 1.3813085556030273, G Loss: 0.7707772850990295\n",
      "Epoch 68, batch 70 D Loss: 1.3681962490081787, G Loss: 0.7611260414123535\n",
      "Epoch 68, batch 71 D Loss: 1.3707520961761475, G Loss: 0.7632372975349426\n",
      "Epoch 68, batch 72 D Loss: 1.3534128665924072, G Loss: 0.7741028070449829\n",
      "Epoch 68, batch 73 D Loss: 1.3578612804412842, G Loss: 0.7691733837127686\n",
      "Epoch 68, batch 74 D Loss: 1.3801405429840088, G Loss: 0.756199061870575\n",
      "Epoch 68, batch 75 D Loss: 1.37044358253479, G Loss: 0.779620349407196\n",
      "Epoch 68, batch 76 D Loss: 1.3712940216064453, G Loss: 0.8054019212722778\n",
      "Epoch 68, batch 77 D Loss: 1.3705596923828125, G Loss: 0.777344822883606\n",
      "Epoch 68, batch 78 D Loss: 1.389089822769165, G Loss: 0.7670786380767822\n",
      "Epoch 68, batch 79 D Loss: 1.3793058395385742, G Loss: 0.7849868535995483\n",
      "Epoch 68, batch 80 D Loss: 1.3579952716827393, G Loss: 0.7766851186752319\n",
      "Epoch 68, batch 81 D Loss: 1.3607113361358643, G Loss: 0.7815462350845337\n",
      "Epoch 68, batch 82 D Loss: 1.351954460144043, G Loss: 0.7889044880867004\n",
      "Epoch 68, batch 83 D Loss: 1.394702434539795, G Loss: 0.781327486038208\n",
      "Epoch 68, batch 84 D Loss: 1.3916420936584473, G Loss: 0.7673165798187256\n",
      "Epoch 68, batch 85 D Loss: 1.3769285678863525, G Loss: 0.760123610496521\n",
      "Epoch 68, batch 86 D Loss: 1.380354404449463, G Loss: 0.7622185349464417\n",
      "Epoch 68, batch 87 D Loss: 1.3448338508605957, G Loss: 0.782698392868042\n",
      "Epoch 68, batch 88 D Loss: 1.4039371013641357, G Loss: 0.7594254016876221\n",
      "Epoch 68, batch 89 D Loss: 1.3665556907653809, G Loss: 0.7773418426513672\n",
      "Epoch 68, batch 90 D Loss: 1.3463828563690186, G Loss: 0.7826576828956604\n",
      "Epoch 68, batch 91 D Loss: 1.3318196535110474, G Loss: 0.7819951772689819\n",
      "Epoch 68, batch 92 D Loss: 1.3945207595825195, G Loss: 0.7748615741729736\n",
      "Epoch 68, batch 93 D Loss: 1.378367304801941, G Loss: 0.7452818155288696\n",
      "Epoch 68, batch 94 D Loss: 1.3575286865234375, G Loss: 0.7722198963165283\n",
      "Epoch 68, batch 95 D Loss: 1.3654825687408447, G Loss: 0.7799054980278015\n",
      "Epoch 68, batch 96 D Loss: 1.372917652130127, G Loss: 0.7682021856307983\n",
      "Epoch 68, batch 97 D Loss: 1.3703573942184448, G Loss: 0.7804211378097534\n",
      "Epoch 68, batch 98 D Loss: 1.3561561107635498, G Loss: 0.7790998220443726\n",
      "Epoch 68, batch 99 D Loss: 1.364375114440918, G Loss: 0.7659530639648438\n",
      "Epoch 68, batch 100 D Loss: 1.369490623474121, G Loss: 0.7449490427970886\n",
      "Epoch 68, batch 101 D Loss: 1.3950446844100952, G Loss: 0.745201826095581\n",
      "Epoch 68, batch 102 D Loss: 1.3030364513397217, G Loss: 0.8025382161140442\n",
      "Epoch 68, batch 103 D Loss: 1.3474856615066528, G Loss: 0.7676205635070801\n",
      "Epoch 68, batch 104 D Loss: 1.3245203495025635, G Loss: 0.8020657896995544\n",
      "Epoch 68, batch 105 D Loss: 1.3520877361297607, G Loss: 0.7501204609870911\n",
      "Epoch 68, batch 106 D Loss: 1.3630247116088867, G Loss: 0.762168288230896\n",
      "Epoch 68, batch 107 D Loss: 1.385080099105835, G Loss: 0.7451422214508057\n",
      "Epoch 68, batch 108 D Loss: 1.387500286102295, G Loss: 0.7476403713226318\n",
      "Epoch 68, batch 109 D Loss: 1.382723093032837, G Loss: 0.7450562119483948\n",
      "Epoch 68, batch 110 D Loss: 1.3911025524139404, G Loss: 0.7320310473442078\n",
      "Epoch 68, batch 111 D Loss: 1.388145923614502, G Loss: 0.7281690239906311\n",
      "Epoch 68, batch 112 D Loss: 1.427070140838623, G Loss: 0.705106258392334\n",
      "Epoch 68, batch 113 D Loss: 1.4170552492141724, G Loss: 0.7183759212493896\n",
      "Epoch 68, batch 114 D Loss: 1.3876936435699463, G Loss: 0.7199963331222534\n",
      "Epoch 68, batch 115 D Loss: 1.3580970764160156, G Loss: 0.7275621891021729\n",
      "Epoch 68, batch 116 D Loss: 1.366443157196045, G Loss: 0.7244409918785095\n",
      "Epoch 68, batch 117 D Loss: 1.4300569295883179, G Loss: 0.6983447074890137\n",
      "Epoch 68, batch 118 D Loss: 1.3742111921310425, G Loss: 0.7423834800720215\n",
      "Epoch 68, batch 119 D Loss: 1.352649450302124, G Loss: 0.722495436668396\n",
      "Epoch 68, batch 120 D Loss: 1.4150527715682983, G Loss: 0.7062601447105408\n",
      "Epoch 68, batch 121 D Loss: 1.4217026233673096, G Loss: 0.693595826625824\n",
      "Epoch 68, batch 122 D Loss: 1.405346155166626, G Loss: 0.6890086531639099\n",
      "Epoch 68, batch 123 D Loss: 1.4353336095809937, G Loss: 0.6926668286323547\n",
      "Epoch 68, batch 124 D Loss: 1.4013769626617432, G Loss: 0.7090773582458496\n",
      "Epoch 68, batch 125 D Loss: 1.438164234161377, G Loss: 0.6786831021308899\n",
      "Epoch 68, batch 126 D Loss: 1.4057648181915283, G Loss: 0.6993523240089417\n",
      "Epoch 68, batch 127 D Loss: 1.3913408517837524, G Loss: 0.6899078488349915\n",
      "Epoch 68, batch 128 D Loss: 1.4024953842163086, G Loss: 0.6982772946357727\n",
      "Epoch 68, batch 129 D Loss: 1.415299415588379, G Loss: 0.6637389659881592\n",
      "Epoch 68, batch 130 D Loss: 1.413191795349121, G Loss: 0.6947385668754578\n",
      "Epoch 68, batch 131 D Loss: 1.3865387439727783, G Loss: 0.6965795755386353\n",
      "Epoch 68, batch 132 D Loss: 1.4216630458831787, G Loss: 0.685422956943512\n",
      "Epoch 68, batch 133 D Loss: 1.4270919561386108, G Loss: 0.6853301525115967\n",
      "Epoch 68, batch 134 D Loss: 1.4298404455184937, G Loss: 0.6860406994819641\n",
      "Epoch 68, batch 135 D Loss: 1.3731474876403809, G Loss: 0.6894862651824951\n",
      "Epoch 68, batch 136 D Loss: 1.4375383853912354, G Loss: 0.6858423352241516\n",
      "Epoch 68, batch 137 D Loss: 1.3951501846313477, G Loss: 0.695712149143219\n",
      "Epoch 68, batch 138 D Loss: 1.4222289323806763, G Loss: 0.6856735944747925\n",
      "Epoch 68, batch 139 D Loss: 1.3848180770874023, G Loss: 0.6932133436203003\n",
      "Epoch 68, batch 140 D Loss: 1.4170105457305908, G Loss: 0.6904668211936951\n",
      "Epoch 68, batch 141 D Loss: 1.3908791542053223, G Loss: 0.6897923946380615\n",
      "Epoch 68, batch 142 D Loss: 1.3986921310424805, G Loss: 0.6835917830467224\n",
      "Epoch 68, batch 143 D Loss: 1.3779001235961914, G Loss: 0.6960757970809937\n",
      "Epoch 68, batch 144 D Loss: 1.360731601715088, G Loss: 0.6905810832977295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, batch 145 D Loss: 1.394028663635254, G Loss: 0.6959100961685181\n",
      "Epoch 68, batch 146 D Loss: 1.3814153671264648, G Loss: 0.6894158720970154\n",
      "Epoch 68, batch 147 D Loss: 1.3675247430801392, G Loss: 0.7131061553955078\n",
      "Epoch 68, batch 148 D Loss: 1.350839376449585, G Loss: 0.7067933678627014\n",
      "Epoch 68, batch 149 D Loss: 1.353046178817749, G Loss: 0.7216652035713196\n",
      "Epoch 68, batch 150 D Loss: 1.3415236473083496, G Loss: 0.7047646045684814\n",
      "Epoch 68, batch 151 D Loss: 1.3687703609466553, G Loss: 0.7029239535331726\n",
      "Epoch 68, batch 152 D Loss: 1.388824462890625, G Loss: 0.7055364847183228\n",
      "Epoch 68, batch 153 D Loss: 1.3702068328857422, G Loss: 0.6989446878433228\n",
      "Epoch 68, batch 154 D Loss: 1.3734784126281738, G Loss: 0.7057482004165649\n",
      "Epoch 68, batch 155 D Loss: 1.3604884147644043, G Loss: 0.7241746783256531\n",
      "Epoch 68, batch 156 D Loss: 1.322840690612793, G Loss: 0.7380713820457458\n",
      "Epoch 68, batch 157 D Loss: 1.3467557430267334, G Loss: 0.7084876894950867\n",
      "Epoch 68, batch 158 D Loss: 1.3499537706375122, G Loss: 0.7200908660888672\n",
      "Epoch 68, batch 159 D Loss: 1.329336166381836, G Loss: 0.7126815915107727\n",
      "Epoch 68, batch 160 D Loss: 1.3363535404205322, G Loss: 0.7134115695953369\n",
      "Epoch 68, batch 161 D Loss: 1.367363691329956, G Loss: 0.7009335160255432\n",
      "Epoch 68, batch 162 D Loss: 1.3298159837722778, G Loss: 0.7079712748527527\n",
      "Epoch 68, batch 163 D Loss: 1.326198935508728, G Loss: 0.7148458957672119\n",
      "Epoch 68, batch 164 D Loss: 1.345198631286621, G Loss: 0.7186551690101624\n",
      "Epoch 68, batch 165 D Loss: 1.3084917068481445, G Loss: 0.7729576826095581\n",
      "Epoch 68, batch 166 D Loss: 1.3313093185424805, G Loss: 0.73332679271698\n",
      "Epoch 68, batch 167 D Loss: 1.350721836090088, G Loss: 0.7146686315536499\n",
      "Epoch 68, batch 168 D Loss: 1.3270671367645264, G Loss: 0.7339367866516113\n",
      "Epoch 68, batch 169 D Loss: 1.321678876876831, G Loss: 0.7269092798233032\n",
      "Epoch 68, batch 170 D Loss: 1.3180546760559082, G Loss: 0.7454097270965576\n",
      "Epoch 68, batch 171 D Loss: 1.3275704383850098, G Loss: 0.7333223223686218\n",
      "Epoch 68, batch 172 D Loss: 1.3027024269104004, G Loss: 0.7128679752349854\n",
      "Epoch 68, batch 173 D Loss: 1.3131062984466553, G Loss: 0.7321633100509644\n",
      "Epoch 68, batch 174 D Loss: 1.3710960149765015, G Loss: 0.7149317860603333\n",
      "Epoch 68, batch 175 D Loss: 1.2718007564544678, G Loss: 0.7814093232154846\n",
      "Epoch 68, batch 176 D Loss: 1.3302369117736816, G Loss: 0.7724720239639282\n",
      "Epoch 68, batch 177 D Loss: 1.3289027214050293, G Loss: 0.7087111473083496\n",
      "Epoch 68, batch 178 D Loss: 1.3326598405838013, G Loss: 0.7171227335929871\n",
      "Epoch 68, batch 179 D Loss: 1.2802376747131348, G Loss: 0.7486986517906189\n",
      "Epoch 68, batch 180 D Loss: 1.3604506254196167, G Loss: 0.7235777378082275\n",
      "Epoch 68, batch 181 D Loss: 1.3336706161499023, G Loss: 0.689399242401123\n",
      "Epoch 68, batch 182 D Loss: 1.3086915016174316, G Loss: 0.7334758043289185\n",
      "Epoch 68, batch 183 D Loss: 1.328472375869751, G Loss: 0.691098690032959\n",
      "Epoch 68, batch 184 D Loss: 1.338500738143921, G Loss: 0.7226802706718445\n",
      "Epoch 68, batch 185 D Loss: 1.2925078868865967, G Loss: 0.7217268347740173\n",
      "Epoch 68, batch 186 D Loss: 1.3408067226409912, G Loss: 0.696540117263794\n",
      "Epoch 68, batch 187 D Loss: 1.3723595142364502, G Loss: 0.7041457295417786\n",
      "Epoch 68, batch 188 D Loss: 1.3789582252502441, G Loss: 0.670950710773468\n",
      "Epoch 68, batch 189 D Loss: 1.3035399913787842, G Loss: 0.7306199073791504\n",
      "Epoch 68, batch 190 D Loss: 1.386430263519287, G Loss: 0.6778967976570129\n",
      "Epoch 68, batch 191 D Loss: 1.3857979774475098, G Loss: 0.673228919506073\n",
      "Epoch 68, batch 192 D Loss: 1.4179209470748901, G Loss: 0.6845541596412659\n",
      "Epoch 68, batch 193 D Loss: 1.3691986799240112, G Loss: 0.6429919600486755\n",
      "Epoch 68, batch 194 D Loss: 1.351983666419983, G Loss: 0.6956651210784912\n",
      "Epoch 68, batch 195 D Loss: 1.3671574592590332, G Loss: 0.6800472736358643\n",
      "Epoch 68, batch 196 D Loss: 1.4528253078460693, G Loss: 0.6319658756256104\n",
      "Epoch 68, batch 197 D Loss: 1.4830470085144043, G Loss: 0.6134601831436157\n",
      "Epoch 68, batch 198 D Loss: 1.511193037033081, G Loss: 0.626351535320282\n",
      "Epoch 68, batch 199 D Loss: 1.4200129508972168, G Loss: 0.643674910068512\n",
      "Epoch 68, batch 200 D Loss: 1.4843753576278687, G Loss: 0.6123206615447998\n",
      "Epoch 69, batch 1 D Loss: 1.4479713439941406, G Loss: 0.6214928030967712\n",
      "Epoch 69, batch 2 D Loss: 1.4380202293395996, G Loss: 0.6399158835411072\n",
      "Epoch 69, batch 3 D Loss: 1.4724979400634766, G Loss: 0.6285018920898438\n",
      "Epoch 69, batch 4 D Loss: 1.4537713527679443, G Loss: 0.635590672492981\n",
      "Epoch 69, batch 5 D Loss: 1.4823650121688843, G Loss: 0.6501843929290771\n",
      "Epoch 69, batch 6 D Loss: 1.4222626686096191, G Loss: 0.6657736897468567\n",
      "Epoch 69, batch 7 D Loss: 1.4843631982803345, G Loss: 0.622349739074707\n",
      "Epoch 69, batch 8 D Loss: 1.471872329711914, G Loss: 0.6330262422561646\n",
      "Epoch 69, batch 9 D Loss: 1.4357304573059082, G Loss: 0.6486961245536804\n",
      "Epoch 69, batch 10 D Loss: 1.458191156387329, G Loss: 0.6553909182548523\n",
      "Epoch 69, batch 11 D Loss: 1.452401876449585, G Loss: 0.6403767466545105\n",
      "Epoch 69, batch 12 D Loss: 1.4800466299057007, G Loss: 0.641883134841919\n",
      "Epoch 69, batch 13 D Loss: 1.4010558128356934, G Loss: 0.6777523159980774\n",
      "Epoch 69, batch 14 D Loss: 1.4582854509353638, G Loss: 0.6549167633056641\n",
      "Epoch 69, batch 15 D Loss: 1.455747365951538, G Loss: 0.6378974318504333\n",
      "Epoch 69, batch 16 D Loss: 1.4215888977050781, G Loss: 0.6658766865730286\n",
      "Epoch 69, batch 17 D Loss: 1.437169075012207, G Loss: 0.6445381045341492\n",
      "Epoch 69, batch 18 D Loss: 1.4624011516571045, G Loss: 0.6435856819152832\n",
      "Epoch 69, batch 19 D Loss: 1.3966871500015259, G Loss: 0.6844544410705566\n",
      "Epoch 69, batch 20 D Loss: 1.418818712234497, G Loss: 0.7104045748710632\n",
      "Epoch 69, batch 21 D Loss: 1.4132864475250244, G Loss: 0.7149380445480347\n",
      "Epoch 69, batch 22 D Loss: 1.4157891273498535, G Loss: 0.7081373333930969\n",
      "Epoch 69, batch 23 D Loss: 1.4385294914245605, G Loss: 0.7076064348220825\n",
      "Epoch 69, batch 24 D Loss: 1.3900647163391113, G Loss: 0.7293474078178406\n",
      "Epoch 69, batch 25 D Loss: 1.4265835285186768, G Loss: 0.6938183307647705\n",
      "Epoch 69, batch 26 D Loss: 1.4038910865783691, G Loss: 0.7178988456726074\n",
      "Epoch 69, batch 27 D Loss: 1.373498797416687, G Loss: 0.7374153137207031\n",
      "Epoch 69, batch 28 D Loss: 1.4314422607421875, G Loss: 0.7305143475532532\n",
      "Epoch 69, batch 29 D Loss: 1.4148368835449219, G Loss: 0.7218766808509827\n",
      "Epoch 69, batch 30 D Loss: 1.38917875289917, G Loss: 0.7575950026512146\n",
      "Epoch 69, batch 31 D Loss: 1.350061297416687, G Loss: 0.7597236633300781\n",
      "Epoch 69, batch 32 D Loss: 1.3887319564819336, G Loss: 0.7559040188789368\n",
      "Epoch 69, batch 33 D Loss: 1.4456520080566406, G Loss: 0.7390732765197754\n",
      "Epoch 69, batch 34 D Loss: 1.450115442276001, G Loss: 0.7083265781402588\n",
      "Epoch 69, batch 35 D Loss: 1.3286113739013672, G Loss: 0.8382031917572021\n",
      "Epoch 69, batch 36 D Loss: 1.372148871421814, G Loss: 0.7716583013534546\n",
      "Epoch 69, batch 37 D Loss: 1.327965497970581, G Loss: 0.8339879512786865\n",
      "Epoch 69, batch 38 D Loss: 1.369551420211792, G Loss: 0.7906200289726257\n",
      "Epoch 69, batch 39 D Loss: 1.4271990060806274, G Loss: 0.7875635027885437\n",
      "Epoch 69, batch 40 D Loss: 1.3988043069839478, G Loss: 0.7739393711090088\n",
      "Epoch 69, batch 41 D Loss: 1.3974196910858154, G Loss: 0.7563278079032898\n",
      "Epoch 69, batch 42 D Loss: 1.38628351688385, G Loss: 0.8150879740715027\n",
      "Epoch 69, batch 43 D Loss: 1.4203546047210693, G Loss: 0.7752766609191895\n",
      "Epoch 69, batch 44 D Loss: 1.426063895225525, G Loss: 0.7447718977928162\n",
      "Epoch 69, batch 45 D Loss: 1.389789342880249, G Loss: 0.7982367873191833\n",
      "Epoch 69, batch 46 D Loss: 1.4128623008728027, G Loss: 0.7357141375541687\n",
      "Epoch 69, batch 47 D Loss: 1.4234790802001953, G Loss: 0.709970235824585\n",
      "Epoch 69, batch 48 D Loss: 1.4570424556732178, G Loss: 0.7299769520759583\n",
      "Epoch 69, batch 49 D Loss: 1.4544343948364258, G Loss: 0.717551589012146\n",
      "Epoch 69, batch 50 D Loss: 1.4323842525482178, G Loss: 0.7003833055496216\n",
      "Epoch 69, batch 51 D Loss: 1.417853593826294, G Loss: 0.7260332703590393\n",
      "Epoch 69, batch 52 D Loss: 1.469515085220337, G Loss: 0.6907302141189575\n",
      "Epoch 69, batch 53 D Loss: 1.4249286651611328, G Loss: 0.7046225070953369\n",
      "Epoch 69, batch 54 D Loss: 1.4489068984985352, G Loss: 0.6964260935783386\n",
      "Epoch 69, batch 55 D Loss: 1.4316084384918213, G Loss: 0.7097627520561218\n",
      "Epoch 69, batch 56 D Loss: 1.4328064918518066, G Loss: 0.7122474908828735\n",
      "Epoch 69, batch 57 D Loss: 1.4408891201019287, G Loss: 0.6990048289299011\n",
      "Epoch 69, batch 58 D Loss: 1.5019075870513916, G Loss: 0.6809509992599487\n",
      "Epoch 69, batch 59 D Loss: 1.4219626188278198, G Loss: 0.6974235773086548\n",
      "Epoch 69, batch 60 D Loss: 1.4425976276397705, G Loss: 0.6851792931556702\n",
      "Epoch 69, batch 61 D Loss: 1.468725323677063, G Loss: 0.6891367435455322\n",
      "Epoch 69, batch 62 D Loss: 1.4288983345031738, G Loss: 0.6991848945617676\n",
      "Epoch 69, batch 63 D Loss: 1.4766775369644165, G Loss: 0.6887716054916382\n",
      "Epoch 69, batch 64 D Loss: 1.4425628185272217, G Loss: 0.6955084204673767\n",
      "Epoch 69, batch 65 D Loss: 1.4379992485046387, G Loss: 0.6908064484596252\n",
      "Epoch 69, batch 66 D Loss: 1.4531792402267456, G Loss: 0.6859779357910156\n",
      "Epoch 69, batch 67 D Loss: 1.450336217880249, G Loss: 0.6934760212898254\n",
      "Epoch 69, batch 68 D Loss: 1.465165376663208, G Loss: 0.7040459513664246\n",
      "Epoch 69, batch 69 D Loss: 1.4216276407241821, G Loss: 0.7075451612472534\n",
      "Epoch 69, batch 70 D Loss: 1.4013839960098267, G Loss: 0.7181454300880432\n",
      "Epoch 69, batch 71 D Loss: 1.4514541625976562, G Loss: 0.7043372392654419\n",
      "Epoch 69, batch 72 D Loss: 1.430708646774292, G Loss: 0.7085075974464417\n",
      "Epoch 69, batch 73 D Loss: 1.3977839946746826, G Loss: 0.711561918258667\n",
      "Epoch 69, batch 74 D Loss: 1.4279518127441406, G Loss: 0.7106019854545593\n",
      "Epoch 69, batch 75 D Loss: 1.4024772644042969, G Loss: 0.7117664217948914\n",
      "Epoch 69, batch 76 D Loss: 1.495138168334961, G Loss: 0.712170422077179\n",
      "Epoch 69, batch 77 D Loss: 1.446000099182129, G Loss: 0.7181247472763062\n",
      "Epoch 69, batch 78 D Loss: 1.4259088039398193, G Loss: 0.7194309830665588\n",
      "Epoch 69, batch 79 D Loss: 1.3964468240737915, G Loss: 0.7201802134513855\n",
      "Epoch 69, batch 80 D Loss: 1.405083179473877, G Loss: 0.7206409573554993\n",
      "Epoch 69, batch 81 D Loss: 1.40470552444458, G Loss: 0.7263357639312744\n",
      "Epoch 69, batch 82 D Loss: 1.3865296840667725, G Loss: 0.7334908843040466\n",
      "Epoch 69, batch 83 D Loss: 1.3884258270263672, G Loss: 0.7266412377357483\n",
      "Epoch 69, batch 84 D Loss: 1.3740930557250977, G Loss: 0.7344436049461365\n",
      "Epoch 69, batch 85 D Loss: 1.3890414237976074, G Loss: 0.7407529354095459\n",
      "Epoch 69, batch 86 D Loss: 1.3889868259429932, G Loss: 0.7340173125267029\n",
      "Epoch 69, batch 87 D Loss: 1.3779041767120361, G Loss: 0.7389580011367798\n",
      "Epoch 69, batch 88 D Loss: 1.383805513381958, G Loss: 0.7335677146911621\n",
      "Epoch 69, batch 89 D Loss: 1.408827781677246, G Loss: 0.7426969408988953\n",
      "Epoch 69, batch 90 D Loss: 1.3819403648376465, G Loss: 0.7466277480125427\n",
      "Epoch 69, batch 91 D Loss: 1.3624415397644043, G Loss: 0.7517247796058655\n",
      "Epoch 69, batch 92 D Loss: 1.3804078102111816, G Loss: 0.7520478963851929\n",
      "Epoch 69, batch 93 D Loss: 1.378465175628662, G Loss: 0.7682846784591675\n",
      "Epoch 69, batch 94 D Loss: 1.3965938091278076, G Loss: 0.7430973052978516\n",
      "Epoch 69, batch 95 D Loss: 1.3732002973556519, G Loss: 0.7587447166442871\n",
      "Epoch 69, batch 96 D Loss: 1.3279736042022705, G Loss: 0.7615538239479065\n",
      "Epoch 69, batch 97 D Loss: 1.3577262163162231, G Loss: 0.7475477457046509\n",
      "Epoch 69, batch 98 D Loss: 1.3614509105682373, G Loss: 0.7545672059059143\n",
      "Epoch 69, batch 99 D Loss: 1.3426152467727661, G Loss: 0.7649189829826355\n",
      "Epoch 69, batch 100 D Loss: 1.3410234451293945, G Loss: 0.7680099606513977\n",
      "Epoch 69, batch 101 D Loss: 1.3445439338684082, G Loss: 0.7674936652183533\n",
      "Epoch 69, batch 102 D Loss: 1.3723821640014648, G Loss: 0.754814624786377\n",
      "Epoch 69, batch 103 D Loss: 1.3306204080581665, G Loss: 0.7719833254814148\n",
      "Epoch 69, batch 104 D Loss: 1.3660098314285278, G Loss: 0.7538238763809204\n",
      "Epoch 69, batch 105 D Loss: 1.3664897680282593, G Loss: 0.7699560523033142\n",
      "Epoch 69, batch 106 D Loss: 1.3587491512298584, G Loss: 0.7565455436706543\n",
      "Epoch 69, batch 107 D Loss: 1.3537148237228394, G Loss: 0.763701319694519\n",
      "Epoch 69, batch 108 D Loss: 1.3455870151519775, G Loss: 0.7659255862236023\n",
      "Epoch 69, batch 109 D Loss: 1.3394193649291992, G Loss: 0.7668519616127014\n",
      "Epoch 69, batch 110 D Loss: 1.3638226985931396, G Loss: 0.7645487189292908\n",
      "Epoch 69, batch 111 D Loss: 1.3449366092681885, G Loss: 0.7514286637306213\n",
      "Epoch 69, batch 112 D Loss: 1.318015456199646, G Loss: 0.7708482146263123\n",
      "Epoch 69, batch 113 D Loss: 1.3402211666107178, G Loss: 0.7546371221542358\n",
      "Epoch 69, batch 114 D Loss: 1.3494449853897095, G Loss: 0.7454797625541687\n",
      "Epoch 69, batch 115 D Loss: 1.3562875986099243, G Loss: 0.7513091564178467\n",
      "Epoch 69, batch 116 D Loss: 1.3267295360565186, G Loss: 0.7677938938140869\n",
      "Epoch 69, batch 117 D Loss: 1.3221001625061035, G Loss: 0.7606947422027588\n",
      "Epoch 69, batch 118 D Loss: 1.3034855127334595, G Loss: 0.7722649574279785\n",
      "Epoch 69, batch 119 D Loss: 1.3194518089294434, G Loss: 0.7565891742706299\n",
      "Epoch 69, batch 120 D Loss: 1.306161642074585, G Loss: 0.7684341669082642\n",
      "Epoch 69, batch 121 D Loss: 1.3323791027069092, G Loss: 0.754234254360199\n",
      "Epoch 69, batch 122 D Loss: 1.3223758935928345, G Loss: 0.7457801699638367\n",
      "Epoch 69, batch 123 D Loss: 1.3213335275650024, G Loss: 0.7587999701499939\n",
      "Epoch 69, batch 124 D Loss: 1.3266284465789795, G Loss: 0.7419292330741882\n",
      "Epoch 69, batch 125 D Loss: 1.3188971281051636, G Loss: 0.7383760213851929\n",
      "Epoch 69, batch 126 D Loss: 1.299381971359253, G Loss: 0.747530460357666\n",
      "Epoch 69, batch 127 D Loss: 1.336669921875, G Loss: 0.7391489148139954\n",
      "Epoch 69, batch 128 D Loss: 1.361466407775879, G Loss: 0.7196217179298401\n",
      "Epoch 69, batch 129 D Loss: 1.3146698474884033, G Loss: 0.7180923223495483\n",
      "Epoch 69, batch 130 D Loss: 1.3010506629943848, G Loss: 0.7414398789405823\n",
      "Epoch 69, batch 131 D Loss: 1.2928483486175537, G Loss: 0.7340003252029419\n",
      "Epoch 69, batch 132 D Loss: 1.293257236480713, G Loss: 0.7230445742607117\n",
      "Epoch 69, batch 133 D Loss: 1.2929251194000244, G Loss: 0.7255834341049194\n",
      "Epoch 69, batch 134 D Loss: 1.322148323059082, G Loss: 0.7286004424095154\n",
      "Epoch 69, batch 135 D Loss: 1.3170795440673828, G Loss: 0.7158781290054321\n",
      "Epoch 69, batch 136 D Loss: 1.3027374744415283, G Loss: 0.7174310088157654\n",
      "Epoch 69, batch 137 D Loss: 1.3548176288604736, G Loss: 0.7136500477790833\n",
      "Epoch 69, batch 138 D Loss: 1.3287742137908936, G Loss: 0.7068533301353455\n",
      "Epoch 69, batch 139 D Loss: 1.293644905090332, G Loss: 0.7138242125511169\n",
      "Epoch 69, batch 140 D Loss: 1.3066020011901855, G Loss: 0.7014610767364502\n",
      "Epoch 69, batch 141 D Loss: 1.3650648593902588, G Loss: 0.6984930634498596\n",
      "Epoch 69, batch 142 D Loss: 1.3768446445465088, G Loss: 0.6830958724021912\n",
      "Epoch 69, batch 143 D Loss: 1.3625719547271729, G Loss: 0.6949231624603271\n",
      "Epoch 69, batch 144 D Loss: 1.3381545543670654, G Loss: 0.6764077544212341\n",
      "Epoch 69, batch 145 D Loss: 1.3417651653289795, G Loss: 0.6997670531272888\n",
      "Epoch 69, batch 146 D Loss: 1.3549638986587524, G Loss: 0.6717398166656494\n",
      "Epoch 69, batch 147 D Loss: 1.3385826349258423, G Loss: 0.6774578094482422\n",
      "Epoch 69, batch 148 D Loss: 1.355607271194458, G Loss: 0.6832137107849121\n",
      "Epoch 69, batch 149 D Loss: 1.3712648153305054, G Loss: 0.6510493755340576\n",
      "Epoch 69, batch 150 D Loss: 1.3722960948944092, G Loss: 0.654695451259613\n",
      "Epoch 69, batch 151 D Loss: 1.39430570602417, G Loss: 0.6429846882820129\n",
      "Epoch 69, batch 152 D Loss: 1.358095407485962, G Loss: 0.6646302938461304\n",
      "Epoch 69, batch 153 D Loss: 1.320970058441162, G Loss: 0.6535448431968689\n",
      "Epoch 69, batch 154 D Loss: 1.395366907119751, G Loss: 0.6234105825424194\n",
      "Epoch 69, batch 155 D Loss: 1.429858684539795, G Loss: 0.6450822949409485\n",
      "Epoch 69, batch 156 D Loss: 1.4493951797485352, G Loss: 0.6333691477775574\n",
      "Epoch 69, batch 157 D Loss: 1.40931236743927, G Loss: 0.6047305464744568\n",
      "Epoch 69, batch 158 D Loss: 1.4275331497192383, G Loss: 0.6095023155212402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, batch 159 D Loss: 1.447232723236084, G Loss: 0.6049155592918396\n",
      "Epoch 69, batch 160 D Loss: 1.4602336883544922, G Loss: 0.6045164465904236\n",
      "Epoch 69, batch 161 D Loss: 1.4088513851165771, G Loss: 0.6051299571990967\n",
      "Epoch 69, batch 162 D Loss: 1.4352481365203857, G Loss: 0.578163743019104\n",
      "Epoch 69, batch 163 D Loss: 1.4465562105178833, G Loss: 0.6047477126121521\n",
      "Epoch 69, batch 164 D Loss: 1.4751904010772705, G Loss: 0.592660129070282\n",
      "Epoch 69, batch 165 D Loss: 1.4114646911621094, G Loss: 0.61748868227005\n",
      "Epoch 69, batch 166 D Loss: 1.4103370904922485, G Loss: 0.5989142656326294\n",
      "Epoch 69, batch 167 D Loss: 1.4574594497680664, G Loss: 0.586395263671875\n",
      "Epoch 69, batch 168 D Loss: 1.4841194152832031, G Loss: 0.5862863659858704\n",
      "Epoch 69, batch 169 D Loss: 1.4923605918884277, G Loss: 0.5859867334365845\n",
      "Epoch 69, batch 170 D Loss: 1.489678978919983, G Loss: 0.577339231967926\n",
      "Epoch 69, batch 171 D Loss: 1.528889536857605, G Loss: 0.5552530884742737\n",
      "Epoch 69, batch 172 D Loss: 1.4319998025894165, G Loss: 0.5849152207374573\n",
      "Epoch 69, batch 173 D Loss: 1.4325201511383057, G Loss: 0.5808717608451843\n",
      "Epoch 69, batch 174 D Loss: 1.416168212890625, G Loss: 0.5781883001327515\n",
      "Epoch 69, batch 175 D Loss: 1.4643619060516357, G Loss: 0.5906524062156677\n",
      "Epoch 69, batch 176 D Loss: 1.4771668910980225, G Loss: 0.5750001668930054\n",
      "Epoch 69, batch 177 D Loss: 1.4690780639648438, G Loss: 0.5841783285140991\n",
      "Epoch 69, batch 178 D Loss: 1.5319492816925049, G Loss: 0.5793244242668152\n",
      "Epoch 69, batch 179 D Loss: 1.5083823204040527, G Loss: 0.5691806077957153\n",
      "Epoch 69, batch 180 D Loss: 1.534248948097229, G Loss: 0.5820237994194031\n",
      "Epoch 69, batch 181 D Loss: 1.5700762271881104, G Loss: 0.5415049195289612\n",
      "Epoch 69, batch 182 D Loss: 1.4793353080749512, G Loss: 0.5796246528625488\n",
      "Epoch 69, batch 183 D Loss: 1.5260958671569824, G Loss: 0.5760552883148193\n",
      "Epoch 69, batch 184 D Loss: 1.4872305393218994, G Loss: 0.5883065462112427\n",
      "Epoch 69, batch 185 D Loss: 1.5239927768707275, G Loss: 0.5780660510063171\n",
      "Epoch 69, batch 186 D Loss: 1.55008864402771, G Loss: 0.5685250163078308\n",
      "Epoch 69, batch 187 D Loss: 1.5181649923324585, G Loss: 0.5931462645530701\n",
      "Epoch 69, batch 188 D Loss: 1.508742332458496, G Loss: 0.5765557289123535\n",
      "Epoch 69, batch 189 D Loss: 1.4315598011016846, G Loss: 0.5996431112289429\n",
      "Epoch 69, batch 190 D Loss: 1.513230323791504, G Loss: 0.5815476775169373\n",
      "Epoch 69, batch 191 D Loss: 1.4964094161987305, G Loss: 0.5937910079956055\n",
      "Epoch 69, batch 192 D Loss: 1.4807653427124023, G Loss: 0.6080504059791565\n",
      "Epoch 69, batch 193 D Loss: 1.512495756149292, G Loss: 0.5969861745834351\n",
      "Epoch 69, batch 194 D Loss: 1.4612452983856201, G Loss: 0.6276907324790955\n",
      "Epoch 69, batch 195 D Loss: 1.4936338663101196, G Loss: 0.5945677757263184\n",
      "Epoch 69, batch 196 D Loss: 1.4613192081451416, G Loss: 0.6255330443382263\n",
      "Epoch 69, batch 197 D Loss: 1.4856992959976196, G Loss: 0.6151553392410278\n",
      "Epoch 69, batch 198 D Loss: 1.454162836074829, G Loss: 0.6279857158660889\n",
      "Epoch 69, batch 199 D Loss: 1.4612388610839844, G Loss: 0.634071409702301\n",
      "Epoch 69, batch 200 D Loss: 1.4725176095962524, G Loss: 0.6263189911842346\n",
      "Epoch 70, batch 1 D Loss: 1.4213303327560425, G Loss: 0.6424078941345215\n",
      "Epoch 70, batch 2 D Loss: 1.423262119293213, G Loss: 0.6545696258544922\n",
      "Epoch 70, batch 3 D Loss: 1.4201329946517944, G Loss: 0.6582016944885254\n",
      "Epoch 70, batch 4 D Loss: 1.4404797554016113, G Loss: 0.6523988246917725\n",
      "Epoch 70, batch 5 D Loss: 1.424257755279541, G Loss: 0.6743461489677429\n",
      "Epoch 70, batch 6 D Loss: 1.39621901512146, G Loss: 0.6787565350532532\n",
      "Epoch 70, batch 7 D Loss: 1.3948981761932373, G Loss: 0.6836450099945068\n",
      "Epoch 70, batch 8 D Loss: 1.408794641494751, G Loss: 0.6906906366348267\n",
      "Epoch 70, batch 9 D Loss: 1.396938681602478, G Loss: 0.6895201802253723\n",
      "Epoch 70, batch 10 D Loss: 1.3847990036010742, G Loss: 0.7018852233886719\n",
      "Epoch 70, batch 11 D Loss: 1.380948543548584, G Loss: 0.7131848335266113\n",
      "Epoch 70, batch 12 D Loss: 1.3519465923309326, G Loss: 0.7277793288230896\n",
      "Epoch 70, batch 13 D Loss: 1.3426673412322998, G Loss: 0.7390415072441101\n",
      "Epoch 70, batch 14 D Loss: 1.3523226976394653, G Loss: 0.7414872050285339\n",
      "Epoch 70, batch 15 D Loss: 1.327671766281128, G Loss: 0.7499176263809204\n",
      "Epoch 70, batch 16 D Loss: 1.339600920677185, G Loss: 0.7484419345855713\n",
      "Epoch 70, batch 17 D Loss: 1.3254835605621338, G Loss: 0.7745389342308044\n",
      "Epoch 70, batch 18 D Loss: 1.3132810592651367, G Loss: 0.7599895000457764\n",
      "Epoch 70, batch 19 D Loss: 1.2969341278076172, G Loss: 0.788236677646637\n",
      "Epoch 70, batch 20 D Loss: 1.3128412961959839, G Loss: 0.7780295014381409\n",
      "Epoch 70, batch 21 D Loss: 1.3026331663131714, G Loss: 0.7965610027313232\n",
      "Epoch 70, batch 22 D Loss: 1.271918535232544, G Loss: 0.831895112991333\n",
      "Epoch 70, batch 23 D Loss: 1.2992520332336426, G Loss: 0.784956157207489\n",
      "Epoch 70, batch 24 D Loss: 1.2479772567749023, G Loss: 0.8339800238609314\n",
      "Epoch 70, batch 25 D Loss: 1.2685480117797852, G Loss: 0.8428173065185547\n",
      "Epoch 70, batch 26 D Loss: 1.2445378303527832, G Loss: 0.841632068157196\n",
      "Epoch 70, batch 27 D Loss: 1.2821539640426636, G Loss: 0.8057085275650024\n",
      "Epoch 70, batch 28 D Loss: 1.235968828201294, G Loss: 0.8428683280944824\n",
      "Epoch 70, batch 29 D Loss: 1.2576186656951904, G Loss: 0.8203771710395813\n",
      "Epoch 70, batch 30 D Loss: 1.2440849542617798, G Loss: 0.8602846264839172\n",
      "Epoch 70, batch 31 D Loss: 1.1952190399169922, G Loss: 0.8586855530738831\n",
      "Epoch 70, batch 32 D Loss: 1.2544585466384888, G Loss: 0.825696587562561\n",
      "Epoch 70, batch 33 D Loss: 1.2339401245117188, G Loss: 0.8728535175323486\n",
      "Epoch 70, batch 34 D Loss: 1.237979769706726, G Loss: 0.8453044295310974\n",
      "Epoch 70, batch 35 D Loss: 1.2604727745056152, G Loss: 0.8034529685974121\n",
      "Epoch 70, batch 36 D Loss: 1.2296644449234009, G Loss: 0.8796905279159546\n",
      "Epoch 70, batch 37 D Loss: 1.2390503883361816, G Loss: 0.8438429832458496\n",
      "Epoch 70, batch 38 D Loss: 1.2130639553070068, G Loss: 0.8201848864555359\n",
      "Epoch 70, batch 39 D Loss: 1.2236837148666382, G Loss: 0.820526123046875\n",
      "Epoch 70, batch 40 D Loss: 1.239965558052063, G Loss: 0.8521767258644104\n",
      "Epoch 70, batch 41 D Loss: 1.2267510890960693, G Loss: 0.8408312797546387\n",
      "Epoch 70, batch 42 D Loss: 1.2932857275009155, G Loss: 0.7958670258522034\n",
      "Epoch 70, batch 43 D Loss: 1.2164061069488525, G Loss: 0.7958633303642273\n",
      "Epoch 70, batch 44 D Loss: 1.2136270999908447, G Loss: 0.7827447652816772\n",
      "Epoch 70, batch 45 D Loss: 1.2777423858642578, G Loss: 0.7695934176445007\n",
      "Epoch 70, batch 46 D Loss: 1.2643578052520752, G Loss: 0.7432585954666138\n",
      "Epoch 70, batch 47 D Loss: 1.2404732704162598, G Loss: 0.807325005531311\n",
      "Epoch 70, batch 48 D Loss: 1.2327107191085815, G Loss: 0.7585642337799072\n",
      "Epoch 70, batch 49 D Loss: 1.3213266134262085, G Loss: 0.7397435903549194\n",
      "Epoch 70, batch 50 D Loss: 1.3128001689910889, G Loss: 0.735920250415802\n",
      "Epoch 70, batch 51 D Loss: 1.3078038692474365, G Loss: 0.6999778151512146\n",
      "Epoch 70, batch 52 D Loss: 1.2913082838058472, G Loss: 0.738833487033844\n",
      "Epoch 70, batch 53 D Loss: 1.280320405960083, G Loss: 0.7260008454322815\n",
      "Epoch 70, batch 54 D Loss: 1.2540366649627686, G Loss: 0.7177380323410034\n",
      "Epoch 70, batch 55 D Loss: 1.2987068891525269, G Loss: 0.7221288084983826\n",
      "Epoch 70, batch 56 D Loss: 1.3242480754852295, G Loss: 0.7019975185394287\n",
      "Epoch 70, batch 57 D Loss: 1.3351058959960938, G Loss: 0.6896495819091797\n",
      "Epoch 70, batch 58 D Loss: 1.337472915649414, G Loss: 0.6608507633209229\n",
      "Epoch 70, batch 59 D Loss: 1.351213812828064, G Loss: 0.6529572010040283\n",
      "Epoch 70, batch 60 D Loss: 1.3331553936004639, G Loss: 0.6419102549552917\n",
      "Epoch 70, batch 61 D Loss: 1.4009822607040405, G Loss: 0.6464080810546875\n",
      "Epoch 70, batch 62 D Loss: 1.360337734222412, G Loss: 0.6332314014434814\n",
      "Epoch 70, batch 63 D Loss: 1.3997149467468262, G Loss: 0.6131774187088013\n",
      "Epoch 70, batch 64 D Loss: 1.4586913585662842, G Loss: 0.6180967092514038\n",
      "Epoch 70, batch 65 D Loss: 1.4442272186279297, G Loss: 0.5944933891296387\n",
      "Epoch 70, batch 66 D Loss: 1.3531253337860107, G Loss: 0.6172683238983154\n",
      "Epoch 70, batch 67 D Loss: 1.3744831085205078, G Loss: 0.6056192517280579\n",
      "Epoch 70, batch 68 D Loss: 1.4342529773712158, G Loss: 0.6043581962585449\n",
      "Epoch 70, batch 69 D Loss: 1.5229463577270508, G Loss: 0.5979791879653931\n",
      "Epoch 70, batch 70 D Loss: 1.4721177816390991, G Loss: 0.5888351202011108\n",
      "Epoch 70, batch 71 D Loss: 1.4973021745681763, G Loss: 0.5756998658180237\n",
      "Epoch 70, batch 72 D Loss: 1.5284850597381592, G Loss: 0.5715903043746948\n",
      "Epoch 70, batch 73 D Loss: 1.570424199104309, G Loss: 0.5648912787437439\n",
      "Epoch 70, batch 74 D Loss: 1.5125259160995483, G Loss: 0.5789735913276672\n",
      "Epoch 70, batch 75 D Loss: 1.5003571510314941, G Loss: 0.5455120205879211\n",
      "Epoch 70, batch 76 D Loss: 1.4810370206832886, G Loss: 0.5652477741241455\n",
      "Epoch 70, batch 77 D Loss: 1.4818766117095947, G Loss: 0.5530454516410828\n",
      "Epoch 70, batch 78 D Loss: 1.581979513168335, G Loss: 0.5584703683853149\n",
      "Epoch 70, batch 79 D Loss: 1.5083763599395752, G Loss: 0.5505459904670715\n",
      "Epoch 70, batch 80 D Loss: 1.6029582023620605, G Loss: 0.5302548408508301\n",
      "Epoch 70, batch 81 D Loss: 1.4518766403198242, G Loss: 0.5863195657730103\n",
      "Epoch 70, batch 82 D Loss: 1.5084624290466309, G Loss: 0.528041660785675\n",
      "Epoch 70, batch 83 D Loss: 1.5104014873504639, G Loss: 0.5865620970726013\n",
      "Epoch 70, batch 84 D Loss: 1.5876669883728027, G Loss: 0.5424541234970093\n",
      "Epoch 70, batch 85 D Loss: 1.5790977478027344, G Loss: 0.568740963935852\n",
      "Epoch 70, batch 86 D Loss: 1.5357093811035156, G Loss: 0.5352644920349121\n",
      "Epoch 70, batch 87 D Loss: 1.5333911180496216, G Loss: 0.575070321559906\n",
      "Epoch 70, batch 88 D Loss: 1.52364182472229, G Loss: 0.5825443863868713\n",
      "Epoch 70, batch 89 D Loss: 1.5439356565475464, G Loss: 0.5956748723983765\n",
      "Epoch 70, batch 90 D Loss: 1.4708856344223022, G Loss: 0.6294271945953369\n",
      "Epoch 70, batch 91 D Loss: 1.5128257274627686, G Loss: 0.5993440747261047\n",
      "Epoch 70, batch 92 D Loss: 1.5067925453186035, G Loss: 0.6161603331565857\n",
      "Epoch 70, batch 93 D Loss: 1.4983422756195068, G Loss: 0.6182010769844055\n",
      "Epoch 70, batch 94 D Loss: 1.547626256942749, G Loss: 0.6201534867286682\n",
      "Epoch 70, batch 95 D Loss: 1.568946361541748, G Loss: 0.6199942231178284\n",
      "Epoch 70, batch 96 D Loss: 1.4543557167053223, G Loss: 0.6097192168235779\n",
      "Epoch 70, batch 97 D Loss: 1.5118741989135742, G Loss: 0.6558371782302856\n",
      "Epoch 70, batch 98 D Loss: 1.482236385345459, G Loss: 0.6333291530609131\n",
      "Epoch 70, batch 99 D Loss: 1.4167048931121826, G Loss: 0.7047663927078247\n",
      "Epoch 70, batch 100 D Loss: 1.3760830163955688, G Loss: 0.6973316669464111\n",
      "Epoch 70, batch 101 D Loss: 1.4678115844726562, G Loss: 0.6950023174285889\n",
      "Epoch 70, batch 102 D Loss: 1.4689068794250488, G Loss: 0.6843338012695312\n",
      "Epoch 70, batch 103 D Loss: 1.444844365119934, G Loss: 0.7271196246147156\n",
      "Epoch 70, batch 104 D Loss: 1.4325162172317505, G Loss: 0.718764066696167\n",
      "Epoch 70, batch 105 D Loss: 1.400260090827942, G Loss: 0.7201337218284607\n",
      "Epoch 70, batch 106 D Loss: 1.3703078031539917, G Loss: 0.7478678226470947\n",
      "Epoch 70, batch 107 D Loss: 1.4586882591247559, G Loss: 0.7381665110588074\n",
      "Epoch 70, batch 108 D Loss: 1.4241273403167725, G Loss: 0.7268394231796265\n",
      "Epoch 70, batch 109 D Loss: 1.327270746231079, G Loss: 0.7937344312667847\n",
      "Epoch 70, batch 110 D Loss: 1.3573700189590454, G Loss: 0.8116740584373474\n",
      "Epoch 70, batch 111 D Loss: 1.3646337985992432, G Loss: 0.794945478439331\n",
      "Epoch 70, batch 112 D Loss: 1.4010450839996338, G Loss: 0.8037615418434143\n",
      "Epoch 70, batch 113 D Loss: 1.3234755992889404, G Loss: 0.822559118270874\n",
      "Epoch 70, batch 114 D Loss: 1.3670823574066162, G Loss: 0.816581130027771\n",
      "Epoch 70, batch 115 D Loss: 1.3432682752609253, G Loss: 0.8428038954734802\n",
      "Epoch 70, batch 116 D Loss: 1.3638877868652344, G Loss: 0.8489582538604736\n",
      "Epoch 70, batch 117 D Loss: 1.2540802955627441, G Loss: 0.8497325778007507\n",
      "Epoch 70, batch 118 D Loss: 1.3400847911834717, G Loss: 0.8290573358535767\n",
      "Epoch 70, batch 119 D Loss: 1.2729713916778564, G Loss: 0.9574699997901917\n",
      "Epoch 70, batch 120 D Loss: 1.3131515979766846, G Loss: 0.9480999708175659\n",
      "Epoch 70, batch 121 D Loss: 1.2854655981063843, G Loss: 0.9815931916236877\n",
      "Epoch 70, batch 122 D Loss: 1.2564418315887451, G Loss: 0.9291341304779053\n",
      "Epoch 70, batch 123 D Loss: 1.2928194999694824, G Loss: 0.9015173316001892\n",
      "Epoch 70, batch 124 D Loss: 1.233628749847412, G Loss: 0.9463759660720825\n",
      "Epoch 70, batch 125 D Loss: 1.3043618202209473, G Loss: 0.9134376645088196\n",
      "Epoch 70, batch 126 D Loss: 1.2249517440795898, G Loss: 0.9232194423675537\n",
      "Epoch 70, batch 127 D Loss: 1.288370966911316, G Loss: 0.9407100081443787\n",
      "Epoch 70, batch 128 D Loss: 1.38893461227417, G Loss: 0.8397465348243713\n",
      "Epoch 70, batch 129 D Loss: 1.3152925968170166, G Loss: 0.8550538420677185\n",
      "Epoch 70, batch 130 D Loss: 1.3032788038253784, G Loss: 0.9135748147964478\n",
      "Epoch 70, batch 131 D Loss: 1.316400408744812, G Loss: 0.870342493057251\n",
      "Epoch 70, batch 132 D Loss: 1.312880277633667, G Loss: 0.8380553722381592\n",
      "Epoch 70, batch 133 D Loss: 1.3346238136291504, G Loss: 0.7753816246986389\n",
      "Epoch 70, batch 134 D Loss: 1.343740701675415, G Loss: 0.8562286496162415\n",
      "Epoch 70, batch 135 D Loss: 1.1918995380401611, G Loss: 0.9307115077972412\n",
      "Epoch 70, batch 136 D Loss: 1.3762520551681519, G Loss: 0.8027288317680359\n",
      "Epoch 70, batch 137 D Loss: 1.2824532985687256, G Loss: 0.8611716628074646\n",
      "Epoch 70, batch 138 D Loss: 1.40313720703125, G Loss: 0.7943791151046753\n",
      "Epoch 70, batch 139 D Loss: 1.293370008468628, G Loss: 0.7950019240379333\n",
      "Epoch 70, batch 140 D Loss: 1.3467429876327515, G Loss: 0.801853358745575\n",
      "Epoch 70, batch 141 D Loss: 1.3754212856292725, G Loss: 0.7767131924629211\n",
      "Epoch 70, batch 142 D Loss: 1.352217674255371, G Loss: 0.7902654409408569\n",
      "Epoch 70, batch 143 D Loss: 1.258989691734314, G Loss: 0.8041872978210449\n",
      "Epoch 70, batch 144 D Loss: 1.3617876768112183, G Loss: 0.820909857749939\n",
      "Epoch 70, batch 145 D Loss: 1.3666894435882568, G Loss: 0.7958488464355469\n",
      "Epoch 70, batch 146 D Loss: 1.3234461545944214, G Loss: 0.8091757297515869\n",
      "Epoch 70, batch 147 D Loss: 1.3278236389160156, G Loss: 0.7691242694854736\n",
      "Epoch 70, batch 148 D Loss: 1.3050272464752197, G Loss: 0.8155913352966309\n",
      "Epoch 70, batch 149 D Loss: 1.3522069454193115, G Loss: 0.8056322336196899\n",
      "Epoch 70, batch 150 D Loss: 1.3218904733657837, G Loss: 0.8191673159599304\n",
      "Epoch 70, batch 151 D Loss: 1.3313889503479004, G Loss: 0.8391319513320923\n",
      "Epoch 70, batch 152 D Loss: 1.237992286682129, G Loss: 0.8408955931663513\n",
      "Epoch 70, batch 153 D Loss: 1.3114595413208008, G Loss: 0.8080434203147888\n",
      "Epoch 70, batch 154 D Loss: 1.3408247232437134, G Loss: 0.7983951568603516\n",
      "Epoch 70, batch 155 D Loss: 1.2614178657531738, G Loss: 0.8976020216941833\n",
      "Epoch 70, batch 156 D Loss: 1.3663969039916992, G Loss: 0.8073089718818665\n",
      "Epoch 70, batch 157 D Loss: 1.3624173402786255, G Loss: 0.8564796447753906\n",
      "Epoch 70, batch 158 D Loss: 1.3325722217559814, G Loss: 0.8577459454536438\n",
      "Epoch 70, batch 159 D Loss: 1.2596566677093506, G Loss: 0.8608301281929016\n",
      "Epoch 70, batch 160 D Loss: 1.3007125854492188, G Loss: 0.8333141207695007\n",
      "Epoch 70, batch 161 D Loss: 1.3283231258392334, G Loss: 0.8817915916442871\n",
      "Epoch 70, batch 162 D Loss: 1.27028226852417, G Loss: 0.8756490349769592\n",
      "Epoch 70, batch 163 D Loss: 1.2920202016830444, G Loss: 0.8812258839607239\n",
      "Epoch 70, batch 164 D Loss: 1.2722091674804688, G Loss: 0.9077455401420593\n",
      "Epoch 70, batch 165 D Loss: 1.3121967315673828, G Loss: 0.8685044646263123\n",
      "Epoch 70, batch 166 D Loss: 1.2592129707336426, G Loss: 0.9214289784431458\n",
      "Epoch 70, batch 167 D Loss: 1.3525227308273315, G Loss: 0.8948910236358643\n",
      "Epoch 70, batch 168 D Loss: 1.39299738407135, G Loss: 0.8781933784484863\n",
      "Epoch 70, batch 169 D Loss: 1.2681903839111328, G Loss: 0.8568509817123413\n",
      "Epoch 70, batch 170 D Loss: 1.4429758787155151, G Loss: 0.8760521411895752\n",
      "Epoch 70, batch 171 D Loss: 1.2894959449768066, G Loss: 0.9724293351173401\n",
      "Epoch 70, batch 172 D Loss: 1.2286001443862915, G Loss: 0.9733796119689941\n",
      "Epoch 70, batch 173 D Loss: 1.2408804893493652, G Loss: 1.0488497018814087\n",
      "Epoch 70, batch 174 D Loss: 1.3243738412857056, G Loss: 0.8213302493095398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, batch 175 D Loss: 1.3924291133880615, G Loss: 0.8413320779800415\n",
      "Epoch 70, batch 176 D Loss: 1.3754428625106812, G Loss: 0.785432755947113\n",
      "Epoch 70, batch 177 D Loss: 1.3334896564483643, G Loss: 0.8831997513771057\n",
      "Epoch 70, batch 178 D Loss: 1.3261723518371582, G Loss: 0.9263896346092224\n",
      "Epoch 70, batch 179 D Loss: 1.4057966470718384, G Loss: 0.8800637722015381\n",
      "Epoch 70, batch 180 D Loss: 1.405069351196289, G Loss: 0.7867709398269653\n",
      "Epoch 70, batch 181 D Loss: 1.4064326286315918, G Loss: 0.8219363689422607\n",
      "Epoch 70, batch 182 D Loss: 1.515964388847351, G Loss: 0.8406839966773987\n",
      "Epoch 70, batch 183 D Loss: 1.4836995601654053, G Loss: 0.8831695318222046\n",
      "Epoch 70, batch 184 D Loss: 1.4673385620117188, G Loss: 0.7167481780052185\n",
      "Epoch 70, batch 185 D Loss: 1.3587126731872559, G Loss: 0.78862464427948\n",
      "Epoch 70, batch 186 D Loss: 1.5577678680419922, G Loss: 0.6802311539649963\n",
      "Epoch 70, batch 187 D Loss: 1.4873507022857666, G Loss: 0.7007835507392883\n",
      "Epoch 70, batch 188 D Loss: 1.4149515628814697, G Loss: 0.7411531209945679\n",
      "Epoch 70, batch 189 D Loss: 1.5224370956420898, G Loss: 0.668274998664856\n",
      "Epoch 70, batch 190 D Loss: 1.578714370727539, G Loss: 0.6838231086730957\n",
      "Epoch 70, batch 191 D Loss: 1.4784879684448242, G Loss: 0.7256357073783875\n",
      "Epoch 70, batch 192 D Loss: 1.5474213361740112, G Loss: 0.6364361643791199\n",
      "Epoch 70, batch 193 D Loss: 1.4606513977050781, G Loss: 0.6739445328712463\n",
      "Epoch 70, batch 194 D Loss: 1.636490821838379, G Loss: 0.6614363789558411\n",
      "Epoch 70, batch 195 D Loss: 1.5090380907058716, G Loss: 0.6305846571922302\n",
      "Epoch 70, batch 196 D Loss: 1.5045526027679443, G Loss: 0.6637111902236938\n",
      "Epoch 70, batch 197 D Loss: 1.6274304389953613, G Loss: 0.6190949082374573\n",
      "Epoch 70, batch 198 D Loss: 1.5753087997436523, G Loss: 0.6574751138687134\n",
      "Epoch 70, batch 199 D Loss: 1.5404515266418457, G Loss: 0.6373291611671448\n",
      "Epoch 70, batch 200 D Loss: 1.626816749572754, G Loss: 0.6140998005867004\n",
      "Epoch 71, batch 1 D Loss: 1.7065948247909546, G Loss: 0.6190980672836304\n",
      "Epoch 71, batch 2 D Loss: 1.6115578413009644, G Loss: 0.6162721514701843\n",
      "Epoch 71, batch 3 D Loss: 1.5891923904418945, G Loss: 0.638159453868866\n",
      "Epoch 71, batch 4 D Loss: 1.4661519527435303, G Loss: 0.6808133721351624\n",
      "Epoch 71, batch 5 D Loss: 1.6196439266204834, G Loss: 0.6043699383735657\n",
      "Epoch 71, batch 6 D Loss: 1.51869797706604, G Loss: 0.648191511631012\n",
      "Epoch 71, batch 7 D Loss: 1.5083062648773193, G Loss: 0.6445720195770264\n",
      "Epoch 71, batch 8 D Loss: 1.570777416229248, G Loss: 0.6693588495254517\n",
      "Epoch 71, batch 9 D Loss: 1.596414566040039, G Loss: 0.6433223485946655\n",
      "Epoch 71, batch 10 D Loss: 1.5246315002441406, G Loss: 0.7272671461105347\n",
      "Epoch 71, batch 11 D Loss: 1.4640154838562012, G Loss: 0.7245916724205017\n",
      "Epoch 71, batch 12 D Loss: 1.4964990615844727, G Loss: 0.7338300347328186\n",
      "Epoch 71, batch 13 D Loss: 1.5408496856689453, G Loss: 0.6925146579742432\n",
      "Epoch 71, batch 14 D Loss: 1.5078685283660889, G Loss: 0.7165927886962891\n",
      "Epoch 71, batch 15 D Loss: 1.530582308769226, G Loss: 0.7599522471427917\n",
      "Epoch 71, batch 16 D Loss: 1.4743804931640625, G Loss: 0.775417149066925\n",
      "Epoch 71, batch 17 D Loss: 1.5414316654205322, G Loss: 0.7729993462562561\n",
      "Epoch 71, batch 18 D Loss: 1.5544021129608154, G Loss: 0.7999930381774902\n",
      "Epoch 71, batch 19 D Loss: 1.5215610265731812, G Loss: 0.7991875410079956\n",
      "Epoch 71, batch 20 D Loss: 1.4498004913330078, G Loss: 0.7978716492652893\n",
      "Epoch 71, batch 21 D Loss: 1.451120138168335, G Loss: 0.8131248354911804\n",
      "Epoch 71, batch 22 D Loss: 1.4282913208007812, G Loss: 0.7670921087265015\n",
      "Epoch 71, batch 23 D Loss: 1.3532123565673828, G Loss: 0.7986185550689697\n",
      "Epoch 71, batch 24 D Loss: 1.3806276321411133, G Loss: 0.8592256307601929\n",
      "Epoch 71, batch 25 D Loss: 1.423510193824768, G Loss: 0.7882183194160461\n",
      "Epoch 71, batch 26 D Loss: 1.5042623281478882, G Loss: 0.7653569579124451\n",
      "Epoch 71, batch 27 D Loss: 1.3362572193145752, G Loss: 0.8279507160186768\n",
      "Epoch 71, batch 28 D Loss: 1.4008851051330566, G Loss: 0.8225553631782532\n",
      "Epoch 71, batch 29 D Loss: 1.4334113597869873, G Loss: 0.8089512586593628\n",
      "Epoch 71, batch 30 D Loss: 1.4072136878967285, G Loss: 0.7943717241287231\n",
      "Epoch 71, batch 31 D Loss: 1.3380184173583984, G Loss: 0.8313360810279846\n",
      "Epoch 71, batch 32 D Loss: 1.4010887145996094, G Loss: 0.867770254611969\n",
      "Epoch 71, batch 33 D Loss: 1.3673152923583984, G Loss: 0.829983651638031\n",
      "Epoch 71, batch 34 D Loss: 1.2999706268310547, G Loss: 0.8776242136955261\n",
      "Epoch 71, batch 35 D Loss: 1.3553094863891602, G Loss: 0.8266192674636841\n",
      "Epoch 71, batch 36 D Loss: 1.3282243013381958, G Loss: 0.8325616717338562\n",
      "Epoch 71, batch 37 D Loss: 1.367980718612671, G Loss: 0.8387560844421387\n",
      "Epoch 71, batch 38 D Loss: 1.3692293167114258, G Loss: 0.835187554359436\n",
      "Epoch 71, batch 39 D Loss: 1.4184346199035645, G Loss: 0.8744661211967468\n",
      "Epoch 71, batch 40 D Loss: 1.313751459121704, G Loss: 0.8731683492660522\n",
      "Epoch 71, batch 41 D Loss: 1.3693459033966064, G Loss: 0.8565248847007751\n",
      "Epoch 71, batch 42 D Loss: 1.276030421257019, G Loss: 0.8773411512374878\n",
      "Epoch 71, batch 43 D Loss: 1.2885587215423584, G Loss: 0.8889233469963074\n",
      "Epoch 71, batch 44 D Loss: 1.2840802669525146, G Loss: 0.8888555765151978\n",
      "Epoch 71, batch 45 D Loss: 1.2946405410766602, G Loss: 0.8947462439537048\n",
      "Epoch 71, batch 46 D Loss: 1.328519344329834, G Loss: 0.8553630113601685\n",
      "Epoch 71, batch 47 D Loss: 1.3324718475341797, G Loss: 0.8674618601799011\n",
      "Epoch 71, batch 48 D Loss: 1.2417997121810913, G Loss: 0.8961748480796814\n",
      "Epoch 71, batch 49 D Loss: 1.2187426090240479, G Loss: 0.8614704608917236\n",
      "Epoch 71, batch 50 D Loss: 1.2670588493347168, G Loss: 0.8612090945243835\n",
      "Epoch 71, batch 51 D Loss: 1.3222585916519165, G Loss: 0.8524954319000244\n",
      "Epoch 71, batch 52 D Loss: 1.267371654510498, G Loss: 0.8892509341239929\n",
      "Epoch 71, batch 53 D Loss: 1.2911794185638428, G Loss: 0.8384546041488647\n",
      "Epoch 71, batch 54 D Loss: 1.266709327697754, G Loss: 0.8934724926948547\n",
      "Epoch 71, batch 55 D Loss: 1.2844287157058716, G Loss: 0.8694412112236023\n",
      "Epoch 71, batch 56 D Loss: 1.258471965789795, G Loss: 0.882312536239624\n",
      "Epoch 71, batch 57 D Loss: 1.2063274383544922, G Loss: 0.9145702123641968\n",
      "Epoch 71, batch 58 D Loss: 1.2446632385253906, G Loss: 0.8739467859268188\n",
      "Epoch 71, batch 59 D Loss: 1.224210500717163, G Loss: 0.8735291361808777\n",
      "Epoch 71, batch 60 D Loss: 1.1848435401916504, G Loss: 0.8590453863143921\n",
      "Epoch 71, batch 61 D Loss: 1.1663262844085693, G Loss: 0.8426603078842163\n",
      "Epoch 71, batch 62 D Loss: 1.2821519374847412, G Loss: 0.8311805725097656\n",
      "Epoch 71, batch 63 D Loss: 1.2324988842010498, G Loss: 0.8521251678466797\n",
      "Epoch 71, batch 64 D Loss: 1.1962811946868896, G Loss: 0.8331844210624695\n",
      "Epoch 71, batch 65 D Loss: 1.2258838415145874, G Loss: 0.8342359662055969\n",
      "Epoch 71, batch 66 D Loss: 1.2618263959884644, G Loss: 0.8652022480964661\n",
      "Epoch 71, batch 67 D Loss: 1.2842519283294678, G Loss: 0.8522817492485046\n",
      "Epoch 71, batch 68 D Loss: 1.2304024696350098, G Loss: 0.8123666644096375\n",
      "Epoch 71, batch 69 D Loss: 1.2005513906478882, G Loss: 0.8375840187072754\n",
      "Epoch 71, batch 70 D Loss: 1.163875937461853, G Loss: 0.8376586437225342\n",
      "Epoch 71, batch 71 D Loss: 1.1865134239196777, G Loss: 0.8608705401420593\n",
      "Epoch 71, batch 72 D Loss: 1.2408993244171143, G Loss: 0.8342105150222778\n",
      "Epoch 71, batch 73 D Loss: 1.241919994354248, G Loss: 0.8404909372329712\n",
      "Epoch 71, batch 74 D Loss: 1.132328748703003, G Loss: 0.8494552373886108\n",
      "Epoch 71, batch 75 D Loss: 1.2343686819076538, G Loss: 0.8204016089439392\n",
      "Epoch 71, batch 76 D Loss: 1.148542881011963, G Loss: 0.82342928647995\n",
      "Epoch 71, batch 77 D Loss: 1.2111200094223022, G Loss: 0.8370961546897888\n",
      "Epoch 71, batch 78 D Loss: 1.146465539932251, G Loss: 0.8507962822914124\n",
      "Epoch 71, batch 79 D Loss: 1.1559933423995972, G Loss: 0.8465379476547241\n",
      "Epoch 71, batch 80 D Loss: 1.2521260976791382, G Loss: 0.853890597820282\n",
      "Epoch 71, batch 81 D Loss: 1.1531383991241455, G Loss: 0.8507745265960693\n",
      "Epoch 71, batch 82 D Loss: 1.2388556003570557, G Loss: 0.8096778988838196\n",
      "Epoch 71, batch 83 D Loss: 1.086257815361023, G Loss: 0.8535451292991638\n",
      "Epoch 71, batch 84 D Loss: 1.1888651847839355, G Loss: 0.8510399460792542\n",
      "Epoch 71, batch 85 D Loss: 1.1819100379943848, G Loss: 0.8231686949729919\n",
      "Epoch 71, batch 86 D Loss: 1.1819024085998535, G Loss: 0.847646176815033\n",
      "Epoch 71, batch 87 D Loss: 1.14033842086792, G Loss: 0.8765119314193726\n",
      "Epoch 71, batch 88 D Loss: 1.1388252973556519, G Loss: 0.8249001502990723\n",
      "Epoch 71, batch 89 D Loss: 1.2304818630218506, G Loss: 0.8557115197181702\n",
      "Epoch 71, batch 90 D Loss: 1.1561710834503174, G Loss: 0.9115315079689026\n",
      "Epoch 71, batch 91 D Loss: 1.1968193054199219, G Loss: 0.8572688102722168\n",
      "Epoch 71, batch 92 D Loss: 1.1578571796417236, G Loss: 0.8730370998382568\n",
      "Epoch 71, batch 93 D Loss: 1.1565725803375244, G Loss: 0.9115748405456543\n",
      "Epoch 71, batch 94 D Loss: 1.105374813079834, G Loss: 0.8928570747375488\n",
      "Epoch 71, batch 95 D Loss: 1.0603351593017578, G Loss: 1.0412306785583496\n",
      "Epoch 71, batch 96 D Loss: 1.1279444694519043, G Loss: 0.9170238971710205\n",
      "Epoch 71, batch 97 D Loss: 1.2202675342559814, G Loss: 0.8856347799301147\n",
      "Epoch 71, batch 98 D Loss: 1.1448383331298828, G Loss: 0.9550252556800842\n",
      "Epoch 71, batch 99 D Loss: 1.1237142086029053, G Loss: 0.9099140167236328\n",
      "Epoch 71, batch 100 D Loss: 1.1424822807312012, G Loss: 0.8818187713623047\n",
      "Epoch 71, batch 101 D Loss: 1.1032483577728271, G Loss: 0.931135892868042\n",
      "Epoch 71, batch 102 D Loss: 1.083669662475586, G Loss: 0.9187150597572327\n",
      "Epoch 71, batch 103 D Loss: 1.1723004579544067, G Loss: 0.9999611377716064\n",
      "Epoch 71, batch 104 D Loss: 1.195101022720337, G Loss: 0.8798048496246338\n",
      "Epoch 71, batch 105 D Loss: 1.2373714447021484, G Loss: 0.9142459034919739\n",
      "Epoch 71, batch 106 D Loss: 1.2168631553649902, G Loss: 0.8630915284156799\n",
      "Epoch 71, batch 107 D Loss: 1.3190659284591675, G Loss: 0.8402882218360901\n",
      "Epoch 71, batch 108 D Loss: 1.3075230121612549, G Loss: 0.7743848562240601\n",
      "Epoch 71, batch 109 D Loss: 1.1941686868667603, G Loss: 0.8547550439834595\n",
      "Epoch 71, batch 110 D Loss: 1.2121913433074951, G Loss: 0.8244465589523315\n",
      "Epoch 71, batch 111 D Loss: 1.264026165008545, G Loss: 0.8012591600418091\n",
      "Epoch 71, batch 112 D Loss: 1.2698545455932617, G Loss: 0.7261608839035034\n",
      "Epoch 71, batch 113 D Loss: 1.4020259380340576, G Loss: 0.7280476093292236\n",
      "Epoch 71, batch 114 D Loss: 1.384956955909729, G Loss: 0.6621255278587341\n",
      "Epoch 71, batch 115 D Loss: 1.2626945972442627, G Loss: 0.707268476486206\n",
      "Epoch 71, batch 116 D Loss: 1.5196640491485596, G Loss: 0.6593846678733826\n",
      "Epoch 71, batch 117 D Loss: 1.4175474643707275, G Loss: 0.6608380079269409\n",
      "Epoch 71, batch 118 D Loss: 1.351832628250122, G Loss: 0.6622033715248108\n",
      "Epoch 71, batch 119 D Loss: 1.5022400617599487, G Loss: 0.6021419167518616\n",
      "Epoch 71, batch 120 D Loss: 1.5389432907104492, G Loss: 0.5349009037017822\n",
      "Epoch 71, batch 121 D Loss: 1.632413625717163, G Loss: 0.5418254137039185\n",
      "Epoch 71, batch 122 D Loss: 1.4012001752853394, G Loss: 0.5915255546569824\n",
      "Epoch 71, batch 123 D Loss: 1.5618860721588135, G Loss: 0.576899528503418\n",
      "Epoch 71, batch 124 D Loss: 1.5479786396026611, G Loss: 0.6094195246696472\n",
      "Epoch 71, batch 125 D Loss: 1.630338191986084, G Loss: 0.557311475276947\n",
      "Epoch 71, batch 126 D Loss: 1.5283490419387817, G Loss: 0.5644555687904358\n",
      "Epoch 71, batch 127 D Loss: 1.708042860031128, G Loss: 0.5438671708106995\n",
      "Epoch 71, batch 128 D Loss: 1.633453130722046, G Loss: 0.5521038770675659\n",
      "Epoch 71, batch 129 D Loss: 1.6205036640167236, G Loss: 0.5543239116668701\n",
      "Epoch 71, batch 130 D Loss: 1.7843854427337646, G Loss: 0.5368118286132812\n",
      "Epoch 71, batch 131 D Loss: 1.4933046102523804, G Loss: 0.5674290657043457\n",
      "Epoch 71, batch 132 D Loss: 1.506325364112854, G Loss: 0.533053457736969\n",
      "Epoch 71, batch 133 D Loss: 1.5505576133728027, G Loss: 0.5207626223564148\n",
      "Epoch 71, batch 134 D Loss: 1.599330186843872, G Loss: 0.5414411425590515\n",
      "Epoch 71, batch 135 D Loss: 1.5737147331237793, G Loss: 0.5508759021759033\n",
      "Epoch 71, batch 136 D Loss: 1.6789238452911377, G Loss: 0.5113204717636108\n",
      "Epoch 71, batch 137 D Loss: 1.645188331604004, G Loss: 0.5180498361587524\n",
      "Epoch 71, batch 138 D Loss: 1.5729845762252808, G Loss: 0.5763850212097168\n",
      "Epoch 71, batch 139 D Loss: 1.5377620458602905, G Loss: 0.6011987328529358\n",
      "Epoch 71, batch 140 D Loss: 1.6410853862762451, G Loss: 0.5665579438209534\n",
      "Epoch 71, batch 141 D Loss: 1.5125105381011963, G Loss: 0.5475003719329834\n",
      "Epoch 71, batch 142 D Loss: 1.472920298576355, G Loss: 0.5919698476791382\n",
      "Epoch 71, batch 143 D Loss: 1.647202968597412, G Loss: 0.582233726978302\n",
      "Epoch 71, batch 144 D Loss: 1.5365443229675293, G Loss: 0.619702160358429\n",
      "Epoch 71, batch 145 D Loss: 1.367728352546692, G Loss: 0.6398420929908752\n",
      "Epoch 71, batch 146 D Loss: 1.4162623882293701, G Loss: 0.6354126930236816\n",
      "Epoch 71, batch 147 D Loss: 1.5191454887390137, G Loss: 0.6160498261451721\n",
      "Epoch 71, batch 148 D Loss: 1.4518418312072754, G Loss: 0.6289980411529541\n",
      "Epoch 71, batch 149 D Loss: 1.5113017559051514, G Loss: 0.5916138291358948\n",
      "Epoch 71, batch 150 D Loss: 1.6654186248779297, G Loss: 0.6282219290733337\n",
      "Epoch 71, batch 151 D Loss: 1.4482004642486572, G Loss: 0.6612954139709473\n",
      "Epoch 71, batch 152 D Loss: 1.505614995956421, G Loss: 0.6262653470039368\n",
      "Epoch 71, batch 153 D Loss: 1.4528684616088867, G Loss: 0.6342589855194092\n",
      "Epoch 71, batch 154 D Loss: 1.5613162517547607, G Loss: 0.6659060716629028\n",
      "Epoch 71, batch 155 D Loss: 1.499934434890747, G Loss: 0.7178738117218018\n",
      "Epoch 71, batch 156 D Loss: 1.4036520719528198, G Loss: 0.7144448757171631\n",
      "Epoch 71, batch 157 D Loss: 1.5686345100402832, G Loss: 0.6816353797912598\n",
      "Epoch 71, batch 158 D Loss: 1.387446641921997, G Loss: 0.7599822878837585\n",
      "Epoch 71, batch 159 D Loss: 1.322466492652893, G Loss: 0.7754266858100891\n",
      "Epoch 71, batch 160 D Loss: 1.469876766204834, G Loss: 0.7730972170829773\n",
      "Epoch 71, batch 161 D Loss: 1.3640251159667969, G Loss: 0.7682465314865112\n",
      "Epoch 71, batch 162 D Loss: 1.3998101949691772, G Loss: 0.7436258792877197\n",
      "Epoch 71, batch 163 D Loss: 1.5105119943618774, G Loss: 0.7894639372825623\n",
      "Epoch 71, batch 164 D Loss: 1.4121134281158447, G Loss: 0.7979419827461243\n",
      "Epoch 71, batch 165 D Loss: 1.2871603965759277, G Loss: 0.8265008330345154\n",
      "Epoch 71, batch 166 D Loss: 1.354297399520874, G Loss: 0.863938570022583\n",
      "Epoch 71, batch 167 D Loss: 1.4321784973144531, G Loss: 0.9198051691055298\n",
      "Epoch 71, batch 168 D Loss: 1.4790966510772705, G Loss: 0.8908230662345886\n",
      "Epoch 71, batch 169 D Loss: 1.2884842157363892, G Loss: 0.8976955413818359\n",
      "Epoch 71, batch 170 D Loss: 1.3134363889694214, G Loss: 0.9563426375389099\n",
      "Epoch 71, batch 171 D Loss: 1.3778069019317627, G Loss: 0.9735761284828186\n",
      "Epoch 71, batch 172 D Loss: 1.3171510696411133, G Loss: 0.8871533274650574\n",
      "Epoch 71, batch 173 D Loss: 1.35689377784729, G Loss: 1.0186766386032104\n",
      "Epoch 71, batch 174 D Loss: 1.4894840717315674, G Loss: 0.9710507392883301\n",
      "Epoch 71, batch 175 D Loss: 1.3941755294799805, G Loss: 1.0513629913330078\n",
      "Epoch 71, batch 176 D Loss: 1.2822738885879517, G Loss: 0.9934578537940979\n",
      "Epoch 71, batch 177 D Loss: 1.2083086967468262, G Loss: 1.0726908445358276\n",
      "Epoch 71, batch 178 D Loss: 1.3954148292541504, G Loss: 1.133225440979004\n",
      "Epoch 71, batch 179 D Loss: 1.2444015741348267, G Loss: 1.3040039539337158\n",
      "Epoch 71, batch 180 D Loss: 1.4293478727340698, G Loss: 1.1011440753936768\n",
      "Epoch 71, batch 181 D Loss: 1.4011530876159668, G Loss: 0.9445315599441528\n",
      "Epoch 71, batch 182 D Loss: 1.6981146335601807, G Loss: 1.0416665077209473\n",
      "Epoch 71, batch 183 D Loss: 1.5120346546173096, G Loss: 1.0271096229553223\n",
      "Epoch 71, batch 184 D Loss: 1.6204296350479126, G Loss: 1.0850965976715088\n",
      "Epoch 71, batch 185 D Loss: 1.5235977172851562, G Loss: 0.8949013352394104\n",
      "Epoch 71, batch 186 D Loss: 1.547028660774231, G Loss: 0.89404296875\n",
      "Epoch 71, batch 187 D Loss: 1.7752059698104858, G Loss: 1.0733933448791504\n",
      "Epoch 71, batch 188 D Loss: 1.612020492553711, G Loss: 0.808907151222229\n",
      "Epoch 71, batch 189 D Loss: 1.8446167707443237, G Loss: 0.7267404198646545\n",
      "Epoch 71, batch 190 D Loss: 1.6082954406738281, G Loss: 0.7362169623374939\n",
      "Epoch 71, batch 191 D Loss: 1.878225564956665, G Loss: 0.7851628065109253\n",
      "Epoch 71, batch 192 D Loss: 1.6474583148956299, G Loss: 0.8243882060050964\n",
      "Epoch 71, batch 193 D Loss: 1.9389092922210693, G Loss: 0.7330811023712158\n",
      "Epoch 71, batch 194 D Loss: 1.9894015789031982, G Loss: 0.6738338470458984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, batch 195 D Loss: 1.9713401794433594, G Loss: 0.6468849182128906\n",
      "Epoch 71, batch 196 D Loss: 1.7958648204803467, G Loss: 0.52312833070755\n",
      "Epoch 71, batch 197 D Loss: 1.734777808189392, G Loss: 0.5560209155082703\n",
      "Epoch 71, batch 198 D Loss: 1.8569092750549316, G Loss: 0.6373775005340576\n",
      "Epoch 71, batch 199 D Loss: 2.0766072273254395, G Loss: 0.5143701434135437\n",
      "Epoch 71, batch 200 D Loss: 2.0526719093322754, G Loss: 0.556100606918335\n",
      "Epoch 72, batch 1 D Loss: 1.9246987104415894, G Loss: 0.5224971771240234\n",
      "Epoch 72, batch 2 D Loss: 1.9593596458435059, G Loss: 0.49412277340888977\n",
      "Epoch 72, batch 3 D Loss: 2.2295172214508057, G Loss: 0.49532485008239746\n",
      "Epoch 72, batch 4 D Loss: 2.239426612854004, G Loss: 0.4822838306427002\n",
      "Epoch 72, batch 5 D Loss: 2.0121846199035645, G Loss: 0.531821608543396\n",
      "Epoch 72, batch 6 D Loss: 2.1260881423950195, G Loss: 0.5078511238098145\n",
      "Epoch 72, batch 7 D Loss: 2.0437979698181152, G Loss: 0.4898930788040161\n",
      "Epoch 72, batch 8 D Loss: 2.219309091567993, G Loss: 0.5474494099617004\n",
      "Epoch 72, batch 9 D Loss: 2.092557430267334, G Loss: 0.48792561888694763\n",
      "Epoch 72, batch 10 D Loss: 1.7315218448638916, G Loss: 0.517754316329956\n",
      "Epoch 72, batch 11 D Loss: 1.9190151691436768, G Loss: 0.5520790219306946\n",
      "Epoch 72, batch 12 D Loss: 2.0000858306884766, G Loss: 0.5315414071083069\n",
      "Epoch 72, batch 13 D Loss: 1.7970212697982788, G Loss: 0.5525500774383545\n",
      "Epoch 72, batch 14 D Loss: 2.0065691471099854, G Loss: 0.5676357746124268\n",
      "Epoch 72, batch 15 D Loss: 1.8116977214813232, G Loss: 0.5529512166976929\n",
      "Epoch 72, batch 16 D Loss: 1.8113956451416016, G Loss: 0.5690810084342957\n",
      "Epoch 72, batch 17 D Loss: 1.9680476188659668, G Loss: 0.5606558918952942\n",
      "Epoch 72, batch 18 D Loss: 1.8836784362792969, G Loss: 0.6284196376800537\n",
      "Epoch 72, batch 19 D Loss: 1.8978855609893799, G Loss: 0.5993145108222961\n",
      "Epoch 72, batch 20 D Loss: 1.7255208492279053, G Loss: 0.6114679574966431\n",
      "Epoch 72, batch 21 D Loss: 1.7378982305526733, G Loss: 0.6820346713066101\n",
      "Epoch 72, batch 22 D Loss: 1.891139030456543, G Loss: 0.6529586911201477\n",
      "Epoch 72, batch 23 D Loss: 1.659808874130249, G Loss: 0.6632335782051086\n",
      "Epoch 72, batch 24 D Loss: 1.7555530071258545, G Loss: 0.6611273288726807\n",
      "Epoch 72, batch 25 D Loss: 1.8178162574768066, G Loss: 0.684112012386322\n",
      "Epoch 72, batch 26 D Loss: 1.5843740701675415, G Loss: 0.6944360136985779\n",
      "Epoch 72, batch 27 D Loss: 1.6580259799957275, G Loss: 0.698628306388855\n",
      "Epoch 72, batch 28 D Loss: 1.6802685260772705, G Loss: 0.7004134654998779\n",
      "Epoch 72, batch 29 D Loss: 1.593300461769104, G Loss: 0.7698248028755188\n",
      "Epoch 72, batch 30 D Loss: 1.7120269536972046, G Loss: 0.7295137047767639\n",
      "Epoch 72, batch 31 D Loss: 1.6749191284179688, G Loss: 0.735988438129425\n",
      "Epoch 72, batch 32 D Loss: 1.5566000938415527, G Loss: 0.7263598442077637\n",
      "Epoch 72, batch 33 D Loss: 1.4870107173919678, G Loss: 0.7594214081764221\n",
      "Epoch 72, batch 34 D Loss: 1.568166732788086, G Loss: 0.8218833804130554\n",
      "Epoch 72, batch 35 D Loss: 1.5338901281356812, G Loss: 0.7853235006332397\n",
      "Epoch 72, batch 36 D Loss: 1.4661788940429688, G Loss: 0.8270456194877625\n",
      "Epoch 72, batch 37 D Loss: 1.612534999847412, G Loss: 0.7677552700042725\n",
      "Epoch 72, batch 38 D Loss: 1.4582130908966064, G Loss: 0.8276746273040771\n",
      "Epoch 72, batch 39 D Loss: 1.619182825088501, G Loss: 0.8672131299972534\n",
      "Epoch 72, batch 40 D Loss: 1.4315364360809326, G Loss: 0.857323408126831\n",
      "Epoch 72, batch 41 D Loss: 1.401903748512268, G Loss: 0.8465897440910339\n",
      "Epoch 72, batch 42 D Loss: 1.4816420078277588, G Loss: 0.8375729918479919\n",
      "Epoch 72, batch 43 D Loss: 1.3567321300506592, G Loss: 0.9453330039978027\n",
      "Epoch 72, batch 44 D Loss: 1.3919004201889038, G Loss: 0.9263485670089722\n",
      "Epoch 72, batch 45 D Loss: 1.3068597316741943, G Loss: 0.9792627096176147\n",
      "Epoch 72, batch 46 D Loss: 1.3227858543395996, G Loss: 0.8857152462005615\n",
      "Epoch 72, batch 47 D Loss: 1.3075593709945679, G Loss: 0.8842906951904297\n",
      "Epoch 72, batch 48 D Loss: 1.2807891368865967, G Loss: 0.9643194079399109\n",
      "Epoch 72, batch 49 D Loss: 1.2396129369735718, G Loss: 0.931130051612854\n",
      "Epoch 72, batch 50 D Loss: 1.2745099067687988, G Loss: 0.9621047973632812\n",
      "Epoch 72, batch 51 D Loss: 1.2758944034576416, G Loss: 0.9513185024261475\n",
      "Epoch 72, batch 52 D Loss: 1.320243000984192, G Loss: 0.9328595995903015\n",
      "Epoch 72, batch 53 D Loss: 1.2642033100128174, G Loss: 0.9506399035453796\n",
      "Epoch 72, batch 54 D Loss: 1.196603536605835, G Loss: 1.0680654048919678\n",
      "Epoch 72, batch 55 D Loss: 1.156745195388794, G Loss: 1.0090701580047607\n",
      "Epoch 72, batch 56 D Loss: 1.231404423713684, G Loss: 0.9860305190086365\n",
      "Epoch 72, batch 57 D Loss: 1.172653317451477, G Loss: 0.9818824529647827\n",
      "Epoch 72, batch 58 D Loss: 1.1303430795669556, G Loss: 1.01383638381958\n",
      "Epoch 72, batch 59 D Loss: 1.2073676586151123, G Loss: 0.9457977414131165\n",
      "Epoch 72, batch 60 D Loss: 1.0857181549072266, G Loss: 1.0183124542236328\n",
      "Epoch 72, batch 61 D Loss: 1.1045596599578857, G Loss: 1.0078011751174927\n",
      "Epoch 72, batch 62 D Loss: 1.1068358421325684, G Loss: 1.0516932010650635\n",
      "Epoch 72, batch 63 D Loss: 1.151026964187622, G Loss: 1.0154080390930176\n",
      "Epoch 72, batch 64 D Loss: 1.1736948490142822, G Loss: 0.9913146495819092\n",
      "Epoch 72, batch 65 D Loss: 1.159553050994873, G Loss: 1.0038267374038696\n",
      "Epoch 72, batch 66 D Loss: 1.091557502746582, G Loss: 1.0136302709579468\n",
      "Epoch 72, batch 67 D Loss: 1.104411244392395, G Loss: 0.922320544719696\n",
      "Epoch 72, batch 68 D Loss: 1.2251019477844238, G Loss: 0.9561730027198792\n",
      "Epoch 72, batch 69 D Loss: 1.0928890705108643, G Loss: 0.9872562885284424\n",
      "Epoch 72, batch 70 D Loss: 1.130008339881897, G Loss: 1.0261465311050415\n",
      "Epoch 72, batch 71 D Loss: 1.1971796751022339, G Loss: 0.953392505645752\n",
      "Epoch 72, batch 72 D Loss: 1.236720323562622, G Loss: 0.9628362059593201\n",
      "Epoch 72, batch 73 D Loss: 1.1465734243392944, G Loss: 0.8826695084571838\n",
      "Epoch 72, batch 74 D Loss: 1.2518281936645508, G Loss: 0.9078699350357056\n",
      "Epoch 72, batch 75 D Loss: 1.1516048908233643, G Loss: 0.8954622149467468\n",
      "Epoch 72, batch 76 D Loss: 1.1873350143432617, G Loss: 0.9008336067199707\n",
      "Epoch 72, batch 77 D Loss: 1.1356624364852905, G Loss: 0.8938779234886169\n",
      "Epoch 72, batch 78 D Loss: 1.2161133289337158, G Loss: 0.8384867310523987\n",
      "Epoch 72, batch 79 D Loss: 1.078399419784546, G Loss: 0.840346097946167\n",
      "Epoch 72, batch 80 D Loss: 1.1591551303863525, G Loss: 0.8568072319030762\n",
      "Epoch 72, batch 81 D Loss: 1.09235417842865, G Loss: 0.8451207876205444\n",
      "Epoch 72, batch 82 D Loss: 1.1419999599456787, G Loss: 0.800499677658081\n",
      "Epoch 72, batch 83 D Loss: 1.1748026609420776, G Loss: 0.7996826767921448\n",
      "Epoch 72, batch 84 D Loss: 1.2481387853622437, G Loss: 0.7712540626525879\n",
      "Epoch 72, batch 85 D Loss: 1.2212014198303223, G Loss: 0.774787187576294\n",
      "Epoch 72, batch 86 D Loss: 1.2885122299194336, G Loss: 0.7491106986999512\n",
      "Epoch 72, batch 87 D Loss: 1.204757809638977, G Loss: 0.7312009930610657\n",
      "Epoch 72, batch 88 D Loss: 1.320906400680542, G Loss: 0.7497095465660095\n",
      "Epoch 72, batch 89 D Loss: 1.2380986213684082, G Loss: 0.760183572769165\n",
      "Epoch 72, batch 90 D Loss: 1.231013298034668, G Loss: 0.6965850591659546\n",
      "Epoch 72, batch 91 D Loss: 1.2564647197723389, G Loss: 0.7035791873931885\n",
      "Epoch 72, batch 92 D Loss: 1.231689453125, G Loss: 0.7004528641700745\n",
      "Epoch 72, batch 93 D Loss: 1.3161797523498535, G Loss: 0.6864898204803467\n",
      "Epoch 72, batch 94 D Loss: 1.247378945350647, G Loss: 0.6302171349525452\n",
      "Epoch 72, batch 95 D Loss: 1.3924317359924316, G Loss: 0.6645638346672058\n",
      "Epoch 72, batch 96 D Loss: 1.186700463294983, G Loss: 0.7284518480300903\n",
      "Epoch 72, batch 97 D Loss: 1.3033156394958496, G Loss: 0.6474378705024719\n",
      "Epoch 72, batch 98 D Loss: 1.1318024396896362, G Loss: 0.6762621998786926\n",
      "Epoch 72, batch 99 D Loss: 1.3036383390426636, G Loss: 0.6689886450767517\n",
      "Epoch 72, batch 100 D Loss: 1.3014034032821655, G Loss: 0.6653091311454773\n",
      "Epoch 72, batch 101 D Loss: 1.2249568700790405, G Loss: 0.6671948432922363\n",
      "Epoch 72, batch 102 D Loss: 1.296502709388733, G Loss: 0.7069553136825562\n",
      "Epoch 72, batch 103 D Loss: 1.3693724870681763, G Loss: 0.6513183116912842\n",
      "Epoch 72, batch 104 D Loss: 1.2554843425750732, G Loss: 0.678408682346344\n",
      "Epoch 72, batch 105 D Loss: 1.2577261924743652, G Loss: 0.6465044617652893\n",
      "Epoch 72, batch 106 D Loss: 1.2945928573608398, G Loss: 0.6895326375961304\n",
      "Epoch 72, batch 107 D Loss: 1.3507845401763916, G Loss: 0.666949450969696\n",
      "Epoch 72, batch 108 D Loss: 1.2918901443481445, G Loss: 0.6775147914886475\n",
      "Epoch 72, batch 109 D Loss: 1.3058538436889648, G Loss: 0.7024151682853699\n",
      "Epoch 72, batch 110 D Loss: 1.2804417610168457, G Loss: 0.6914778351783752\n",
      "Epoch 72, batch 111 D Loss: 1.216452717781067, G Loss: 0.7040973901748657\n",
      "Epoch 72, batch 112 D Loss: 1.2637115716934204, G Loss: 0.7085044980049133\n",
      "Epoch 72, batch 113 D Loss: 1.303635597229004, G Loss: 0.7110002636909485\n",
      "Epoch 72, batch 114 D Loss: 1.2402334213256836, G Loss: 0.6974790692329407\n",
      "Epoch 72, batch 115 D Loss: 1.1107208728790283, G Loss: 0.7233083248138428\n",
      "Epoch 72, batch 116 D Loss: 1.2241380214691162, G Loss: 0.7251508831977844\n",
      "Epoch 72, batch 117 D Loss: 1.301038146018982, G Loss: 0.7111791372299194\n",
      "Epoch 72, batch 118 D Loss: 1.2590599060058594, G Loss: 0.7488856315612793\n",
      "Epoch 72, batch 119 D Loss: 1.1926665306091309, G Loss: 0.7047408223152161\n",
      "Epoch 72, batch 120 D Loss: 1.2020361423492432, G Loss: 0.7546863555908203\n",
      "Epoch 72, batch 121 D Loss: 1.2397406101226807, G Loss: 0.7728489637374878\n",
      "Epoch 72, batch 122 D Loss: 1.2651110887527466, G Loss: 0.743895411491394\n",
      "Epoch 72, batch 123 D Loss: 1.221498727798462, G Loss: 0.7612650394439697\n",
      "Epoch 72, batch 124 D Loss: 1.2485979795455933, G Loss: 0.768983006477356\n",
      "Epoch 72, batch 125 D Loss: 1.2189335823059082, G Loss: 0.7862831354141235\n",
      "Epoch 72, batch 126 D Loss: 1.2224478721618652, G Loss: 0.8242141008377075\n",
      "Epoch 72, batch 127 D Loss: 1.249267339706421, G Loss: 0.8437322974205017\n",
      "Epoch 72, batch 128 D Loss: 1.1800954341888428, G Loss: 0.8757871985435486\n",
      "Epoch 72, batch 129 D Loss: 1.2601189613342285, G Loss: 0.8890228271484375\n",
      "Epoch 72, batch 130 D Loss: 1.3469765186309814, G Loss: 0.8216776251792908\n",
      "Epoch 72, batch 131 D Loss: 1.248009204864502, G Loss: 0.8905513882637024\n",
      "Epoch 72, batch 132 D Loss: 1.1255767345428467, G Loss: 0.880635678768158\n",
      "Epoch 72, batch 133 D Loss: 1.3399255275726318, G Loss: 0.7532486915588379\n",
      "Epoch 72, batch 134 D Loss: 1.2636058330535889, G Loss: 0.912204384803772\n",
      "Epoch 72, batch 135 D Loss: 1.2070144414901733, G Loss: 0.8160231113433838\n",
      "Epoch 72, batch 136 D Loss: 1.2524633407592773, G Loss: 0.8683609962463379\n",
      "Epoch 72, batch 137 D Loss: 1.195319414138794, G Loss: 0.8543895483016968\n",
      "Epoch 72, batch 138 D Loss: 1.2752900123596191, G Loss: 0.839782178401947\n",
      "Epoch 72, batch 139 D Loss: 1.2971866130828857, G Loss: 0.8574458956718445\n",
      "Epoch 72, batch 140 D Loss: 1.374218225479126, G Loss: 0.7668076157569885\n",
      "Epoch 72, batch 141 D Loss: 1.215843677520752, G Loss: 0.8028711080551147\n",
      "Epoch 72, batch 142 D Loss: 1.1270041465759277, G Loss: 0.9112679362297058\n",
      "Epoch 72, batch 143 D Loss: 1.2579939365386963, G Loss: 0.8294441103935242\n",
      "Epoch 72, batch 144 D Loss: 1.237990140914917, G Loss: 0.8708321452140808\n",
      "Epoch 72, batch 145 D Loss: 1.3572238683700562, G Loss: 0.8469364047050476\n",
      "Epoch 72, batch 146 D Loss: 1.1264104843139648, G Loss: 0.8122899532318115\n",
      "Epoch 72, batch 147 D Loss: 1.2663936614990234, G Loss: 0.8418314456939697\n",
      "Epoch 72, batch 148 D Loss: 1.374765157699585, G Loss: 0.8241960406303406\n",
      "Epoch 72, batch 149 D Loss: 1.333721399307251, G Loss: 0.8184347748756409\n",
      "Epoch 72, batch 150 D Loss: 1.2595579624176025, G Loss: 0.8911412954330444\n",
      "Epoch 72, batch 151 D Loss: 1.2862510681152344, G Loss: 0.7269824147224426\n",
      "Epoch 72, batch 152 D Loss: 1.2583980560302734, G Loss: 0.7465296387672424\n",
      "Epoch 72, batch 153 D Loss: 1.2533578872680664, G Loss: 0.7810633182525635\n",
      "Epoch 72, batch 154 D Loss: 1.19366455078125, G Loss: 0.8491807579994202\n",
      "Epoch 72, batch 155 D Loss: 1.6301921606063843, G Loss: 0.7288841009140015\n",
      "Epoch 72, batch 156 D Loss: 1.4112995862960815, G Loss: 0.7539299726486206\n",
      "Epoch 72, batch 157 D Loss: 1.4271646738052368, G Loss: 0.6739850640296936\n",
      "Epoch 72, batch 158 D Loss: 1.4764270782470703, G Loss: 0.654391884803772\n",
      "Epoch 72, batch 159 D Loss: 1.4719204902648926, G Loss: 0.6574292778968811\n",
      "Epoch 72, batch 160 D Loss: 1.4538259506225586, G Loss: 0.5977970957756042\n",
      "Epoch 72, batch 161 D Loss: 1.4897428750991821, G Loss: 0.644763708114624\n",
      "Epoch 72, batch 162 D Loss: 1.486776351928711, G Loss: 0.6660657525062561\n",
      "Epoch 72, batch 163 D Loss: 1.3930385112762451, G Loss: 0.615639328956604\n",
      "Epoch 72, batch 164 D Loss: 1.2807697057724, G Loss: 0.7362921237945557\n",
      "Epoch 72, batch 165 D Loss: 1.4578227996826172, G Loss: 0.5727884769439697\n",
      "Epoch 72, batch 166 D Loss: 1.4950143098831177, G Loss: 0.6158503890037537\n",
      "Epoch 72, batch 167 D Loss: 1.454738736152649, G Loss: 0.6069284081459045\n",
      "Epoch 72, batch 168 D Loss: 1.6791467666625977, G Loss: 0.5284323692321777\n",
      "Epoch 72, batch 169 D Loss: 1.4819867610931396, G Loss: 0.5835106372833252\n",
      "Epoch 72, batch 170 D Loss: 1.487525463104248, G Loss: 0.6172296404838562\n",
      "Epoch 72, batch 171 D Loss: 1.3307703733444214, G Loss: 0.6046094298362732\n",
      "Epoch 72, batch 172 D Loss: 1.5179895162582397, G Loss: 0.5528055429458618\n",
      "Epoch 72, batch 173 D Loss: 1.3433139324188232, G Loss: 0.5758981704711914\n",
      "Epoch 72, batch 174 D Loss: 1.2468783855438232, G Loss: 0.6395263075828552\n",
      "Epoch 72, batch 175 D Loss: 1.4353419542312622, G Loss: 0.5777682662010193\n",
      "Epoch 72, batch 176 D Loss: 1.446210265159607, G Loss: 0.6101752519607544\n",
      "Epoch 72, batch 177 D Loss: 1.2896158695220947, G Loss: 0.5846717953681946\n",
      "Epoch 72, batch 178 D Loss: 1.3458365201950073, G Loss: 0.6307687163352966\n",
      "Epoch 72, batch 179 D Loss: 1.3400793075561523, G Loss: 0.6678874492645264\n",
      "Epoch 72, batch 180 D Loss: 1.4456384181976318, G Loss: 0.6446377635002136\n",
      "Epoch 72, batch 181 D Loss: 1.2423558235168457, G Loss: 0.6895935535430908\n",
      "Epoch 72, batch 182 D Loss: 1.3548789024353027, G Loss: 0.6522065997123718\n",
      "Epoch 72, batch 183 D Loss: 1.2951991558074951, G Loss: 0.6600448489189148\n",
      "Epoch 72, batch 184 D Loss: 1.225299596786499, G Loss: 0.6697888374328613\n",
      "Epoch 72, batch 185 D Loss: 1.4355567693710327, G Loss: 0.6591024994850159\n",
      "Epoch 72, batch 186 D Loss: 1.3521883487701416, G Loss: 0.715640127658844\n",
      "Epoch 72, batch 187 D Loss: 1.2171123027801514, G Loss: 0.7326622605323792\n",
      "Epoch 72, batch 188 D Loss: 1.3014779090881348, G Loss: 0.751019299030304\n",
      "Epoch 72, batch 189 D Loss: 1.358879566192627, G Loss: 0.7122759819030762\n",
      "Epoch 72, batch 190 D Loss: 1.3524763584136963, G Loss: 0.7026687860488892\n",
      "Epoch 72, batch 191 D Loss: 1.3531262874603271, G Loss: 0.7094986438751221\n",
      "Epoch 72, batch 192 D Loss: 1.312894344329834, G Loss: 0.7647810578346252\n",
      "Epoch 72, batch 193 D Loss: 1.3506457805633545, G Loss: 0.7327352166175842\n",
      "Epoch 72, batch 194 D Loss: 1.2097302675247192, G Loss: 0.8183465003967285\n",
      "Epoch 72, batch 195 D Loss: 1.3828922510147095, G Loss: 0.7323355078697205\n",
      "Epoch 72, batch 196 D Loss: 1.3788330554962158, G Loss: 0.7742928862571716\n",
      "Epoch 72, batch 197 D Loss: 1.1883857250213623, G Loss: 0.7752662897109985\n",
      "Epoch 72, batch 198 D Loss: 1.2362289428710938, G Loss: 0.833579957485199\n",
      "Epoch 72, batch 199 D Loss: 1.2360384464263916, G Loss: 0.8670652508735657\n",
      "Epoch 72, batch 200 D Loss: 1.1888352632522583, G Loss: 0.8303999304771423\n",
      "Epoch 73, batch 1 D Loss: 1.2474844455718994, G Loss: 0.8550965785980225\n",
      "Epoch 73, batch 2 D Loss: 1.1125404834747314, G Loss: 0.8890909552574158\n",
      "Epoch 73, batch 3 D Loss: 1.390190839767456, G Loss: 0.834545910358429\n",
      "Epoch 73, batch 4 D Loss: 1.2586944103240967, G Loss: 0.8623594641685486\n",
      "Epoch 73, batch 5 D Loss: 1.0753765106201172, G Loss: 0.9157556891441345\n",
      "Epoch 73, batch 6 D Loss: 1.2872658967971802, G Loss: 0.9061076641082764\n",
      "Epoch 73, batch 7 D Loss: 1.2727468013763428, G Loss: 0.8825585246086121\n",
      "Epoch 73, batch 8 D Loss: 1.126010537147522, G Loss: 0.9168874621391296\n",
      "Epoch 73, batch 9 D Loss: 1.2615517377853394, G Loss: 0.8980381488800049\n",
      "Epoch 73, batch 10 D Loss: 1.1004185676574707, G Loss: 0.9810720682144165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, batch 11 D Loss: 1.196053147315979, G Loss: 0.8891551494598389\n",
      "Epoch 73, batch 12 D Loss: 1.235471487045288, G Loss: 0.9462209343910217\n",
      "Epoch 73, batch 13 D Loss: 1.0124149322509766, G Loss: 0.9691924452781677\n",
      "Epoch 73, batch 14 D Loss: 1.143734097480774, G Loss: 1.0561550855636597\n",
      "Epoch 73, batch 15 D Loss: 1.142937421798706, G Loss: 0.9533568620681763\n",
      "Epoch 73, batch 16 D Loss: 1.275336503982544, G Loss: 1.0530091524124146\n",
      "Epoch 73, batch 17 D Loss: 1.2282119989395142, G Loss: 0.9813284873962402\n",
      "Epoch 73, batch 18 D Loss: 1.1532809734344482, G Loss: 1.0280730724334717\n",
      "Epoch 73, batch 19 D Loss: 1.155688762664795, G Loss: 1.0734333992004395\n",
      "Epoch 73, batch 20 D Loss: 1.133628487586975, G Loss: 1.03057861328125\n",
      "Epoch 73, batch 21 D Loss: 1.2372543811798096, G Loss: 1.067959189414978\n",
      "Epoch 73, batch 22 D Loss: 1.164055585861206, G Loss: 1.1022279262542725\n",
      "Epoch 73, batch 23 D Loss: 1.2408051490783691, G Loss: 1.033545970916748\n",
      "Epoch 73, batch 24 D Loss: 1.1671884059906006, G Loss: 1.0952550172805786\n",
      "Epoch 73, batch 25 D Loss: 1.1926757097244263, G Loss: 1.13996422290802\n",
      "Epoch 73, batch 26 D Loss: 1.3192239999771118, G Loss: 1.0841455459594727\n",
      "Epoch 73, batch 27 D Loss: 1.0639983415603638, G Loss: 1.0244450569152832\n",
      "Epoch 73, batch 28 D Loss: 1.394132375717163, G Loss: 1.0411335229873657\n",
      "Epoch 73, batch 29 D Loss: 1.2233047485351562, G Loss: 0.9886419773101807\n",
      "Epoch 73, batch 30 D Loss: 1.101534366607666, G Loss: 1.0291376113891602\n",
      "Epoch 73, batch 31 D Loss: 1.4652278423309326, G Loss: 1.077559232711792\n",
      "Epoch 73, batch 32 D Loss: 1.338754653930664, G Loss: 1.0347285270690918\n",
      "Epoch 73, batch 33 D Loss: 1.1566598415374756, G Loss: 1.0398190021514893\n",
      "Epoch 73, batch 34 D Loss: 1.187692403793335, G Loss: 0.9795930981636047\n",
      "Epoch 73, batch 35 D Loss: 1.0811846256256104, G Loss: 1.0317904949188232\n",
      "Epoch 73, batch 36 D Loss: 1.2953083515167236, G Loss: 0.9708322286605835\n",
      "Epoch 73, batch 37 D Loss: 1.3212511539459229, G Loss: 0.9821626543998718\n",
      "Epoch 73, batch 38 D Loss: 1.37445068359375, G Loss: 0.9901223182678223\n",
      "Epoch 73, batch 39 D Loss: 1.2496083974838257, G Loss: 0.9653111100196838\n",
      "Epoch 73, batch 40 D Loss: 1.298119306564331, G Loss: 0.9463809728622437\n",
      "Epoch 73, batch 41 D Loss: 1.2569599151611328, G Loss: 0.8940500617027283\n",
      "Epoch 73, batch 42 D Loss: 1.3312921524047852, G Loss: 0.9366165995597839\n",
      "Epoch 73, batch 43 D Loss: 1.4729716777801514, G Loss: 0.8942319750785828\n",
      "Epoch 73, batch 44 D Loss: 1.2891690731048584, G Loss: 0.9271436929702759\n",
      "Epoch 73, batch 45 D Loss: 1.2730151414871216, G Loss: 0.8758167028427124\n",
      "Epoch 73, batch 46 D Loss: 1.2468211650848389, G Loss: 0.8491854667663574\n",
      "Epoch 73, batch 47 D Loss: 1.6709027290344238, G Loss: 0.8004190325737\n",
      "Epoch 73, batch 48 D Loss: 1.3567273616790771, G Loss: 0.7871493697166443\n",
      "Epoch 73, batch 49 D Loss: 1.5256266593933105, G Loss: 0.8060877919197083\n",
      "Epoch 73, batch 50 D Loss: 1.3818132877349854, G Loss: 0.8067836165428162\n",
      "Epoch 73, batch 51 D Loss: 1.3639287948608398, G Loss: 0.7963681221008301\n",
      "Epoch 73, batch 52 D Loss: 1.3803558349609375, G Loss: 0.7480273246765137\n",
      "Epoch 73, batch 53 D Loss: 1.4313362836837769, G Loss: 0.6924685835838318\n",
      "Epoch 73, batch 54 D Loss: 1.656349778175354, G Loss: 0.6068460941314697\n",
      "Epoch 73, batch 55 D Loss: 1.6126158237457275, G Loss: 0.6510197520256042\n",
      "Epoch 73, batch 56 D Loss: 1.6874028444290161, G Loss: 0.600531280040741\n",
      "Epoch 73, batch 57 D Loss: 1.7553465366363525, G Loss: 0.5801699757575989\n",
      "Epoch 73, batch 58 D Loss: 1.685248613357544, G Loss: 0.5713260769844055\n",
      "Epoch 73, batch 59 D Loss: 2.0072805881500244, G Loss: 0.5381497144699097\n",
      "Epoch 73, batch 60 D Loss: 1.6960787773132324, G Loss: 0.5777137279510498\n",
      "Epoch 73, batch 61 D Loss: 1.8328185081481934, G Loss: 0.568824052810669\n",
      "Epoch 73, batch 62 D Loss: 1.880482792854309, G Loss: 0.5147754549980164\n",
      "Epoch 73, batch 63 D Loss: 1.9572734832763672, G Loss: 0.4336099624633789\n",
      "Epoch 73, batch 64 D Loss: 1.9593391418457031, G Loss: 0.5189648866653442\n",
      "Epoch 73, batch 65 D Loss: 1.9930427074432373, G Loss: 0.5489808917045593\n",
      "Epoch 73, batch 66 D Loss: 2.098379373550415, G Loss: 0.4051969647407532\n",
      "Epoch 73, batch 67 D Loss: 1.9639246463775635, G Loss: 0.401229590177536\n",
      "Epoch 73, batch 68 D Loss: 1.966721534729004, G Loss: 0.4576726257801056\n",
      "Epoch 73, batch 69 D Loss: 2.0082952976226807, G Loss: 0.46006762981414795\n",
      "Epoch 73, batch 70 D Loss: 2.0187153816223145, G Loss: 0.42443040013313293\n",
      "Epoch 73, batch 71 D Loss: 2.071868896484375, G Loss: 0.4236553907394409\n",
      "Epoch 73, batch 72 D Loss: 2.075502872467041, G Loss: 0.46732452511787415\n",
      "Epoch 73, batch 73 D Loss: 2.35402774810791, G Loss: 0.35277384519577026\n",
      "Epoch 73, batch 74 D Loss: 2.090881824493408, G Loss: 0.38353222608566284\n",
      "Epoch 73, batch 75 D Loss: 2.066221237182617, G Loss: 0.4148043394088745\n",
      "Epoch 73, batch 76 D Loss: 2.180793285369873, G Loss: 0.43098071217536926\n",
      "Epoch 73, batch 77 D Loss: 1.9302012920379639, G Loss: 0.404449462890625\n",
      "Epoch 73, batch 78 D Loss: 1.9594852924346924, G Loss: 0.4721716046333313\n",
      "Epoch 73, batch 79 D Loss: 2.080223560333252, G Loss: 0.47486621141433716\n",
      "Epoch 73, batch 80 D Loss: 2.0219428539276123, G Loss: 0.4138014614582062\n",
      "Epoch 73, batch 81 D Loss: 1.9830818176269531, G Loss: 0.4508426785469055\n",
      "Epoch 73, batch 82 D Loss: 2.010911464691162, G Loss: 0.4378777742385864\n",
      "Epoch 73, batch 83 D Loss: 1.8585917949676514, G Loss: 0.5184981822967529\n",
      "Epoch 73, batch 84 D Loss: 1.8598904609680176, G Loss: 0.515313982963562\n",
      "Epoch 73, batch 85 D Loss: 1.930173635482788, G Loss: 0.5168468356132507\n",
      "Epoch 73, batch 86 D Loss: 1.8640189170837402, G Loss: 0.530659556388855\n",
      "Epoch 73, batch 87 D Loss: 1.9559513330459595, G Loss: 0.5707527995109558\n",
      "Epoch 73, batch 88 D Loss: 1.8418405055999756, G Loss: 0.525865375995636\n",
      "Epoch 73, batch 89 D Loss: 1.8048683404922485, G Loss: 0.5885167121887207\n",
      "Epoch 73, batch 90 D Loss: 1.9442871809005737, G Loss: 0.6128800511360168\n",
      "Epoch 73, batch 91 D Loss: 1.6306302547454834, G Loss: 0.6584841012954712\n",
      "Epoch 73, batch 92 D Loss: 1.850728988647461, G Loss: 0.6686679124832153\n",
      "Epoch 73, batch 93 D Loss: 1.5189106464385986, G Loss: 0.6800674200057983\n",
      "Epoch 73, batch 94 D Loss: 1.7367043495178223, G Loss: 0.7187852263450623\n",
      "Epoch 73, batch 95 D Loss: 1.6574361324310303, G Loss: 0.7349284887313843\n",
      "Epoch 73, batch 96 D Loss: 1.5670011043548584, G Loss: 0.7989746332168579\n",
      "Epoch 73, batch 97 D Loss: 1.7246719598770142, G Loss: 0.815259575843811\n",
      "Epoch 73, batch 98 D Loss: 1.5534712076187134, G Loss: 0.7936353087425232\n",
      "Epoch 73, batch 99 D Loss: 1.601059913635254, G Loss: 0.8658596873283386\n",
      "Epoch 73, batch 100 D Loss: 1.5420089960098267, G Loss: 0.914039134979248\n",
      "Epoch 73, batch 101 D Loss: 1.626232385635376, G Loss: 0.9230561852455139\n",
      "Epoch 73, batch 102 D Loss: 1.473067045211792, G Loss: 0.98927903175354\n",
      "Epoch 73, batch 103 D Loss: 1.4139609336853027, G Loss: 1.0576519966125488\n",
      "Epoch 73, batch 104 D Loss: 1.5625096559524536, G Loss: 1.0907052755355835\n",
      "Epoch 73, batch 105 D Loss: 1.4498928785324097, G Loss: 1.1711785793304443\n",
      "Epoch 73, batch 106 D Loss: 1.5328075885772705, G Loss: 1.1129653453826904\n",
      "Epoch 73, batch 107 D Loss: 1.3632495403289795, G Loss: 1.1711839437484741\n",
      "Epoch 73, batch 108 D Loss: 1.5761463642120361, G Loss: 1.1851990222930908\n",
      "Epoch 73, batch 109 D Loss: 1.2685693502426147, G Loss: 1.2409950494766235\n",
      "Epoch 73, batch 110 D Loss: 1.3888065814971924, G Loss: 1.3279697895050049\n",
      "Epoch 73, batch 111 D Loss: 1.5150411128997803, G Loss: 1.2981311082839966\n",
      "Epoch 73, batch 112 D Loss: 1.3978314399719238, G Loss: 1.3595454692840576\n",
      "Epoch 73, batch 113 D Loss: 1.5279569625854492, G Loss: 1.278483271598816\n",
      "Epoch 73, batch 114 D Loss: 1.3443727493286133, G Loss: 1.5031957626342773\n",
      "Epoch 73, batch 115 D Loss: 1.4860831499099731, G Loss: 1.4062846899032593\n",
      "Epoch 73, batch 116 D Loss: 1.4287478923797607, G Loss: 1.4132527112960815\n",
      "Epoch 73, batch 117 D Loss: 1.273270606994629, G Loss: 1.4253114461898804\n",
      "Epoch 73, batch 118 D Loss: 1.3984782695770264, G Loss: 1.391591191291809\n",
      "Epoch 73, batch 119 D Loss: 1.2861601114273071, G Loss: 1.3135342597961426\n",
      "Epoch 73, batch 120 D Loss: 1.4769012928009033, G Loss: 1.3050050735473633\n",
      "Epoch 73, batch 121 D Loss: 1.4316542148590088, G Loss: 1.3686827421188354\n",
      "Epoch 73, batch 122 D Loss: 1.3215999603271484, G Loss: 1.541353702545166\n",
      "Epoch 73, batch 123 D Loss: 1.2955471277236938, G Loss: 1.4379775524139404\n",
      "Epoch 73, batch 124 D Loss: 1.4074100255966187, G Loss: 1.3381599187850952\n",
      "Epoch 73, batch 125 D Loss: 1.3133058547973633, G Loss: 1.3362412452697754\n",
      "Epoch 73, batch 126 D Loss: 1.3039276599884033, G Loss: 1.404272198677063\n",
      "Epoch 73, batch 127 D Loss: 1.4373592138290405, G Loss: 1.326551079750061\n",
      "Epoch 73, batch 128 D Loss: 1.2368285655975342, G Loss: 1.2641831636428833\n",
      "Epoch 73, batch 129 D Loss: 1.381983757019043, G Loss: 1.3657838106155396\n",
      "Epoch 73, batch 130 D Loss: 1.4393585920333862, G Loss: 1.356743335723877\n",
      "Epoch 73, batch 131 D Loss: 1.3714679479599, G Loss: 1.3149739503860474\n",
      "Epoch 73, batch 132 D Loss: 1.4202814102172852, G Loss: 1.2344573736190796\n",
      "Epoch 73, batch 133 D Loss: 1.4654003381729126, G Loss: 1.330968976020813\n",
      "Epoch 73, batch 134 D Loss: 1.5814294815063477, G Loss: 1.2124125957489014\n",
      "Epoch 73, batch 135 D Loss: 1.3999241590499878, G Loss: 1.2869200706481934\n",
      "Epoch 73, batch 136 D Loss: 1.4258875846862793, G Loss: 1.2154910564422607\n",
      "Epoch 73, batch 137 D Loss: 1.5430783033370972, G Loss: 1.2433685064315796\n",
      "Epoch 73, batch 138 D Loss: 1.344231128692627, G Loss: 1.145808458328247\n",
      "Epoch 73, batch 139 D Loss: 1.499272108078003, G Loss: 1.153305172920227\n",
      "Epoch 73, batch 140 D Loss: 1.5725032091140747, G Loss: 1.1833635568618774\n",
      "Epoch 73, batch 141 D Loss: 1.3921964168548584, G Loss: 1.1499334573745728\n",
      "Epoch 73, batch 142 D Loss: 1.460050344467163, G Loss: 1.0731602907180786\n",
      "Epoch 73, batch 143 D Loss: 1.4516117572784424, G Loss: 1.0934466123580933\n",
      "Epoch 73, batch 144 D Loss: 1.6082431077957153, G Loss: 1.1248400211334229\n",
      "Epoch 73, batch 145 D Loss: 1.3839362859725952, G Loss: 1.091313123703003\n",
      "Epoch 73, batch 146 D Loss: 1.5033471584320068, G Loss: 1.0466609001159668\n",
      "Epoch 73, batch 147 D Loss: 1.4665518999099731, G Loss: 1.0964398384094238\n",
      "Epoch 73, batch 148 D Loss: 1.3041000366210938, G Loss: 1.1359210014343262\n",
      "Epoch 73, batch 149 D Loss: 1.41035795211792, G Loss: 1.0768681764602661\n",
      "Epoch 73, batch 150 D Loss: 1.4766019582748413, G Loss: 1.125734806060791\n",
      "Epoch 73, batch 151 D Loss: 1.4851287603378296, G Loss: 1.0253204107284546\n",
      "Epoch 73, batch 152 D Loss: 1.5682331323623657, G Loss: 1.0701327323913574\n",
      "Epoch 73, batch 153 D Loss: 1.5242477655410767, G Loss: 1.034531831741333\n",
      "Epoch 73, batch 154 D Loss: 1.6242424249649048, G Loss: 0.9947158098220825\n",
      "Epoch 73, batch 155 D Loss: 1.4247303009033203, G Loss: 1.0615038871765137\n",
      "Epoch 73, batch 156 D Loss: 1.5578465461730957, G Loss: 1.0517314672470093\n",
      "Epoch 73, batch 157 D Loss: 1.34392511844635, G Loss: 1.0059866905212402\n",
      "Epoch 73, batch 158 D Loss: 1.594360113143921, G Loss: 0.9822075366973877\n",
      "Epoch 73, batch 159 D Loss: 1.4541962146759033, G Loss: 1.0303072929382324\n",
      "Epoch 73, batch 160 D Loss: 1.4219255447387695, G Loss: 0.9625186920166016\n",
      "Epoch 73, batch 161 D Loss: 1.2803890705108643, G Loss: 1.0323517322540283\n",
      "Epoch 73, batch 162 D Loss: 1.4745110273361206, G Loss: 1.0037221908569336\n",
      "Epoch 73, batch 163 D Loss: 1.4014513492584229, G Loss: 0.9987637400627136\n",
      "Epoch 73, batch 164 D Loss: 1.4208238124847412, G Loss: 0.9961460828781128\n",
      "Epoch 73, batch 165 D Loss: 1.459761381149292, G Loss: 0.9731460809707642\n",
      "Epoch 73, batch 166 D Loss: 1.5232094526290894, G Loss: 1.0057692527770996\n",
      "Epoch 73, batch 167 D Loss: 1.4108269214630127, G Loss: 1.0010654926300049\n",
      "Epoch 73, batch 168 D Loss: 1.381190538406372, G Loss: 0.9782208204269409\n",
      "Epoch 73, batch 169 D Loss: 1.230032205581665, G Loss: 0.9662063121795654\n",
      "Epoch 73, batch 170 D Loss: 1.2799853086471558, G Loss: 1.068312644958496\n",
      "Epoch 73, batch 171 D Loss: 1.3718938827514648, G Loss: 0.9522181153297424\n",
      "Epoch 73, batch 172 D Loss: 1.43597412109375, G Loss: 0.9514835476875305\n",
      "Epoch 73, batch 173 D Loss: 1.4019780158996582, G Loss: 1.008957862854004\n",
      "Epoch 73, batch 174 D Loss: 1.6027610301971436, G Loss: 0.973039984703064\n",
      "Epoch 73, batch 175 D Loss: 1.439316987991333, G Loss: 0.923729658126831\n",
      "Epoch 73, batch 176 D Loss: 1.511955976486206, G Loss: 0.9782110452651978\n",
      "Epoch 73, batch 177 D Loss: 1.5544085502624512, G Loss: 1.0094820261001587\n",
      "Epoch 73, batch 178 D Loss: 1.4652996063232422, G Loss: 0.9484046697616577\n",
      "Epoch 73, batch 179 D Loss: 1.5157660245895386, G Loss: 0.9633010029792786\n",
      "Epoch 73, batch 180 D Loss: 1.4203392267227173, G Loss: 0.9452608227729797\n",
      "Epoch 73, batch 181 D Loss: 1.5600804090499878, G Loss: 0.8845746517181396\n",
      "Epoch 73, batch 182 D Loss: 1.6082322597503662, G Loss: 0.9280354976654053\n",
      "Epoch 73, batch 183 D Loss: 1.3670947551727295, G Loss: 0.9900751709938049\n",
      "Epoch 73, batch 184 D Loss: 1.5170412063598633, G Loss: 0.9613966941833496\n",
      "Epoch 73, batch 185 D Loss: 1.4697288274765015, G Loss: 0.8976970911026001\n",
      "Epoch 73, batch 186 D Loss: 1.5399420261383057, G Loss: 0.8532209992408752\n",
      "Epoch 73, batch 187 D Loss: 1.6463754177093506, G Loss: 0.8534638285636902\n",
      "Epoch 73, batch 188 D Loss: 1.5162498950958252, G Loss: 0.9173442721366882\n",
      "Epoch 73, batch 189 D Loss: 1.827761173248291, G Loss: 0.8214769959449768\n",
      "Epoch 73, batch 190 D Loss: 1.5471134185791016, G Loss: 0.7679591178894043\n",
      "Epoch 73, batch 191 D Loss: 1.501677393913269, G Loss: 0.8188651204109192\n",
      "Epoch 73, batch 192 D Loss: 1.4992411136627197, G Loss: 0.7923592925071716\n",
      "Epoch 73, batch 193 D Loss: 1.6329333782196045, G Loss: 0.7810865044593811\n",
      "Epoch 73, batch 194 D Loss: 1.69553542137146, G Loss: 0.8097112774848938\n",
      "Epoch 73, batch 195 D Loss: 1.6775362491607666, G Loss: 0.8543684482574463\n",
      "Epoch 73, batch 196 D Loss: 1.6286952495574951, G Loss: 0.7673800587654114\n",
      "Epoch 73, batch 197 D Loss: 1.522937536239624, G Loss: 0.7715088129043579\n",
      "Epoch 73, batch 198 D Loss: 1.5332627296447754, G Loss: 0.8257631063461304\n",
      "Epoch 73, batch 199 D Loss: 1.6114239692687988, G Loss: 0.7688743472099304\n",
      "Epoch 73, batch 200 D Loss: 1.6540162563323975, G Loss: 0.72376549243927\n",
      "Epoch 74, batch 1 D Loss: 1.5018329620361328, G Loss: 0.711239218711853\n",
      "Epoch 74, batch 2 D Loss: 1.552276611328125, G Loss: 0.6871081590652466\n",
      "Epoch 74, batch 3 D Loss: 1.404080867767334, G Loss: 0.7510446906089783\n",
      "Epoch 74, batch 4 D Loss: 1.5028126239776611, G Loss: 0.6891049742698669\n",
      "Epoch 74, batch 5 D Loss: 1.684324026107788, G Loss: 0.7169229388237\n",
      "Epoch 74, batch 6 D Loss: 1.663175106048584, G Loss: 0.6575920581817627\n",
      "Epoch 74, batch 7 D Loss: 1.7018461227416992, G Loss: 0.6915070414543152\n",
      "Epoch 74, batch 8 D Loss: 1.7724380493164062, G Loss: 0.6807892322540283\n",
      "Epoch 74, batch 9 D Loss: 1.4620211124420166, G Loss: 0.6887702345848083\n",
      "Epoch 74, batch 10 D Loss: 1.3784880638122559, G Loss: 0.7239534258842468\n",
      "Epoch 74, batch 11 D Loss: 1.4343290328979492, G Loss: 0.6982290744781494\n",
      "Epoch 74, batch 12 D Loss: 1.575695514678955, G Loss: 0.6965571045875549\n",
      "Epoch 74, batch 13 D Loss: 1.4723975658416748, G Loss: 0.6941644549369812\n",
      "Epoch 74, batch 14 D Loss: 1.544904112815857, G Loss: 0.7018520832061768\n",
      "Epoch 74, batch 15 D Loss: 1.6132805347442627, G Loss: 0.6838743686676025\n",
      "Epoch 74, batch 16 D Loss: 1.4242256879806519, G Loss: 0.6795187592506409\n",
      "Epoch 74, batch 17 D Loss: 1.4964861869812012, G Loss: 0.709138035774231\n",
      "Epoch 74, batch 18 D Loss: 1.399420976638794, G Loss: 0.7062116861343384\n",
      "Epoch 74, batch 19 D Loss: 1.4020566940307617, G Loss: 0.7120000720024109\n",
      "Epoch 74, batch 20 D Loss: 1.395352840423584, G Loss: 0.703001081943512\n",
      "Epoch 74, batch 21 D Loss: 1.4262592792510986, G Loss: 0.7355284094810486\n",
      "Epoch 74, batch 22 D Loss: 1.5383840799331665, G Loss: 0.7541470527648926\n",
      "Epoch 74, batch 23 D Loss: 1.363433837890625, G Loss: 0.764487087726593\n",
      "Epoch 74, batch 24 D Loss: 1.3771061897277832, G Loss: 0.7406818270683289\n",
      "Epoch 74, batch 25 D Loss: 1.4265525341033936, G Loss: 0.7425622344017029\n",
      "Epoch 74, batch 26 D Loss: 1.4173119068145752, G Loss: 0.7603506445884705\n",
      "Epoch 74, batch 27 D Loss: 1.2745424509048462, G Loss: 0.7658177018165588\n",
      "Epoch 74, batch 28 D Loss: 1.4220658540725708, G Loss: 0.781892716884613\n",
      "Epoch 74, batch 29 D Loss: 1.3495337963104248, G Loss: 0.7741308808326721\n",
      "Epoch 74, batch 30 D Loss: 1.356924057006836, G Loss: 0.773578941822052\n",
      "Epoch 74, batch 31 D Loss: 1.291916847229004, G Loss: 0.8005209565162659\n",
      "Epoch 74, batch 32 D Loss: 1.3534873723983765, G Loss: 0.7837336659431458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, batch 33 D Loss: 1.2805161476135254, G Loss: 0.7832522392272949\n",
      "Epoch 74, batch 34 D Loss: 1.2827246189117432, G Loss: 0.8046663403511047\n",
      "Epoch 74, batch 35 D Loss: 1.2673206329345703, G Loss: 0.8118589520454407\n",
      "Epoch 74, batch 36 D Loss: 1.2143930196762085, G Loss: 0.8233836889266968\n",
      "Epoch 74, batch 37 D Loss: 1.237937331199646, G Loss: 0.8425273895263672\n",
      "Epoch 74, batch 38 D Loss: 1.1321420669555664, G Loss: 0.8469783067703247\n",
      "Epoch 74, batch 39 D Loss: 1.1862876415252686, G Loss: 0.8321974873542786\n",
      "Epoch 74, batch 40 D Loss: 1.2619577646255493, G Loss: 0.8586812615394592\n",
      "Epoch 74, batch 41 D Loss: 1.1237285137176514, G Loss: 0.8730521202087402\n",
      "Epoch 74, batch 42 D Loss: 1.1899759769439697, G Loss: 0.8455350995063782\n",
      "Epoch 74, batch 43 D Loss: 1.2994842529296875, G Loss: 0.8794780969619751\n",
      "Epoch 74, batch 44 D Loss: 1.2446842193603516, G Loss: 0.8759506940841675\n",
      "Epoch 74, batch 45 D Loss: 1.1851791143417358, G Loss: 0.8976855278015137\n",
      "Epoch 74, batch 46 D Loss: 1.1327521800994873, G Loss: 0.8786752223968506\n",
      "Epoch 74, batch 47 D Loss: 1.2223961353302002, G Loss: 0.9160416126251221\n",
      "Epoch 74, batch 48 D Loss: 1.1310282945632935, G Loss: 0.8832672238349915\n",
      "Epoch 74, batch 49 D Loss: 1.208369255065918, G Loss: 0.8968465328216553\n",
      "Epoch 74, batch 50 D Loss: 1.1707757711410522, G Loss: 0.8838726282119751\n",
      "Epoch 74, batch 51 D Loss: 1.121842861175537, G Loss: 0.906190812587738\n",
      "Epoch 74, batch 52 D Loss: 1.0438170433044434, G Loss: 0.8936783671379089\n",
      "Epoch 74, batch 53 D Loss: 1.2194730043411255, G Loss: 0.9015893340110779\n",
      "Epoch 74, batch 54 D Loss: 1.1115844249725342, G Loss: 0.9227374792098999\n",
      "Epoch 74, batch 55 D Loss: 1.2311034202575684, G Loss: 0.8491696715354919\n",
      "Epoch 74, batch 56 D Loss: 1.1390970945358276, G Loss: 0.9021156430244446\n",
      "Epoch 74, batch 57 D Loss: 1.0961010456085205, G Loss: 0.9079150557518005\n",
      "Epoch 74, batch 58 D Loss: 1.1407818794250488, G Loss: 0.9104581475257874\n",
      "Epoch 74, batch 59 D Loss: 1.2217695713043213, G Loss: 0.8978899121284485\n",
      "Epoch 74, batch 60 D Loss: 1.2457592487335205, G Loss: 0.870319128036499\n",
      "Epoch 74, batch 61 D Loss: 1.0902239084243774, G Loss: 0.9053061604499817\n",
      "Epoch 74, batch 62 D Loss: 1.0787889957427979, G Loss: 0.9112891554832458\n",
      "Epoch 74, batch 63 D Loss: 1.1224201917648315, G Loss: 0.9017115831375122\n",
      "Epoch 74, batch 64 D Loss: 1.1441922187805176, G Loss: 0.9044756889343262\n",
      "Epoch 74, batch 65 D Loss: 1.0861482620239258, G Loss: 0.921112060546875\n",
      "Epoch 74, batch 66 D Loss: 1.2189574241638184, G Loss: 0.8790029883384705\n",
      "Epoch 74, batch 67 D Loss: 1.2215956449508667, G Loss: 0.8502436280250549\n",
      "Epoch 74, batch 68 D Loss: 1.14970064163208, G Loss: 0.871798038482666\n",
      "Epoch 74, batch 69 D Loss: 1.2236634492874146, G Loss: 0.9014831781387329\n",
      "Epoch 74, batch 70 D Loss: 1.160742998123169, G Loss: 0.8532863855361938\n",
      "Epoch 74, batch 71 D Loss: 1.153145432472229, G Loss: 0.9267930388450623\n",
      "Epoch 74, batch 72 D Loss: 1.1887085437774658, G Loss: 0.8371984958648682\n",
      "Epoch 74, batch 73 D Loss: 1.2014484405517578, G Loss: 0.8712080121040344\n",
      "Epoch 74, batch 74 D Loss: 1.1961400508880615, G Loss: 0.8463330864906311\n",
      "Epoch 74, batch 75 D Loss: 1.257317066192627, G Loss: 0.8190516829490662\n",
      "Epoch 74, batch 76 D Loss: 1.1689493656158447, G Loss: 0.8956388235092163\n",
      "Epoch 74, batch 77 D Loss: 1.2504122257232666, G Loss: 0.8738239407539368\n",
      "Epoch 74, batch 78 D Loss: 1.054288387298584, G Loss: 0.8844510912895203\n",
      "Epoch 74, batch 79 D Loss: 1.2771389484405518, G Loss: 0.7598885297775269\n",
      "Epoch 74, batch 80 D Loss: 1.204004168510437, G Loss: 0.8615291118621826\n",
      "Epoch 74, batch 81 D Loss: 1.1714798212051392, G Loss: 0.8034636974334717\n",
      "Epoch 74, batch 82 D Loss: 1.110429286956787, G Loss: 0.846618115901947\n",
      "Epoch 74, batch 83 D Loss: 1.2573325634002686, G Loss: 0.782626748085022\n",
      "Epoch 74, batch 84 D Loss: 1.3255820274353027, G Loss: 0.791134238243103\n",
      "Epoch 74, batch 85 D Loss: 1.3039119243621826, G Loss: 0.7646656036376953\n",
      "Epoch 74, batch 86 D Loss: 1.2703509330749512, G Loss: 0.760074257850647\n",
      "Epoch 74, batch 87 D Loss: 1.4847660064697266, G Loss: 0.7580999732017517\n",
      "Epoch 74, batch 88 D Loss: 1.3222970962524414, G Loss: 0.8132659196853638\n",
      "Epoch 74, batch 89 D Loss: 1.2533190250396729, G Loss: 0.7917813658714294\n",
      "Epoch 74, batch 90 D Loss: 1.212144374847412, G Loss: 0.8389581441879272\n",
      "Epoch 74, batch 91 D Loss: 1.278381586074829, G Loss: 0.8023301959037781\n",
      "Epoch 74, batch 92 D Loss: 1.2792800664901733, G Loss: 0.7803407311439514\n",
      "Epoch 74, batch 93 D Loss: 1.3018121719360352, G Loss: 0.8071011900901794\n",
      "Epoch 74, batch 94 D Loss: 1.28164803981781, G Loss: 0.8381590843200684\n",
      "Epoch 74, batch 95 D Loss: 1.2992146015167236, G Loss: 0.8357334733009338\n",
      "Epoch 74, batch 96 D Loss: 1.3273816108703613, G Loss: 0.7889246940612793\n",
      "Epoch 74, batch 97 D Loss: 1.2399557828903198, G Loss: 0.8689365386962891\n",
      "Epoch 74, batch 98 D Loss: 1.330214500427246, G Loss: 0.813783586025238\n",
      "Epoch 74, batch 99 D Loss: 1.4489433765411377, G Loss: 0.8229661583900452\n",
      "Epoch 74, batch 100 D Loss: 1.3299576044082642, G Loss: 0.81097811460495\n",
      "Epoch 74, batch 101 D Loss: 1.3410248756408691, G Loss: 0.7797092199325562\n",
      "Epoch 74, batch 102 D Loss: 1.3095777034759521, G Loss: 0.8275551795959473\n",
      "Epoch 74, batch 103 D Loss: 1.4379658699035645, G Loss: 0.8069828748703003\n",
      "Epoch 74, batch 104 D Loss: 1.314065933227539, G Loss: 0.8580362200737\n",
      "Epoch 74, batch 105 D Loss: 1.243245005607605, G Loss: 0.8791773915290833\n",
      "Epoch 74, batch 106 D Loss: 1.243501901626587, G Loss: 0.853512167930603\n",
      "Epoch 74, batch 107 D Loss: 1.4823896884918213, G Loss: 0.8001196980476379\n",
      "Epoch 74, batch 108 D Loss: 1.3159693479537964, G Loss: 0.8236276507377625\n",
      "Epoch 74, batch 109 D Loss: 1.1958162784576416, G Loss: 0.8923757672309875\n",
      "Epoch 74, batch 110 D Loss: 1.2834256887435913, G Loss: 0.9155470132827759\n",
      "Epoch 74, batch 111 D Loss: 1.1914331912994385, G Loss: 0.8977845907211304\n",
      "Epoch 74, batch 112 D Loss: 1.434646725654602, G Loss: 0.9197061657905579\n",
      "Epoch 74, batch 113 D Loss: 1.2578561305999756, G Loss: 0.907533586025238\n",
      "Epoch 74, batch 114 D Loss: 1.23789381980896, G Loss: 0.8705909252166748\n",
      "Epoch 74, batch 115 D Loss: 1.2220934629440308, G Loss: 0.8648046851158142\n",
      "Epoch 74, batch 116 D Loss: 1.353417158126831, G Loss: 0.8882476687431335\n",
      "Epoch 74, batch 117 D Loss: 1.28385329246521, G Loss: 1.0170567035675049\n",
      "Epoch 74, batch 118 D Loss: 1.2074894905090332, G Loss: 0.90876305103302\n",
      "Epoch 74, batch 119 D Loss: 1.3616079092025757, G Loss: 1.0000262260437012\n",
      "Epoch 74, batch 120 D Loss: 1.2875925302505493, G Loss: 0.8965926170349121\n",
      "Epoch 74, batch 121 D Loss: 1.3013267517089844, G Loss: 0.9256111979484558\n",
      "Epoch 74, batch 122 D Loss: 1.3319289684295654, G Loss: 0.9247686266899109\n",
      "Epoch 74, batch 123 D Loss: 1.3666696548461914, G Loss: 0.9204932451248169\n",
      "Epoch 74, batch 124 D Loss: 1.3057544231414795, G Loss: 0.9042495489120483\n",
      "Epoch 74, batch 125 D Loss: 1.4181804656982422, G Loss: 0.9505348205566406\n",
      "Epoch 74, batch 126 D Loss: 1.1899347305297852, G Loss: 0.9961279034614563\n",
      "Epoch 74, batch 127 D Loss: 1.1747578382492065, G Loss: 0.93888258934021\n",
      "Epoch 74, batch 128 D Loss: 1.2309012413024902, G Loss: 0.9782249927520752\n",
      "Epoch 74, batch 129 D Loss: 1.3637747764587402, G Loss: 0.8826103806495667\n",
      "Epoch 74, batch 130 D Loss: 1.23561429977417, G Loss: 0.9262027144432068\n",
      "Epoch 74, batch 131 D Loss: 1.478113055229187, G Loss: 0.9004191756248474\n",
      "Epoch 74, batch 132 D Loss: 1.415290355682373, G Loss: 0.8750560879707336\n",
      "Epoch 74, batch 133 D Loss: 1.326520323753357, G Loss: 0.9287564158439636\n",
      "Epoch 74, batch 134 D Loss: 1.3481696844100952, G Loss: 0.9224951863288879\n",
      "Epoch 74, batch 135 D Loss: 1.384366750717163, G Loss: 0.8823590874671936\n",
      "Epoch 74, batch 136 D Loss: 1.316129446029663, G Loss: 0.8617221713066101\n",
      "Epoch 74, batch 137 D Loss: 1.392863154411316, G Loss: 0.9015709757804871\n",
      "Epoch 74, batch 138 D Loss: 1.509488582611084, G Loss: 0.8434551954269409\n",
      "Epoch 74, batch 139 D Loss: 1.1833813190460205, G Loss: 0.9095708727836609\n",
      "Epoch 74, batch 140 D Loss: 1.324427843093872, G Loss: 0.834208607673645\n",
      "Epoch 74, batch 141 D Loss: 1.3944501876831055, G Loss: 0.834126353263855\n",
      "Epoch 74, batch 142 D Loss: 1.2704076766967773, G Loss: 0.8878049254417419\n",
      "Epoch 74, batch 143 D Loss: 1.3693010807037354, G Loss: 0.8491301536560059\n",
      "Epoch 74, batch 144 D Loss: 1.4059937000274658, G Loss: 0.8419262766838074\n",
      "Epoch 74, batch 145 D Loss: 1.3886836767196655, G Loss: 0.835841953754425\n",
      "Epoch 74, batch 146 D Loss: 1.4124691486358643, G Loss: 0.8672273755073547\n",
      "Epoch 74, batch 147 D Loss: 1.357177495956421, G Loss: 0.7734026312828064\n",
      "Epoch 74, batch 148 D Loss: 1.4215648174285889, G Loss: 0.8325140476226807\n",
      "Epoch 74, batch 149 D Loss: 1.5279591083526611, G Loss: 0.7729546427726746\n",
      "Epoch 74, batch 150 D Loss: 1.5006310939788818, G Loss: 0.7830529808998108\n",
      "Epoch 74, batch 151 D Loss: 1.3885502815246582, G Loss: 0.8266952633857727\n",
      "Epoch 74, batch 152 D Loss: 1.446359395980835, G Loss: 0.7975836992263794\n",
      "Epoch 74, batch 153 D Loss: 1.3237617015838623, G Loss: 0.7792120575904846\n",
      "Epoch 74, batch 154 D Loss: 1.6324520111083984, G Loss: 0.7767968773841858\n",
      "Epoch 74, batch 155 D Loss: 1.3528358936309814, G Loss: 0.7563716769218445\n",
      "Epoch 74, batch 156 D Loss: 1.4692901372909546, G Loss: 0.8468214273452759\n",
      "Epoch 74, batch 157 D Loss: 1.5681171417236328, G Loss: 0.7902758717536926\n",
      "Epoch 74, batch 158 D Loss: 1.475404143333435, G Loss: 0.7572354674339294\n",
      "Epoch 74, batch 159 D Loss: 1.4669427871704102, G Loss: 0.6829502582550049\n",
      "Epoch 74, batch 160 D Loss: 1.4585371017456055, G Loss: 0.7177307605743408\n",
      "Epoch 74, batch 161 D Loss: 1.4288005828857422, G Loss: 0.7210761308670044\n",
      "Epoch 74, batch 162 D Loss: 1.5769264698028564, G Loss: 0.7073115706443787\n",
      "Epoch 74, batch 163 D Loss: 1.5412061214447021, G Loss: 0.6937829852104187\n",
      "Epoch 74, batch 164 D Loss: 1.6363980770111084, G Loss: 0.6544878482818604\n",
      "Epoch 74, batch 165 D Loss: 1.6152527332305908, G Loss: 0.6736328601837158\n",
      "Epoch 74, batch 166 D Loss: 1.6205604076385498, G Loss: 0.6765510439872742\n",
      "Epoch 74, batch 167 D Loss: 1.5086352825164795, G Loss: 0.6995697021484375\n",
      "Epoch 74, batch 168 D Loss: 1.7796580791473389, G Loss: 0.6445329189300537\n",
      "Epoch 74, batch 169 D Loss: 1.6391782760620117, G Loss: 0.6761162281036377\n",
      "Epoch 74, batch 170 D Loss: 1.6289541721343994, G Loss: 0.6252533197402954\n",
      "Epoch 74, batch 171 D Loss: 1.5329331159591675, G Loss: 0.6391404867172241\n",
      "Epoch 74, batch 172 D Loss: 1.4938569068908691, G Loss: 0.7056636214256287\n",
      "Epoch 74, batch 173 D Loss: 1.5458638668060303, G Loss: 0.7074103355407715\n",
      "Epoch 74, batch 174 D Loss: 1.6882553100585938, G Loss: 0.6425051093101501\n",
      "Epoch 74, batch 175 D Loss: 1.50886869430542, G Loss: 0.6878835558891296\n",
      "Epoch 74, batch 176 D Loss: 1.5334057807922363, G Loss: 0.6065555214881897\n",
      "Epoch 74, batch 177 D Loss: 1.5182161331176758, G Loss: 0.6894018650054932\n",
      "Epoch 74, batch 178 D Loss: 1.5534591674804688, G Loss: 0.5958071351051331\n",
      "Epoch 74, batch 179 D Loss: 1.593597173690796, G Loss: 0.6899509429931641\n",
      "Epoch 74, batch 180 D Loss: 1.4086647033691406, G Loss: 0.7122393846511841\n",
      "Epoch 74, batch 181 D Loss: 1.6003575325012207, G Loss: 0.6342288255691528\n",
      "Epoch 74, batch 182 D Loss: 1.5607048273086548, G Loss: 0.6351791620254517\n",
      "Epoch 74, batch 183 D Loss: 1.5254039764404297, G Loss: 0.6566423773765564\n",
      "Epoch 74, batch 184 D Loss: 1.5619099140167236, G Loss: 0.660788893699646\n",
      "Epoch 74, batch 185 D Loss: 1.581242322921753, G Loss: 0.6651784777641296\n",
      "Epoch 74, batch 186 D Loss: 1.3843305110931396, G Loss: 0.7115453481674194\n",
      "Epoch 74, batch 187 D Loss: 1.4105489253997803, G Loss: 0.7339268326759338\n",
      "Epoch 74, batch 188 D Loss: 1.5108599662780762, G Loss: 0.6650239825248718\n",
      "Epoch 74, batch 189 D Loss: 1.646667242050171, G Loss: 0.6891354322433472\n",
      "Epoch 74, batch 190 D Loss: 1.5308911800384521, G Loss: 0.7312434911727905\n",
      "Epoch 74, batch 191 D Loss: 1.527474045753479, G Loss: 0.7047560214996338\n",
      "Epoch 74, batch 192 D Loss: 1.4426283836364746, G Loss: 0.7144941687583923\n",
      "Epoch 74, batch 193 D Loss: 1.3648531436920166, G Loss: 0.8097745776176453\n",
      "Epoch 74, batch 194 D Loss: 1.596527338027954, G Loss: 0.7947630882263184\n",
      "Epoch 74, batch 195 D Loss: 1.5167925357818604, G Loss: 0.7025062441825867\n",
      "Epoch 74, batch 196 D Loss: 1.580458164215088, G Loss: 0.7410686016082764\n",
      "Epoch 74, batch 197 D Loss: 1.5454124212265015, G Loss: 0.8293419480323792\n",
      "Epoch 74, batch 198 D Loss: 1.544569492340088, G Loss: 0.782112181186676\n",
      "Epoch 74, batch 199 D Loss: 1.5365996360778809, G Loss: 0.8046269416809082\n",
      "Epoch 74, batch 200 D Loss: 1.426224946975708, G Loss: 0.818760097026825\n",
      "Epoch 75, batch 1 D Loss: 1.4358389377593994, G Loss: 0.7588188052177429\n",
      "Epoch 75, batch 2 D Loss: 1.3560316562652588, G Loss: 0.7829572558403015\n",
      "Epoch 75, batch 3 D Loss: 1.4793829917907715, G Loss: 0.7907370924949646\n",
      "Epoch 75, batch 4 D Loss: 1.3556177616119385, G Loss: 0.8580554127693176\n",
      "Epoch 75, batch 5 D Loss: 1.3933758735656738, G Loss: 0.8857944011688232\n",
      "Epoch 75, batch 6 D Loss: 1.4405500888824463, G Loss: 0.8232966661453247\n",
      "Epoch 75, batch 7 D Loss: 1.2714695930480957, G Loss: 0.944225549697876\n",
      "Epoch 75, batch 8 D Loss: 1.4450805187225342, G Loss: 0.8309652209281921\n",
      "Epoch 75, batch 9 D Loss: 1.3580480813980103, G Loss: 0.9203768372535706\n",
      "Epoch 75, batch 10 D Loss: 1.4811720848083496, G Loss: 0.8195478916168213\n",
      "Epoch 75, batch 11 D Loss: 1.42070472240448, G Loss: 0.8473402261734009\n",
      "Epoch 75, batch 12 D Loss: 1.5157115459442139, G Loss: 0.9126469492912292\n",
      "Epoch 75, batch 13 D Loss: 1.3365668058395386, G Loss: 0.8915081024169922\n",
      "Epoch 75, batch 14 D Loss: 1.2978423833847046, G Loss: 0.8769996762275696\n",
      "Epoch 75, batch 15 D Loss: 1.502168893814087, G Loss: 0.8442642688751221\n",
      "Epoch 75, batch 16 D Loss: 1.4768415689468384, G Loss: 0.9704867005348206\n",
      "Epoch 75, batch 17 D Loss: 1.475166916847229, G Loss: 0.851747453212738\n",
      "Epoch 75, batch 18 D Loss: 1.3989536762237549, G Loss: 0.8647280335426331\n",
      "Epoch 75, batch 19 D Loss: 1.4860317707061768, G Loss: 0.90532386302948\n",
      "Epoch 75, batch 20 D Loss: 1.4194507598876953, G Loss: 0.8794149160385132\n",
      "Epoch 75, batch 21 D Loss: 1.4936339855194092, G Loss: 0.8273018598556519\n",
      "Epoch 75, batch 22 D Loss: 1.3493390083312988, G Loss: 0.8869050741195679\n",
      "Epoch 75, batch 23 D Loss: 1.462476372718811, G Loss: 0.8566272258758545\n",
      "Epoch 75, batch 24 D Loss: 1.4334849119186401, G Loss: 0.8567008972167969\n",
      "Epoch 75, batch 25 D Loss: 1.569040298461914, G Loss: 0.871035635471344\n",
      "Epoch 75, batch 26 D Loss: 1.2182974815368652, G Loss: 0.924839198589325\n",
      "Epoch 75, batch 27 D Loss: 1.5037542581558228, G Loss: 0.8715546131134033\n",
      "Epoch 75, batch 28 D Loss: 1.3269052505493164, G Loss: 0.9047709107398987\n",
      "Epoch 75, batch 29 D Loss: 1.4473226070404053, G Loss: 0.8744121789932251\n",
      "Epoch 75, batch 30 D Loss: 1.3156286478042603, G Loss: 0.8616957068443298\n",
      "Epoch 75, batch 31 D Loss: 1.5605897903442383, G Loss: 0.8581870198249817\n",
      "Epoch 75, batch 32 D Loss: 1.4135582447052002, G Loss: 0.8270061016082764\n",
      "Epoch 75, batch 33 D Loss: 1.3461222648620605, G Loss: 0.835064172744751\n",
      "Epoch 75, batch 34 D Loss: 1.4472854137420654, G Loss: 0.8412587642669678\n",
      "Epoch 75, batch 35 D Loss: 1.3785661458969116, G Loss: 0.8858762383460999\n",
      "Epoch 75, batch 36 D Loss: 1.6209419965744019, G Loss: 0.8545790314674377\n",
      "Epoch 75, batch 37 D Loss: 1.4398951530456543, G Loss: 0.8286924958229065\n",
      "Epoch 75, batch 38 D Loss: 1.522491693496704, G Loss: 0.8454619646072388\n",
      "Epoch 75, batch 39 D Loss: 1.3832674026489258, G Loss: 0.8466439843177795\n",
      "Epoch 75, batch 40 D Loss: 1.3850347995758057, G Loss: 0.8059084415435791\n",
      "Epoch 75, batch 41 D Loss: 1.3352127075195312, G Loss: 0.8476621508598328\n",
      "Epoch 75, batch 42 D Loss: 1.5344016551971436, G Loss: 0.8360843658447266\n",
      "Epoch 75, batch 43 D Loss: 1.3625378608703613, G Loss: 0.8305413126945496\n",
      "Epoch 75, batch 44 D Loss: 1.5241053104400635, G Loss: 0.834177553653717\n",
      "Epoch 75, batch 45 D Loss: 1.3274719715118408, G Loss: 0.8431246280670166\n",
      "Epoch 75, batch 46 D Loss: 1.374631404876709, G Loss: 0.7981903553009033\n",
      "Epoch 75, batch 47 D Loss: 1.353184700012207, G Loss: 0.846291720867157\n",
      "Epoch 75, batch 48 D Loss: 1.3707637786865234, G Loss: 0.8138884902000427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, batch 49 D Loss: 1.3820898532867432, G Loss: 0.8222424387931824\n",
      "Epoch 75, batch 50 D Loss: 1.4882259368896484, G Loss: 0.8366817235946655\n",
      "Epoch 75, batch 51 D Loss: 1.3874006271362305, G Loss: 0.8291795253753662\n",
      "Epoch 75, batch 52 D Loss: 1.4695007801055908, G Loss: 0.7850407361984253\n",
      "Epoch 75, batch 53 D Loss: 1.4585340023040771, G Loss: 0.8484646677970886\n",
      "Epoch 75, batch 54 D Loss: 1.3816115856170654, G Loss: 0.851169764995575\n",
      "Epoch 75, batch 55 D Loss: 1.5091612339019775, G Loss: 0.8231765031814575\n",
      "Epoch 75, batch 56 D Loss: 1.4215867519378662, G Loss: 0.8434110879898071\n",
      "Epoch 75, batch 57 D Loss: 1.399552345275879, G Loss: 0.8432424664497375\n",
      "Epoch 75, batch 58 D Loss: 1.3926620483398438, G Loss: 0.8458379507064819\n",
      "Epoch 75, batch 59 D Loss: 1.2676459550857544, G Loss: 0.8374196887016296\n",
      "Epoch 75, batch 60 D Loss: 1.344626545906067, G Loss: 0.8498477935791016\n",
      "Epoch 75, batch 61 D Loss: 1.3796840906143188, G Loss: 0.8293478488922119\n",
      "Epoch 75, batch 62 D Loss: 1.2891719341278076, G Loss: 0.8780330419540405\n",
      "Epoch 75, batch 63 D Loss: 1.2816073894500732, G Loss: 0.8249585628509521\n",
      "Epoch 75, batch 64 D Loss: 1.371656894683838, G Loss: 0.8779972195625305\n",
      "Epoch 75, batch 65 D Loss: 1.389497995376587, G Loss: 0.8578269481658936\n",
      "Epoch 75, batch 66 D Loss: 1.4356645345687866, G Loss: 0.8303751349449158\n",
      "Epoch 75, batch 67 D Loss: 1.2146756649017334, G Loss: 0.9300157427787781\n",
      "Epoch 75, batch 68 D Loss: 1.3214575052261353, G Loss: 0.8579510450363159\n",
      "Epoch 75, batch 69 D Loss: 1.287688970565796, G Loss: 0.8678513169288635\n",
      "Epoch 75, batch 70 D Loss: 1.608404278755188, G Loss: 0.8438632488250732\n",
      "Epoch 75, batch 71 D Loss: 1.3643016815185547, G Loss: 0.8847882747650146\n",
      "Epoch 75, batch 72 D Loss: 1.4228698015213013, G Loss: 0.8319991230964661\n",
      "Epoch 75, batch 73 D Loss: 1.4394729137420654, G Loss: 0.8287846446037292\n",
      "Epoch 75, batch 74 D Loss: 1.4456734657287598, G Loss: 0.8295629620552063\n",
      "Epoch 75, batch 75 D Loss: 1.2671020030975342, G Loss: 0.8900393843650818\n",
      "Epoch 75, batch 76 D Loss: 1.2753901481628418, G Loss: 0.8657106161117554\n",
      "Epoch 75, batch 77 D Loss: 1.4130101203918457, G Loss: 0.858543872833252\n",
      "Epoch 75, batch 78 D Loss: 1.1982601881027222, G Loss: 0.8836939334869385\n",
      "Epoch 75, batch 79 D Loss: 1.3570175170898438, G Loss: 0.8551326990127563\n",
      "Epoch 75, batch 80 D Loss: 1.358107328414917, G Loss: 0.8328697681427002\n",
      "Epoch 75, batch 81 D Loss: 1.3320727348327637, G Loss: 0.8262429237365723\n",
      "Epoch 75, batch 82 D Loss: 1.3931398391723633, G Loss: 0.8291172981262207\n",
      "Epoch 75, batch 83 D Loss: 1.3585095405578613, G Loss: 0.8299961686134338\n",
      "Epoch 75, batch 84 D Loss: 1.330393671989441, G Loss: 0.8092438578605652\n",
      "Epoch 75, batch 85 D Loss: 1.3061776161193848, G Loss: 0.8438200950622559\n",
      "Epoch 75, batch 86 D Loss: 1.3162121772766113, G Loss: 0.8468365669250488\n",
      "Epoch 75, batch 87 D Loss: 1.3936831951141357, G Loss: 0.7989360690116882\n",
      "Epoch 75, batch 88 D Loss: 1.3694920539855957, G Loss: 0.8341426253318787\n",
      "Epoch 75, batch 89 D Loss: 1.3479137420654297, G Loss: 0.8055474758148193\n",
      "Epoch 75, batch 90 D Loss: 1.3508039712905884, G Loss: 0.820448637008667\n",
      "Epoch 75, batch 91 D Loss: 1.4429346323013306, G Loss: 0.8364865183830261\n",
      "Epoch 75, batch 92 D Loss: 1.3520245552062988, G Loss: 0.8367762565612793\n",
      "Epoch 75, batch 93 D Loss: 1.3088152408599854, G Loss: 0.8191317915916443\n",
      "Epoch 75, batch 94 D Loss: 1.561774730682373, G Loss: 0.8175147771835327\n",
      "Epoch 75, batch 95 D Loss: 1.5590848922729492, G Loss: 0.8315941691398621\n",
      "Epoch 75, batch 96 D Loss: 1.2542904615402222, G Loss: 0.8002435564994812\n",
      "Epoch 75, batch 97 D Loss: 1.3068771362304688, G Loss: 0.8209748864173889\n",
      "Epoch 75, batch 98 D Loss: 1.4584250450134277, G Loss: 0.7765133380889893\n",
      "Epoch 75, batch 99 D Loss: 1.516419768333435, G Loss: 0.8081627488136292\n",
      "Epoch 75, batch 100 D Loss: 1.4216134548187256, G Loss: 0.7615366578102112\n",
      "Epoch 75, batch 101 D Loss: 1.3576767444610596, G Loss: 0.8446307182312012\n",
      "Epoch 75, batch 102 D Loss: 1.5129828453063965, G Loss: 0.8024185299873352\n",
      "Epoch 75, batch 103 D Loss: 1.53916335105896, G Loss: 0.7700002789497375\n",
      "Epoch 75, batch 104 D Loss: 1.4496288299560547, G Loss: 0.7735543847084045\n",
      "Epoch 75, batch 105 D Loss: 1.3755149841308594, G Loss: 0.8024376034736633\n",
      "Epoch 75, batch 106 D Loss: 1.50417160987854, G Loss: 0.7734577655792236\n",
      "Epoch 75, batch 107 D Loss: 1.4082876443862915, G Loss: 0.7579269409179688\n",
      "Epoch 75, batch 108 D Loss: 1.4481656551361084, G Loss: 0.7510673999786377\n",
      "Epoch 75, batch 109 D Loss: 1.4354221820831299, G Loss: 0.7479324340820312\n",
      "Epoch 75, batch 110 D Loss: 1.4633104801177979, G Loss: 0.6982138156890869\n",
      "Epoch 75, batch 111 D Loss: 1.3139991760253906, G Loss: 0.7597813606262207\n",
      "Epoch 75, batch 112 D Loss: 1.343538761138916, G Loss: 0.7681154012680054\n",
      "Epoch 75, batch 113 D Loss: 1.3959033489227295, G Loss: 0.7881554961204529\n",
      "Epoch 75, batch 114 D Loss: 1.4211711883544922, G Loss: 0.7778494954109192\n",
      "Epoch 75, batch 115 D Loss: 1.2878029346466064, G Loss: 0.7410239577293396\n",
      "Epoch 75, batch 116 D Loss: 1.4048516750335693, G Loss: 0.779887318611145\n",
      "Epoch 75, batch 117 D Loss: 1.3766684532165527, G Loss: 0.7705888152122498\n",
      "Epoch 75, batch 118 D Loss: 1.3577808141708374, G Loss: 0.7386683821678162\n",
      "Epoch 75, batch 119 D Loss: 1.3827686309814453, G Loss: 0.7353202104568481\n",
      "Epoch 75, batch 120 D Loss: 1.4891644716262817, G Loss: 0.7478076815605164\n",
      "Epoch 75, batch 121 D Loss: 1.4100816249847412, G Loss: 0.7514970898628235\n",
      "Epoch 75, batch 122 D Loss: 1.270616054534912, G Loss: 0.7438226342201233\n",
      "Epoch 75, batch 123 D Loss: 1.3919579982757568, G Loss: 0.7373724579811096\n",
      "Epoch 75, batch 124 D Loss: 1.4068830013275146, G Loss: 0.7184790968894958\n",
      "Epoch 75, batch 125 D Loss: 1.460220456123352, G Loss: 0.6954894065856934\n",
      "Epoch 75, batch 126 D Loss: 1.4328248500823975, G Loss: 0.7406026721000671\n",
      "Epoch 75, batch 127 D Loss: 1.3230197429656982, G Loss: 0.7034119963645935\n",
      "Epoch 75, batch 128 D Loss: 1.5335595607757568, G Loss: 0.6930267810821533\n",
      "Epoch 75, batch 129 D Loss: 1.4121768474578857, G Loss: 0.685676634311676\n",
      "Epoch 75, batch 130 D Loss: 1.4807758331298828, G Loss: 0.6771324872970581\n",
      "Epoch 75, batch 131 D Loss: 1.4595309495925903, G Loss: 0.6566194295883179\n",
      "Epoch 75, batch 132 D Loss: 1.3489046096801758, G Loss: 0.6513961553573608\n",
      "Epoch 75, batch 133 D Loss: 1.4667038917541504, G Loss: 0.6979445815086365\n",
      "Epoch 75, batch 134 D Loss: 1.4192516803741455, G Loss: 0.6820994019508362\n",
      "Epoch 75, batch 135 D Loss: 1.5522804260253906, G Loss: 0.6442282199859619\n",
      "Epoch 75, batch 136 D Loss: 1.3570058345794678, G Loss: 0.6793656349182129\n",
      "Epoch 75, batch 137 D Loss: 1.466353178024292, G Loss: 0.6522257328033447\n",
      "Epoch 75, batch 138 D Loss: 1.4965782165527344, G Loss: 0.6525949239730835\n",
      "Epoch 75, batch 139 D Loss: 1.4454065561294556, G Loss: 0.6105692386627197\n",
      "Epoch 75, batch 140 D Loss: 1.434196949005127, G Loss: 0.6474220156669617\n",
      "Epoch 75, batch 141 D Loss: 1.5093557834625244, G Loss: 0.6300417184829712\n",
      "Epoch 75, batch 142 D Loss: 1.4335002899169922, G Loss: 0.6496912240982056\n",
      "Epoch 75, batch 143 D Loss: 1.4240427017211914, G Loss: 0.6664673089981079\n",
      "Epoch 75, batch 144 D Loss: 1.4613124132156372, G Loss: 0.6494492888450623\n",
      "Epoch 75, batch 145 D Loss: 1.4084534645080566, G Loss: 0.6667835712432861\n",
      "Epoch 75, batch 146 D Loss: 1.4835562705993652, G Loss: 0.6023275852203369\n",
      "Epoch 75, batch 147 D Loss: 1.45930814743042, G Loss: 0.6455885171890259\n",
      "Epoch 75, batch 148 D Loss: 1.5069656372070312, G Loss: 0.6580307483673096\n",
      "Epoch 75, batch 149 D Loss: 1.3975191116333008, G Loss: 0.6544220447540283\n",
      "Epoch 75, batch 150 D Loss: 1.3597757816314697, G Loss: 0.6841455101966858\n",
      "Epoch 75, batch 151 D Loss: 1.3507707118988037, G Loss: 0.7005929350852966\n",
      "Epoch 75, batch 152 D Loss: 1.4443800449371338, G Loss: 0.669116199016571\n",
      "Epoch 75, batch 153 D Loss: 1.5248526334762573, G Loss: 0.6618741750717163\n",
      "Epoch 75, batch 154 D Loss: 1.4432122707366943, G Loss: 0.682578444480896\n",
      "Epoch 75, batch 155 D Loss: 1.4299341440200806, G Loss: 0.6863419413566589\n",
      "Epoch 75, batch 156 D Loss: 1.435292363166809, G Loss: 0.6828824877738953\n",
      "Epoch 75, batch 157 D Loss: 1.378598928451538, G Loss: 0.6945599317550659\n",
      "Epoch 75, batch 158 D Loss: 1.4395136833190918, G Loss: 0.7074190378189087\n",
      "Epoch 75, batch 159 D Loss: 1.451385498046875, G Loss: 0.6832476258277893\n",
      "Epoch 75, batch 160 D Loss: 1.4107494354248047, G Loss: 0.6907595992088318\n",
      "Epoch 75, batch 161 D Loss: 1.3695062398910522, G Loss: 0.7515435814857483\n",
      "Epoch 75, batch 162 D Loss: 1.390073299407959, G Loss: 0.704471230506897\n",
      "Epoch 75, batch 163 D Loss: 1.4787266254425049, G Loss: 0.7318757772445679\n",
      "Epoch 75, batch 164 D Loss: 1.3087544441223145, G Loss: 0.7531842589378357\n",
      "Epoch 75, batch 165 D Loss: 1.305274486541748, G Loss: 0.7877374291419983\n",
      "Epoch 75, batch 166 D Loss: 1.3710951805114746, G Loss: 0.7342957258224487\n",
      "Epoch 75, batch 167 D Loss: 1.307565450668335, G Loss: 0.7643824815750122\n",
      "Epoch 75, batch 168 D Loss: 1.206664800643921, G Loss: 0.7939990758895874\n",
      "Epoch 75, batch 169 D Loss: 1.217355489730835, G Loss: 0.8089241981506348\n",
      "Epoch 75, batch 170 D Loss: 1.200186014175415, G Loss: 0.821918249130249\n",
      "Epoch 75, batch 171 D Loss: 1.2327238321304321, G Loss: 0.7991849780082703\n",
      "Epoch 75, batch 172 D Loss: 1.2736470699310303, G Loss: 0.8247380256652832\n",
      "Epoch 75, batch 173 D Loss: 1.2261645793914795, G Loss: 0.845889687538147\n",
      "Epoch 75, batch 174 D Loss: 1.2084360122680664, G Loss: 0.8760079741477966\n",
      "Epoch 75, batch 175 D Loss: 1.242645025253296, G Loss: 0.8459939360618591\n",
      "Epoch 75, batch 176 D Loss: 1.2456340789794922, G Loss: 0.8898031115531921\n",
      "Epoch 75, batch 177 D Loss: 1.2894587516784668, G Loss: 0.8539465069770813\n",
      "Epoch 75, batch 178 D Loss: 1.1730537414550781, G Loss: 0.8355693221092224\n",
      "Epoch 75, batch 179 D Loss: 1.1984896659851074, G Loss: 0.8722708225250244\n",
      "Epoch 75, batch 180 D Loss: 1.1232420206069946, G Loss: 0.9616706967353821\n",
      "Epoch 75, batch 181 D Loss: 1.0377998352050781, G Loss: 0.9415174722671509\n",
      "Epoch 75, batch 182 D Loss: 1.1398266553878784, G Loss: 0.9471662640571594\n",
      "Epoch 75, batch 183 D Loss: 1.1896382570266724, G Loss: 0.9242495894432068\n",
      "Epoch 75, batch 184 D Loss: 1.1963157653808594, G Loss: 0.9235093593597412\n",
      "Epoch 75, batch 185 D Loss: 1.2056869268417358, G Loss: 0.9258608222007751\n",
      "Epoch 75, batch 186 D Loss: 1.1749567985534668, G Loss: 0.9273049235343933\n",
      "Epoch 75, batch 187 D Loss: 1.1721699237823486, G Loss: 0.9421356320381165\n",
      "Epoch 75, batch 188 D Loss: 1.2209374904632568, G Loss: 1.0286327600479126\n",
      "Epoch 75, batch 189 D Loss: 1.167449712753296, G Loss: 0.995637059211731\n",
      "Epoch 75, batch 190 D Loss: 1.0904927253723145, G Loss: 0.997668445110321\n",
      "Epoch 75, batch 191 D Loss: 1.150319218635559, G Loss: 1.0168547630310059\n",
      "Epoch 75, batch 192 D Loss: 1.0918399095535278, G Loss: 1.059281587600708\n",
      "Epoch 75, batch 193 D Loss: 0.9960893392562866, G Loss: 1.0897465944290161\n",
      "Epoch 75, batch 194 D Loss: 1.2580835819244385, G Loss: 0.9403746128082275\n",
      "Epoch 75, batch 195 D Loss: 1.1118346452713013, G Loss: 1.082930088043213\n",
      "Epoch 75, batch 196 D Loss: 1.0671429634094238, G Loss: 1.0295052528381348\n",
      "Epoch 75, batch 197 D Loss: 1.1354669332504272, G Loss: 1.062685489654541\n",
      "Epoch 75, batch 198 D Loss: 1.048758625984192, G Loss: 1.0349293947219849\n",
      "Epoch 75, batch 199 D Loss: 1.0994720458984375, G Loss: 1.0447688102722168\n",
      "Epoch 75, batch 200 D Loss: 1.1981630325317383, G Loss: 1.0843226909637451\n",
      "Epoch 76, batch 1 D Loss: 1.115272045135498, G Loss: 1.0583990812301636\n",
      "Epoch 76, batch 2 D Loss: 1.0668489933013916, G Loss: 1.0286016464233398\n",
      "Epoch 76, batch 3 D Loss: 1.2162582874298096, G Loss: 1.116665005683899\n",
      "Epoch 76, batch 4 D Loss: 1.1171016693115234, G Loss: 1.0458427667617798\n",
      "Epoch 76, batch 5 D Loss: 1.2493994235992432, G Loss: 1.0360007286071777\n",
      "Epoch 76, batch 6 D Loss: 1.1441895961761475, G Loss: 1.054117202758789\n",
      "Epoch 76, batch 7 D Loss: 1.1037395000457764, G Loss: 1.065484881401062\n",
      "Epoch 76, batch 8 D Loss: 1.196578025817871, G Loss: 1.045820951461792\n",
      "Epoch 76, batch 9 D Loss: 1.0883262157440186, G Loss: 1.1088045835494995\n",
      "Epoch 76, batch 10 D Loss: 1.1738487482070923, G Loss: 1.1635053157806396\n",
      "Epoch 76, batch 11 D Loss: 1.0891437530517578, G Loss: 1.0971009731292725\n",
      "Epoch 76, batch 12 D Loss: 1.186468482017517, G Loss: 1.041642427444458\n",
      "Epoch 76, batch 13 D Loss: 1.1362769603729248, G Loss: 1.138519048690796\n",
      "Epoch 76, batch 14 D Loss: 1.2362264394760132, G Loss: 1.0822439193725586\n",
      "Epoch 76, batch 15 D Loss: 1.2214319705963135, G Loss: 1.0201350450515747\n",
      "Epoch 76, batch 16 D Loss: 1.1478147506713867, G Loss: 1.0680053234100342\n",
      "Epoch 76, batch 17 D Loss: 1.0096263885498047, G Loss: 1.1079789400100708\n",
      "Epoch 76, batch 18 D Loss: 1.1419833898544312, G Loss: 1.0482664108276367\n",
      "Epoch 76, batch 19 D Loss: 1.151179313659668, G Loss: 1.079817533493042\n",
      "Epoch 76, batch 20 D Loss: 1.0699586868286133, G Loss: 1.0665298700332642\n",
      "Epoch 76, batch 21 D Loss: 1.0730650424957275, G Loss: 1.100711464881897\n",
      "Epoch 76, batch 22 D Loss: 1.343552589416504, G Loss: 1.0451555252075195\n",
      "Epoch 76, batch 23 D Loss: 1.1284241676330566, G Loss: 1.1141767501831055\n",
      "Epoch 76, batch 24 D Loss: 1.1181604862213135, G Loss: 1.1344056129455566\n",
      "Epoch 76, batch 25 D Loss: 1.1313766241073608, G Loss: 1.1048507690429688\n",
      "Epoch 76, batch 26 D Loss: 1.1393928527832031, G Loss: 1.0626407861709595\n",
      "Epoch 76, batch 27 D Loss: 1.1827608346939087, G Loss: 1.0720648765563965\n",
      "Epoch 76, batch 28 D Loss: 1.0705034732818604, G Loss: 1.0608140230178833\n",
      "Epoch 76, batch 29 D Loss: 1.164557695388794, G Loss: 1.0614310503005981\n",
      "Epoch 76, batch 30 D Loss: 1.1278862953186035, G Loss: 1.1270685195922852\n",
      "Epoch 76, batch 31 D Loss: 1.039438247680664, G Loss: 1.0873123407363892\n",
      "Epoch 76, batch 32 D Loss: 1.0872211456298828, G Loss: 1.0386760234832764\n",
      "Epoch 76, batch 33 D Loss: 1.2411973476409912, G Loss: 1.0160752534866333\n",
      "Epoch 76, batch 34 D Loss: 1.1252620220184326, G Loss: 1.1442769765853882\n",
      "Epoch 76, batch 35 D Loss: 1.073488473892212, G Loss: 1.090583324432373\n",
      "Epoch 76, batch 36 D Loss: 1.226008653640747, G Loss: 1.094262957572937\n",
      "Epoch 76, batch 37 D Loss: 1.1389710903167725, G Loss: 1.094294786453247\n",
      "Epoch 76, batch 38 D Loss: 1.1699131727218628, G Loss: 1.0020290613174438\n",
      "Epoch 76, batch 39 D Loss: 1.2661906480789185, G Loss: 1.0086753368377686\n",
      "Epoch 76, batch 40 D Loss: 1.2187774181365967, G Loss: 1.004196286201477\n",
      "Epoch 76, batch 41 D Loss: 1.2064568996429443, G Loss: 0.9545260071754456\n",
      "Epoch 76, batch 42 D Loss: 1.124024748802185, G Loss: 1.018052577972412\n",
      "Epoch 76, batch 43 D Loss: 1.1794332265853882, G Loss: 1.0399484634399414\n",
      "Epoch 76, batch 44 D Loss: 1.3104183673858643, G Loss: 0.986280083656311\n",
      "Epoch 76, batch 45 D Loss: 1.1134283542633057, G Loss: 0.998863160610199\n",
      "Epoch 76, batch 46 D Loss: 1.291551113128662, G Loss: 0.9318119287490845\n",
      "Epoch 76, batch 47 D Loss: 1.3405016660690308, G Loss: 0.9543516635894775\n",
      "Epoch 76, batch 48 D Loss: 1.166383981704712, G Loss: 0.9559645056724548\n",
      "Epoch 76, batch 49 D Loss: 1.254399299621582, G Loss: 0.904205322265625\n",
      "Epoch 76, batch 50 D Loss: 1.1437474489212036, G Loss: 0.95766282081604\n",
      "Epoch 76, batch 51 D Loss: 1.3527140617370605, G Loss: 0.8887696266174316\n",
      "Epoch 76, batch 52 D Loss: 1.3075778484344482, G Loss: 0.9375376105308533\n",
      "Epoch 76, batch 53 D Loss: 1.254875659942627, G Loss: 0.9213690161705017\n",
      "Epoch 76, batch 54 D Loss: 1.3346118927001953, G Loss: 0.8765313625335693\n",
      "Epoch 76, batch 55 D Loss: 1.1851260662078857, G Loss: 0.870175838470459\n",
      "Epoch 76, batch 56 D Loss: 1.230259656906128, G Loss: 0.8741376996040344\n",
      "Epoch 76, batch 57 D Loss: 1.338709831237793, G Loss: 0.8843621611595154\n",
      "Epoch 76, batch 58 D Loss: 1.3625496625900269, G Loss: 0.8724424839019775\n",
      "Epoch 76, batch 59 D Loss: 1.28484308719635, G Loss: 0.8340203762054443\n",
      "Epoch 76, batch 60 D Loss: 1.183660626411438, G Loss: 0.8478826284408569\n",
      "Epoch 76, batch 61 D Loss: 1.128273844718933, G Loss: 0.8382386565208435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, batch 62 D Loss: 1.242570161819458, G Loss: 0.8294525742530823\n",
      "Epoch 76, batch 63 D Loss: 1.4708516597747803, G Loss: 0.8173539638519287\n",
      "Epoch 76, batch 64 D Loss: 1.1451270580291748, G Loss: 0.8422142863273621\n",
      "Epoch 76, batch 65 D Loss: 1.2367525100708008, G Loss: 0.8680216073989868\n",
      "Epoch 76, batch 66 D Loss: 1.3715224266052246, G Loss: 0.8489823341369629\n",
      "Epoch 76, batch 67 D Loss: 1.3141958713531494, G Loss: 0.8153469562530518\n",
      "Epoch 76, batch 68 D Loss: 1.4049904346466064, G Loss: 0.8293898701667786\n",
      "Epoch 76, batch 69 D Loss: 1.412900686264038, G Loss: 0.8335810303688049\n",
      "Epoch 76, batch 70 D Loss: 1.3778460025787354, G Loss: 0.8072912096977234\n",
      "Epoch 76, batch 71 D Loss: 1.3495495319366455, G Loss: 0.8051337599754333\n",
      "Epoch 76, batch 72 D Loss: 1.2554672956466675, G Loss: 0.8022586107254028\n",
      "Epoch 76, batch 73 D Loss: 1.2711105346679688, G Loss: 0.8260980248451233\n",
      "Epoch 76, batch 74 D Loss: 1.3966939449310303, G Loss: 0.7926739454269409\n",
      "Epoch 76, batch 75 D Loss: 1.355963945388794, G Loss: 0.7965860962867737\n",
      "Epoch 76, batch 76 D Loss: 1.4119409322738647, G Loss: 0.803514838218689\n",
      "Epoch 76, batch 77 D Loss: 1.503549337387085, G Loss: 0.8294811248779297\n",
      "Epoch 76, batch 78 D Loss: 1.4299297332763672, G Loss: 0.7925065755844116\n",
      "Epoch 76, batch 79 D Loss: 1.3144958019256592, G Loss: 0.7608487010002136\n",
      "Epoch 76, batch 80 D Loss: 1.4673713445663452, G Loss: 0.788770318031311\n",
      "Epoch 76, batch 81 D Loss: 1.3356404304504395, G Loss: 0.7913527488708496\n",
      "Epoch 76, batch 82 D Loss: 1.2847325801849365, G Loss: 0.795389711856842\n",
      "Epoch 76, batch 83 D Loss: 1.3985636234283447, G Loss: 0.7835747003555298\n",
      "Epoch 76, batch 84 D Loss: 1.2222514152526855, G Loss: 0.762402355670929\n",
      "Epoch 76, batch 85 D Loss: 1.312207818031311, G Loss: 0.7610961198806763\n",
      "Epoch 76, batch 86 D Loss: 1.4508088827133179, G Loss: 0.7731595039367676\n",
      "Epoch 76, batch 87 D Loss: 1.3720020055770874, G Loss: 0.7630805969238281\n",
      "Epoch 76, batch 88 D Loss: 1.3967151641845703, G Loss: 0.7652958035469055\n",
      "Epoch 76, batch 89 D Loss: 1.2994446754455566, G Loss: 0.7861265540122986\n",
      "Epoch 76, batch 90 D Loss: 1.3763573169708252, G Loss: 0.7837585210800171\n",
      "Epoch 76, batch 91 D Loss: 1.3773248195648193, G Loss: 0.7446893453598022\n",
      "Epoch 76, batch 92 D Loss: 1.4889297485351562, G Loss: 0.7424335479736328\n",
      "Epoch 76, batch 93 D Loss: 1.3292384147644043, G Loss: 0.7536086440086365\n",
      "Epoch 76, batch 94 D Loss: 1.3222136497497559, G Loss: 0.7630993127822876\n",
      "Epoch 76, batch 95 D Loss: 1.4793283939361572, G Loss: 0.7341704368591309\n",
      "Epoch 76, batch 96 D Loss: 1.1790167093276978, G Loss: 0.7717323303222656\n",
      "Epoch 76, batch 97 D Loss: 1.367218017578125, G Loss: 0.765546441078186\n",
      "Epoch 76, batch 98 D Loss: 1.3840055465698242, G Loss: 0.7100808620452881\n",
      "Epoch 76, batch 99 D Loss: 1.318688154220581, G Loss: 0.7305900454521179\n",
      "Epoch 76, batch 100 D Loss: 1.3664233684539795, G Loss: 0.735003650188446\n",
      "Epoch 76, batch 101 D Loss: 1.3915104866027832, G Loss: 0.7126200795173645\n",
      "Epoch 76, batch 102 D Loss: 1.4710068702697754, G Loss: 0.7020078897476196\n",
      "Epoch 76, batch 103 D Loss: 1.3598319292068481, G Loss: 0.710635781288147\n",
      "Epoch 76, batch 104 D Loss: 1.5245797634124756, G Loss: 0.7054721713066101\n",
      "Epoch 76, batch 105 D Loss: 1.2579219341278076, G Loss: 0.7095532417297363\n",
      "Epoch 76, batch 106 D Loss: 1.3914399147033691, G Loss: 0.6834385395050049\n",
      "Epoch 76, batch 107 D Loss: 1.4467345476150513, G Loss: 0.7064246535301208\n",
      "Epoch 76, batch 108 D Loss: 1.3168165683746338, G Loss: 0.6951825022697449\n",
      "Epoch 76, batch 109 D Loss: 1.430942177772522, G Loss: 0.6688796877861023\n",
      "Epoch 76, batch 110 D Loss: 1.347581148147583, G Loss: 0.6928344964981079\n",
      "Epoch 76, batch 111 D Loss: 1.3636984825134277, G Loss: 0.7087005376815796\n",
      "Epoch 76, batch 112 D Loss: 1.4739503860473633, G Loss: 0.6680155992507935\n",
      "Epoch 76, batch 113 D Loss: 1.39121413230896, G Loss: 0.7028846144676208\n",
      "Epoch 76, batch 114 D Loss: 1.7004103660583496, G Loss: 0.6180683374404907\n",
      "Epoch 76, batch 115 D Loss: 1.3783105611801147, G Loss: 0.6306106448173523\n",
      "Epoch 76, batch 116 D Loss: 1.6811679601669312, G Loss: 0.647003173828125\n",
      "Epoch 76, batch 117 D Loss: 1.381096601486206, G Loss: 0.6448336839675903\n",
      "Epoch 76, batch 118 D Loss: 1.4182451963424683, G Loss: 0.6674688458442688\n",
      "Epoch 76, batch 119 D Loss: 1.429213523864746, G Loss: 0.6248840093612671\n",
      "Epoch 76, batch 120 D Loss: 1.446155309677124, G Loss: 0.6559829115867615\n",
      "Epoch 76, batch 121 D Loss: 1.5899699926376343, G Loss: 0.6161699295043945\n",
      "Epoch 76, batch 122 D Loss: 1.5565946102142334, G Loss: 0.6309451460838318\n",
      "Epoch 76, batch 123 D Loss: 1.5882548093795776, G Loss: 0.63252854347229\n",
      "Epoch 76, batch 124 D Loss: 1.421293020248413, G Loss: 0.642111599445343\n",
      "Epoch 76, batch 125 D Loss: 1.4712293148040771, G Loss: 0.5971681475639343\n",
      "Epoch 76, batch 126 D Loss: 1.5671766996383667, G Loss: 0.6104303598403931\n",
      "Epoch 76, batch 127 D Loss: 1.570671796798706, G Loss: 0.593008279800415\n",
      "Epoch 76, batch 128 D Loss: 1.6835991144180298, G Loss: 0.5723779201507568\n",
      "Epoch 76, batch 129 D Loss: 1.5670650005340576, G Loss: 0.616656482219696\n",
      "Epoch 76, batch 130 D Loss: 1.7003188133239746, G Loss: 0.5794718265533447\n",
      "Epoch 76, batch 131 D Loss: 1.748530626296997, G Loss: 0.5793924927711487\n",
      "Epoch 76, batch 132 D Loss: 1.584209680557251, G Loss: 0.5805713534355164\n",
      "Epoch 76, batch 133 D Loss: 1.6605513095855713, G Loss: 0.5594886541366577\n",
      "Epoch 76, batch 134 D Loss: 1.6531307697296143, G Loss: 0.61417555809021\n",
      "Epoch 76, batch 135 D Loss: 1.5586564540863037, G Loss: 0.6183731555938721\n",
      "Epoch 76, batch 136 D Loss: 1.5536534786224365, G Loss: 0.6144742369651794\n",
      "Epoch 76, batch 137 D Loss: 1.681161880493164, G Loss: 0.549447238445282\n",
      "Epoch 76, batch 138 D Loss: 1.5086404085159302, G Loss: 0.6092418432235718\n",
      "Epoch 76, batch 139 D Loss: 1.5383527278900146, G Loss: 0.5555427670478821\n",
      "Epoch 76, batch 140 D Loss: 1.5911035537719727, G Loss: 0.5863729119300842\n",
      "Epoch 76, batch 141 D Loss: 1.5099729299545288, G Loss: 0.5938071012496948\n",
      "Epoch 76, batch 142 D Loss: 1.5941128730773926, G Loss: 0.6161274313926697\n",
      "Epoch 76, batch 143 D Loss: 1.5990034341812134, G Loss: 0.6173042058944702\n",
      "Epoch 76, batch 144 D Loss: 1.5910730361938477, G Loss: 0.564217746257782\n",
      "Epoch 76, batch 145 D Loss: 1.5640649795532227, G Loss: 0.6396459341049194\n",
      "Epoch 76, batch 146 D Loss: 1.5570662021636963, G Loss: 0.600277304649353\n",
      "Epoch 76, batch 147 D Loss: 1.5699541568756104, G Loss: 0.6117146611213684\n",
      "Epoch 76, batch 148 D Loss: 1.6684842109680176, G Loss: 0.5718339681625366\n",
      "Epoch 76, batch 149 D Loss: 1.6429181098937988, G Loss: 0.5823127627372742\n",
      "Epoch 76, batch 150 D Loss: 1.5247490406036377, G Loss: 0.6120509505271912\n",
      "Epoch 76, batch 151 D Loss: 1.4373599290847778, G Loss: 0.6569098830223083\n",
      "Epoch 76, batch 152 D Loss: 1.5756988525390625, G Loss: 0.6107094883918762\n",
      "Epoch 76, batch 153 D Loss: 1.511588454246521, G Loss: 0.6381078362464905\n",
      "Epoch 76, batch 154 D Loss: 1.444427251815796, G Loss: 0.6255765557289124\n",
      "Epoch 76, batch 155 D Loss: 1.5160404443740845, G Loss: 0.6635472774505615\n",
      "Epoch 76, batch 156 D Loss: 1.4874110221862793, G Loss: 0.6107161045074463\n",
      "Epoch 76, batch 157 D Loss: 1.6376346349716187, G Loss: 0.6234867572784424\n",
      "Epoch 76, batch 158 D Loss: 1.4234223365783691, G Loss: 0.6771327257156372\n",
      "Epoch 76, batch 159 D Loss: 1.3938753604888916, G Loss: 0.6986171007156372\n",
      "Epoch 76, batch 160 D Loss: 1.4681673049926758, G Loss: 0.6571549773216248\n",
      "Epoch 76, batch 161 D Loss: 1.4660470485687256, G Loss: 0.6884547472000122\n",
      "Epoch 76, batch 162 D Loss: 1.440556287765503, G Loss: 0.7216613292694092\n",
      "Epoch 76, batch 163 D Loss: 1.6213070154190063, G Loss: 0.6608108282089233\n",
      "Epoch 76, batch 164 D Loss: 1.5044755935668945, G Loss: 0.6911450028419495\n",
      "Epoch 76, batch 165 D Loss: 1.3814209699630737, G Loss: 0.7178666591644287\n",
      "Epoch 76, batch 166 D Loss: 1.3886120319366455, G Loss: 0.7491923570632935\n",
      "Epoch 76, batch 167 D Loss: 1.3756624460220337, G Loss: 0.7271496653556824\n",
      "Epoch 76, batch 168 D Loss: 1.4087951183319092, G Loss: 0.7646373510360718\n",
      "Epoch 76, batch 169 D Loss: 1.303039789199829, G Loss: 0.7940654158592224\n",
      "Epoch 76, batch 170 D Loss: 1.3056175708770752, G Loss: 0.832427978515625\n",
      "Epoch 76, batch 171 D Loss: 1.3852505683898926, G Loss: 0.7950928211212158\n",
      "Epoch 76, batch 172 D Loss: 1.2212953567504883, G Loss: 0.8294193744659424\n",
      "Epoch 76, batch 173 D Loss: 1.3199867010116577, G Loss: 0.8205453753471375\n",
      "Epoch 76, batch 174 D Loss: 1.2656761407852173, G Loss: 0.8312059640884399\n",
      "Epoch 76, batch 175 D Loss: 1.3464171886444092, G Loss: 0.8197996020317078\n",
      "Epoch 76, batch 176 D Loss: 1.2612419128417969, G Loss: 0.8809356689453125\n",
      "Epoch 76, batch 177 D Loss: 1.3523950576782227, G Loss: 0.7985526323318481\n",
      "Epoch 76, batch 178 D Loss: 1.2397520542144775, G Loss: 0.8825806975364685\n",
      "Epoch 76, batch 179 D Loss: 1.1829501390457153, G Loss: 0.8748798966407776\n",
      "Epoch 76, batch 180 D Loss: 1.2406179904937744, G Loss: 0.9400714635848999\n",
      "Epoch 76, batch 181 D Loss: 1.2073445320129395, G Loss: 0.9542297124862671\n",
      "Epoch 76, batch 182 D Loss: 1.3045326471328735, G Loss: 0.8803402185440063\n",
      "Epoch 76, batch 183 D Loss: 1.2456121444702148, G Loss: 0.9464048743247986\n",
      "Epoch 76, batch 184 D Loss: 1.2324090003967285, G Loss: 0.955036461353302\n",
      "Epoch 76, batch 185 D Loss: 1.2913780212402344, G Loss: 0.9705614447593689\n",
      "Epoch 76, batch 186 D Loss: 1.211599588394165, G Loss: 0.9864357113838196\n",
      "Epoch 76, batch 187 D Loss: 1.1846530437469482, G Loss: 0.9640952944755554\n",
      "Epoch 76, batch 188 D Loss: 1.2420918941497803, G Loss: 1.0150368213653564\n",
      "Epoch 76, batch 189 D Loss: 1.277145504951477, G Loss: 0.9822336435317993\n",
      "Epoch 76, batch 190 D Loss: 1.1499230861663818, G Loss: 0.9940510392189026\n",
      "Epoch 76, batch 191 D Loss: 1.1923516988754272, G Loss: 1.0517460107803345\n",
      "Epoch 76, batch 192 D Loss: 1.1395461559295654, G Loss: 1.0251803398132324\n",
      "Epoch 76, batch 193 D Loss: 1.0443472862243652, G Loss: 1.0928086042404175\n",
      "Epoch 76, batch 194 D Loss: 1.1322753429412842, G Loss: 1.0547627210617065\n",
      "Epoch 76, batch 195 D Loss: 1.1362249851226807, G Loss: 1.0443871021270752\n",
      "Epoch 76, batch 196 D Loss: 1.0157727003097534, G Loss: 1.0413870811462402\n",
      "Epoch 76, batch 197 D Loss: 1.0966825485229492, G Loss: 1.0697382688522339\n",
      "Epoch 76, batch 198 D Loss: 1.0349907875061035, G Loss: 1.0604180097579956\n",
      "Epoch 76, batch 199 D Loss: 1.1541426181793213, G Loss: 1.0499287843704224\n",
      "Epoch 76, batch 200 D Loss: 1.1100820302963257, G Loss: 1.079370379447937\n",
      "Epoch 77, batch 1 D Loss: 1.1159238815307617, G Loss: 1.0959923267364502\n",
      "Epoch 77, batch 2 D Loss: 1.108844518661499, G Loss: 1.0398961305618286\n",
      "Epoch 77, batch 3 D Loss: 1.1253180503845215, G Loss: 1.098771095275879\n",
      "Epoch 77, batch 4 D Loss: 1.1627180576324463, G Loss: 1.0874072313308716\n",
      "Epoch 77, batch 5 D Loss: 1.1051349639892578, G Loss: 1.1024270057678223\n",
      "Epoch 77, batch 6 D Loss: 0.9752339720726013, G Loss: 1.1211611032485962\n",
      "Epoch 77, batch 7 D Loss: 1.1723806858062744, G Loss: 1.0396239757537842\n",
      "Epoch 77, batch 8 D Loss: 1.1104648113250732, G Loss: 1.14240300655365\n",
      "Epoch 77, batch 9 D Loss: 1.0955920219421387, G Loss: 1.0758869647979736\n",
      "Epoch 77, batch 10 D Loss: 1.027650237083435, G Loss: 1.1357582807540894\n",
      "Epoch 77, batch 11 D Loss: 0.963842511177063, G Loss: 1.0967758893966675\n",
      "Epoch 77, batch 12 D Loss: 1.0005474090576172, G Loss: 1.1702603101730347\n",
      "Epoch 77, batch 13 D Loss: 1.0110340118408203, G Loss: 1.152780294418335\n",
      "Epoch 77, batch 14 D Loss: 1.2092256546020508, G Loss: 1.1087404489517212\n",
      "Epoch 77, batch 15 D Loss: 1.257007122039795, G Loss: 1.1306719779968262\n",
      "Epoch 77, batch 16 D Loss: 1.1204947233200073, G Loss: 1.1036183834075928\n",
      "Epoch 77, batch 17 D Loss: 1.1297518014907837, G Loss: 1.1376805305480957\n",
      "Epoch 77, batch 18 D Loss: 1.0076289176940918, G Loss: 1.1622275114059448\n",
      "Epoch 77, batch 19 D Loss: 1.0033836364746094, G Loss: 1.1020598411560059\n",
      "Epoch 77, batch 20 D Loss: 1.1412888765335083, G Loss: 1.1031891107559204\n",
      "Epoch 77, batch 21 D Loss: 1.1241880655288696, G Loss: 1.0914344787597656\n",
      "Epoch 77, batch 22 D Loss: 1.0748294591903687, G Loss: 1.1417063474655151\n",
      "Epoch 77, batch 23 D Loss: 0.9463972449302673, G Loss: 1.1034177541732788\n",
      "Epoch 77, batch 24 D Loss: 1.1142358779907227, G Loss: 1.1437842845916748\n",
      "Epoch 77, batch 25 D Loss: 1.033942699432373, G Loss: 1.09157395362854\n",
      "Epoch 77, batch 26 D Loss: 1.0303744077682495, G Loss: 1.121253252029419\n",
      "Epoch 77, batch 27 D Loss: 1.05857253074646, G Loss: 1.1117886304855347\n",
      "Epoch 77, batch 28 D Loss: 1.043792486190796, G Loss: 1.0640734434127808\n",
      "Epoch 77, batch 29 D Loss: 0.9941391944885254, G Loss: 1.071158528327942\n",
      "Epoch 77, batch 30 D Loss: 1.1019091606140137, G Loss: 1.1323764324188232\n",
      "Epoch 77, batch 31 D Loss: 1.1415796279907227, G Loss: 1.0898525714874268\n",
      "Epoch 77, batch 32 D Loss: 1.0859379768371582, G Loss: 1.0805082321166992\n",
      "Epoch 77, batch 33 D Loss: 1.1337676048278809, G Loss: 1.056659460067749\n",
      "Epoch 77, batch 34 D Loss: 1.1271049976348877, G Loss: 1.0982931852340698\n",
      "Epoch 77, batch 35 D Loss: 0.9992725253105164, G Loss: 1.142261266708374\n",
      "Epoch 77, batch 36 D Loss: 0.9286651015281677, G Loss: 1.1127405166625977\n",
      "Epoch 77, batch 37 D Loss: 1.0274269580841064, G Loss: 1.1403697729110718\n",
      "Epoch 77, batch 38 D Loss: 1.07671320438385, G Loss: 1.0850634574890137\n",
      "Epoch 77, batch 39 D Loss: 1.0474573373794556, G Loss: 1.083377480506897\n",
      "Epoch 77, batch 40 D Loss: 1.177966833114624, G Loss: 1.1628820896148682\n",
      "Epoch 77, batch 41 D Loss: 1.0016238689422607, G Loss: 1.1108314990997314\n",
      "Epoch 77, batch 42 D Loss: 1.104546070098877, G Loss: 1.1217715740203857\n",
      "Epoch 77, batch 43 D Loss: 1.103514313697815, G Loss: 1.0836820602416992\n",
      "Epoch 77, batch 44 D Loss: 1.0702646970748901, G Loss: 1.1073496341705322\n",
      "Epoch 77, batch 45 D Loss: 1.0391833782196045, G Loss: 1.0953484773635864\n",
      "Epoch 77, batch 46 D Loss: 1.0918554067611694, G Loss: 1.0679529905319214\n",
      "Epoch 77, batch 47 D Loss: 1.0045439004898071, G Loss: 1.1283299922943115\n",
      "Epoch 77, batch 48 D Loss: 1.0026968717575073, G Loss: 1.1294066905975342\n",
      "Epoch 77, batch 49 D Loss: 1.019365906715393, G Loss: 1.1122545003890991\n",
      "Epoch 77, batch 50 D Loss: 1.1401654481887817, G Loss: 1.0425130128860474\n",
      "Epoch 77, batch 51 D Loss: 1.0880522727966309, G Loss: 1.0890597105026245\n",
      "Epoch 77, batch 52 D Loss: 1.1310616731643677, G Loss: 1.042435884475708\n",
      "Epoch 77, batch 53 D Loss: 1.1627044677734375, G Loss: 0.9842905402183533\n",
      "Epoch 77, batch 54 D Loss: 1.1195430755615234, G Loss: 1.0909863710403442\n",
      "Epoch 77, batch 55 D Loss: 1.2171006202697754, G Loss: 1.033897042274475\n",
      "Epoch 77, batch 56 D Loss: 1.1495468616485596, G Loss: 1.0581212043762207\n",
      "Epoch 77, batch 57 D Loss: 1.0010684728622437, G Loss: 1.0892494916915894\n",
      "Epoch 77, batch 58 D Loss: 1.1626728773117065, G Loss: 1.0212537050247192\n",
      "Epoch 77, batch 59 D Loss: 1.107457160949707, G Loss: 1.087777853012085\n",
      "Epoch 77, batch 60 D Loss: 1.1375616788864136, G Loss: 1.0056672096252441\n",
      "Epoch 77, batch 61 D Loss: 1.1625629663467407, G Loss: 1.0424861907958984\n",
      "Epoch 77, batch 62 D Loss: 1.1135244369506836, G Loss: 0.9976564049720764\n",
      "Epoch 77, batch 63 D Loss: 1.3530564308166504, G Loss: 1.0365893840789795\n",
      "Epoch 77, batch 64 D Loss: 0.9878838062286377, G Loss: 1.0309230089187622\n",
      "Epoch 77, batch 65 D Loss: 1.2535955905914307, G Loss: 1.029931902885437\n",
      "Epoch 77, batch 66 D Loss: 1.1291165351867676, G Loss: 0.9632745385169983\n",
      "Epoch 77, batch 67 D Loss: 1.1841944456100464, G Loss: 0.9456591606140137\n",
      "Epoch 77, batch 68 D Loss: 1.2856557369232178, G Loss: 0.9549912810325623\n",
      "Epoch 77, batch 69 D Loss: 1.1377841234207153, G Loss: 0.9650682210922241\n",
      "Epoch 77, batch 70 D Loss: 1.166367769241333, G Loss: 1.0683437585830688\n",
      "Epoch 77, batch 71 D Loss: 1.1333552598953247, G Loss: 1.0357393026351929\n",
      "Epoch 77, batch 72 D Loss: 1.1994516849517822, G Loss: 1.0290169715881348\n",
      "Epoch 77, batch 73 D Loss: 1.2574653625488281, G Loss: 0.8442996144294739\n",
      "Epoch 77, batch 74 D Loss: 1.1430743932724, G Loss: 1.0162314176559448\n",
      "Epoch 77, batch 75 D Loss: 1.1704899072647095, G Loss: 1.0595241785049438\n",
      "Epoch 77, batch 76 D Loss: 1.186570644378662, G Loss: 0.9588893055915833\n",
      "Epoch 77, batch 77 D Loss: 1.3421998023986816, G Loss: 0.9819886684417725\n",
      "Epoch 77, batch 78 D Loss: 1.230513334274292, G Loss: 0.9533849358558655\n",
      "Epoch 77, batch 79 D Loss: 1.281160831451416, G Loss: 0.9467166066169739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, batch 80 D Loss: 1.1990896463394165, G Loss: 0.8604701161384583\n",
      "Epoch 77, batch 81 D Loss: 1.2062126398086548, G Loss: 0.9302569627761841\n",
      "Epoch 77, batch 82 D Loss: 1.1731538772583008, G Loss: 0.9265667200088501\n",
      "Epoch 77, batch 83 D Loss: 1.2026546001434326, G Loss: 0.9444215297698975\n",
      "Epoch 77, batch 84 D Loss: 1.3692184686660767, G Loss: 0.9243674278259277\n",
      "Epoch 77, batch 85 D Loss: 1.1097662448883057, G Loss: 0.9961216449737549\n",
      "Epoch 77, batch 86 D Loss: 1.1657276153564453, G Loss: 0.9384231567382812\n",
      "Epoch 77, batch 87 D Loss: 1.2544069290161133, G Loss: 1.012069821357727\n",
      "Epoch 77, batch 88 D Loss: 1.0973358154296875, G Loss: 0.985121488571167\n",
      "Epoch 77, batch 89 D Loss: 1.0613701343536377, G Loss: 1.000887155532837\n",
      "Epoch 77, batch 90 D Loss: 1.2037265300750732, G Loss: 0.921185314655304\n",
      "Epoch 77, batch 91 D Loss: 1.309248447418213, G Loss: 0.8954375982284546\n",
      "Epoch 77, batch 92 D Loss: 1.1988143920898438, G Loss: 1.0005507469177246\n",
      "Epoch 77, batch 93 D Loss: 1.3844130039215088, G Loss: 0.768086314201355\n",
      "Epoch 77, batch 94 D Loss: 1.1789801120758057, G Loss: 0.888715386390686\n",
      "Epoch 77, batch 95 D Loss: 1.2166123390197754, G Loss: 0.8495232462882996\n",
      "Epoch 77, batch 96 D Loss: 1.260391354560852, G Loss: 0.7898992300033569\n",
      "Epoch 77, batch 97 D Loss: 1.16164231300354, G Loss: 0.8591227531433105\n",
      "Epoch 77, batch 98 D Loss: 1.2083184719085693, G Loss: 0.8550401926040649\n",
      "Epoch 77, batch 99 D Loss: 1.3197550773620605, G Loss: 0.9433955550193787\n",
      "Epoch 77, batch 100 D Loss: 1.3712143898010254, G Loss: 0.8716716766357422\n",
      "Epoch 77, batch 101 D Loss: 1.303175687789917, G Loss: 0.8891969919204712\n",
      "Epoch 77, batch 102 D Loss: 1.2663332223892212, G Loss: 0.8165999054908752\n",
      "Epoch 77, batch 103 D Loss: 1.4074923992156982, G Loss: 0.730975329875946\n",
      "Epoch 77, batch 104 D Loss: 1.3424009084701538, G Loss: 0.7788010239601135\n",
      "Epoch 77, batch 105 D Loss: 1.1875553131103516, G Loss: 0.8534960746765137\n",
      "Epoch 77, batch 106 D Loss: 1.2821288108825684, G Loss: 0.794979453086853\n",
      "Epoch 77, batch 107 D Loss: 1.1652265787124634, G Loss: 0.7939169406890869\n",
      "Epoch 77, batch 108 D Loss: 1.415812611579895, G Loss: 0.8097435235977173\n",
      "Epoch 77, batch 109 D Loss: 1.315977692604065, G Loss: 0.8537602424621582\n",
      "Epoch 77, batch 110 D Loss: 1.3355354070663452, G Loss: 0.7755950093269348\n",
      "Epoch 77, batch 111 D Loss: 1.1839230060577393, G Loss: 0.7943207025527954\n",
      "Epoch 77, batch 112 D Loss: 1.4704903364181519, G Loss: 0.7262107133865356\n",
      "Epoch 77, batch 113 D Loss: 1.3432923555374146, G Loss: 0.7426244616508484\n",
      "Epoch 77, batch 114 D Loss: 1.2871460914611816, G Loss: 0.7530120611190796\n",
      "Epoch 77, batch 115 D Loss: 1.288744330406189, G Loss: 0.7965285778045654\n",
      "Epoch 77, batch 116 D Loss: 1.3133468627929688, G Loss: 0.8159234523773193\n",
      "Epoch 77, batch 117 D Loss: 1.3779184818267822, G Loss: 0.7316899299621582\n",
      "Epoch 77, batch 118 D Loss: 1.361199140548706, G Loss: 0.7200239300727844\n",
      "Epoch 77, batch 119 D Loss: 1.2944742441177368, G Loss: 0.7693734169006348\n",
      "Epoch 77, batch 120 D Loss: 1.2811249494552612, G Loss: 0.7153728604316711\n",
      "Epoch 77, batch 121 D Loss: 1.2136996984481812, G Loss: 0.7289790511131287\n",
      "Epoch 77, batch 122 D Loss: 1.2919361591339111, G Loss: 0.7639337182044983\n",
      "Epoch 77, batch 123 D Loss: 1.2191232442855835, G Loss: 0.699567437171936\n",
      "Epoch 77, batch 124 D Loss: 1.2466883659362793, G Loss: 0.721099317073822\n",
      "Epoch 77, batch 125 D Loss: 1.1961848735809326, G Loss: 0.7247812747955322\n",
      "Epoch 77, batch 126 D Loss: 1.3932790756225586, G Loss: 0.7241909503936768\n",
      "Epoch 77, batch 127 D Loss: 1.3008880615234375, G Loss: 0.7164131999015808\n",
      "Epoch 77, batch 128 D Loss: 1.3798896074295044, G Loss: 0.7609187960624695\n",
      "Epoch 77, batch 129 D Loss: 1.182870864868164, G Loss: 0.8018304705619812\n",
      "Epoch 77, batch 130 D Loss: 1.3575717210769653, G Loss: 0.7379794120788574\n",
      "Epoch 77, batch 131 D Loss: 1.1915111541748047, G Loss: 0.711754560470581\n",
      "Epoch 77, batch 132 D Loss: 1.2753078937530518, G Loss: 0.7096119523048401\n",
      "Epoch 77, batch 133 D Loss: 1.2602832317352295, G Loss: 0.723893940448761\n",
      "Epoch 77, batch 134 D Loss: 1.2952828407287598, G Loss: 0.7726260423660278\n",
      "Epoch 77, batch 135 D Loss: 1.168607473373413, G Loss: 0.8076626062393188\n",
      "Epoch 77, batch 136 D Loss: 1.2571122646331787, G Loss: 0.7409768104553223\n",
      "Epoch 77, batch 137 D Loss: 1.340279221534729, G Loss: 0.7717707753181458\n",
      "Epoch 77, batch 138 D Loss: 1.2107956409454346, G Loss: 0.7809178829193115\n",
      "Epoch 77, batch 139 D Loss: 1.4430413246154785, G Loss: 0.7406732439994812\n",
      "Epoch 77, batch 140 D Loss: 1.2481318712234497, G Loss: 0.8072294592857361\n",
      "Epoch 77, batch 141 D Loss: 1.46439790725708, G Loss: 0.7582233548164368\n",
      "Epoch 77, batch 142 D Loss: 1.1766268014907837, G Loss: 0.778216540813446\n",
      "Epoch 77, batch 143 D Loss: 1.2004908323287964, G Loss: 0.8302901983261108\n",
      "Epoch 77, batch 144 D Loss: 1.2635458707809448, G Loss: 0.7969802021980286\n",
      "Epoch 77, batch 145 D Loss: 1.2802491188049316, G Loss: 0.7481580972671509\n",
      "Epoch 77, batch 146 D Loss: 1.2830193042755127, G Loss: 0.8006294369697571\n",
      "Epoch 77, batch 147 D Loss: 1.3874588012695312, G Loss: 0.7652435302734375\n",
      "Epoch 77, batch 148 D Loss: 1.3388638496398926, G Loss: 0.7343537211418152\n",
      "Epoch 77, batch 149 D Loss: 1.3063912391662598, G Loss: 0.7734997272491455\n",
      "Epoch 77, batch 150 D Loss: 1.2819935083389282, G Loss: 0.7764779925346375\n",
      "Epoch 77, batch 151 D Loss: 1.221609354019165, G Loss: 0.7747254371643066\n",
      "Epoch 77, batch 152 D Loss: 1.1991451978683472, G Loss: 0.8020362257957458\n",
      "Epoch 77, batch 153 D Loss: 1.3856441974639893, G Loss: 0.7841739654541016\n",
      "Epoch 77, batch 154 D Loss: 1.2922871112823486, G Loss: 0.7982731461524963\n",
      "Epoch 77, batch 155 D Loss: 1.2898600101470947, G Loss: 0.8066011071205139\n",
      "Epoch 77, batch 156 D Loss: 1.3375389575958252, G Loss: 0.7928311824798584\n",
      "Epoch 77, batch 157 D Loss: 1.106825828552246, G Loss: 0.8435155749320984\n",
      "Epoch 77, batch 158 D Loss: 1.249349594116211, G Loss: 0.8379026651382446\n",
      "Epoch 77, batch 159 D Loss: 1.1977167129516602, G Loss: 0.8357040882110596\n",
      "Epoch 77, batch 160 D Loss: 1.431410789489746, G Loss: 0.8547695279121399\n",
      "Epoch 77, batch 161 D Loss: 1.3001517057418823, G Loss: 0.8579466342926025\n",
      "Epoch 77, batch 162 D Loss: 1.2114362716674805, G Loss: 0.8444516062736511\n",
      "Epoch 77, batch 163 D Loss: 1.1216769218444824, G Loss: 0.8852041363716125\n",
      "Epoch 77, batch 164 D Loss: 1.2332488298416138, G Loss: 0.8706558346748352\n",
      "Epoch 77, batch 165 D Loss: 1.2289233207702637, G Loss: 0.8024569749832153\n",
      "Epoch 77, batch 166 D Loss: 1.1365433931350708, G Loss: 0.8945067524909973\n",
      "Epoch 77, batch 167 D Loss: 1.1721978187561035, G Loss: 0.9152706265449524\n",
      "Epoch 77, batch 168 D Loss: 1.185125708580017, G Loss: 0.9028720855712891\n",
      "Epoch 77, batch 169 D Loss: 1.3104443550109863, G Loss: 0.9292457699775696\n",
      "Epoch 77, batch 170 D Loss: 1.1005014181137085, G Loss: 0.9689704775810242\n",
      "Epoch 77, batch 171 D Loss: 1.2210371494293213, G Loss: 0.9124982953071594\n",
      "Epoch 77, batch 172 D Loss: 1.2317640781402588, G Loss: 0.8995843529701233\n",
      "Epoch 77, batch 173 D Loss: 1.2898073196411133, G Loss: 0.8942674398422241\n",
      "Epoch 77, batch 174 D Loss: 1.43044114112854, G Loss: 0.8637479543685913\n",
      "Epoch 77, batch 175 D Loss: 1.279546856880188, G Loss: 0.9229435920715332\n",
      "Epoch 77, batch 176 D Loss: 1.3626022338867188, G Loss: 0.949354887008667\n",
      "Epoch 77, batch 177 D Loss: 1.3015499114990234, G Loss: 0.8237394094467163\n",
      "Epoch 77, batch 178 D Loss: 1.3905247449874878, G Loss: 0.902616560459137\n",
      "Epoch 77, batch 179 D Loss: 1.1892259120941162, G Loss: 0.823253333568573\n",
      "Epoch 77, batch 180 D Loss: 1.0808382034301758, G Loss: 0.9182581901550293\n",
      "Epoch 77, batch 181 D Loss: 1.048394799232483, G Loss: 0.977367639541626\n",
      "Epoch 77, batch 182 D Loss: 1.155961036682129, G Loss: 0.9398604035377502\n",
      "Epoch 77, batch 183 D Loss: 1.245033621788025, G Loss: 0.9010140895843506\n",
      "Epoch 77, batch 184 D Loss: 1.2582893371582031, G Loss: 1.0177679061889648\n",
      "Epoch 77, batch 185 D Loss: 1.2383729219436646, G Loss: 0.9356087446212769\n",
      "Epoch 77, batch 186 D Loss: 1.3199626207351685, G Loss: 0.937497079372406\n",
      "Epoch 77, batch 187 D Loss: 1.1591160297393799, G Loss: 0.9380750060081482\n",
      "Epoch 77, batch 188 D Loss: 1.2025644779205322, G Loss: 0.917559802532196\n",
      "Epoch 77, batch 189 D Loss: 1.1558785438537598, G Loss: 0.9628036618232727\n",
      "Epoch 77, batch 190 D Loss: 1.171759843826294, G Loss: 0.9562603831291199\n",
      "Epoch 77, batch 191 D Loss: 1.037911295890808, G Loss: 1.0215389728546143\n",
      "Epoch 77, batch 192 D Loss: 1.1557891368865967, G Loss: 0.9368417263031006\n",
      "Epoch 77, batch 193 D Loss: 1.2187962532043457, G Loss: 0.9705265164375305\n",
      "Epoch 77, batch 194 D Loss: 1.2443355321884155, G Loss: 0.9123592376708984\n",
      "Epoch 77, batch 195 D Loss: 1.1410996913909912, G Loss: 1.0177984237670898\n",
      "Epoch 77, batch 196 D Loss: 1.2424101829528809, G Loss: 0.9686237573623657\n",
      "Epoch 77, batch 197 D Loss: 1.1782032251358032, G Loss: 0.930788516998291\n",
      "Epoch 77, batch 198 D Loss: 1.243260145187378, G Loss: 0.8946830034255981\n",
      "Epoch 77, batch 199 D Loss: 1.3112683296203613, G Loss: 0.9033069014549255\n",
      "Epoch 77, batch 200 D Loss: 1.4400397539138794, G Loss: 0.8473412990570068\n",
      "Epoch 78, batch 1 D Loss: 1.1946940422058105, G Loss: 0.9925493001937866\n",
      "Epoch 78, batch 2 D Loss: 1.172957420349121, G Loss: 0.934832751750946\n",
      "Epoch 78, batch 3 D Loss: 1.1790571212768555, G Loss: 1.0181421041488647\n",
      "Epoch 78, batch 4 D Loss: 1.0787676572799683, G Loss: 1.0146682262420654\n",
      "Epoch 78, batch 5 D Loss: 1.1917471885681152, G Loss: 0.9302350878715515\n",
      "Epoch 78, batch 6 D Loss: 1.271923542022705, G Loss: 0.8817965984344482\n",
      "Epoch 78, batch 7 D Loss: 1.297881841659546, G Loss: 0.9656072854995728\n",
      "Epoch 78, batch 8 D Loss: 1.139928936958313, G Loss: 0.9948663115501404\n",
      "Epoch 78, batch 9 D Loss: 1.1920318603515625, G Loss: 0.9194371700286865\n",
      "Epoch 78, batch 10 D Loss: 1.2457369565963745, G Loss: 0.9475917220115662\n",
      "Epoch 78, batch 11 D Loss: 1.0714867115020752, G Loss: 0.9949498176574707\n",
      "Epoch 78, batch 12 D Loss: 1.0589501857757568, G Loss: 0.9802911877632141\n",
      "Epoch 78, batch 13 D Loss: 1.2263832092285156, G Loss: 0.9339340329170227\n",
      "Epoch 78, batch 14 D Loss: 1.2481346130371094, G Loss: 0.9184035658836365\n",
      "Epoch 78, batch 15 D Loss: 1.239331841468811, G Loss: 1.0424004793167114\n",
      "Epoch 78, batch 16 D Loss: 1.1834132671356201, G Loss: 0.9515804052352905\n",
      "Epoch 78, batch 17 D Loss: 1.2193171977996826, G Loss: 0.9137817621231079\n",
      "Epoch 78, batch 18 D Loss: 1.1418780088424683, G Loss: 1.0210649967193604\n",
      "Epoch 78, batch 19 D Loss: 1.2666062116622925, G Loss: 0.9520036578178406\n",
      "Epoch 78, batch 20 D Loss: 1.2195475101470947, G Loss: 1.0265141725540161\n",
      "Epoch 78, batch 21 D Loss: 1.0019588470458984, G Loss: 1.1080775260925293\n",
      "Epoch 78, batch 22 D Loss: 1.2101190090179443, G Loss: 1.0756785869598389\n",
      "Epoch 78, batch 23 D Loss: 1.1906306743621826, G Loss: 1.0449120998382568\n",
      "Epoch 78, batch 24 D Loss: 1.1108314990997314, G Loss: 1.0285992622375488\n",
      "Epoch 78, batch 25 D Loss: 1.1581454277038574, G Loss: 1.0153025388717651\n",
      "Epoch 78, batch 26 D Loss: 1.4058263301849365, G Loss: 0.9549014568328857\n",
      "Epoch 78, batch 27 D Loss: 1.1897461414337158, G Loss: 1.0339760780334473\n",
      "Epoch 78, batch 28 D Loss: 1.1011290550231934, G Loss: 1.0193277597427368\n",
      "Epoch 78, batch 29 D Loss: 1.2107244729995728, G Loss: 1.0319234132766724\n",
      "Epoch 78, batch 30 D Loss: 1.2329745292663574, G Loss: 1.0289820432662964\n",
      "Epoch 78, batch 31 D Loss: 1.1442337036132812, G Loss: 1.0496857166290283\n",
      "Epoch 78, batch 32 D Loss: 1.2652416229248047, G Loss: 1.0086960792541504\n",
      "Epoch 78, batch 33 D Loss: 1.1366517543792725, G Loss: 0.985715925693512\n",
      "Epoch 78, batch 34 D Loss: 1.2087829113006592, G Loss: 0.994953989982605\n",
      "Epoch 78, batch 35 D Loss: 1.2301464080810547, G Loss: 0.9837303161621094\n",
      "Epoch 78, batch 36 D Loss: 1.1341116428375244, G Loss: 1.1006923913955688\n",
      "Epoch 78, batch 37 D Loss: 1.2442984580993652, G Loss: 0.9787124395370483\n",
      "Epoch 78, batch 38 D Loss: 1.3271440267562866, G Loss: 1.0440586805343628\n",
      "Epoch 78, batch 39 D Loss: 1.1810202598571777, G Loss: 1.0600194931030273\n",
      "Epoch 78, batch 40 D Loss: 1.1956326961517334, G Loss: 1.0749776363372803\n",
      "Epoch 78, batch 41 D Loss: 1.1184756755828857, G Loss: 1.0632785558700562\n",
      "Epoch 78, batch 42 D Loss: 1.1551491022109985, G Loss: 1.0926493406295776\n",
      "Epoch 78, batch 43 D Loss: 1.1583638191223145, G Loss: 1.0905014276504517\n",
      "Epoch 78, batch 44 D Loss: 1.3835411071777344, G Loss: 1.0518912076950073\n",
      "Epoch 78, batch 45 D Loss: 1.076646327972412, G Loss: 1.1387629508972168\n",
      "Epoch 78, batch 46 D Loss: 1.1585686206817627, G Loss: 1.0668511390686035\n",
      "Epoch 78, batch 47 D Loss: 1.0758516788482666, G Loss: 1.1611590385437012\n",
      "Epoch 78, batch 48 D Loss: 1.126960277557373, G Loss: 1.0987309217453003\n",
      "Epoch 78, batch 49 D Loss: 1.1047630310058594, G Loss: 1.1296626329421997\n",
      "Epoch 78, batch 50 D Loss: 1.1187822818756104, G Loss: 1.1126658916473389\n",
      "Epoch 78, batch 51 D Loss: 1.1798858642578125, G Loss: 1.2417840957641602\n",
      "Epoch 78, batch 52 D Loss: 1.0342512130737305, G Loss: 1.0850502252578735\n",
      "Epoch 78, batch 53 D Loss: 1.1879234313964844, G Loss: 1.0733118057250977\n",
      "Epoch 78, batch 54 D Loss: 1.3566772937774658, G Loss: 1.0650912523269653\n",
      "Epoch 78, batch 55 D Loss: 1.2698981761932373, G Loss: 1.1445289850234985\n",
      "Epoch 78, batch 56 D Loss: 1.1230111122131348, G Loss: 1.117601990699768\n",
      "Epoch 78, batch 57 D Loss: 1.2081201076507568, G Loss: 1.0287132263183594\n",
      "Epoch 78, batch 58 D Loss: 1.0427050590515137, G Loss: 1.2282685041427612\n",
      "Epoch 78, batch 59 D Loss: 1.0139321088790894, G Loss: 1.1873821020126343\n",
      "Epoch 78, batch 60 D Loss: 1.0188701152801514, G Loss: 1.2236818075180054\n",
      "Epoch 78, batch 61 D Loss: 1.2608890533447266, G Loss: 1.187248706817627\n",
      "Epoch 78, batch 62 D Loss: 1.2696921825408936, G Loss: 1.1798092126846313\n",
      "Epoch 78, batch 63 D Loss: 1.1029964685440063, G Loss: 1.1536787748336792\n",
      "Epoch 78, batch 64 D Loss: 1.1637275218963623, G Loss: 1.1569170951843262\n",
      "Epoch 78, batch 65 D Loss: 1.0757216215133667, G Loss: 1.142139196395874\n",
      "Epoch 78, batch 66 D Loss: 1.1541517972946167, G Loss: 1.2213129997253418\n",
      "Epoch 78, batch 67 D Loss: 1.045308232307434, G Loss: 1.1908036470413208\n",
      "Epoch 78, batch 68 D Loss: 1.1023445129394531, G Loss: 1.1596524715423584\n",
      "Epoch 78, batch 69 D Loss: 1.235650897026062, G Loss: 1.179297924041748\n",
      "Epoch 78, batch 70 D Loss: 1.2752126455307007, G Loss: 1.1961466073989868\n",
      "Epoch 78, batch 71 D Loss: 1.1908551454544067, G Loss: 1.0636967420578003\n",
      "Epoch 78, batch 72 D Loss: 1.2191411256790161, G Loss: 1.146704912185669\n",
      "Epoch 78, batch 73 D Loss: 1.2806651592254639, G Loss: 1.197360873222351\n",
      "Epoch 78, batch 74 D Loss: 0.9920544624328613, G Loss: 1.2051305770874023\n",
      "Epoch 78, batch 75 D Loss: 1.207883358001709, G Loss: 1.089808702468872\n",
      "Epoch 78, batch 76 D Loss: 1.1280750036239624, G Loss: 1.1188013553619385\n",
      "Epoch 78, batch 77 D Loss: 1.0981073379516602, G Loss: 1.154000997543335\n",
      "Epoch 78, batch 78 D Loss: 1.1286710500717163, G Loss: 1.183173656463623\n",
      "Epoch 78, batch 79 D Loss: 1.2107224464416504, G Loss: 1.1818581819534302\n",
      "Epoch 78, batch 80 D Loss: 1.2651262283325195, G Loss: 1.0405070781707764\n",
      "Epoch 78, batch 81 D Loss: 1.1593248844146729, G Loss: 1.1464625597000122\n",
      "Epoch 78, batch 82 D Loss: 1.0383623838424683, G Loss: 1.1680686473846436\n",
      "Epoch 78, batch 83 D Loss: 1.156498908996582, G Loss: 1.1493605375289917\n",
      "Epoch 78, batch 84 D Loss: 1.1984615325927734, G Loss: 1.0684151649475098\n",
      "Epoch 78, batch 85 D Loss: 1.1679943799972534, G Loss: 1.0498120784759521\n",
      "Epoch 78, batch 86 D Loss: 1.2979015111923218, G Loss: 1.0940145254135132\n",
      "Epoch 78, batch 87 D Loss: 1.3177820444107056, G Loss: 1.0817956924438477\n",
      "Epoch 78, batch 88 D Loss: 1.33840012550354, G Loss: 1.0191973447799683\n",
      "Epoch 78, batch 89 D Loss: 1.2578582763671875, G Loss: 1.0193110704421997\n",
      "Epoch 78, batch 90 D Loss: 1.3061487674713135, G Loss: 0.9962277412414551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, batch 91 D Loss: 1.2671867609024048, G Loss: 1.0304954051971436\n",
      "Epoch 78, batch 92 D Loss: 1.2255204916000366, G Loss: 1.0346124172210693\n",
      "Epoch 78, batch 93 D Loss: 1.1550536155700684, G Loss: 1.0737448930740356\n",
      "Epoch 78, batch 94 D Loss: 1.3765199184417725, G Loss: 0.9445743560791016\n",
      "Epoch 78, batch 95 D Loss: 1.1940253973007202, G Loss: 0.9586469531059265\n",
      "Epoch 78, batch 96 D Loss: 1.164487361907959, G Loss: 0.9920909404754639\n",
      "Epoch 78, batch 97 D Loss: 1.2137832641601562, G Loss: 0.9747357964515686\n",
      "Epoch 78, batch 98 D Loss: 1.3117010593414307, G Loss: 0.9532231092453003\n",
      "Epoch 78, batch 99 D Loss: 1.42316472530365, G Loss: 0.8996846079826355\n",
      "Epoch 78, batch 100 D Loss: 1.2943850755691528, G Loss: 0.9299108386039734\n",
      "Epoch 78, batch 101 D Loss: 1.202796220779419, G Loss: 0.9683211445808411\n",
      "Epoch 78, batch 102 D Loss: 1.411178469657898, G Loss: 0.8336901068687439\n",
      "Epoch 78, batch 103 D Loss: 1.522146224975586, G Loss: 0.8577151298522949\n",
      "Epoch 78, batch 104 D Loss: 1.342755675315857, G Loss: 0.876926064491272\n",
      "Epoch 78, batch 105 D Loss: 1.4475505352020264, G Loss: 0.9175931811332703\n",
      "Epoch 78, batch 106 D Loss: 1.7071276903152466, G Loss: 0.8174566030502319\n",
      "Epoch 78, batch 107 D Loss: 1.6740813255310059, G Loss: 0.8074038028717041\n",
      "Epoch 78, batch 108 D Loss: 1.4892557859420776, G Loss: 0.8361527323722839\n",
      "Epoch 78, batch 109 D Loss: 1.4234665632247925, G Loss: 0.8169522881507874\n",
      "Epoch 78, batch 110 D Loss: 1.6269950866699219, G Loss: 0.7241285443305969\n",
      "Epoch 78, batch 111 D Loss: 1.3794116973876953, G Loss: 0.8335075378417969\n",
      "Epoch 78, batch 112 D Loss: 1.493001103401184, G Loss: 0.8196462392807007\n",
      "Epoch 78, batch 113 D Loss: 1.6922048330307007, G Loss: 0.7436502575874329\n",
      "Epoch 78, batch 114 D Loss: 1.626500129699707, G Loss: 0.7998998761177063\n",
      "Epoch 78, batch 115 D Loss: 1.5437157154083252, G Loss: 0.7825331091880798\n",
      "Epoch 78, batch 116 D Loss: 1.611912488937378, G Loss: 0.832343339920044\n",
      "Epoch 78, batch 117 D Loss: 1.588526725769043, G Loss: 0.7307031750679016\n",
      "Epoch 78, batch 118 D Loss: 1.6743824481964111, G Loss: 0.7897851467132568\n",
      "Epoch 78, batch 119 D Loss: 1.4094529151916504, G Loss: 0.8014917969703674\n",
      "Epoch 78, batch 120 D Loss: 1.6814064979553223, G Loss: 0.7700741291046143\n",
      "Epoch 78, batch 121 D Loss: 1.7249168157577515, G Loss: 0.7680309414863586\n",
      "Epoch 78, batch 122 D Loss: 1.2806626558303833, G Loss: 0.8278295397758484\n",
      "Epoch 78, batch 123 D Loss: 1.4173328876495361, G Loss: 0.8065531253814697\n",
      "Epoch 78, batch 124 D Loss: 1.3691905736923218, G Loss: 0.8436628580093384\n",
      "Epoch 78, batch 125 D Loss: 1.4530858993530273, G Loss: 0.8283277153968811\n",
      "Epoch 78, batch 126 D Loss: 1.3614028692245483, G Loss: 0.9111404418945312\n",
      "Epoch 78, batch 127 D Loss: 1.531085729598999, G Loss: 0.8486654758453369\n",
      "Epoch 78, batch 128 D Loss: 1.5211923122406006, G Loss: 0.9279842376708984\n",
      "Epoch 78, batch 129 D Loss: 1.505835771560669, G Loss: 1.0049644708633423\n",
      "Epoch 78, batch 130 D Loss: 1.3981354236602783, G Loss: 0.8978674411773682\n",
      "Epoch 78, batch 131 D Loss: 1.6413884162902832, G Loss: 0.9059886932373047\n",
      "Epoch 78, batch 132 D Loss: 1.4876296520233154, G Loss: 0.9394638538360596\n",
      "Epoch 78, batch 133 D Loss: 1.504088282585144, G Loss: 0.9601355195045471\n",
      "Epoch 78, batch 134 D Loss: 1.5163732767105103, G Loss: 1.0677409172058105\n",
      "Epoch 78, batch 135 D Loss: 1.5113431215286255, G Loss: 0.9691506028175354\n",
      "Epoch 78, batch 136 D Loss: 1.344056487083435, G Loss: 0.9862349033355713\n",
      "Epoch 78, batch 137 D Loss: 1.5435194969177246, G Loss: 0.9440762996673584\n",
      "Epoch 78, batch 138 D Loss: 1.477988600730896, G Loss: 0.9689352512359619\n",
      "Epoch 78, batch 139 D Loss: 1.4321279525756836, G Loss: 1.0084718465805054\n",
      "Epoch 78, batch 140 D Loss: 1.4770451784133911, G Loss: 1.2450555562973022\n",
      "Epoch 78, batch 141 D Loss: 1.3066279888153076, G Loss: 1.192574143409729\n",
      "Epoch 78, batch 142 D Loss: 1.3815193176269531, G Loss: 1.1857928037643433\n",
      "Epoch 78, batch 143 D Loss: 1.558050274848938, G Loss: 1.056809663772583\n",
      "Epoch 78, batch 144 D Loss: 1.2401304244995117, G Loss: 1.2994412183761597\n",
      "Epoch 78, batch 145 D Loss: 1.3852052688598633, G Loss: 1.2490925788879395\n",
      "Epoch 78, batch 146 D Loss: 1.4409499168395996, G Loss: 1.1436285972595215\n",
      "Epoch 78, batch 147 D Loss: 1.4704370498657227, G Loss: 1.1772361993789673\n",
      "Epoch 78, batch 148 D Loss: 1.530278205871582, G Loss: 1.240783452987671\n",
      "Epoch 78, batch 149 D Loss: 1.645546793937683, G Loss: 1.2011327743530273\n",
      "Epoch 78, batch 150 D Loss: 1.3271087408065796, G Loss: 1.2258356809616089\n",
      "Epoch 78, batch 151 D Loss: 1.6672697067260742, G Loss: 1.3112716674804688\n",
      "Epoch 78, batch 152 D Loss: 1.627041220664978, G Loss: 1.2605971097946167\n",
      "Epoch 78, batch 153 D Loss: 1.3829872608184814, G Loss: 1.305273175239563\n",
      "Epoch 78, batch 154 D Loss: 1.3348205089569092, G Loss: 1.2358543872833252\n",
      "Epoch 78, batch 155 D Loss: 1.771193504333496, G Loss: 1.3093620538711548\n",
      "Epoch 78, batch 156 D Loss: 1.4202661514282227, G Loss: 1.3136993646621704\n",
      "Epoch 78, batch 157 D Loss: 1.6065146923065186, G Loss: 1.2385530471801758\n",
      "Epoch 78, batch 158 D Loss: 1.3818447589874268, G Loss: 1.2064168453216553\n",
      "Epoch 78, batch 159 D Loss: 1.6552857160568237, G Loss: 1.207771897315979\n",
      "Epoch 78, batch 160 D Loss: 1.3331820964813232, G Loss: 1.2297862768173218\n",
      "Epoch 78, batch 161 D Loss: 1.367828130722046, G Loss: 1.1926565170288086\n",
      "Epoch 78, batch 162 D Loss: 1.6764798164367676, G Loss: 1.3154634237289429\n",
      "Epoch 78, batch 163 D Loss: 1.5845140218734741, G Loss: 1.1451417207717896\n",
      "Epoch 78, batch 164 D Loss: 1.3459986448287964, G Loss: 1.0820013284683228\n",
      "Epoch 78, batch 165 D Loss: 1.5694384574890137, G Loss: 1.306972622871399\n",
      "Epoch 78, batch 166 D Loss: 1.3912136554718018, G Loss: 1.2146146297454834\n",
      "Epoch 78, batch 167 D Loss: 1.3549354076385498, G Loss: 1.0557641983032227\n",
      "Epoch 78, batch 168 D Loss: 1.2321085929870605, G Loss: 1.333178997039795\n",
      "Epoch 78, batch 169 D Loss: 1.296307921409607, G Loss: 1.139041543006897\n",
      "Epoch 78, batch 170 D Loss: 1.4328725337982178, G Loss: 1.0417897701263428\n",
      "Epoch 78, batch 171 D Loss: 1.359030842781067, G Loss: 1.009742021560669\n",
      "Epoch 78, batch 172 D Loss: 1.7231839895248413, G Loss: 0.9926403164863586\n",
      "Epoch 78, batch 173 D Loss: 1.4565123319625854, G Loss: 1.2212607860565186\n",
      "Epoch 78, batch 174 D Loss: 1.214507818222046, G Loss: 1.1183788776397705\n",
      "Epoch 78, batch 175 D Loss: 1.4947409629821777, G Loss: 0.8534223437309265\n",
      "Epoch 78, batch 176 D Loss: 1.574516773223877, G Loss: 0.9436553120613098\n",
      "Epoch 78, batch 177 D Loss: 1.4085302352905273, G Loss: 0.9350720047950745\n",
      "Epoch 78, batch 178 D Loss: 1.5053069591522217, G Loss: 1.1064579486846924\n",
      "Epoch 78, batch 179 D Loss: 1.749307632446289, G Loss: 0.9232025742530823\n",
      "Epoch 78, batch 180 D Loss: 1.664081335067749, G Loss: 0.9722270369529724\n",
      "Epoch 78, batch 181 D Loss: 1.477837085723877, G Loss: 1.0346606969833374\n",
      "Epoch 78, batch 182 D Loss: 1.5237572193145752, G Loss: 0.9292966723442078\n",
      "Epoch 78, batch 183 D Loss: 1.3911651372909546, G Loss: 0.9723924398422241\n",
      "Epoch 78, batch 184 D Loss: 1.5032267570495605, G Loss: 1.0092419385910034\n",
      "Epoch 78, batch 185 D Loss: 1.7480356693267822, G Loss: 0.9004315733909607\n",
      "Epoch 78, batch 186 D Loss: 1.8957446813583374, G Loss: 0.9274095296859741\n",
      "Epoch 78, batch 187 D Loss: 1.4872422218322754, G Loss: 0.9709409475326538\n",
      "Epoch 78, batch 188 D Loss: 1.807643175125122, G Loss: 1.031866431236267\n",
      "Epoch 78, batch 189 D Loss: 1.6729828119277954, G Loss: 0.8765766024589539\n",
      "Epoch 78, batch 190 D Loss: 1.511336088180542, G Loss: 0.9134875535964966\n",
      "Epoch 78, batch 191 D Loss: 1.7082698345184326, G Loss: 0.8380303978919983\n",
      "Epoch 78, batch 192 D Loss: 1.5489434003829956, G Loss: 0.89040207862854\n",
      "Epoch 78, batch 193 D Loss: 1.3425133228302002, G Loss: 0.8083028197288513\n",
      "Epoch 78, batch 194 D Loss: 1.4689152240753174, G Loss: 0.8341051340103149\n",
      "Epoch 78, batch 195 D Loss: 1.8223670721054077, G Loss: 0.846592128276825\n",
      "Epoch 78, batch 196 D Loss: 1.5002570152282715, G Loss: 0.9271363615989685\n",
      "Epoch 78, batch 197 D Loss: 1.2835299968719482, G Loss: 0.8684924244880676\n",
      "Epoch 78, batch 198 D Loss: 1.6143720149993896, G Loss: 0.8597337603569031\n",
      "Epoch 78, batch 199 D Loss: 1.6522719860076904, G Loss: 0.8385440111160278\n",
      "Epoch 78, batch 200 D Loss: 1.548487901687622, G Loss: 0.8499990105628967\n",
      "Epoch 79, batch 1 D Loss: 1.66817045211792, G Loss: 0.854666531085968\n",
      "Epoch 79, batch 2 D Loss: 1.508737325668335, G Loss: 0.871875524520874\n",
      "Epoch 79, batch 3 D Loss: 1.904679536819458, G Loss: 0.8208192586898804\n",
      "Epoch 79, batch 4 D Loss: 1.6222209930419922, G Loss: 0.8585749864578247\n",
      "Epoch 79, batch 5 D Loss: 1.4293384552001953, G Loss: 0.8565859794616699\n",
      "Epoch 79, batch 6 D Loss: 1.6468982696533203, G Loss: 0.8434068560600281\n",
      "Epoch 79, batch 7 D Loss: 1.495620846748352, G Loss: 0.8165470361709595\n",
      "Epoch 79, batch 8 D Loss: 1.6488001346588135, G Loss: 0.8267495632171631\n",
      "Epoch 79, batch 9 D Loss: 1.3478388786315918, G Loss: 0.8788751363754272\n",
      "Epoch 79, batch 10 D Loss: 1.5887622833251953, G Loss: 0.8239209651947021\n",
      "Epoch 79, batch 11 D Loss: 1.5314395427703857, G Loss: 0.8873654007911682\n",
      "Epoch 79, batch 12 D Loss: 1.7269948720932007, G Loss: 0.8073022365570068\n",
      "Epoch 79, batch 13 D Loss: 1.612933874130249, G Loss: 0.8504366874694824\n",
      "Epoch 79, batch 14 D Loss: 1.4011415243148804, G Loss: 0.8820034265518188\n",
      "Epoch 79, batch 15 D Loss: 1.373119831085205, G Loss: 0.8557456135749817\n",
      "Epoch 79, batch 16 D Loss: 1.241219162940979, G Loss: 0.896211564540863\n",
      "Epoch 79, batch 17 D Loss: 1.416597604751587, G Loss: 0.8797324895858765\n",
      "Epoch 79, batch 18 D Loss: 1.392861247062683, G Loss: 0.9228256344795227\n",
      "Epoch 79, batch 19 D Loss: 1.464714527130127, G Loss: 0.9170249700546265\n",
      "Epoch 79, batch 20 D Loss: 1.4565441608428955, G Loss: 0.8680106997489929\n",
      "Epoch 79, batch 21 D Loss: 1.3686920404434204, G Loss: 0.8121143579483032\n",
      "Epoch 79, batch 22 D Loss: 1.3038012981414795, G Loss: 0.8595423698425293\n",
      "Epoch 79, batch 23 D Loss: 1.3339941501617432, G Loss: 0.9188570380210876\n",
      "Epoch 79, batch 24 D Loss: 1.2285183668136597, G Loss: 0.9223153591156006\n",
      "Epoch 79, batch 25 D Loss: 1.2157179117202759, G Loss: 0.8916252851486206\n",
      "Epoch 79, batch 26 D Loss: 1.300575852394104, G Loss: 0.8927487134933472\n",
      "Epoch 79, batch 27 D Loss: 1.502976417541504, G Loss: 0.9243057370185852\n",
      "Epoch 79, batch 28 D Loss: 1.3726654052734375, G Loss: 0.9439327120780945\n",
      "Epoch 79, batch 29 D Loss: 1.2206116914749146, G Loss: 0.9475050568580627\n",
      "Epoch 79, batch 30 D Loss: 1.4658315181732178, G Loss: 0.9105439186096191\n",
      "Epoch 79, batch 31 D Loss: 1.5218425989151, G Loss: 0.9075587391853333\n",
      "Epoch 79, batch 32 D Loss: 1.5450153350830078, G Loss: 0.8863914012908936\n",
      "Epoch 79, batch 33 D Loss: 1.2561521530151367, G Loss: 0.9390254020690918\n",
      "Epoch 79, batch 34 D Loss: 1.3961009979248047, G Loss: 0.8980554938316345\n",
      "Epoch 79, batch 35 D Loss: 1.307076096534729, G Loss: 0.9468443989753723\n",
      "Epoch 79, batch 36 D Loss: 1.2569870948791504, G Loss: 0.9464426636695862\n",
      "Epoch 79, batch 37 D Loss: 1.4401781558990479, G Loss: 0.9149963855743408\n",
      "Epoch 79, batch 38 D Loss: 1.431990146636963, G Loss: 0.9857710003852844\n",
      "Epoch 79, batch 39 D Loss: 1.3804218769073486, G Loss: 0.9600805044174194\n",
      "Epoch 79, batch 40 D Loss: 1.3894245624542236, G Loss: 0.895199179649353\n",
      "Epoch 79, batch 41 D Loss: 1.303312063217163, G Loss: 0.94463050365448\n",
      "Epoch 79, batch 42 D Loss: 1.297932505607605, G Loss: 0.9768481254577637\n",
      "Epoch 79, batch 43 D Loss: 1.2549684047698975, G Loss: 0.9561663866043091\n",
      "Epoch 79, batch 44 D Loss: 1.357820987701416, G Loss: 0.9271286725997925\n",
      "Epoch 79, batch 45 D Loss: 1.284043788909912, G Loss: 0.9545532464981079\n",
      "Epoch 79, batch 46 D Loss: 1.3300065994262695, G Loss: 0.9462568759918213\n",
      "Epoch 79, batch 47 D Loss: 1.2823402881622314, G Loss: 0.980614423751831\n",
      "Epoch 79, batch 48 D Loss: 1.540370225906372, G Loss: 0.8986164927482605\n",
      "Epoch 79, batch 49 D Loss: 1.3131484985351562, G Loss: 0.9513280391693115\n",
      "Epoch 79, batch 50 D Loss: 1.2011590003967285, G Loss: 0.9521198868751526\n",
      "Epoch 79, batch 51 D Loss: 1.2214696407318115, G Loss: 0.928050696849823\n",
      "Epoch 79, batch 52 D Loss: 1.2376036643981934, G Loss: 0.8857461810112\n",
      "Epoch 79, batch 53 D Loss: 1.219637155532837, G Loss: 0.9682577252388\n",
      "Epoch 79, batch 54 D Loss: 1.2352852821350098, G Loss: 0.9707199335098267\n",
      "Epoch 79, batch 55 D Loss: 1.10639488697052, G Loss: 0.9502909779548645\n",
      "Epoch 79, batch 56 D Loss: 1.1807745695114136, G Loss: 0.9487727880477905\n",
      "Epoch 79, batch 57 D Loss: 1.234407901763916, G Loss: 0.9419650435447693\n",
      "Epoch 79, batch 58 D Loss: 1.3385365009307861, G Loss: 0.925340473651886\n",
      "Epoch 79, batch 59 D Loss: 1.2097482681274414, G Loss: 0.9375149607658386\n",
      "Epoch 79, batch 60 D Loss: 1.311685562133789, G Loss: 0.8947255611419678\n",
      "Epoch 79, batch 61 D Loss: 1.280165672302246, G Loss: 0.8924873471260071\n",
      "Epoch 79, batch 62 D Loss: 1.2672408819198608, G Loss: 0.913038432598114\n",
      "Epoch 79, batch 63 D Loss: 1.233306646347046, G Loss: 0.9034628868103027\n",
      "Epoch 79, batch 64 D Loss: 1.3107707500457764, G Loss: 0.9039021134376526\n",
      "Epoch 79, batch 65 D Loss: 1.1652846336364746, G Loss: 0.8980399966239929\n",
      "Epoch 79, batch 66 D Loss: 1.2729661464691162, G Loss: 0.912936270236969\n",
      "Epoch 79, batch 67 D Loss: 1.1638398170471191, G Loss: 0.9139158129692078\n",
      "Epoch 79, batch 68 D Loss: 1.3679773807525635, G Loss: 0.8921502828598022\n",
      "Epoch 79, batch 69 D Loss: 1.1255288124084473, G Loss: 0.9031480550765991\n",
      "Epoch 79, batch 70 D Loss: 1.2813011407852173, G Loss: 0.8696855902671814\n",
      "Epoch 79, batch 71 D Loss: 1.2429931163787842, G Loss: 0.8897824287414551\n",
      "Epoch 79, batch 72 D Loss: 1.2457630634307861, G Loss: 0.8759157657623291\n",
      "Epoch 79, batch 73 D Loss: 1.3253462314605713, G Loss: 0.9023683667182922\n",
      "Epoch 79, batch 74 D Loss: 1.2598834037780762, G Loss: 0.8393297791481018\n",
      "Epoch 79, batch 75 D Loss: 1.4172561168670654, G Loss: 0.7868396043777466\n",
      "Epoch 79, batch 76 D Loss: 1.2859474420547485, G Loss: 0.8570366501808167\n",
      "Epoch 79, batch 77 D Loss: 1.3401315212249756, G Loss: 0.8410359025001526\n",
      "Epoch 79, batch 78 D Loss: 1.2533727884292603, G Loss: 0.8256135582923889\n",
      "Epoch 79, batch 79 D Loss: 1.3297703266143799, G Loss: 0.8325691819190979\n",
      "Epoch 79, batch 80 D Loss: 1.2825496196746826, G Loss: 0.878205418586731\n",
      "Epoch 79, batch 81 D Loss: 1.201456069946289, G Loss: 0.8186699748039246\n",
      "Epoch 79, batch 82 D Loss: 1.2696595191955566, G Loss: 0.8463070392608643\n",
      "Epoch 79, batch 83 D Loss: 1.243869662284851, G Loss: 0.8221806287765503\n",
      "Epoch 79, batch 84 D Loss: 1.2213070392608643, G Loss: 0.7765957117080688\n",
      "Epoch 79, batch 85 D Loss: 1.2635350227355957, G Loss: 0.7698547840118408\n",
      "Epoch 79, batch 86 D Loss: 1.3443878889083862, G Loss: 0.7620086669921875\n",
      "Epoch 79, batch 87 D Loss: 1.271193504333496, G Loss: 0.7560189962387085\n",
      "Epoch 79, batch 88 D Loss: 1.2713638544082642, G Loss: 0.7593840956687927\n",
      "Epoch 79, batch 89 D Loss: 1.2542028427124023, G Loss: 0.8333004713058472\n",
      "Epoch 79, batch 90 D Loss: 1.2811845541000366, G Loss: 0.7701478004455566\n",
      "Epoch 79, batch 91 D Loss: 1.3262295722961426, G Loss: 0.7654829621315002\n",
      "Epoch 79, batch 92 D Loss: 1.3207201957702637, G Loss: 0.7272999286651611\n",
      "Epoch 79, batch 93 D Loss: 1.2202661037445068, G Loss: 0.8067519664764404\n",
      "Epoch 79, batch 94 D Loss: 1.3511238098144531, G Loss: 0.7322238683700562\n",
      "Epoch 79, batch 95 D Loss: 1.2009286880493164, G Loss: 0.8548848032951355\n",
      "Epoch 79, batch 96 D Loss: 1.2700319290161133, G Loss: 0.7204046845436096\n",
      "Epoch 79, batch 97 D Loss: 1.2716915607452393, G Loss: 0.7991288900375366\n",
      "Epoch 79, batch 98 D Loss: 1.1901373863220215, G Loss: 0.7990053296089172\n",
      "Epoch 79, batch 99 D Loss: 1.3398716449737549, G Loss: 0.688689112663269\n",
      "Epoch 79, batch 100 D Loss: 1.3457527160644531, G Loss: 0.7604162693023682\n",
      "Epoch 79, batch 101 D Loss: 1.4089000225067139, G Loss: 0.7237399220466614\n",
      "Epoch 79, batch 102 D Loss: 1.3450909852981567, G Loss: 0.7094610333442688\n",
      "Epoch 79, batch 103 D Loss: 1.270480751991272, G Loss: 0.7491928339004517\n",
      "Epoch 79, batch 104 D Loss: 1.3774940967559814, G Loss: 0.7327597141265869\n",
      "Epoch 79, batch 105 D Loss: 1.2896625995635986, G Loss: 0.725191593170166\n",
      "Epoch 79, batch 106 D Loss: 1.327791690826416, G Loss: 0.7696887850761414\n",
      "Epoch 79, batch 107 D Loss: 1.243344783782959, G Loss: 0.750729501247406\n",
      "Epoch 79, batch 108 D Loss: 1.228057622909546, G Loss: 0.7194803357124329\n",
      "Epoch 79, batch 109 D Loss: 1.3038184642791748, G Loss: 0.7555654644966125\n",
      "Epoch 79, batch 110 D Loss: 1.3389716148376465, G Loss: 0.7619576454162598\n",
      "Epoch 79, batch 111 D Loss: 1.363788366317749, G Loss: 0.7548133134841919\n",
      "Epoch 79, batch 112 D Loss: 1.3334519863128662, G Loss: 0.7174224257469177\n",
      "Epoch 79, batch 113 D Loss: 1.2773983478546143, G Loss: 0.7122969031333923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, batch 114 D Loss: 1.358507513999939, G Loss: 0.6682596802711487\n",
      "Epoch 79, batch 115 D Loss: 1.2971935272216797, G Loss: 0.6866045594215393\n",
      "Epoch 79, batch 116 D Loss: 1.3548359870910645, G Loss: 0.6962961554527283\n",
      "Epoch 79, batch 117 D Loss: 1.3321735858917236, G Loss: 0.6570925712585449\n",
      "Epoch 79, batch 118 D Loss: 1.285743236541748, G Loss: 0.7278771996498108\n",
      "Epoch 79, batch 119 D Loss: 1.2964308261871338, G Loss: 0.7497987151145935\n",
      "Epoch 79, batch 120 D Loss: 1.277064561843872, G Loss: 0.7353092432022095\n",
      "Epoch 79, batch 121 D Loss: 1.3088841438293457, G Loss: 0.7450977563858032\n",
      "Epoch 79, batch 122 D Loss: 1.29191255569458, G Loss: 0.7646899223327637\n",
      "Epoch 79, batch 123 D Loss: 1.403511881828308, G Loss: 0.6961979866027832\n",
      "Epoch 79, batch 124 D Loss: 1.2653090953826904, G Loss: 0.7381305694580078\n",
      "Epoch 79, batch 125 D Loss: 1.2583351135253906, G Loss: 0.7252142429351807\n",
      "Epoch 79, batch 126 D Loss: 1.1978446245193481, G Loss: 0.7475296258926392\n",
      "Epoch 79, batch 127 D Loss: 1.2264416217803955, G Loss: 0.7327054738998413\n",
      "Epoch 79, batch 128 D Loss: 1.3276171684265137, G Loss: 0.738372266292572\n",
      "Epoch 79, batch 129 D Loss: 1.3332247734069824, G Loss: 0.7233801484107971\n",
      "Epoch 79, batch 130 D Loss: 1.1958673000335693, G Loss: 0.7543827295303345\n",
      "Epoch 79, batch 131 D Loss: 1.233132243156433, G Loss: 0.7741962671279907\n",
      "Epoch 79, batch 132 D Loss: 1.2504048347473145, G Loss: 0.7774455547332764\n",
      "Epoch 79, batch 133 D Loss: 1.207205891609192, G Loss: 0.7980602383613586\n",
      "Epoch 79, batch 134 D Loss: 1.2709980010986328, G Loss: 0.7722140550613403\n",
      "Epoch 79, batch 135 D Loss: 1.1396281719207764, G Loss: 0.801072895526886\n",
      "Epoch 79, batch 136 D Loss: 1.1893956661224365, G Loss: 0.7736117839813232\n",
      "Epoch 79, batch 137 D Loss: 1.2708498239517212, G Loss: 0.8271971344947815\n",
      "Epoch 79, batch 138 D Loss: 1.1704962253570557, G Loss: 0.8050363659858704\n",
      "Epoch 79, batch 139 D Loss: 1.2626601457595825, G Loss: 0.8130767941474915\n",
      "Epoch 79, batch 140 D Loss: 1.1789956092834473, G Loss: 0.7977302670478821\n",
      "Epoch 79, batch 141 D Loss: 1.1307302713394165, G Loss: 0.8066866397857666\n",
      "Epoch 79, batch 142 D Loss: 1.224699854850769, G Loss: 0.8150793313980103\n",
      "Epoch 79, batch 143 D Loss: 1.2214272022247314, G Loss: 0.814372181892395\n",
      "Epoch 79, batch 144 D Loss: 1.1750004291534424, G Loss: 0.8089967966079712\n",
      "Epoch 79, batch 145 D Loss: 1.2246391773223877, G Loss: 0.8398084044456482\n",
      "Epoch 79, batch 146 D Loss: 1.1472768783569336, G Loss: 0.834463357925415\n",
      "Epoch 79, batch 147 D Loss: 1.1460468769073486, G Loss: 0.8565061688423157\n",
      "Epoch 79, batch 148 D Loss: 1.1996086835861206, G Loss: 0.8894861340522766\n",
      "Epoch 79, batch 149 D Loss: 1.136635661125183, G Loss: 0.8441863059997559\n",
      "Epoch 79, batch 150 D Loss: 1.147986650466919, G Loss: 0.8968595266342163\n",
      "Epoch 79, batch 151 D Loss: 1.1122117042541504, G Loss: 0.8579926490783691\n",
      "Epoch 79, batch 152 D Loss: 1.0650687217712402, G Loss: 0.9136340618133545\n",
      "Epoch 79, batch 153 D Loss: 1.1618152856826782, G Loss: 0.8930516242980957\n",
      "Epoch 79, batch 154 D Loss: 1.1755216121673584, G Loss: 0.9168574810028076\n",
      "Epoch 79, batch 155 D Loss: 1.1970484256744385, G Loss: 0.9023181200027466\n",
      "Epoch 79, batch 156 D Loss: 0.9866260290145874, G Loss: 0.9553259015083313\n",
      "Epoch 79, batch 157 D Loss: 1.1269819736480713, G Loss: 0.9361526370048523\n",
      "Epoch 79, batch 158 D Loss: 1.0538740158081055, G Loss: 0.9573914408683777\n",
      "Epoch 79, batch 159 D Loss: 1.3333864212036133, G Loss: 0.9422823190689087\n",
      "Epoch 79, batch 160 D Loss: 1.093632698059082, G Loss: 0.8909379839897156\n",
      "Epoch 79, batch 161 D Loss: 1.2548351287841797, G Loss: 0.9173638820648193\n",
      "Epoch 79, batch 162 D Loss: 1.0476800203323364, G Loss: 0.9494374990463257\n",
      "Epoch 79, batch 163 D Loss: 1.0998191833496094, G Loss: 0.9181418418884277\n",
      "Epoch 79, batch 164 D Loss: 1.0830127000808716, G Loss: 0.9741594195365906\n",
      "Epoch 79, batch 165 D Loss: 1.1995763778686523, G Loss: 0.9806475043296814\n",
      "Epoch 79, batch 166 D Loss: 1.0798370838165283, G Loss: 0.9795675873756409\n",
      "Epoch 79, batch 167 D Loss: 1.119577169418335, G Loss: 0.9773703813552856\n",
      "Epoch 79, batch 168 D Loss: 1.2386399507522583, G Loss: 0.9495331048965454\n",
      "Epoch 79, batch 169 D Loss: 1.056164026260376, G Loss: 0.9338241815567017\n",
      "Epoch 79, batch 170 D Loss: 1.0430686473846436, G Loss: 0.9702865481376648\n",
      "Epoch 79, batch 171 D Loss: 1.0089267492294312, G Loss: 1.0035324096679688\n",
      "Epoch 79, batch 172 D Loss: 1.0617334842681885, G Loss: 0.9617637395858765\n",
      "Epoch 79, batch 173 D Loss: 1.0592191219329834, G Loss: 0.9313071966171265\n",
      "Epoch 79, batch 174 D Loss: 1.1233311891555786, G Loss: 0.9436209797859192\n",
      "Epoch 79, batch 175 D Loss: 1.1280803680419922, G Loss: 0.9876848459243774\n",
      "Epoch 79, batch 176 D Loss: 1.006633996963501, G Loss: 0.9584972262382507\n",
      "Epoch 79, batch 177 D Loss: 1.0567219257354736, G Loss: 0.9855129718780518\n",
      "Epoch 79, batch 178 D Loss: 1.1851921081542969, G Loss: 1.0036728382110596\n",
      "Epoch 79, batch 179 D Loss: 1.0252405405044556, G Loss: 0.9707450270652771\n",
      "Epoch 79, batch 180 D Loss: 0.9886802434921265, G Loss: 0.9926080107688904\n",
      "Epoch 79, batch 181 D Loss: 1.1160099506378174, G Loss: 0.9706854224205017\n",
      "Epoch 79, batch 182 D Loss: 1.208171010017395, G Loss: 0.9887093305587769\n",
      "Epoch 79, batch 183 D Loss: 1.1227078437805176, G Loss: 1.030856728553772\n",
      "Epoch 79, batch 184 D Loss: 1.0050792694091797, G Loss: 1.0670658349990845\n",
      "Epoch 79, batch 185 D Loss: 1.0702240467071533, G Loss: 1.0458279848098755\n",
      "Epoch 79, batch 186 D Loss: 1.2059752941131592, G Loss: 0.9591076374053955\n",
      "Epoch 79, batch 187 D Loss: 1.1553972959518433, G Loss: 0.978455662727356\n",
      "Epoch 79, batch 188 D Loss: 0.8978419303894043, G Loss: 1.0060560703277588\n",
      "Epoch 79, batch 189 D Loss: 0.9839129447937012, G Loss: 1.028255581855774\n",
      "Epoch 79, batch 190 D Loss: 1.0194480419158936, G Loss: 0.9687926769256592\n",
      "Epoch 79, batch 191 D Loss: 0.9961187243461609, G Loss: 1.0328295230865479\n",
      "Epoch 79, batch 192 D Loss: 1.118992805480957, G Loss: 1.0162092447280884\n",
      "Epoch 79, batch 193 D Loss: 1.0994141101837158, G Loss: 1.0385273694992065\n",
      "Epoch 79, batch 194 D Loss: 1.061910629272461, G Loss: 1.0512726306915283\n",
      "Epoch 79, batch 195 D Loss: 1.1176363229751587, G Loss: 0.9745451211929321\n",
      "Epoch 79, batch 196 D Loss: 1.0209954977035522, G Loss: 1.028358817100525\n",
      "Epoch 79, batch 197 D Loss: 1.0912100076675415, G Loss: 1.0346457958221436\n",
      "Epoch 79, batch 198 D Loss: 1.1058645248413086, G Loss: 1.0367364883422852\n",
      "Epoch 79, batch 199 D Loss: 0.9502034187316895, G Loss: 0.9917080402374268\n",
      "Epoch 79, batch 200 D Loss: 1.156402349472046, G Loss: 1.1135541200637817\n",
      "Epoch 80, batch 1 D Loss: 1.1660995483398438, G Loss: 1.024169921875\n",
      "Epoch 80, batch 2 D Loss: 0.9798736572265625, G Loss: 1.0534193515777588\n",
      "Epoch 80, batch 3 D Loss: 1.1852856874465942, G Loss: 0.9734984040260315\n",
      "Epoch 80, batch 4 D Loss: 1.094979166984558, G Loss: 1.004894733428955\n",
      "Epoch 80, batch 5 D Loss: 1.0876469612121582, G Loss: 1.0452593564987183\n",
      "Epoch 80, batch 6 D Loss: 0.9713108539581299, G Loss: 1.1487995386123657\n",
      "Epoch 80, batch 7 D Loss: 0.9955620169639587, G Loss: 1.0366699695587158\n",
      "Epoch 80, batch 8 D Loss: 1.1406681537628174, G Loss: 1.0213823318481445\n",
      "Epoch 80, batch 9 D Loss: 1.1402230262756348, G Loss: 0.9808399081230164\n",
      "Epoch 80, batch 10 D Loss: 1.0592159032821655, G Loss: 1.072708010673523\n",
      "Epoch 80, batch 11 D Loss: 0.9235993027687073, G Loss: 1.0900295972824097\n",
      "Epoch 80, batch 12 D Loss: 1.1305391788482666, G Loss: 0.9945722222328186\n",
      "Epoch 80, batch 13 D Loss: 1.1522202491760254, G Loss: 1.0037930011749268\n",
      "Epoch 80, batch 14 D Loss: 1.0576814413070679, G Loss: 1.0668129920959473\n",
      "Epoch 80, batch 15 D Loss: 1.2015984058380127, G Loss: 1.0728976726531982\n",
      "Epoch 80, batch 16 D Loss: 1.072291612625122, G Loss: 1.0222861766815186\n",
      "Epoch 80, batch 17 D Loss: 1.1529055833816528, G Loss: 1.0738708972930908\n",
      "Epoch 80, batch 18 D Loss: 1.0615837574005127, G Loss: 1.0083273649215698\n",
      "Epoch 80, batch 19 D Loss: 1.0414952039718628, G Loss: 1.0296635627746582\n",
      "Epoch 80, batch 20 D Loss: 0.9879239797592163, G Loss: 1.0692719221115112\n",
      "Epoch 80, batch 21 D Loss: 0.955835223197937, G Loss: 1.0576132535934448\n",
      "Epoch 80, batch 22 D Loss: 1.15352201461792, G Loss: 0.9409005045890808\n",
      "Epoch 80, batch 23 D Loss: 1.150728464126587, G Loss: 1.0922366380691528\n",
      "Epoch 80, batch 24 D Loss: 1.015122413635254, G Loss: 1.0659254789352417\n",
      "Epoch 80, batch 25 D Loss: 0.9776795506477356, G Loss: 1.0531041622161865\n",
      "Epoch 80, batch 26 D Loss: 1.0703480243682861, G Loss: 1.070818543434143\n",
      "Epoch 80, batch 27 D Loss: 1.0601567029953003, G Loss: 1.0878088474273682\n",
      "Epoch 80, batch 28 D Loss: 1.0187983512878418, G Loss: 1.0967170000076294\n",
      "Epoch 80, batch 29 D Loss: 1.0659652948379517, G Loss: 1.0344761610031128\n",
      "Epoch 80, batch 30 D Loss: 1.1453359127044678, G Loss: 1.1071698665618896\n",
      "Epoch 80, batch 31 D Loss: 1.103357195854187, G Loss: 1.0808504819869995\n",
      "Epoch 80, batch 32 D Loss: 0.9309368133544922, G Loss: 1.1490004062652588\n",
      "Epoch 80, batch 33 D Loss: 1.069486141204834, G Loss: 1.0587239265441895\n",
      "Epoch 80, batch 34 D Loss: 1.06999671459198, G Loss: 1.1032625436782837\n",
      "Epoch 80, batch 35 D Loss: 1.1342308521270752, G Loss: 1.0590641498565674\n",
      "Epoch 80, batch 36 D Loss: 1.144851803779602, G Loss: 1.135817527770996\n",
      "Epoch 80, batch 37 D Loss: 1.0283478498458862, G Loss: 1.08143150806427\n",
      "Epoch 80, batch 38 D Loss: 1.0349133014678955, G Loss: 1.0986056327819824\n",
      "Epoch 80, batch 39 D Loss: 1.0335454940795898, G Loss: 1.0550686120986938\n",
      "Epoch 80, batch 40 D Loss: 1.1333245038986206, G Loss: 1.1109344959259033\n",
      "Epoch 80, batch 41 D Loss: 1.1117489337921143, G Loss: 1.1172562837600708\n",
      "Epoch 80, batch 42 D Loss: 1.1096203327178955, G Loss: 1.0403194427490234\n",
      "Epoch 80, batch 43 D Loss: 1.1936336755752563, G Loss: 1.0494943857192993\n",
      "Epoch 80, batch 44 D Loss: 1.0608415603637695, G Loss: 1.0714813470840454\n",
      "Epoch 80, batch 45 D Loss: 1.0309209823608398, G Loss: 1.0467970371246338\n",
      "Epoch 80, batch 46 D Loss: 0.9509406685829163, G Loss: 1.0951789617538452\n",
      "Epoch 80, batch 47 D Loss: 0.9736263751983643, G Loss: 1.0002684593200684\n",
      "Epoch 80, batch 48 D Loss: 1.060673713684082, G Loss: 1.1746914386749268\n",
      "Epoch 80, batch 49 D Loss: 1.0654821395874023, G Loss: 1.0492438077926636\n",
      "Epoch 80, batch 50 D Loss: 1.0346391201019287, G Loss: 1.1179887056350708\n",
      "Epoch 80, batch 51 D Loss: 1.2223504781723022, G Loss: 1.0449960231781006\n",
      "Epoch 80, batch 52 D Loss: 1.1212255954742432, G Loss: 1.0502642393112183\n",
      "Epoch 80, batch 53 D Loss: 1.2088762521743774, G Loss: 1.1117912530899048\n",
      "Epoch 80, batch 54 D Loss: 0.9628763198852539, G Loss: 1.0703003406524658\n",
      "Epoch 80, batch 55 D Loss: 0.8693118095397949, G Loss: 1.045962929725647\n",
      "Epoch 80, batch 56 D Loss: 0.9437881708145142, G Loss: 1.0695827007293701\n",
      "Epoch 80, batch 57 D Loss: 0.9607939720153809, G Loss: 1.1557555198669434\n",
      "Epoch 80, batch 58 D Loss: 0.9934179782867432, G Loss: 1.0621787309646606\n",
      "Epoch 80, batch 59 D Loss: 1.245607614517212, G Loss: 1.0393487215042114\n",
      "Epoch 80, batch 60 D Loss: 0.9861104488372803, G Loss: 1.1892541646957397\n",
      "Epoch 80, batch 61 D Loss: 1.0305429697036743, G Loss: 1.022890567779541\n",
      "Epoch 80, batch 62 D Loss: 1.0128248929977417, G Loss: 1.0735201835632324\n",
      "Epoch 80, batch 63 D Loss: 1.1117188930511475, G Loss: 1.1063381433486938\n",
      "Epoch 80, batch 64 D Loss: 0.9727770686149597, G Loss: 1.0842620134353638\n",
      "Epoch 80, batch 65 D Loss: 0.9929219484329224, G Loss: 1.1063638925552368\n",
      "Epoch 80, batch 66 D Loss: 0.9389216899871826, G Loss: 1.0927413702011108\n",
      "Epoch 80, batch 67 D Loss: 1.075358510017395, G Loss: 1.0284533500671387\n",
      "Epoch 80, batch 68 D Loss: 0.988635778427124, G Loss: 1.069443702697754\n",
      "Epoch 80, batch 69 D Loss: 1.0099302530288696, G Loss: 1.051012396812439\n",
      "Epoch 80, batch 70 D Loss: 0.7851957082748413, G Loss: 1.1483590602874756\n",
      "Epoch 80, batch 71 D Loss: 1.1472082138061523, G Loss: 1.0706086158752441\n",
      "Epoch 80, batch 72 D Loss: 0.9425255656242371, G Loss: 1.0570818185806274\n",
      "Epoch 80, batch 73 D Loss: 1.065713882446289, G Loss: 1.0753661394119263\n",
      "Epoch 80, batch 74 D Loss: 1.1271703243255615, G Loss: 1.0994813442230225\n",
      "Epoch 80, batch 75 D Loss: 1.111681580543518, G Loss: 1.0642311573028564\n",
      "Epoch 80, batch 76 D Loss: 1.0481823682785034, G Loss: 1.057435154914856\n",
      "Epoch 80, batch 77 D Loss: 1.0758490562438965, G Loss: 1.1206096410751343\n",
      "Epoch 80, batch 78 D Loss: 1.0388786792755127, G Loss: 1.102540373802185\n",
      "Epoch 80, batch 79 D Loss: 1.105875849723816, G Loss: 1.1139618158340454\n",
      "Epoch 80, batch 80 D Loss: 1.0101959705352783, G Loss: 1.0435278415679932\n",
      "Epoch 80, batch 81 D Loss: 0.9632239937782288, G Loss: 1.032545566558838\n",
      "Epoch 80, batch 82 D Loss: 1.0414866209030151, G Loss: 1.0433318614959717\n",
      "Epoch 80, batch 83 D Loss: 1.0382078886032104, G Loss: 1.0526615381240845\n",
      "Epoch 80, batch 84 D Loss: 0.9150869250297546, G Loss: 1.0803481340408325\n",
      "Epoch 80, batch 85 D Loss: 0.9986370205879211, G Loss: 1.0033793449401855\n",
      "Epoch 80, batch 86 D Loss: 1.0127867460250854, G Loss: 0.983852207660675\n",
      "Epoch 80, batch 87 D Loss: 1.2073092460632324, G Loss: 1.0349488258361816\n",
      "Epoch 80, batch 88 D Loss: 1.059201717376709, G Loss: 1.021916389465332\n",
      "Epoch 80, batch 89 D Loss: 1.1028807163238525, G Loss: 1.1028963327407837\n",
      "Epoch 80, batch 90 D Loss: 1.1555020809173584, G Loss: 1.0210670232772827\n",
      "Epoch 80, batch 91 D Loss: 0.9580413699150085, G Loss: 1.0169485807418823\n",
      "Epoch 80, batch 92 D Loss: 1.1086382865905762, G Loss: 1.0452114343643188\n",
      "Epoch 80, batch 93 D Loss: 0.9100822806358337, G Loss: 1.008461833000183\n",
      "Epoch 80, batch 94 D Loss: 0.9991046786308289, G Loss: 1.0520424842834473\n",
      "Epoch 80, batch 95 D Loss: 1.1567047834396362, G Loss: 1.1054600477218628\n",
      "Epoch 80, batch 96 D Loss: 1.1994186639785767, G Loss: 1.064934492111206\n",
      "Epoch 80, batch 97 D Loss: 1.029720425605774, G Loss: 1.065363883972168\n",
      "Epoch 80, batch 98 D Loss: 0.8708351850509644, G Loss: 1.0243501663208008\n",
      "Epoch 80, batch 99 D Loss: 0.9988329410552979, G Loss: 1.0162349939346313\n",
      "Epoch 80, batch 100 D Loss: 1.1128071546554565, G Loss: 0.9460088610649109\n",
      "Epoch 80, batch 101 D Loss: 1.1444125175476074, G Loss: 0.944873571395874\n",
      "Epoch 80, batch 102 D Loss: 0.937953770160675, G Loss: 1.0396475791931152\n",
      "Epoch 80, batch 103 D Loss: 1.0289616584777832, G Loss: 0.9817478060722351\n",
      "Epoch 80, batch 104 D Loss: 0.9330092668533325, G Loss: 1.0084527730941772\n",
      "Epoch 80, batch 105 D Loss: 0.9782136678695679, G Loss: 0.9927483201026917\n",
      "Epoch 80, batch 106 D Loss: 1.0836327075958252, G Loss: 0.9684916138648987\n",
      "Epoch 80, batch 107 D Loss: 1.2957203388214111, G Loss: 0.9550633430480957\n",
      "Epoch 80, batch 108 D Loss: 0.9924874305725098, G Loss: 0.9495470523834229\n",
      "Epoch 80, batch 109 D Loss: 1.0729533433914185, G Loss: 0.9821842908859253\n",
      "Epoch 80, batch 110 D Loss: 0.9474121928215027, G Loss: 1.0068979263305664\n",
      "Epoch 80, batch 111 D Loss: 1.1659321784973145, G Loss: 0.9530693292617798\n",
      "Epoch 80, batch 112 D Loss: 1.076838731765747, G Loss: 0.9685347080230713\n",
      "Epoch 80, batch 113 D Loss: 1.003804087638855, G Loss: 0.9499754309654236\n",
      "Epoch 80, batch 114 D Loss: 1.088232159614563, G Loss: 0.9884532690048218\n",
      "Epoch 80, batch 115 D Loss: 1.2299752235412598, G Loss: 0.982027530670166\n",
      "Epoch 80, batch 116 D Loss: 0.8889552354812622, G Loss: 0.9264318943023682\n",
      "Epoch 80, batch 117 D Loss: 1.2408106327056885, G Loss: 0.919880747795105\n",
      "Epoch 80, batch 118 D Loss: 0.9254693984985352, G Loss: 0.9146516919136047\n",
      "Epoch 80, batch 119 D Loss: 1.2842493057250977, G Loss: 0.8374394774436951\n",
      "Epoch 80, batch 120 D Loss: 1.0330570936203003, G Loss: 0.9523413181304932\n",
      "Epoch 80, batch 121 D Loss: 1.1394623517990112, G Loss: 0.8341214060783386\n",
      "Epoch 80, batch 122 D Loss: 1.1367719173431396, G Loss: 0.9033448696136475\n",
      "Epoch 80, batch 123 D Loss: 1.232276439666748, G Loss: 0.9084525108337402\n",
      "Epoch 80, batch 124 D Loss: 0.9785833358764648, G Loss: 0.9012922048568726\n",
      "Epoch 80, batch 125 D Loss: 1.193274974822998, G Loss: 0.8871157765388489\n",
      "Epoch 80, batch 126 D Loss: 1.2549071311950684, G Loss: 0.8579772114753723\n",
      "Epoch 80, batch 127 D Loss: 1.074556589126587, G Loss: 0.9219806790351868\n",
      "Epoch 80, batch 128 D Loss: 1.107905626296997, G Loss: 0.844896674156189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, batch 129 D Loss: 1.2885539531707764, G Loss: 0.831831157207489\n",
      "Epoch 80, batch 130 D Loss: 1.3144750595092773, G Loss: 0.719409167766571\n",
      "Epoch 80, batch 131 D Loss: 1.273064136505127, G Loss: 0.7816728353500366\n",
      "Epoch 80, batch 132 D Loss: 1.2236254215240479, G Loss: 0.7526956796646118\n",
      "Epoch 80, batch 133 D Loss: 1.1896685361862183, G Loss: 0.7923158407211304\n",
      "Epoch 80, batch 134 D Loss: 1.1622055768966675, G Loss: 0.7526094913482666\n",
      "Epoch 80, batch 135 D Loss: 1.2916128635406494, G Loss: 0.7370818853378296\n",
      "Epoch 80, batch 136 D Loss: 1.1987652778625488, G Loss: 0.7958996295928955\n",
      "Epoch 80, batch 137 D Loss: 1.318028450012207, G Loss: 0.7546209096908569\n",
      "Epoch 80, batch 138 D Loss: 1.2992699146270752, G Loss: 0.7655483484268188\n",
      "Epoch 80, batch 139 D Loss: 1.2836084365844727, G Loss: 0.7670343518257141\n",
      "Epoch 80, batch 140 D Loss: 1.5009331703186035, G Loss: 0.7236852049827576\n",
      "Epoch 80, batch 141 D Loss: 1.4208142757415771, G Loss: 0.664719820022583\n",
      "Epoch 80, batch 142 D Loss: 1.182690978050232, G Loss: 0.7889430522918701\n",
      "Epoch 80, batch 143 D Loss: 1.2166858911514282, G Loss: 0.7913501262664795\n",
      "Epoch 80, batch 144 D Loss: 1.4109441041946411, G Loss: 0.7157475352287292\n",
      "Epoch 80, batch 145 D Loss: 1.30814528465271, G Loss: 0.7270101308822632\n",
      "Epoch 80, batch 146 D Loss: 1.446543574333191, G Loss: 0.7160162925720215\n",
      "Epoch 80, batch 147 D Loss: 1.392318606376648, G Loss: 0.6758376955986023\n",
      "Epoch 80, batch 148 D Loss: 1.394458293914795, G Loss: 0.691673994064331\n",
      "Epoch 80, batch 149 D Loss: 1.346006155014038, G Loss: 0.6385363936424255\n",
      "Epoch 80, batch 150 D Loss: 1.5602240562438965, G Loss: 0.6166918277740479\n",
      "Epoch 80, batch 151 D Loss: 1.4734865427017212, G Loss: 0.6468443274497986\n",
      "Epoch 80, batch 152 D Loss: 1.4616966247558594, G Loss: 0.6818242073059082\n",
      "Epoch 80, batch 153 D Loss: 1.2571160793304443, G Loss: 0.6979808807373047\n",
      "Epoch 80, batch 154 D Loss: 1.5745326280593872, G Loss: 0.6503756642341614\n",
      "Epoch 80, batch 155 D Loss: 1.3097577095031738, G Loss: 0.7415259480476379\n",
      "Epoch 80, batch 156 D Loss: 1.587258219718933, G Loss: 0.6602981686592102\n",
      "Epoch 80, batch 157 D Loss: 1.3251118659973145, G Loss: 0.6287710666656494\n",
      "Epoch 80, batch 158 D Loss: 1.5382862091064453, G Loss: 0.6312465667724609\n",
      "Epoch 80, batch 159 D Loss: 1.194195032119751, G Loss: 0.6467107534408569\n",
      "Epoch 80, batch 160 D Loss: 1.4220499992370605, G Loss: 0.6482087969779968\n",
      "Epoch 80, batch 161 D Loss: 1.225266933441162, G Loss: 0.6897560954093933\n",
      "Epoch 80, batch 162 D Loss: 1.5628238916397095, G Loss: 0.6230338215827942\n",
      "Epoch 80, batch 163 D Loss: 1.3530304431915283, G Loss: 0.6553048491477966\n",
      "Epoch 80, batch 164 D Loss: 1.377976417541504, G Loss: 0.686341404914856\n",
      "Epoch 80, batch 165 D Loss: 1.2190637588500977, G Loss: 0.6714038252830505\n",
      "Epoch 80, batch 166 D Loss: 1.4144220352172852, G Loss: 0.6608663201332092\n",
      "Epoch 80, batch 167 D Loss: 1.4172179698944092, G Loss: 0.702883780002594\n",
      "Epoch 80, batch 168 D Loss: 1.4609851837158203, G Loss: 0.6524214744567871\n",
      "Epoch 80, batch 169 D Loss: 1.3451454639434814, G Loss: 0.7083795666694641\n",
      "Epoch 80, batch 170 D Loss: 1.4657986164093018, G Loss: 0.6591312885284424\n",
      "Epoch 80, batch 171 D Loss: 1.4292819499969482, G Loss: 0.649670422077179\n",
      "Epoch 80, batch 172 D Loss: 1.7268816232681274, G Loss: 0.6582059264183044\n",
      "Epoch 80, batch 173 D Loss: 1.24922513961792, G Loss: 0.7447631359100342\n",
      "Epoch 80, batch 174 D Loss: 1.5398731231689453, G Loss: 0.6894533038139343\n",
      "Epoch 80, batch 175 D Loss: 1.4163480997085571, G Loss: 0.6832983493804932\n",
      "Epoch 80, batch 176 D Loss: 1.5210143327713013, G Loss: 0.6980329751968384\n",
      "Epoch 80, batch 177 D Loss: 1.2918198108673096, G Loss: 0.7716437578201294\n",
      "Epoch 80, batch 178 D Loss: 1.5587997436523438, G Loss: 0.6268380284309387\n",
      "Epoch 80, batch 179 D Loss: 1.3647810220718384, G Loss: 0.7321015000343323\n",
      "Epoch 80, batch 180 D Loss: 1.3636221885681152, G Loss: 0.6633803844451904\n",
      "Epoch 80, batch 181 D Loss: 1.411741852760315, G Loss: 0.7807509899139404\n",
      "Epoch 80, batch 182 D Loss: 1.3755991458892822, G Loss: 0.734977126121521\n",
      "Epoch 80, batch 183 D Loss: 1.4419225454330444, G Loss: 0.8764826059341431\n",
      "Epoch 80, batch 184 D Loss: 1.3459970951080322, G Loss: 0.8435894250869751\n",
      "Epoch 80, batch 185 D Loss: 1.4346789121627808, G Loss: 0.7610884308815002\n",
      "Epoch 80, batch 186 D Loss: 1.5577058792114258, G Loss: 0.7392197251319885\n",
      "Epoch 80, batch 187 D Loss: 1.4516193866729736, G Loss: 0.8000466227531433\n",
      "Epoch 80, batch 188 D Loss: 1.3520442247390747, G Loss: 0.8575737476348877\n",
      "Epoch 80, batch 189 D Loss: 1.4572949409484863, G Loss: 0.9124186038970947\n",
      "Epoch 80, batch 190 D Loss: 1.3182775974273682, G Loss: 0.8427568674087524\n",
      "Epoch 80, batch 191 D Loss: 1.4814016819000244, G Loss: 0.8325996994972229\n",
      "Epoch 80, batch 192 D Loss: 1.5642800331115723, G Loss: 0.8029384613037109\n",
      "Epoch 80, batch 193 D Loss: 1.294236660003662, G Loss: 0.8778912425041199\n",
      "Epoch 80, batch 194 D Loss: 1.445375680923462, G Loss: 0.8073357343673706\n",
      "Epoch 80, batch 195 D Loss: 1.2669786214828491, G Loss: 0.9123745560646057\n",
      "Epoch 80, batch 196 D Loss: 1.4625123739242554, G Loss: 0.8226559162139893\n",
      "Epoch 80, batch 197 D Loss: 1.3078672885894775, G Loss: 0.9630785584449768\n",
      "Epoch 80, batch 198 D Loss: 1.4212640523910522, G Loss: 0.872109591960907\n",
      "Epoch 80, batch 199 D Loss: 1.268570899963379, G Loss: 0.9322752356529236\n",
      "Epoch 80, batch 200 D Loss: 1.3610154390335083, G Loss: 0.8803104162216187\n",
      "Epoch 81, batch 1 D Loss: 1.3023653030395508, G Loss: 0.9231913685798645\n",
      "Epoch 81, batch 2 D Loss: 1.4574692249298096, G Loss: 0.9279280304908752\n",
      "Epoch 81, batch 3 D Loss: 1.5495171546936035, G Loss: 0.922109067440033\n",
      "Epoch 81, batch 4 D Loss: 1.364342451095581, G Loss: 0.9070603251457214\n",
      "Epoch 81, batch 5 D Loss: 1.4160056114196777, G Loss: 0.8257383704185486\n",
      "Epoch 81, batch 6 D Loss: 1.2819013595581055, G Loss: 1.0562541484832764\n",
      "Epoch 81, batch 7 D Loss: 1.3854233026504517, G Loss: 0.8701176643371582\n",
      "Epoch 81, batch 8 D Loss: 1.4268536567687988, G Loss: 0.8478147983551025\n",
      "Epoch 81, batch 9 D Loss: 1.5542261600494385, G Loss: 0.826962411403656\n",
      "Epoch 81, batch 10 D Loss: 1.4265317916870117, G Loss: 1.1427905559539795\n",
      "Epoch 81, batch 11 D Loss: 1.345872402191162, G Loss: 0.9390049576759338\n",
      "Epoch 81, batch 12 D Loss: 1.4835314750671387, G Loss: 0.9852374792098999\n",
      "Epoch 81, batch 13 D Loss: 1.4499201774597168, G Loss: 0.9845095872879028\n",
      "Epoch 81, batch 14 D Loss: 1.4034748077392578, G Loss: 0.8220193386077881\n",
      "Epoch 81, batch 15 D Loss: 1.5439453125, G Loss: 0.8800601363182068\n",
      "Epoch 81, batch 16 D Loss: 1.38155198097229, G Loss: 0.9837150573730469\n",
      "Epoch 81, batch 17 D Loss: 1.4247002601623535, G Loss: 0.9390516877174377\n",
      "Epoch 81, batch 18 D Loss: 1.5519675016403198, G Loss: 0.8570509552955627\n",
      "Epoch 81, batch 19 D Loss: 1.325491189956665, G Loss: 0.9975540041923523\n",
      "Epoch 81, batch 20 D Loss: 1.58809494972229, G Loss: 0.9231430292129517\n",
      "Epoch 81, batch 21 D Loss: 1.5382006168365479, G Loss: 0.8678798079490662\n",
      "Epoch 81, batch 22 D Loss: 1.2655506134033203, G Loss: 0.9209201335906982\n",
      "Epoch 81, batch 23 D Loss: 1.3630218505859375, G Loss: 0.9225000739097595\n",
      "Epoch 81, batch 24 D Loss: 1.534160852432251, G Loss: 0.8488644361495972\n",
      "Epoch 81, batch 25 D Loss: 1.2712209224700928, G Loss: 0.9643067717552185\n",
      "Epoch 81, batch 26 D Loss: 1.2839784622192383, G Loss: 0.9195790886878967\n",
      "Epoch 81, batch 27 D Loss: 1.3862955570220947, G Loss: 0.9141497611999512\n",
      "Epoch 81, batch 28 D Loss: 1.3909668922424316, G Loss: 1.013560175895691\n",
      "Epoch 81, batch 29 D Loss: 1.300283670425415, G Loss: 0.9471021294593811\n",
      "Epoch 81, batch 30 D Loss: 1.572256326675415, G Loss: 0.8142657279968262\n",
      "Epoch 81, batch 31 D Loss: 1.2983412742614746, G Loss: 0.823300838470459\n",
      "Epoch 81, batch 32 D Loss: 1.4089410305023193, G Loss: 0.8651823401451111\n",
      "Epoch 81, batch 33 D Loss: 1.302199125289917, G Loss: 0.9132728576660156\n",
      "Epoch 81, batch 34 D Loss: 1.427947759628296, G Loss: 0.7647351026535034\n",
      "Epoch 81, batch 35 D Loss: 1.3246607780456543, G Loss: 0.9022009968757629\n",
      "Epoch 81, batch 36 D Loss: 1.5115935802459717, G Loss: 0.937925398349762\n",
      "Epoch 81, batch 37 D Loss: 1.6138279438018799, G Loss: 0.8007877469062805\n",
      "Epoch 81, batch 38 D Loss: 1.3967915773391724, G Loss: 0.8597942590713501\n",
      "Epoch 81, batch 39 D Loss: 1.4784965515136719, G Loss: 0.8752791881561279\n",
      "Epoch 81, batch 40 D Loss: 1.424147129058838, G Loss: 0.7773834466934204\n",
      "Epoch 81, batch 41 D Loss: 1.295128345489502, G Loss: 0.9080074429512024\n",
      "Epoch 81, batch 42 D Loss: 1.4829363822937012, G Loss: 0.7999209761619568\n",
      "Epoch 81, batch 43 D Loss: 1.2029067277908325, G Loss: 0.8797404766082764\n",
      "Epoch 81, batch 44 D Loss: 1.472929835319519, G Loss: 0.8936195969581604\n",
      "Epoch 81, batch 45 D Loss: 1.2718267440795898, G Loss: 0.8360812664031982\n",
      "Epoch 81, batch 46 D Loss: 1.3465397357940674, G Loss: 0.9522057175636292\n",
      "Epoch 81, batch 47 D Loss: 1.4076170921325684, G Loss: 0.8669534921646118\n",
      "Epoch 81, batch 48 D Loss: 1.495421290397644, G Loss: 0.8660789728164673\n",
      "Epoch 81, batch 49 D Loss: 1.219414472579956, G Loss: 0.8872065544128418\n",
      "Epoch 81, batch 50 D Loss: 1.2046287059783936, G Loss: 0.8621614575386047\n",
      "Epoch 81, batch 51 D Loss: 1.4723294973373413, G Loss: 0.8287538886070251\n",
      "Epoch 81, batch 52 D Loss: 1.329695701599121, G Loss: 0.9455887079238892\n",
      "Epoch 81, batch 53 D Loss: 1.3858742713928223, G Loss: 0.8426328301429749\n",
      "Epoch 81, batch 54 D Loss: 1.593120813369751, G Loss: 0.8176425695419312\n",
      "Epoch 81, batch 55 D Loss: 1.4192500114440918, G Loss: 0.8479868173599243\n",
      "Epoch 81, batch 56 D Loss: 1.3000547885894775, G Loss: 0.8018997311592102\n",
      "Epoch 81, batch 57 D Loss: 1.2437407970428467, G Loss: 0.9451610445976257\n",
      "Epoch 81, batch 58 D Loss: 1.4267213344573975, G Loss: 0.8317475914955139\n",
      "Epoch 81, batch 59 D Loss: 1.274655818939209, G Loss: 0.8904939293861389\n",
      "Epoch 81, batch 60 D Loss: 1.4240942001342773, G Loss: 0.920025646686554\n",
      "Epoch 81, batch 61 D Loss: 1.3158520460128784, G Loss: 0.9066225290298462\n",
      "Epoch 81, batch 62 D Loss: 1.5841290950775146, G Loss: 0.8281412720680237\n",
      "Epoch 81, batch 63 D Loss: 1.6458020210266113, G Loss: 0.8402054309844971\n",
      "Epoch 81, batch 64 D Loss: 1.31070876121521, G Loss: 0.9104655385017395\n",
      "Epoch 81, batch 65 D Loss: 1.285236120223999, G Loss: 0.922496497631073\n",
      "Epoch 81, batch 66 D Loss: 1.5004940032958984, G Loss: 0.8721086382865906\n",
      "Epoch 81, batch 67 D Loss: 1.3307008743286133, G Loss: 0.9126203656196594\n",
      "Epoch 81, batch 68 D Loss: 1.4932053089141846, G Loss: 0.8464314341545105\n",
      "Epoch 81, batch 69 D Loss: 1.7023320198059082, G Loss: 0.8662488460540771\n",
      "Epoch 81, batch 70 D Loss: 1.4618966579437256, G Loss: 0.88982754945755\n",
      "Epoch 81, batch 71 D Loss: 1.3041764497756958, G Loss: 0.9288569688796997\n",
      "Epoch 81, batch 72 D Loss: 1.2880477905273438, G Loss: 0.9264630675315857\n",
      "Epoch 81, batch 73 D Loss: 1.3192260265350342, G Loss: 0.8804168105125427\n",
      "Epoch 81, batch 74 D Loss: 1.4809789657592773, G Loss: 0.8815010786056519\n",
      "Epoch 81, batch 75 D Loss: 1.489911437034607, G Loss: 0.9166821241378784\n",
      "Epoch 81, batch 76 D Loss: 1.2594327926635742, G Loss: 0.9355660080909729\n",
      "Epoch 81, batch 77 D Loss: 1.4216315746307373, G Loss: 0.9544607400894165\n",
      "Epoch 81, batch 78 D Loss: 1.5826654434204102, G Loss: 0.9558384418487549\n",
      "Epoch 81, batch 79 D Loss: 1.3046287298202515, G Loss: 0.9212492108345032\n",
      "Epoch 81, batch 80 D Loss: 1.3736531734466553, G Loss: 0.888854444026947\n",
      "Epoch 81, batch 81 D Loss: 1.3911771774291992, G Loss: 0.8949489593505859\n",
      "Epoch 81, batch 82 D Loss: 1.47389554977417, G Loss: 0.9100841283798218\n",
      "Epoch 81, batch 83 D Loss: 1.4235302209854126, G Loss: 0.8321942090988159\n",
      "Epoch 81, batch 84 D Loss: 1.4156572818756104, G Loss: 0.9154365062713623\n",
      "Epoch 81, batch 85 D Loss: 1.4714410305023193, G Loss: 0.9167079925537109\n",
      "Epoch 81, batch 86 D Loss: 1.287672996520996, G Loss: 0.877478837966919\n",
      "Epoch 81, batch 87 D Loss: 1.3793087005615234, G Loss: 0.8737631440162659\n",
      "Epoch 81, batch 88 D Loss: 1.4750025272369385, G Loss: 0.8231382966041565\n",
      "Epoch 81, batch 89 D Loss: 1.3963433504104614, G Loss: 0.8443306088447571\n",
      "Epoch 81, batch 90 D Loss: 1.4715862274169922, G Loss: 0.942912220954895\n",
      "Epoch 81, batch 91 D Loss: 1.3396434783935547, G Loss: 0.958035409450531\n",
      "Epoch 81, batch 92 D Loss: 1.4403138160705566, G Loss: 0.8604944348335266\n",
      "Epoch 81, batch 93 D Loss: 1.4065287113189697, G Loss: 0.8670251369476318\n",
      "Epoch 81, batch 94 D Loss: 1.58982253074646, G Loss: 0.8793177008628845\n",
      "Epoch 81, batch 95 D Loss: 1.5382859706878662, G Loss: 0.8325899243354797\n",
      "Epoch 81, batch 96 D Loss: 1.7917778491973877, G Loss: 0.7710044384002686\n",
      "Epoch 81, batch 97 D Loss: 1.3146276473999023, G Loss: 0.9012776017189026\n",
      "Epoch 81, batch 98 D Loss: 1.3904396295547485, G Loss: 0.9277665019035339\n",
      "Epoch 81, batch 99 D Loss: 1.4651131629943848, G Loss: 0.791938304901123\n",
      "Epoch 81, batch 100 D Loss: 1.13258957862854, G Loss: 0.8943908214569092\n",
      "Epoch 81, batch 101 D Loss: 1.4076042175292969, G Loss: 0.9610260725021362\n",
      "Epoch 81, batch 102 D Loss: 1.2961469888687134, G Loss: 0.954484760761261\n",
      "Epoch 81, batch 103 D Loss: 1.4916176795959473, G Loss: 0.8225980997085571\n",
      "Epoch 81, batch 104 D Loss: 1.389195203781128, G Loss: 0.9762897491455078\n",
      "Epoch 81, batch 105 D Loss: 1.5155646800994873, G Loss: 0.9186965227127075\n",
      "Epoch 81, batch 106 D Loss: 1.5803271532058716, G Loss: 0.9384078979492188\n",
      "Epoch 81, batch 107 D Loss: 1.415584921836853, G Loss: 0.9064038991928101\n",
      "Epoch 81, batch 108 D Loss: 1.4735848903656006, G Loss: 0.8227809071540833\n",
      "Epoch 81, batch 109 D Loss: 1.5032528638839722, G Loss: 0.8723034858703613\n",
      "Epoch 81, batch 110 D Loss: 1.3872392177581787, G Loss: 0.9246591925621033\n",
      "Epoch 81, batch 111 D Loss: 1.4633406400680542, G Loss: 0.8691507577896118\n",
      "Epoch 81, batch 112 D Loss: 1.401871919631958, G Loss: 0.8643467426300049\n",
      "Epoch 81, batch 113 D Loss: 1.196800708770752, G Loss: 0.9008073210716248\n",
      "Epoch 81, batch 114 D Loss: 1.577189326286316, G Loss: 0.9010367393493652\n",
      "Epoch 81, batch 115 D Loss: 1.267707109451294, G Loss: 0.9044038653373718\n",
      "Epoch 81, batch 116 D Loss: 1.3267908096313477, G Loss: 0.9219370484352112\n",
      "Epoch 81, batch 117 D Loss: 1.426947832107544, G Loss: 0.8889555931091309\n",
      "Epoch 81, batch 118 D Loss: 1.4157644510269165, G Loss: 0.8682795763015747\n",
      "Epoch 81, batch 119 D Loss: 1.1313302516937256, G Loss: 0.9574010372161865\n",
      "Epoch 81, batch 120 D Loss: 1.223008632659912, G Loss: 0.8644224405288696\n",
      "Epoch 81, batch 121 D Loss: 1.4388468265533447, G Loss: 0.8338276743888855\n",
      "Epoch 81, batch 122 D Loss: 1.3384978771209717, G Loss: 0.8831499218940735\n",
      "Epoch 81, batch 123 D Loss: 1.4083174467086792, G Loss: 0.8582667708396912\n",
      "Epoch 81, batch 124 D Loss: 1.3852654695510864, G Loss: 0.9976718425750732\n",
      "Epoch 81, batch 125 D Loss: 1.3454148769378662, G Loss: 0.9505693316459656\n",
      "Epoch 81, batch 126 D Loss: 1.3781216144561768, G Loss: 0.9432575106620789\n",
      "Epoch 81, batch 127 D Loss: 1.446045160293579, G Loss: 0.9056192636489868\n",
      "Epoch 81, batch 128 D Loss: 1.3138781785964966, G Loss: 0.9232255816459656\n",
      "Epoch 81, batch 129 D Loss: 1.3345677852630615, G Loss: 0.9452135562896729\n",
      "Epoch 81, batch 130 D Loss: 1.3656872510910034, G Loss: 0.9392089247703552\n",
      "Epoch 81, batch 131 D Loss: 1.5105085372924805, G Loss: 0.8579252362251282\n",
      "Epoch 81, batch 132 D Loss: 1.5179779529571533, G Loss: 0.905036211013794\n",
      "Epoch 81, batch 133 D Loss: 1.311241626739502, G Loss: 0.9022202491760254\n",
      "Epoch 81, batch 134 D Loss: 1.2979508638381958, G Loss: 0.9730885028839111\n",
      "Epoch 81, batch 135 D Loss: 1.2645221948623657, G Loss: 0.9448997378349304\n",
      "Epoch 81, batch 136 D Loss: 1.3201850652694702, G Loss: 0.9221807718276978\n",
      "Epoch 81, batch 137 D Loss: 1.3672704696655273, G Loss: 0.9379738569259644\n",
      "Epoch 81, batch 138 D Loss: 1.2676396369934082, G Loss: 0.9860610365867615\n",
      "Epoch 81, batch 139 D Loss: 1.2148017883300781, G Loss: 1.0299465656280518\n",
      "Epoch 81, batch 140 D Loss: 1.3415124416351318, G Loss: 0.9290673732757568\n",
      "Epoch 81, batch 141 D Loss: 1.247610330581665, G Loss: 0.9815344214439392\n",
      "Epoch 81, batch 142 D Loss: 1.3035115003585815, G Loss: 0.9204591512680054\n",
      "Epoch 81, batch 143 D Loss: 1.4329279661178589, G Loss: 0.9208986163139343\n",
      "Epoch 81, batch 144 D Loss: 1.349202275276184, G Loss: 0.9454270005226135\n",
      "Epoch 81, batch 145 D Loss: 1.2644076347351074, G Loss: 0.9669317603111267\n",
      "Epoch 81, batch 146 D Loss: 1.2647149562835693, G Loss: 1.0020335912704468\n",
      "Epoch 81, batch 147 D Loss: 1.4075795412063599, G Loss: 0.9049350619316101\n",
      "Epoch 81, batch 148 D Loss: 1.382420539855957, G Loss: 0.9196727275848389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, batch 149 D Loss: 1.2778531312942505, G Loss: 1.0373653173446655\n",
      "Epoch 81, batch 150 D Loss: 1.372671127319336, G Loss: 0.9532738327980042\n",
      "Epoch 81, batch 151 D Loss: 1.3217895030975342, G Loss: 0.9913169145584106\n",
      "Epoch 81, batch 152 D Loss: 1.4191044569015503, G Loss: 0.8869087100028992\n",
      "Epoch 81, batch 153 D Loss: 1.535435438156128, G Loss: 0.8750545978546143\n",
      "Epoch 81, batch 154 D Loss: 1.3605597019195557, G Loss: 0.9773380756378174\n",
      "Epoch 81, batch 155 D Loss: 1.3238728046417236, G Loss: 0.9541147351264954\n",
      "Epoch 81, batch 156 D Loss: 1.3349145650863647, G Loss: 0.9755361676216125\n",
      "Epoch 81, batch 157 D Loss: 1.320053219795227, G Loss: 0.9928712248802185\n",
      "Epoch 81, batch 158 D Loss: 1.366861343383789, G Loss: 0.9970040321350098\n",
      "Epoch 81, batch 159 D Loss: 1.340421199798584, G Loss: 0.9654022455215454\n",
      "Epoch 81, batch 160 D Loss: 1.3047738075256348, G Loss: 0.9941655993461609\n",
      "Epoch 81, batch 161 D Loss: 1.3597803115844727, G Loss: 0.9593866467475891\n",
      "Epoch 81, batch 162 D Loss: 1.323244571685791, G Loss: 0.9635486602783203\n",
      "Epoch 81, batch 163 D Loss: 1.2948360443115234, G Loss: 1.0359716415405273\n",
      "Epoch 81, batch 164 D Loss: 1.322322964668274, G Loss: 1.0437498092651367\n",
      "Epoch 81, batch 165 D Loss: 1.206451654434204, G Loss: 1.0258980989456177\n",
      "Epoch 81, batch 166 D Loss: 1.1932600736618042, G Loss: 1.061475396156311\n",
      "Epoch 81, batch 167 D Loss: 1.2543866634368896, G Loss: 0.9917829632759094\n",
      "Epoch 81, batch 168 D Loss: 1.156660795211792, G Loss: 0.9901208281517029\n",
      "Epoch 81, batch 169 D Loss: 1.2584788799285889, G Loss: 1.0064003467559814\n",
      "Epoch 81, batch 170 D Loss: 1.1626102924346924, G Loss: 1.0351996421813965\n",
      "Epoch 81, batch 171 D Loss: 1.3108880519866943, G Loss: 0.986177384853363\n",
      "Epoch 81, batch 172 D Loss: 1.3488730192184448, G Loss: 0.9669576287269592\n",
      "Epoch 81, batch 173 D Loss: 1.254776954650879, G Loss: 1.0073766708374023\n",
      "Epoch 81, batch 174 D Loss: 1.2941231727600098, G Loss: 1.0601598024368286\n",
      "Epoch 81, batch 175 D Loss: 1.2141084671020508, G Loss: 1.0367867946624756\n",
      "Epoch 81, batch 176 D Loss: 1.2296127080917358, G Loss: 0.9671638607978821\n",
      "Epoch 81, batch 177 D Loss: 1.23874032497406, G Loss: 1.0325887203216553\n",
      "Epoch 81, batch 178 D Loss: 1.1600492000579834, G Loss: 1.102948784828186\n",
      "Epoch 81, batch 179 D Loss: 1.1624348163604736, G Loss: 1.1242988109588623\n",
      "Epoch 81, batch 180 D Loss: 1.1778510808944702, G Loss: 1.1490925550460815\n",
      "Epoch 81, batch 181 D Loss: 1.1751463413238525, G Loss: 1.1270347833633423\n",
      "Epoch 81, batch 182 D Loss: 1.092342734336853, G Loss: 1.1081361770629883\n",
      "Epoch 81, batch 183 D Loss: 1.0470439195632935, G Loss: 1.1416093111038208\n",
      "Epoch 81, batch 184 D Loss: 1.0861072540283203, G Loss: 1.1064708232879639\n",
      "Epoch 81, batch 185 D Loss: 1.137071967124939, G Loss: 1.1444549560546875\n",
      "Epoch 81, batch 186 D Loss: 1.1047441959381104, G Loss: 1.20143461227417\n",
      "Epoch 81, batch 187 D Loss: 1.1864168643951416, G Loss: 1.1857951879501343\n",
      "Epoch 81, batch 188 D Loss: 1.1519893407821655, G Loss: 1.100744366645813\n",
      "Epoch 81, batch 189 D Loss: 1.2210674285888672, G Loss: 1.142594337463379\n",
      "Epoch 81, batch 190 D Loss: 1.1915242671966553, G Loss: 1.1117156744003296\n",
      "Epoch 81, batch 191 D Loss: 1.0891361236572266, G Loss: 1.1881173849105835\n",
      "Epoch 81, batch 192 D Loss: 1.1104835271835327, G Loss: 1.1910871267318726\n",
      "Epoch 81, batch 193 D Loss: 1.308098316192627, G Loss: 1.135460376739502\n",
      "Epoch 81, batch 194 D Loss: 1.1952154636383057, G Loss: 1.2142510414123535\n",
      "Epoch 81, batch 195 D Loss: 1.0816154479980469, G Loss: 1.203092098236084\n",
      "Epoch 81, batch 196 D Loss: 1.1475605964660645, G Loss: 1.139722228050232\n",
      "Epoch 81, batch 197 D Loss: 1.0899319648742676, G Loss: 1.203830599784851\n",
      "Epoch 81, batch 198 D Loss: 0.9402965307235718, G Loss: 1.2307262420654297\n",
      "Epoch 81, batch 199 D Loss: 1.2656224966049194, G Loss: 1.15572190284729\n",
      "Epoch 81, batch 200 D Loss: 1.148026943206787, G Loss: 1.1617121696472168\n",
      "Epoch 82, batch 1 D Loss: 1.0415105819702148, G Loss: 1.1563695669174194\n",
      "Epoch 82, batch 2 D Loss: 1.0318118333816528, G Loss: 1.2092654705047607\n",
      "Epoch 82, batch 3 D Loss: 1.1750006675720215, G Loss: 1.121975064277649\n",
      "Epoch 82, batch 4 D Loss: 1.2854771614074707, G Loss: 1.1028778553009033\n",
      "Epoch 82, batch 5 D Loss: 1.2365576028823853, G Loss: 1.1663260459899902\n",
      "Epoch 82, batch 6 D Loss: 1.025728464126587, G Loss: 1.214369773864746\n",
      "Epoch 82, batch 7 D Loss: 1.0138370990753174, G Loss: 1.2206408977508545\n",
      "Epoch 82, batch 8 D Loss: 1.0448795557022095, G Loss: 1.2226978540420532\n",
      "Epoch 82, batch 9 D Loss: 1.1501601934432983, G Loss: 1.1988962888717651\n",
      "Epoch 82, batch 10 D Loss: 1.103645920753479, G Loss: 1.1752090454101562\n",
      "Epoch 82, batch 11 D Loss: 1.2038153409957886, G Loss: 1.200263500213623\n",
      "Epoch 82, batch 12 D Loss: 1.113997459411621, G Loss: 1.1307923793792725\n",
      "Epoch 82, batch 13 D Loss: 1.137939214706421, G Loss: 1.2093833684921265\n",
      "Epoch 82, batch 14 D Loss: 1.1583008766174316, G Loss: 1.0637202262878418\n",
      "Epoch 82, batch 15 D Loss: 1.1907628774642944, G Loss: 1.1659196615219116\n",
      "Epoch 82, batch 16 D Loss: 1.0792220830917358, G Loss: 1.1153056621551514\n",
      "Epoch 82, batch 17 D Loss: 1.1239244937896729, G Loss: 1.1369280815124512\n",
      "Epoch 82, batch 18 D Loss: 1.2169427871704102, G Loss: 1.1196720600128174\n",
      "Epoch 82, batch 19 D Loss: 1.1274534463882446, G Loss: 1.1271134614944458\n",
      "Epoch 82, batch 20 D Loss: 1.1825311183929443, G Loss: 1.2505900859832764\n",
      "Epoch 82, batch 21 D Loss: 1.115033745765686, G Loss: 1.051346778869629\n",
      "Epoch 82, batch 22 D Loss: 1.1670576333999634, G Loss: 1.103303074836731\n",
      "Epoch 82, batch 23 D Loss: 1.1414515972137451, G Loss: 1.1011104583740234\n",
      "Epoch 82, batch 24 D Loss: 1.170027732849121, G Loss: 1.139740228652954\n",
      "Epoch 82, batch 25 D Loss: 1.2219812870025635, G Loss: 1.086614727973938\n",
      "Epoch 82, batch 26 D Loss: 1.0983655452728271, G Loss: 1.145682692527771\n",
      "Epoch 82, batch 27 D Loss: 1.1635745763778687, G Loss: 1.1327570676803589\n",
      "Epoch 82, batch 28 D Loss: 1.093899130821228, G Loss: 0.9959815144538879\n",
      "Epoch 82, batch 29 D Loss: 1.1431493759155273, G Loss: 1.0536121129989624\n",
      "Epoch 82, batch 30 D Loss: 1.118112564086914, G Loss: 1.0873425006866455\n",
      "Epoch 82, batch 31 D Loss: 1.0933586359024048, G Loss: 1.0329272747039795\n",
      "Epoch 82, batch 32 D Loss: 1.1373172998428345, G Loss: 1.1037288904190063\n",
      "Epoch 82, batch 33 D Loss: 1.0203293561935425, G Loss: 1.0819042921066284\n",
      "Epoch 82, batch 34 D Loss: 1.2043695449829102, G Loss: 1.0882023572921753\n",
      "Epoch 82, batch 35 D Loss: 1.1105867624282837, G Loss: 1.1420807838439941\n",
      "Epoch 82, batch 36 D Loss: 1.236269235610962, G Loss: 1.040420651435852\n",
      "Epoch 82, batch 37 D Loss: 1.0958195924758911, G Loss: 1.2008435726165771\n",
      "Epoch 82, batch 38 D Loss: 1.1432766914367676, G Loss: 1.09714937210083\n",
      "Epoch 82, batch 39 D Loss: 1.0266231298446655, G Loss: 1.0568194389343262\n",
      "Epoch 82, batch 40 D Loss: 1.1861045360565186, G Loss: 1.0614686012268066\n",
      "Epoch 82, batch 41 D Loss: 1.108465313911438, G Loss: 1.127047061920166\n",
      "Epoch 82, batch 42 D Loss: 1.1490682363510132, G Loss: 1.0383286476135254\n",
      "Epoch 82, batch 43 D Loss: 1.045385479927063, G Loss: 1.0613312721252441\n",
      "Epoch 82, batch 44 D Loss: 1.138253927230835, G Loss: 1.0568876266479492\n",
      "Epoch 82, batch 45 D Loss: 1.2564680576324463, G Loss: 1.0470669269561768\n",
      "Epoch 82, batch 46 D Loss: 1.0910053253173828, G Loss: 1.052657961845398\n",
      "Epoch 82, batch 47 D Loss: 1.0857771635055542, G Loss: 1.0995951890945435\n",
      "Epoch 82, batch 48 D Loss: 1.135891079902649, G Loss: 1.1082243919372559\n",
      "Epoch 82, batch 49 D Loss: 1.190308690071106, G Loss: 1.07792329788208\n",
      "Epoch 82, batch 50 D Loss: 1.1369545459747314, G Loss: 1.1214632987976074\n",
      "Epoch 82, batch 51 D Loss: 1.0817261934280396, G Loss: 1.0938359498977661\n",
      "Epoch 82, batch 52 D Loss: 1.0111924409866333, G Loss: 1.1612213850021362\n",
      "Epoch 82, batch 53 D Loss: 1.1278986930847168, G Loss: 1.1066615581512451\n",
      "Epoch 82, batch 54 D Loss: 0.9621418714523315, G Loss: 1.0480862855911255\n",
      "Epoch 82, batch 55 D Loss: 1.0056633949279785, G Loss: 1.0847645998001099\n",
      "Epoch 82, batch 56 D Loss: 1.0704622268676758, G Loss: 1.0643305778503418\n",
      "Epoch 82, batch 57 D Loss: 0.972533106803894, G Loss: 1.1837220191955566\n",
      "Epoch 82, batch 58 D Loss: 1.2334932088851929, G Loss: 1.1722849607467651\n",
      "Epoch 82, batch 59 D Loss: 1.1981866359710693, G Loss: 1.1852632761001587\n",
      "Epoch 82, batch 60 D Loss: 1.1770954132080078, G Loss: 1.2501717805862427\n",
      "Epoch 82, batch 61 D Loss: 0.9965592622756958, G Loss: 1.2545738220214844\n",
      "Epoch 82, batch 62 D Loss: 1.1848349571228027, G Loss: 1.1043227910995483\n",
      "Epoch 82, batch 63 D Loss: 0.9216594696044922, G Loss: 1.3372405767440796\n",
      "Epoch 82, batch 64 D Loss: 1.283263087272644, G Loss: 1.2027406692504883\n",
      "Epoch 82, batch 65 D Loss: 1.0961413383483887, G Loss: 1.1166149377822876\n",
      "Epoch 82, batch 66 D Loss: 1.1209094524383545, G Loss: 1.261045217514038\n",
      "Epoch 82, batch 67 D Loss: 1.032149314880371, G Loss: 1.3335232734680176\n",
      "Epoch 82, batch 68 D Loss: 1.0307176113128662, G Loss: 1.1817290782928467\n",
      "Epoch 82, batch 69 D Loss: 1.0712954998016357, G Loss: 1.1121388673782349\n",
      "Epoch 82, batch 70 D Loss: 1.121949553489685, G Loss: 1.2557611465454102\n",
      "Epoch 82, batch 71 D Loss: 1.0800912380218506, G Loss: 1.074758768081665\n",
      "Epoch 82, batch 72 D Loss: 1.1702336072921753, G Loss: 1.180185317993164\n",
      "Epoch 82, batch 73 D Loss: 1.0535613298416138, G Loss: 1.0836882591247559\n",
      "Epoch 82, batch 74 D Loss: 1.1610559225082397, G Loss: 1.1346940994262695\n",
      "Epoch 82, batch 75 D Loss: 1.229363203048706, G Loss: 1.044012188911438\n",
      "Epoch 82, batch 76 D Loss: 1.1159470081329346, G Loss: 1.150612235069275\n",
      "Epoch 82, batch 77 D Loss: 1.3340046405792236, G Loss: 1.05279541015625\n",
      "Epoch 82, batch 78 D Loss: 1.2990325689315796, G Loss: 1.1264451742172241\n",
      "Epoch 82, batch 79 D Loss: 1.280374526977539, G Loss: 1.0043072700500488\n",
      "Epoch 82, batch 80 D Loss: 1.1362439393997192, G Loss: 1.0914151668548584\n",
      "Epoch 82, batch 81 D Loss: 1.257331132888794, G Loss: 1.012050986289978\n",
      "Epoch 82, batch 82 D Loss: 1.1191415786743164, G Loss: 1.0801186561584473\n",
      "Epoch 82, batch 83 D Loss: 1.1550228595733643, G Loss: 0.9514224529266357\n",
      "Epoch 82, batch 84 D Loss: 1.1633497476577759, G Loss: 1.0454155206680298\n",
      "Epoch 82, batch 85 D Loss: 1.3154335021972656, G Loss: 1.0161207914352417\n",
      "Epoch 82, batch 86 D Loss: 1.2475814819335938, G Loss: 0.9603294134140015\n",
      "Epoch 82, batch 87 D Loss: 1.1475409269332886, G Loss: 0.9593587517738342\n",
      "Epoch 82, batch 88 D Loss: 1.2972583770751953, G Loss: 0.9428955316543579\n",
      "Epoch 82, batch 89 D Loss: 0.9954580664634705, G Loss: 0.940257728099823\n",
      "Epoch 82, batch 90 D Loss: 1.0565118789672852, G Loss: 0.9752550721168518\n",
      "Epoch 82, batch 91 D Loss: 1.1889833211898804, G Loss: 0.9369457960128784\n",
      "Epoch 82, batch 92 D Loss: 1.2221778631210327, G Loss: 0.9153926968574524\n",
      "Epoch 82, batch 93 D Loss: 1.3366589546203613, G Loss: 0.919676661491394\n",
      "Epoch 82, batch 94 D Loss: 1.201488971710205, G Loss: 0.885343611240387\n",
      "Epoch 82, batch 95 D Loss: 1.227128267288208, G Loss: 0.9165219664573669\n",
      "Epoch 82, batch 96 D Loss: 0.986920952796936, G Loss: 0.9450387358665466\n",
      "Epoch 82, batch 97 D Loss: 1.2970682382583618, G Loss: 0.8359625339508057\n",
      "Epoch 82, batch 98 D Loss: 1.3003255128860474, G Loss: 0.8601182699203491\n",
      "Epoch 82, batch 99 D Loss: 1.3932712078094482, G Loss: 0.8921418190002441\n",
      "Epoch 82, batch 100 D Loss: 1.2768616676330566, G Loss: 0.9533242583274841\n",
      "Epoch 82, batch 101 D Loss: 1.1237685680389404, G Loss: 0.8968072533607483\n",
      "Epoch 82, batch 102 D Loss: 1.2489204406738281, G Loss: 0.8921344876289368\n",
      "Epoch 82, batch 103 D Loss: 1.1508342027664185, G Loss: 0.9245142340660095\n",
      "Epoch 82, batch 104 D Loss: 1.238097071647644, G Loss: 0.8956508040428162\n",
      "Epoch 82, batch 105 D Loss: 1.2123262882232666, G Loss: 0.9044421911239624\n",
      "Epoch 82, batch 106 D Loss: 1.419661521911621, G Loss: 0.8422920107841492\n",
      "Epoch 82, batch 107 D Loss: 1.5097410678863525, G Loss: 0.8861747980117798\n",
      "Epoch 82, batch 108 D Loss: 1.264212727546692, G Loss: 0.8473586440086365\n",
      "Epoch 82, batch 109 D Loss: 1.126516342163086, G Loss: 0.8886749744415283\n",
      "Epoch 82, batch 110 D Loss: 1.242026686668396, G Loss: 0.86370450258255\n",
      "Epoch 82, batch 111 D Loss: 1.3684543371200562, G Loss: 0.7932181358337402\n",
      "Epoch 82, batch 112 D Loss: 1.3426392078399658, G Loss: 0.8360071778297424\n",
      "Epoch 82, batch 113 D Loss: 1.1841018199920654, G Loss: 0.8362157344818115\n",
      "Epoch 82, batch 114 D Loss: 1.3858451843261719, G Loss: 0.8119390606880188\n",
      "Epoch 82, batch 115 D Loss: 1.2382512092590332, G Loss: 0.810710072517395\n",
      "Epoch 82, batch 116 D Loss: 1.4452786445617676, G Loss: 0.8033960461616516\n",
      "Epoch 82, batch 117 D Loss: 1.2254290580749512, G Loss: 0.788483738899231\n",
      "Epoch 82, batch 118 D Loss: 1.432219386100769, G Loss: 0.7710146903991699\n",
      "Epoch 82, batch 119 D Loss: 1.426737666130066, G Loss: 0.769255518913269\n",
      "Epoch 82, batch 120 D Loss: 1.2565439939498901, G Loss: 0.7867873907089233\n",
      "Epoch 82, batch 121 D Loss: 1.3335695266723633, G Loss: 0.770456075668335\n",
      "Epoch 82, batch 122 D Loss: 1.3285515308380127, G Loss: 0.7223057746887207\n",
      "Epoch 82, batch 123 D Loss: 1.5494375228881836, G Loss: 0.7408244609832764\n",
      "Epoch 82, batch 124 D Loss: 1.4210419654846191, G Loss: 0.7279088497161865\n",
      "Epoch 82, batch 125 D Loss: 1.4279131889343262, G Loss: 0.7320039868354797\n",
      "Epoch 82, batch 126 D Loss: 1.4684934616088867, G Loss: 0.6632416248321533\n",
      "Epoch 82, batch 127 D Loss: 1.5251494646072388, G Loss: 0.6378841996192932\n",
      "Epoch 82, batch 128 D Loss: 1.364283800125122, G Loss: 0.6972806453704834\n",
      "Epoch 82, batch 129 D Loss: 1.4580230712890625, G Loss: 0.6429051756858826\n",
      "Epoch 82, batch 130 D Loss: 1.4916796684265137, G Loss: 0.6503900289535522\n",
      "Epoch 82, batch 131 D Loss: 1.255737066268921, G Loss: 0.705840528011322\n",
      "Epoch 82, batch 132 D Loss: 1.5070655345916748, G Loss: 0.6351116895675659\n",
      "Epoch 82, batch 133 D Loss: 1.5161360502243042, G Loss: 0.5949147939682007\n",
      "Epoch 82, batch 134 D Loss: 1.4980756044387817, G Loss: 0.6195197105407715\n",
      "Epoch 82, batch 135 D Loss: 1.588613510131836, G Loss: 0.6216380596160889\n",
      "Epoch 82, batch 136 D Loss: 1.5923597812652588, G Loss: 0.5928093791007996\n",
      "Epoch 82, batch 137 D Loss: 1.4580717086791992, G Loss: 0.6240376234054565\n",
      "Epoch 82, batch 138 D Loss: 1.705865740776062, G Loss: 0.5733978748321533\n",
      "Epoch 82, batch 139 D Loss: 1.5597665309906006, G Loss: 0.6119438409805298\n",
      "Epoch 82, batch 140 D Loss: 1.652259349822998, G Loss: 0.5712400674819946\n",
      "Epoch 82, batch 141 D Loss: 1.6054552793502808, G Loss: 0.5554505586624146\n",
      "Epoch 82, batch 142 D Loss: 1.6490017175674438, G Loss: 0.5426456332206726\n",
      "Epoch 82, batch 143 D Loss: 1.6767560243606567, G Loss: 0.5357587337493896\n",
      "Epoch 82, batch 144 D Loss: 1.4828898906707764, G Loss: 0.5676169991493225\n",
      "Epoch 82, batch 145 D Loss: 1.605848789215088, G Loss: 0.5563575029373169\n",
      "Epoch 82, batch 146 D Loss: 1.69429349899292, G Loss: 0.5072655081748962\n",
      "Epoch 82, batch 147 D Loss: 1.6584481000900269, G Loss: 0.5681858658790588\n",
      "Epoch 82, batch 148 D Loss: 1.3278312683105469, G Loss: 0.6091482043266296\n",
      "Epoch 82, batch 149 D Loss: 1.536076545715332, G Loss: 0.5547021627426147\n",
      "Epoch 82, batch 150 D Loss: 1.6826668977737427, G Loss: 0.6188950538635254\n",
      "Epoch 82, batch 151 D Loss: 1.4873378276824951, G Loss: 0.5639058947563171\n",
      "Epoch 82, batch 152 D Loss: 1.4982496500015259, G Loss: 0.5451605319976807\n",
      "Epoch 82, batch 153 D Loss: 1.5723953247070312, G Loss: 0.5882303714752197\n",
      "Epoch 82, batch 154 D Loss: 1.705329179763794, G Loss: 0.5455067157745361\n",
      "Epoch 82, batch 155 D Loss: 1.6987018585205078, G Loss: 0.5101398229598999\n",
      "Epoch 82, batch 156 D Loss: 1.503504991531372, G Loss: 0.5718812942504883\n",
      "Epoch 82, batch 157 D Loss: 1.692460298538208, G Loss: 0.5716664791107178\n",
      "Epoch 82, batch 158 D Loss: 1.4645168781280518, G Loss: 0.5238473415374756\n",
      "Epoch 82, batch 159 D Loss: 1.7021920680999756, G Loss: 0.5272994041442871\n",
      "Epoch 82, batch 160 D Loss: 1.4989311695098877, G Loss: 0.5732231736183167\n",
      "Epoch 82, batch 161 D Loss: 1.5963666439056396, G Loss: 0.5208438634872437\n",
      "Epoch 82, batch 162 D Loss: 1.5568573474884033, G Loss: 0.5662740468978882\n",
      "Epoch 82, batch 163 D Loss: 1.3578331470489502, G Loss: 0.6735477447509766\n",
      "Epoch 82, batch 164 D Loss: 1.459367275238037, G Loss: 0.6043241024017334\n",
      "Epoch 82, batch 165 D Loss: 1.5631091594696045, G Loss: 0.5568721294403076\n",
      "Epoch 82, batch 166 D Loss: 1.4537410736083984, G Loss: 0.6112112998962402\n",
      "Epoch 82, batch 167 D Loss: 1.5415095090866089, G Loss: 0.61119145154953\n",
      "Epoch 82, batch 168 D Loss: 1.438622236251831, G Loss: 0.6490960121154785\n",
      "Epoch 82, batch 169 D Loss: 1.5458581447601318, G Loss: 0.6638146042823792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, batch 170 D Loss: 1.6294655799865723, G Loss: 0.5939762592315674\n",
      "Epoch 82, batch 171 D Loss: 1.4041087627410889, G Loss: 0.6744260191917419\n",
      "Epoch 82, batch 172 D Loss: 1.6257858276367188, G Loss: 0.571212887763977\n",
      "Epoch 82, batch 173 D Loss: 1.262102723121643, G Loss: 0.7002623081207275\n",
      "Epoch 82, batch 174 D Loss: 1.7071795463562012, G Loss: 0.625128448009491\n",
      "Epoch 82, batch 175 D Loss: 1.475555658340454, G Loss: 0.6980680823326111\n",
      "Epoch 82, batch 176 D Loss: 1.4283642768859863, G Loss: 0.7471739053726196\n",
      "Epoch 82, batch 177 D Loss: 1.599813461303711, G Loss: 0.7045359015464783\n",
      "Epoch 82, batch 178 D Loss: 1.5565247535705566, G Loss: 0.724113941192627\n",
      "Epoch 82, batch 179 D Loss: 1.6130990982055664, G Loss: 0.6569745540618896\n",
      "Epoch 82, batch 180 D Loss: 1.4598326683044434, G Loss: 0.757154643535614\n",
      "Epoch 82, batch 181 D Loss: 1.495776891708374, G Loss: 0.7465305328369141\n",
      "Epoch 82, batch 182 D Loss: 1.4728269577026367, G Loss: 0.7714986205101013\n",
      "Epoch 82, batch 183 D Loss: 1.3656034469604492, G Loss: 0.7581164836883545\n",
      "Epoch 82, batch 184 D Loss: 1.3307750225067139, G Loss: 0.730216920375824\n",
      "Epoch 82, batch 185 D Loss: 1.4067845344543457, G Loss: 0.7703633308410645\n",
      "Epoch 82, batch 186 D Loss: 1.2732056379318237, G Loss: 0.8064558506011963\n",
      "Epoch 82, batch 187 D Loss: 1.4950997829437256, G Loss: 0.790722668170929\n",
      "Epoch 82, batch 188 D Loss: 1.3365174531936646, G Loss: 0.7698665857315063\n",
      "Epoch 82, batch 189 D Loss: 1.3625669479370117, G Loss: 0.8018461465835571\n",
      "Epoch 82, batch 190 D Loss: 1.4050127267837524, G Loss: 0.8301239013671875\n",
      "Epoch 82, batch 191 D Loss: 1.263709306716919, G Loss: 0.8198237419128418\n",
      "Epoch 82, batch 192 D Loss: 1.2119128704071045, G Loss: 0.8440197110176086\n",
      "Epoch 82, batch 193 D Loss: 1.4125269651412964, G Loss: 0.8233783841133118\n",
      "Epoch 82, batch 194 D Loss: 1.384239912033081, G Loss: 0.7655588388442993\n",
      "Epoch 82, batch 195 D Loss: 1.3522051572799683, G Loss: 0.7768226861953735\n",
      "Epoch 82, batch 196 D Loss: 1.340357780456543, G Loss: 0.8744499087333679\n",
      "Epoch 82, batch 197 D Loss: 1.190368890762329, G Loss: 0.883741021156311\n",
      "Epoch 82, batch 198 D Loss: 1.4408912658691406, G Loss: 0.8266527056694031\n",
      "Epoch 82, batch 199 D Loss: 1.2261853218078613, G Loss: 0.9325300455093384\n",
      "Epoch 82, batch 200 D Loss: 1.309556007385254, G Loss: 0.8482372164726257\n",
      "Epoch 83, batch 1 D Loss: 1.349630355834961, G Loss: 0.8580842614173889\n",
      "Epoch 83, batch 2 D Loss: 1.0602467060089111, G Loss: 0.9284313321113586\n",
      "Epoch 83, batch 3 D Loss: 1.406703233718872, G Loss: 0.8248375654220581\n",
      "Epoch 83, batch 4 D Loss: 1.181728482246399, G Loss: 0.8920299410820007\n",
      "Epoch 83, batch 5 D Loss: 1.2497608661651611, G Loss: 0.9209019541740417\n",
      "Epoch 83, batch 6 D Loss: 1.1486856937408447, G Loss: 0.9040017127990723\n",
      "Epoch 83, batch 7 D Loss: 1.2883992195129395, G Loss: 0.8795607686042786\n",
      "Epoch 83, batch 8 D Loss: 1.1470482349395752, G Loss: 0.8935492634773254\n",
      "Epoch 83, batch 9 D Loss: 1.2005739212036133, G Loss: 0.9693955779075623\n",
      "Epoch 83, batch 10 D Loss: 1.2920310497283936, G Loss: 0.890902578830719\n",
      "Epoch 83, batch 11 D Loss: 1.1518244743347168, G Loss: 0.9403959512710571\n",
      "Epoch 83, batch 12 D Loss: 1.3349902629852295, G Loss: 0.8533872365951538\n",
      "Epoch 83, batch 13 D Loss: 1.2608627080917358, G Loss: 0.9461854696273804\n",
      "Epoch 83, batch 14 D Loss: 1.096372365951538, G Loss: 1.0131878852844238\n",
      "Epoch 83, batch 15 D Loss: 1.1966800689697266, G Loss: 0.9417951703071594\n",
      "Epoch 83, batch 16 D Loss: 1.1715447902679443, G Loss: 0.9504607915878296\n",
      "Epoch 83, batch 17 D Loss: 1.2303109169006348, G Loss: 0.9739074110984802\n",
      "Epoch 83, batch 18 D Loss: 1.2223992347717285, G Loss: 0.931541919708252\n",
      "Epoch 83, batch 19 D Loss: 1.0408536195755005, G Loss: 1.0311137437820435\n",
      "Epoch 83, batch 20 D Loss: 1.0392704010009766, G Loss: 1.0393192768096924\n",
      "Epoch 83, batch 21 D Loss: 1.2180709838867188, G Loss: 0.9337179064750671\n",
      "Epoch 83, batch 22 D Loss: 1.1395329236984253, G Loss: 1.0257740020751953\n",
      "Epoch 83, batch 23 D Loss: 1.1145615577697754, G Loss: 0.9716548919677734\n",
      "Epoch 83, batch 24 D Loss: 1.119767189025879, G Loss: 1.052642822265625\n",
      "Epoch 83, batch 25 D Loss: 0.9431402683258057, G Loss: 1.0521959066390991\n",
      "Epoch 83, batch 26 D Loss: 1.081708312034607, G Loss: 1.0052475929260254\n",
      "Epoch 83, batch 27 D Loss: 1.1778193712234497, G Loss: 1.030413269996643\n",
      "Epoch 83, batch 28 D Loss: 0.9851722717285156, G Loss: 1.0807913541793823\n",
      "Epoch 83, batch 29 D Loss: 1.0493800640106201, G Loss: 1.0542290210723877\n",
      "Epoch 83, batch 30 D Loss: 1.1105668544769287, G Loss: 1.0342823266983032\n",
      "Epoch 83, batch 31 D Loss: 1.095387578010559, G Loss: 0.9917594194412231\n",
      "Epoch 83, batch 32 D Loss: 1.0432416200637817, G Loss: 1.0723224878311157\n",
      "Epoch 83, batch 33 D Loss: 1.0984246730804443, G Loss: 1.0086005926132202\n",
      "Epoch 83, batch 34 D Loss: 0.9661322832107544, G Loss: 1.069786787033081\n",
      "Epoch 83, batch 35 D Loss: 1.0042524337768555, G Loss: 0.9866897463798523\n",
      "Epoch 83, batch 36 D Loss: 1.0370588302612305, G Loss: 1.0420056581497192\n",
      "Epoch 83, batch 37 D Loss: 1.041046380996704, G Loss: 1.1043697595596313\n",
      "Epoch 83, batch 38 D Loss: 1.1034483909606934, G Loss: 1.0367494821548462\n",
      "Epoch 83, batch 39 D Loss: 1.0422956943511963, G Loss: 1.0996251106262207\n",
      "Epoch 83, batch 40 D Loss: 0.9944791793823242, G Loss: 1.0888651609420776\n",
      "Epoch 83, batch 41 D Loss: 1.1500722169876099, G Loss: 1.1001642942428589\n",
      "Epoch 83, batch 42 D Loss: 1.011775016784668, G Loss: 1.0387483835220337\n",
      "Epoch 83, batch 43 D Loss: 1.2499313354492188, G Loss: 0.9621984958648682\n",
      "Epoch 83, batch 44 D Loss: 1.072582721710205, G Loss: 1.0829358100891113\n",
      "Epoch 83, batch 45 D Loss: 0.8738322257995605, G Loss: 1.0956817865371704\n",
      "Epoch 83, batch 46 D Loss: 1.0076159238815308, G Loss: 1.1023045778274536\n",
      "Epoch 83, batch 47 D Loss: 0.9047889709472656, G Loss: 1.0988086462020874\n",
      "Epoch 83, batch 48 D Loss: 0.9851910471916199, G Loss: 1.0759291648864746\n",
      "Epoch 83, batch 49 D Loss: 1.0342429876327515, G Loss: 1.0945463180541992\n",
      "Epoch 83, batch 50 D Loss: 0.980724036693573, G Loss: 1.1050509214401245\n",
      "Epoch 83, batch 51 D Loss: 0.8489406108856201, G Loss: 1.1306486129760742\n",
      "Epoch 83, batch 52 D Loss: 1.059771180152893, G Loss: 1.0767585039138794\n",
      "Epoch 83, batch 53 D Loss: 1.1217924356460571, G Loss: 1.0903260707855225\n",
      "Epoch 83, batch 54 D Loss: 0.9964337348937988, G Loss: 1.1172513961791992\n",
      "Epoch 83, batch 55 D Loss: 0.9442517161369324, G Loss: 1.063094139099121\n",
      "Epoch 83, batch 56 D Loss: 0.8604605197906494, G Loss: 1.0789905786514282\n",
      "Epoch 83, batch 57 D Loss: 1.0273557901382446, G Loss: 1.093308448791504\n",
      "Epoch 83, batch 58 D Loss: 1.1149954795837402, G Loss: 1.066759467124939\n",
      "Epoch 83, batch 59 D Loss: 1.0076996088027954, G Loss: 1.1357864141464233\n",
      "Epoch 83, batch 60 D Loss: 1.031864047050476, G Loss: 1.1408867835998535\n",
      "Epoch 83, batch 61 D Loss: 1.0541788339614868, G Loss: 1.1485817432403564\n",
      "Epoch 83, batch 62 D Loss: 0.9947295188903809, G Loss: 1.0615533590316772\n",
      "Epoch 83, batch 63 D Loss: 0.9908428192138672, G Loss: 1.0760958194732666\n",
      "Epoch 83, batch 64 D Loss: 1.0611450672149658, G Loss: 1.0906665325164795\n",
      "Epoch 83, batch 65 D Loss: 0.9478222131729126, G Loss: 1.0157305002212524\n",
      "Epoch 83, batch 66 D Loss: 1.187499761581421, G Loss: 1.039607048034668\n",
      "Epoch 83, batch 67 D Loss: 1.0653244256973267, G Loss: 1.003688097000122\n",
      "Epoch 83, batch 68 D Loss: 1.082362174987793, G Loss: 1.0341942310333252\n",
      "Epoch 83, batch 69 D Loss: 1.1315805912017822, G Loss: 1.1100326776504517\n",
      "Epoch 83, batch 70 D Loss: 0.9967559576034546, G Loss: 1.0617316961288452\n",
      "Epoch 83, batch 71 D Loss: 1.0621857643127441, G Loss: 1.0143910646438599\n",
      "Epoch 83, batch 72 D Loss: 1.0649648904800415, G Loss: 1.054419994354248\n",
      "Epoch 83, batch 73 D Loss: 0.9296656847000122, G Loss: 1.0809565782546997\n",
      "Epoch 83, batch 74 D Loss: 1.0283989906311035, G Loss: 1.0143948793411255\n",
      "Epoch 83, batch 75 D Loss: 1.1068105697631836, G Loss: 1.0095198154449463\n",
      "Epoch 83, batch 76 D Loss: 0.791380763053894, G Loss: 1.0922259092330933\n",
      "Epoch 83, batch 77 D Loss: 1.031369924545288, G Loss: 1.0234911441802979\n",
      "Epoch 83, batch 78 D Loss: 0.9597214460372925, G Loss: 1.0541349649429321\n",
      "Epoch 83, batch 79 D Loss: 1.1902835369110107, G Loss: 0.9976806044578552\n",
      "Epoch 83, batch 80 D Loss: 1.3907177448272705, G Loss: 0.9392340183258057\n",
      "Epoch 83, batch 81 D Loss: 1.171346664428711, G Loss: 0.9621900916099548\n",
      "Epoch 83, batch 82 D Loss: 1.0365633964538574, G Loss: 0.9895706176757812\n",
      "Epoch 83, batch 83 D Loss: 1.0513707399368286, G Loss: 1.0123130083084106\n",
      "Epoch 83, batch 84 D Loss: 1.0278878211975098, G Loss: 0.9763199687004089\n",
      "Epoch 83, batch 85 D Loss: 1.0552293062210083, G Loss: 0.9803332686424255\n",
      "Epoch 83, batch 86 D Loss: 1.1664763689041138, G Loss: 0.9296656847000122\n",
      "Epoch 83, batch 87 D Loss: 0.9815846085548401, G Loss: 0.9914687275886536\n",
      "Epoch 83, batch 88 D Loss: 1.0345754623413086, G Loss: 0.9300482273101807\n",
      "Epoch 83, batch 89 D Loss: 1.111876130104065, G Loss: 0.9788687229156494\n",
      "Epoch 83, batch 90 D Loss: 1.2774513959884644, G Loss: 0.8440307378768921\n",
      "Epoch 83, batch 91 D Loss: 1.0755102634429932, G Loss: 0.8627582788467407\n",
      "Epoch 83, batch 92 D Loss: 1.0919562578201294, G Loss: 0.9352536201477051\n",
      "Epoch 83, batch 93 D Loss: 1.2295619249343872, G Loss: 0.9123271107673645\n",
      "Epoch 83, batch 94 D Loss: 1.2716519832611084, G Loss: 0.8848947882652283\n",
      "Epoch 83, batch 95 D Loss: 1.1696932315826416, G Loss: 0.9018725752830505\n",
      "Epoch 83, batch 96 D Loss: 1.189974308013916, G Loss: 0.889028787612915\n",
      "Epoch 83, batch 97 D Loss: 1.0782389640808105, G Loss: 0.9053745865821838\n",
      "Epoch 83, batch 98 D Loss: 1.216325283050537, G Loss: 0.923663854598999\n",
      "Epoch 83, batch 99 D Loss: 1.1337306499481201, G Loss: 0.8914013504981995\n",
      "Epoch 83, batch 100 D Loss: 1.439476490020752, G Loss: 0.8111883401870728\n",
      "Epoch 83, batch 101 D Loss: 1.3389779329299927, G Loss: 0.910937488079071\n",
      "Epoch 83, batch 102 D Loss: 1.1023064851760864, G Loss: 0.92039954662323\n",
      "Epoch 83, batch 103 D Loss: 1.3971644639968872, G Loss: 0.8069316148757935\n",
      "Epoch 83, batch 104 D Loss: 1.328959345817566, G Loss: 0.8464203476905823\n",
      "Epoch 83, batch 105 D Loss: 1.26194167137146, G Loss: 0.8435627222061157\n",
      "Epoch 83, batch 106 D Loss: 1.305580735206604, G Loss: 0.7785098552703857\n",
      "Epoch 83, batch 107 D Loss: 1.3489166498184204, G Loss: 0.7520392537117004\n",
      "Epoch 83, batch 108 D Loss: 1.273435115814209, G Loss: 0.7845196723937988\n",
      "Epoch 83, batch 109 D Loss: 1.3974493741989136, G Loss: 0.8069061040878296\n",
      "Epoch 83, batch 110 D Loss: 1.2752829790115356, G Loss: 0.8369102478027344\n",
      "Epoch 83, batch 111 D Loss: 1.6166110038757324, G Loss: 0.7253063321113586\n",
      "Epoch 83, batch 112 D Loss: 1.3067433834075928, G Loss: 0.7444509863853455\n",
      "Epoch 83, batch 113 D Loss: 1.182681679725647, G Loss: 0.7701882719993591\n",
      "Epoch 83, batch 114 D Loss: 1.395606279373169, G Loss: 0.7390815019607544\n",
      "Epoch 83, batch 115 D Loss: 1.3236994743347168, G Loss: 0.811587393283844\n",
      "Epoch 83, batch 116 D Loss: 1.2743581533432007, G Loss: 0.8081662058830261\n",
      "Epoch 83, batch 117 D Loss: 1.5227733850479126, G Loss: 0.763018786907196\n",
      "Epoch 83, batch 118 D Loss: 1.5439698696136475, G Loss: 0.7531247138977051\n",
      "Epoch 83, batch 119 D Loss: 1.3917611837387085, G Loss: 0.8091253042221069\n",
      "Epoch 83, batch 120 D Loss: 1.381069540977478, G Loss: 0.857546865940094\n",
      "Epoch 83, batch 121 D Loss: 1.401874303817749, G Loss: 0.7690348625183105\n",
      "Epoch 83, batch 122 D Loss: 1.483003854751587, G Loss: 0.707699179649353\n",
      "Epoch 83, batch 123 D Loss: 1.6219823360443115, G Loss: 0.644349992275238\n",
      "Epoch 83, batch 124 D Loss: 1.4080146551132202, G Loss: 0.7666043639183044\n",
      "Epoch 83, batch 125 D Loss: 1.326056957244873, G Loss: 0.761045515537262\n",
      "Epoch 83, batch 126 D Loss: 1.3757710456848145, G Loss: 0.7408826947212219\n",
      "Epoch 83, batch 127 D Loss: 1.446382999420166, G Loss: 0.6999497413635254\n",
      "Epoch 83, batch 128 D Loss: 1.4173918962478638, G Loss: 0.7941713929176331\n",
      "Epoch 83, batch 129 D Loss: 1.492955207824707, G Loss: 0.7424423098564148\n",
      "Epoch 83, batch 130 D Loss: 1.385776162147522, G Loss: 0.7628625631332397\n",
      "Epoch 83, batch 131 D Loss: 1.285181999206543, G Loss: 0.7442970275878906\n",
      "Epoch 83, batch 132 D Loss: 1.539818286895752, G Loss: 0.6477022767066956\n",
      "Epoch 83, batch 133 D Loss: 1.4233131408691406, G Loss: 0.6745247840881348\n",
      "Epoch 83, batch 134 D Loss: 1.5404716730117798, G Loss: 0.7291411757469177\n",
      "Epoch 83, batch 135 D Loss: 1.6201798915863037, G Loss: 0.6226449608802795\n",
      "Epoch 83, batch 136 D Loss: 1.4678184986114502, G Loss: 0.7218804359436035\n",
      "Epoch 83, batch 137 D Loss: 1.5046300888061523, G Loss: 0.7124649286270142\n",
      "Epoch 83, batch 138 D Loss: 1.4322049617767334, G Loss: 0.70066237449646\n",
      "Epoch 83, batch 139 D Loss: 1.6120601892471313, G Loss: 0.7162256836891174\n",
      "Epoch 83, batch 140 D Loss: 1.6345086097717285, G Loss: 0.634122908115387\n",
      "Epoch 83, batch 141 D Loss: 1.4528427124023438, G Loss: 0.6438373327255249\n",
      "Epoch 83, batch 142 D Loss: 1.6025065183639526, G Loss: 0.6585899591445923\n",
      "Epoch 83, batch 143 D Loss: 1.3838565349578857, G Loss: 0.7017385363578796\n",
      "Epoch 83, batch 144 D Loss: 1.5125343799591064, G Loss: 0.7016707062721252\n",
      "Epoch 83, batch 145 D Loss: 1.6910789012908936, G Loss: 0.6836504936218262\n",
      "Epoch 83, batch 146 D Loss: 1.5089831352233887, G Loss: 0.685710072517395\n",
      "Epoch 83, batch 147 D Loss: 1.5044291019439697, G Loss: 0.6861316561698914\n",
      "Epoch 83, batch 148 D Loss: 1.4420702457427979, G Loss: 0.7168357968330383\n",
      "Epoch 83, batch 149 D Loss: 1.6873657703399658, G Loss: 0.6628692150115967\n",
      "Epoch 83, batch 150 D Loss: 1.5617344379425049, G Loss: 0.6583131551742554\n",
      "Epoch 83, batch 151 D Loss: 1.5669593811035156, G Loss: 0.6762927174568176\n",
      "Epoch 83, batch 152 D Loss: 1.4131414890289307, G Loss: 0.7245786190032959\n",
      "Epoch 83, batch 153 D Loss: 1.678556203842163, G Loss: 0.6014163494110107\n",
      "Epoch 83, batch 154 D Loss: 1.5779838562011719, G Loss: 0.6571536064147949\n",
      "Epoch 83, batch 155 D Loss: 1.5575485229492188, G Loss: 0.6737896800041199\n",
      "Epoch 83, batch 156 D Loss: 1.4934422969818115, G Loss: 0.6786546111106873\n",
      "Epoch 83, batch 157 D Loss: 1.4044454097747803, G Loss: 0.7085882425308228\n",
      "Epoch 83, batch 158 D Loss: 1.5388541221618652, G Loss: 0.6768583655357361\n",
      "Epoch 83, batch 159 D Loss: 1.6346485614776611, G Loss: 0.5998575091362\n",
      "Epoch 83, batch 160 D Loss: 1.5345244407653809, G Loss: 0.6245118975639343\n",
      "Epoch 83, batch 161 D Loss: 1.6039713621139526, G Loss: 0.6127974987030029\n",
      "Epoch 83, batch 162 D Loss: 1.526585578918457, G Loss: 0.6753519177436829\n",
      "Epoch 83, batch 163 D Loss: 1.485825777053833, G Loss: 0.6519960761070251\n",
      "Epoch 83, batch 164 D Loss: 1.5686252117156982, G Loss: 0.6881313920021057\n",
      "Epoch 83, batch 165 D Loss: 1.5773603916168213, G Loss: 0.6229486465454102\n",
      "Epoch 83, batch 166 D Loss: 1.554678201675415, G Loss: 0.702639102935791\n",
      "Epoch 83, batch 167 D Loss: 1.4386465549468994, G Loss: 0.6694905757904053\n",
      "Epoch 83, batch 168 D Loss: 1.59051513671875, G Loss: 0.6796401739120483\n",
      "Epoch 83, batch 169 D Loss: 1.587658166885376, G Loss: 0.6514678001403809\n",
      "Epoch 83, batch 170 D Loss: 1.6419181823730469, G Loss: 0.6344544291496277\n",
      "Epoch 83, batch 171 D Loss: 1.6331396102905273, G Loss: 0.6567978858947754\n",
      "Epoch 83, batch 172 D Loss: 1.4615046977996826, G Loss: 0.7065102458000183\n",
      "Epoch 83, batch 173 D Loss: 1.600738286972046, G Loss: 0.6678617000579834\n",
      "Epoch 83, batch 174 D Loss: 1.5576879978179932, G Loss: 0.6617904901504517\n",
      "Epoch 83, batch 175 D Loss: 1.6385228633880615, G Loss: 0.6133309006690979\n",
      "Epoch 83, batch 176 D Loss: 1.5707066059112549, G Loss: 0.6771479845046997\n",
      "Epoch 83, batch 177 D Loss: 1.5608265399932861, G Loss: 0.6748416423797607\n",
      "Epoch 83, batch 178 D Loss: 1.5908668041229248, G Loss: 0.6825008988380432\n",
      "Epoch 83, batch 179 D Loss: 1.5476815700531006, G Loss: 0.6450011730194092\n",
      "Epoch 83, batch 180 D Loss: 1.5304512977600098, G Loss: 0.6911947727203369\n",
      "Epoch 83, batch 181 D Loss: 1.6187134981155396, G Loss: 0.6708471775054932\n",
      "Epoch 83, batch 182 D Loss: 1.4564892053604126, G Loss: 0.6976023316383362\n",
      "Epoch 83, batch 183 D Loss: 1.468813419342041, G Loss: 0.7498243451118469\n",
      "Epoch 83, batch 184 D Loss: 1.5775293111801147, G Loss: 0.6872457265853882\n",
      "Epoch 83, batch 185 D Loss: 1.5909831523895264, G Loss: 0.686947226524353\n",
      "Epoch 83, batch 186 D Loss: 1.3912405967712402, G Loss: 0.7501777410507202\n",
      "Epoch 83, batch 187 D Loss: 1.4718632698059082, G Loss: 0.7118614912033081\n",
      "Epoch 83, batch 188 D Loss: 1.4556056261062622, G Loss: 0.7699874043464661\n",
      "Epoch 83, batch 189 D Loss: 1.4458754062652588, G Loss: 0.7824032306671143\n",
      "Epoch 83, batch 190 D Loss: 1.5790257453918457, G Loss: 0.71245276927948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, batch 191 D Loss: 1.4625669717788696, G Loss: 0.7379695177078247\n",
      "Epoch 83, batch 192 D Loss: 1.560904860496521, G Loss: 0.7197161912918091\n",
      "Epoch 83, batch 193 D Loss: 1.5595941543579102, G Loss: 0.7118974328041077\n",
      "Epoch 83, batch 194 D Loss: 1.5954818725585938, G Loss: 0.7230302691459656\n",
      "Epoch 83, batch 195 D Loss: 1.4396792650222778, G Loss: 0.774335503578186\n",
      "Epoch 83, batch 196 D Loss: 1.4294867515563965, G Loss: 0.8062106966972351\n",
      "Epoch 83, batch 197 D Loss: 1.474848747253418, G Loss: 0.8391146659851074\n",
      "Epoch 83, batch 198 D Loss: 1.5048308372497559, G Loss: 0.7603854537010193\n",
      "Epoch 83, batch 199 D Loss: 1.4745512008666992, G Loss: 0.7670789957046509\n",
      "Epoch 83, batch 200 D Loss: 1.4701764583587646, G Loss: 0.7789783477783203\n",
      "Epoch 84, batch 1 D Loss: 1.436389446258545, G Loss: 0.7751004099845886\n",
      "Epoch 84, batch 2 D Loss: 1.3827972412109375, G Loss: 0.8608905076980591\n",
      "Epoch 84, batch 3 D Loss: 1.479393482208252, G Loss: 0.8042262196540833\n",
      "Epoch 84, batch 4 D Loss: 1.4329134225845337, G Loss: 0.7795067429542542\n",
      "Epoch 84, batch 5 D Loss: 1.4450011253356934, G Loss: 0.7952934503555298\n",
      "Epoch 84, batch 6 D Loss: 1.409423828125, G Loss: 0.8214688897132874\n",
      "Epoch 84, batch 7 D Loss: 1.4052433967590332, G Loss: 0.8297665119171143\n",
      "Epoch 84, batch 8 D Loss: 1.423753023147583, G Loss: 0.8275080323219299\n",
      "Epoch 84, batch 9 D Loss: 1.4688576459884644, G Loss: 0.7864515781402588\n",
      "Epoch 84, batch 10 D Loss: 1.4514000415802002, G Loss: 0.8156422972679138\n",
      "Epoch 84, batch 11 D Loss: 1.3574450016021729, G Loss: 0.839144766330719\n",
      "Epoch 84, batch 12 D Loss: 1.358964443206787, G Loss: 0.8839255571365356\n",
      "Epoch 84, batch 13 D Loss: 1.4006226062774658, G Loss: 0.8860490918159485\n",
      "Epoch 84, batch 14 D Loss: 1.329404592514038, G Loss: 0.8952711224555969\n",
      "Epoch 84, batch 15 D Loss: 1.370664358139038, G Loss: 0.8747692704200745\n",
      "Epoch 84, batch 16 D Loss: 1.36800217628479, G Loss: 0.8584180474281311\n",
      "Epoch 84, batch 17 D Loss: 1.402430772781372, G Loss: 0.8975421190261841\n",
      "Epoch 84, batch 18 D Loss: 1.3864977359771729, G Loss: 0.8737759590148926\n",
      "Epoch 84, batch 19 D Loss: 1.3484609127044678, G Loss: 0.922336220741272\n",
      "Epoch 84, batch 20 D Loss: 1.3359346389770508, G Loss: 0.9233944416046143\n",
      "Epoch 84, batch 21 D Loss: 1.300351858139038, G Loss: 0.9405441284179688\n",
      "Epoch 84, batch 22 D Loss: 1.3958741426467896, G Loss: 0.94025057554245\n",
      "Epoch 84, batch 23 D Loss: 1.3203119039535522, G Loss: 0.9560959339141846\n",
      "Epoch 84, batch 24 D Loss: 1.2927883863449097, G Loss: 0.9513691067695618\n",
      "Epoch 84, batch 25 D Loss: 1.309661626815796, G Loss: 0.9651405215263367\n",
      "Epoch 84, batch 26 D Loss: 1.2808880805969238, G Loss: 1.0109301805496216\n",
      "Epoch 84, batch 27 D Loss: 1.3574038743972778, G Loss: 1.0099520683288574\n",
      "Epoch 84, batch 28 D Loss: 1.303673267364502, G Loss: 0.9700227379798889\n",
      "Epoch 84, batch 29 D Loss: 1.2617813348770142, G Loss: 1.003943681716919\n",
      "Epoch 84, batch 30 D Loss: 1.2310065031051636, G Loss: 1.0255305767059326\n",
      "Epoch 84, batch 31 D Loss: 1.206727147102356, G Loss: 1.046043872833252\n",
      "Epoch 84, batch 32 D Loss: 1.2750179767608643, G Loss: 1.0325173139572144\n",
      "Epoch 84, batch 33 D Loss: 1.2430747747421265, G Loss: 1.0497829914093018\n",
      "Epoch 84, batch 34 D Loss: 1.256554126739502, G Loss: 1.050370454788208\n",
      "Epoch 84, batch 35 D Loss: 1.2301959991455078, G Loss: 1.066144347190857\n",
      "Epoch 84, batch 36 D Loss: 1.2274798154830933, G Loss: 1.075753092765808\n",
      "Epoch 84, batch 37 D Loss: 1.2176614999771118, G Loss: 1.073022484779358\n",
      "Epoch 84, batch 38 D Loss: 1.2428938150405884, G Loss: 1.1001851558685303\n",
      "Epoch 84, batch 39 D Loss: 1.2981384992599487, G Loss: 1.1008070707321167\n",
      "Epoch 84, batch 40 D Loss: 1.1559078693389893, G Loss: 1.0886542797088623\n",
      "Epoch 84, batch 41 D Loss: 1.1672818660736084, G Loss: 1.1126484870910645\n",
      "Epoch 84, batch 42 D Loss: 1.2628947496414185, G Loss: 1.1024606227874756\n",
      "Epoch 84, batch 43 D Loss: 1.1850624084472656, G Loss: 1.1220221519470215\n",
      "Epoch 84, batch 44 D Loss: 1.2148237228393555, G Loss: 1.1642224788665771\n",
      "Epoch 84, batch 45 D Loss: 1.178039789199829, G Loss: 1.1175544261932373\n",
      "Epoch 84, batch 46 D Loss: 1.2427417039871216, G Loss: 1.1561007499694824\n",
      "Epoch 84, batch 47 D Loss: 1.1633057594299316, G Loss: 1.1328344345092773\n",
      "Epoch 84, batch 48 D Loss: 1.2227413654327393, G Loss: 1.1406439542770386\n",
      "Epoch 84, batch 49 D Loss: 1.218899130821228, G Loss: 1.1343780755996704\n",
      "Epoch 84, batch 50 D Loss: 1.2431461811065674, G Loss: 1.1425063610076904\n",
      "Epoch 84, batch 51 D Loss: 1.179107666015625, G Loss: 1.1724038124084473\n",
      "Epoch 84, batch 52 D Loss: 1.2094300985336304, G Loss: 1.1393755674362183\n",
      "Epoch 84, batch 53 D Loss: 1.2509244680404663, G Loss: 1.1109322309494019\n",
      "Epoch 84, batch 54 D Loss: 1.2191252708435059, G Loss: 1.1259840726852417\n",
      "Epoch 84, batch 55 D Loss: 1.1634893417358398, G Loss: 1.1575113534927368\n",
      "Epoch 84, batch 56 D Loss: 1.1994235515594482, G Loss: 1.1291126012802124\n",
      "Epoch 84, batch 57 D Loss: 1.2403851747512817, G Loss: 1.109724521636963\n",
      "Epoch 84, batch 58 D Loss: 1.1543147563934326, G Loss: 1.1186530590057373\n",
      "Epoch 84, batch 59 D Loss: 1.2053054571151733, G Loss: 1.1099284887313843\n",
      "Epoch 84, batch 60 D Loss: 1.1318533420562744, G Loss: 1.0910000801086426\n",
      "Epoch 84, batch 61 D Loss: 1.1676220893859863, G Loss: 1.1042404174804688\n",
      "Epoch 84, batch 62 D Loss: 1.2112884521484375, G Loss: 1.0734409093856812\n",
      "Epoch 84, batch 63 D Loss: 1.2120693922042847, G Loss: 1.0692147016525269\n",
      "Epoch 84, batch 64 D Loss: 1.2107008695602417, G Loss: 1.1020008325576782\n",
      "Epoch 84, batch 65 D Loss: 1.1826512813568115, G Loss: 1.107250690460205\n",
      "Epoch 84, batch 66 D Loss: 1.1548676490783691, G Loss: 1.102157473564148\n",
      "Epoch 84, batch 67 D Loss: 1.1094975471496582, G Loss: 1.0640571117401123\n",
      "Epoch 84, batch 68 D Loss: 1.1184992790222168, G Loss: 1.0460740327835083\n",
      "Epoch 84, batch 69 D Loss: 1.1994068622589111, G Loss: 1.06291925907135\n",
      "Epoch 84, batch 70 D Loss: 1.1293009519577026, G Loss: 1.0822066068649292\n",
      "Epoch 84, batch 71 D Loss: 1.227391242980957, G Loss: 1.0537981986999512\n",
      "Epoch 84, batch 72 D Loss: 1.1627956628799438, G Loss: 1.1072239875793457\n",
      "Epoch 84, batch 73 D Loss: 1.2215300798416138, G Loss: 1.0437357425689697\n",
      "Epoch 84, batch 74 D Loss: 1.2312136888504028, G Loss: 1.0390264987945557\n",
      "Epoch 84, batch 75 D Loss: 1.1131064891815186, G Loss: 1.0634137392044067\n",
      "Epoch 84, batch 76 D Loss: 1.0999276638031006, G Loss: 1.046401858329773\n",
      "Epoch 84, batch 77 D Loss: 1.1488065719604492, G Loss: 1.0354703664779663\n",
      "Epoch 84, batch 78 D Loss: 1.046809434890747, G Loss: 1.0470702648162842\n",
      "Epoch 84, batch 79 D Loss: 1.1315500736236572, G Loss: 1.0556423664093018\n",
      "Epoch 84, batch 80 D Loss: 1.1756094694137573, G Loss: 1.005089282989502\n",
      "Epoch 84, batch 81 D Loss: 1.247913122177124, G Loss: 1.0195801258087158\n",
      "Epoch 84, batch 82 D Loss: 1.1122565269470215, G Loss: 1.0380768775939941\n",
      "Epoch 84, batch 83 D Loss: 1.1534311771392822, G Loss: 1.005631446838379\n",
      "Epoch 84, batch 84 D Loss: 1.1544287204742432, G Loss: 1.0131406784057617\n",
      "Epoch 84, batch 85 D Loss: 1.1211594343185425, G Loss: 1.0132033824920654\n",
      "Epoch 84, batch 86 D Loss: 1.2033320665359497, G Loss: 1.0164172649383545\n",
      "Epoch 84, batch 87 D Loss: 1.0693246126174927, G Loss: 1.028680443763733\n",
      "Epoch 84, batch 88 D Loss: 1.1375741958618164, G Loss: 0.9998985528945923\n",
      "Epoch 84, batch 89 D Loss: 1.266823410987854, G Loss: 0.9875374436378479\n",
      "Epoch 84, batch 90 D Loss: 1.1279560327529907, G Loss: 0.9995784759521484\n",
      "Epoch 84, batch 91 D Loss: 1.1486114263534546, G Loss: 1.0071814060211182\n",
      "Epoch 84, batch 92 D Loss: 1.1690220832824707, G Loss: 0.9656856656074524\n",
      "Epoch 84, batch 93 D Loss: 1.2122914791107178, G Loss: 0.9804144501686096\n",
      "Epoch 84, batch 94 D Loss: 1.2426848411560059, G Loss: 0.9728796482086182\n",
      "Epoch 84, batch 95 D Loss: 1.1071826219558716, G Loss: 0.9738098382949829\n",
      "Epoch 84, batch 96 D Loss: 1.1541330814361572, G Loss: 0.9835912585258484\n",
      "Epoch 84, batch 97 D Loss: 1.2039448022842407, G Loss: 0.9570378065109253\n",
      "Epoch 84, batch 98 D Loss: 1.0468182563781738, G Loss: 0.9798001050949097\n",
      "Epoch 84, batch 99 D Loss: 1.086670160293579, G Loss: 0.9364769458770752\n",
      "Epoch 84, batch 100 D Loss: 1.2522701025009155, G Loss: 0.936927318572998\n",
      "Epoch 84, batch 101 D Loss: 1.1889785528182983, G Loss: 0.9482391476631165\n",
      "Epoch 84, batch 102 D Loss: 1.1607558727264404, G Loss: 0.9705284833908081\n",
      "Epoch 84, batch 103 D Loss: 1.2617113590240479, G Loss: 0.923077404499054\n",
      "Epoch 84, batch 104 D Loss: 1.2376137971878052, G Loss: 0.9050862193107605\n",
      "Epoch 84, batch 105 D Loss: 1.1354200839996338, G Loss: 0.949281632900238\n",
      "Epoch 84, batch 106 D Loss: 1.3228449821472168, G Loss: 0.9292788505554199\n",
      "Epoch 84, batch 107 D Loss: 1.18898344039917, G Loss: 0.9527413845062256\n",
      "Epoch 84, batch 108 D Loss: 1.1544110774993896, G Loss: 0.8667847514152527\n",
      "Epoch 84, batch 109 D Loss: 1.347784161567688, G Loss: 0.896748960018158\n",
      "Epoch 84, batch 110 D Loss: 1.2706122398376465, G Loss: 0.8923711180686951\n",
      "Epoch 84, batch 111 D Loss: 1.15237557888031, G Loss: 0.8901728987693787\n",
      "Epoch 84, batch 112 D Loss: 1.0877808332443237, G Loss: 0.8318135738372803\n",
      "Epoch 84, batch 113 D Loss: 1.2306103706359863, G Loss: 0.8835674524307251\n",
      "Epoch 84, batch 114 D Loss: 1.2372841835021973, G Loss: 0.8792343139648438\n",
      "Epoch 84, batch 115 D Loss: 1.1334939002990723, G Loss: 0.8441615104675293\n",
      "Epoch 84, batch 116 D Loss: 1.2152141332626343, G Loss: 0.8779261112213135\n",
      "Epoch 84, batch 117 D Loss: 1.2132601737976074, G Loss: 0.836808443069458\n",
      "Epoch 84, batch 118 D Loss: 1.3598299026489258, G Loss: 0.8497522473335266\n",
      "Epoch 84, batch 119 D Loss: 1.2358291149139404, G Loss: 0.8056374192237854\n",
      "Epoch 84, batch 120 D Loss: 1.2411807775497437, G Loss: 0.823652982711792\n",
      "Epoch 84, batch 121 D Loss: 1.3164178133010864, G Loss: 0.7857556939125061\n",
      "Epoch 84, batch 122 D Loss: 1.3398590087890625, G Loss: 0.8543651700019836\n",
      "Epoch 84, batch 123 D Loss: 1.2408475875854492, G Loss: 0.8349540829658508\n",
      "Epoch 84, batch 124 D Loss: 1.388718605041504, G Loss: 0.7702351212501526\n",
      "Epoch 84, batch 125 D Loss: 1.2796766757965088, G Loss: 0.7823328971862793\n",
      "Epoch 84, batch 126 D Loss: 1.2332508563995361, G Loss: 0.8152945041656494\n",
      "Epoch 84, batch 127 D Loss: 1.2201097011566162, G Loss: 0.8028411269187927\n",
      "Epoch 84, batch 128 D Loss: 1.3435297012329102, G Loss: 0.7646015882492065\n",
      "Epoch 84, batch 129 D Loss: 1.2969000339508057, G Loss: 0.7761530876159668\n",
      "Epoch 84, batch 130 D Loss: 1.1941940784454346, G Loss: 0.8220406174659729\n",
      "Epoch 84, batch 131 D Loss: 1.3740978240966797, G Loss: 0.7515302896499634\n",
      "Epoch 84, batch 132 D Loss: 1.4204740524291992, G Loss: 0.7489511966705322\n",
      "Epoch 84, batch 133 D Loss: 1.3233367204666138, G Loss: 0.7269041538238525\n",
      "Epoch 84, batch 134 D Loss: 1.3599228858947754, G Loss: 0.7961801290512085\n",
      "Epoch 84, batch 135 D Loss: 1.2744883298873901, G Loss: 0.7973533868789673\n",
      "Epoch 84, batch 136 D Loss: 1.3128514289855957, G Loss: 0.7213039398193359\n",
      "Epoch 84, batch 137 D Loss: 1.4780247211456299, G Loss: 0.740286648273468\n",
      "Epoch 84, batch 138 D Loss: 1.487399697303772, G Loss: 0.7167382836341858\n",
      "Epoch 84, batch 139 D Loss: 1.3798736333847046, G Loss: 0.7690706849098206\n",
      "Epoch 84, batch 140 D Loss: 1.3338770866394043, G Loss: 0.7764714956283569\n",
      "Epoch 84, batch 141 D Loss: 1.4814913272857666, G Loss: 0.7084610462188721\n",
      "Epoch 84, batch 142 D Loss: 1.4032217264175415, G Loss: 0.7367924451828003\n",
      "Epoch 84, batch 143 D Loss: 1.3492374420166016, G Loss: 0.7326145768165588\n",
      "Epoch 84, batch 144 D Loss: 1.324610710144043, G Loss: 0.7332847118377686\n",
      "Epoch 84, batch 145 D Loss: 1.2084567546844482, G Loss: 0.7519519925117493\n",
      "Epoch 84, batch 146 D Loss: 1.47586190700531, G Loss: 0.638060450553894\n",
      "Epoch 84, batch 147 D Loss: 1.4692459106445312, G Loss: 0.7035536766052246\n",
      "Epoch 84, batch 148 D Loss: 1.2908246517181396, G Loss: 0.7184290885925293\n",
      "Epoch 84, batch 149 D Loss: 1.4617855548858643, G Loss: 0.6939492225646973\n",
      "Epoch 84, batch 150 D Loss: 1.3868212699890137, G Loss: 0.7036272287368774\n",
      "Epoch 84, batch 151 D Loss: 1.4120922088623047, G Loss: 0.711537778377533\n",
      "Epoch 84, batch 152 D Loss: 1.4532265663146973, G Loss: 0.6707236766815186\n",
      "Epoch 84, batch 153 D Loss: 1.3763060569763184, G Loss: 0.7185978889465332\n",
      "Epoch 84, batch 154 D Loss: 1.4546470642089844, G Loss: 0.6774256825447083\n",
      "Epoch 84, batch 155 D Loss: 1.3887207508087158, G Loss: 0.7318878173828125\n",
      "Epoch 84, batch 156 D Loss: 1.4832923412322998, G Loss: 0.6603102087974548\n",
      "Epoch 84, batch 157 D Loss: 1.430731177330017, G Loss: 0.7154695391654968\n",
      "Epoch 84, batch 158 D Loss: 1.3811116218566895, G Loss: 0.7196208834648132\n",
      "Epoch 84, batch 159 D Loss: 1.2838807106018066, G Loss: 0.7574096918106079\n",
      "Epoch 84, batch 160 D Loss: 1.451261043548584, G Loss: 0.7061516046524048\n",
      "Epoch 84, batch 161 D Loss: 1.4772906303405762, G Loss: 0.7155008912086487\n",
      "Epoch 84, batch 162 D Loss: 1.522716760635376, G Loss: 0.6809180378913879\n",
      "Epoch 84, batch 163 D Loss: 1.558119535446167, G Loss: 0.696861982345581\n",
      "Epoch 84, batch 164 D Loss: 1.4622105360031128, G Loss: 0.6777679324150085\n",
      "Epoch 84, batch 165 D Loss: 1.421104907989502, G Loss: 0.6762996912002563\n",
      "Epoch 84, batch 166 D Loss: 1.5510969161987305, G Loss: 0.6877887845039368\n",
      "Epoch 84, batch 167 D Loss: 1.6329946517944336, G Loss: 0.6282109022140503\n",
      "Epoch 84, batch 168 D Loss: 1.3527405261993408, G Loss: 0.7033451199531555\n",
      "Epoch 84, batch 169 D Loss: 1.5073926448822021, G Loss: 0.6419228911399841\n",
      "Epoch 84, batch 170 D Loss: 1.5339796543121338, G Loss: 0.646502673625946\n",
      "Epoch 84, batch 171 D Loss: 1.3953880071640015, G Loss: 0.6351765990257263\n",
      "Epoch 84, batch 172 D Loss: 1.5015770196914673, G Loss: 0.6570591926574707\n",
      "Epoch 84, batch 173 D Loss: 1.4173121452331543, G Loss: 0.6869516968727112\n",
      "Epoch 84, batch 174 D Loss: 1.5618042945861816, G Loss: 0.680025041103363\n",
      "Epoch 84, batch 175 D Loss: 1.4273428916931152, G Loss: 0.7207426428794861\n",
      "Epoch 84, batch 176 D Loss: 1.5321054458618164, G Loss: 0.6814784407615662\n",
      "Epoch 84, batch 177 D Loss: 1.4585157632827759, G Loss: 0.7418023943901062\n",
      "Epoch 84, batch 178 D Loss: 1.6522588729858398, G Loss: 0.6152931451797485\n",
      "Epoch 84, batch 179 D Loss: 1.4973552227020264, G Loss: 0.6640374064445496\n",
      "Epoch 84, batch 180 D Loss: 1.5538926124572754, G Loss: 0.6462996602058411\n",
      "Epoch 84, batch 181 D Loss: 1.5549346208572388, G Loss: 0.6841496229171753\n",
      "Epoch 84, batch 182 D Loss: 1.6095409393310547, G Loss: 0.6241707801818848\n",
      "Epoch 84, batch 183 D Loss: 1.5649678707122803, G Loss: 0.6308963894844055\n",
      "Epoch 84, batch 184 D Loss: 1.3728976249694824, G Loss: 0.7030748128890991\n",
      "Epoch 84, batch 185 D Loss: 1.376159906387329, G Loss: 0.7212924361228943\n",
      "Epoch 84, batch 186 D Loss: 1.4642086029052734, G Loss: 0.6628971695899963\n",
      "Epoch 84, batch 187 D Loss: 1.5122599601745605, G Loss: 0.648735761642456\n",
      "Epoch 84, batch 188 D Loss: 1.486771583557129, G Loss: 0.6824667453765869\n",
      "Epoch 84, batch 189 D Loss: 1.4059947729110718, G Loss: 0.6992623209953308\n",
      "Epoch 84, batch 190 D Loss: 1.423680067062378, G Loss: 0.7214683294296265\n",
      "Epoch 84, batch 191 D Loss: 1.4292570352554321, G Loss: 0.6747274994850159\n",
      "Epoch 84, batch 192 D Loss: 1.433882236480713, G Loss: 0.756459653377533\n",
      "Epoch 84, batch 193 D Loss: 1.4081687927246094, G Loss: 0.7477468848228455\n",
      "Epoch 84, batch 194 D Loss: 1.5724496841430664, G Loss: 0.6559044718742371\n",
      "Epoch 84, batch 195 D Loss: 1.5339159965515137, G Loss: 0.6862706542015076\n",
      "Epoch 84, batch 196 D Loss: 1.4643967151641846, G Loss: 0.6759551167488098\n",
      "Epoch 84, batch 197 D Loss: 1.4341109991073608, G Loss: 0.6948663592338562\n",
      "Epoch 84, batch 198 D Loss: 1.441579818725586, G Loss: 0.7794301509857178\n",
      "Epoch 84, batch 199 D Loss: 1.4773108959197998, G Loss: 0.6803605556488037\n",
      "Epoch 84, batch 200 D Loss: 1.5086956024169922, G Loss: 0.7002838253974915\n",
      "Epoch 85, batch 1 D Loss: 1.3658483028411865, G Loss: 0.7480103373527527\n",
      "Epoch 85, batch 2 D Loss: 1.460998773574829, G Loss: 0.7396809458732605\n",
      "Epoch 85, batch 3 D Loss: 1.3515121936798096, G Loss: 0.8017412424087524\n",
      "Epoch 85, batch 4 D Loss: 1.4377861022949219, G Loss: 0.7018362283706665\n",
      "Epoch 85, batch 5 D Loss: 1.334357738494873, G Loss: 0.764314591884613\n",
      "Epoch 85, batch 6 D Loss: 1.3495886325836182, G Loss: 0.7830064296722412\n",
      "Epoch 85, batch 7 D Loss: 1.4483070373535156, G Loss: 0.7659321427345276\n",
      "Epoch 85, batch 8 D Loss: 1.4431278705596924, G Loss: 0.7570019364356995\n",
      "Epoch 85, batch 9 D Loss: 1.35538649559021, G Loss: 0.7343607544898987\n",
      "Epoch 85, batch 10 D Loss: 1.4124670028686523, G Loss: 0.7540305256843567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, batch 11 D Loss: 1.3621184825897217, G Loss: 0.7994415760040283\n",
      "Epoch 85, batch 12 D Loss: 1.3325703144073486, G Loss: 0.7564539909362793\n",
      "Epoch 85, batch 13 D Loss: 1.3757388591766357, G Loss: 0.7269831895828247\n",
      "Epoch 85, batch 14 D Loss: 1.361976981163025, G Loss: 0.788204550743103\n",
      "Epoch 85, batch 15 D Loss: 1.3302288055419922, G Loss: 0.8166062235832214\n",
      "Epoch 85, batch 16 D Loss: 1.4626145362854004, G Loss: 0.7385386824607849\n",
      "Epoch 85, batch 17 D Loss: 1.3764495849609375, G Loss: 0.7764533758163452\n",
      "Epoch 85, batch 18 D Loss: 1.4164570569992065, G Loss: 0.7742904424667358\n",
      "Epoch 85, batch 19 D Loss: 1.387547492980957, G Loss: 0.8765135407447815\n",
      "Epoch 85, batch 20 D Loss: 1.3809759616851807, G Loss: 0.7939690947532654\n",
      "Epoch 85, batch 21 D Loss: 1.3354487419128418, G Loss: 0.8603983521461487\n",
      "Epoch 85, batch 22 D Loss: 1.3978369235992432, G Loss: 0.7680386900901794\n",
      "Epoch 85, batch 23 D Loss: 1.382543921470642, G Loss: 0.7717917561531067\n",
      "Epoch 85, batch 24 D Loss: 1.2395942211151123, G Loss: 0.8224403262138367\n",
      "Epoch 85, batch 25 D Loss: 1.340134620666504, G Loss: 0.7792129516601562\n",
      "Epoch 85, batch 26 D Loss: 1.2845275402069092, G Loss: 0.7872090339660645\n",
      "Epoch 85, batch 27 D Loss: 1.269089937210083, G Loss: 0.8307912349700928\n",
      "Epoch 85, batch 28 D Loss: 1.3780686855316162, G Loss: 0.768048882484436\n",
      "Epoch 85, batch 29 D Loss: 1.3149993419647217, G Loss: 0.7930868268013\n",
      "Epoch 85, batch 30 D Loss: 1.3011584281921387, G Loss: 0.8361932635307312\n",
      "Epoch 85, batch 31 D Loss: 1.3006163835525513, G Loss: 0.8392667174339294\n",
      "Epoch 85, batch 32 D Loss: 1.3070234060287476, G Loss: 0.8670153617858887\n",
      "Epoch 85, batch 33 D Loss: 1.2971078157424927, G Loss: 0.8613073229789734\n",
      "Epoch 85, batch 34 D Loss: 1.2665302753448486, G Loss: 0.9536257982254028\n",
      "Epoch 85, batch 35 D Loss: 1.2867463827133179, G Loss: 0.8461940288543701\n",
      "Epoch 85, batch 36 D Loss: 1.2790732383728027, G Loss: 0.8689122200012207\n",
      "Epoch 85, batch 37 D Loss: 1.2318835258483887, G Loss: 0.9169541001319885\n",
      "Epoch 85, batch 38 D Loss: 1.299155354499817, G Loss: 0.855724573135376\n",
      "Epoch 85, batch 39 D Loss: 1.2697689533233643, G Loss: 0.8957886695861816\n",
      "Epoch 85, batch 40 D Loss: 1.2448701858520508, G Loss: 0.9390645027160645\n",
      "Epoch 85, batch 41 D Loss: 1.2713418006896973, G Loss: 0.8866525888442993\n",
      "Epoch 85, batch 42 D Loss: 1.136734127998352, G Loss: 0.8914966583251953\n",
      "Epoch 85, batch 43 D Loss: 1.2431750297546387, G Loss: 0.8070515394210815\n",
      "Epoch 85, batch 44 D Loss: 1.2544267177581787, G Loss: 0.9343199133872986\n",
      "Epoch 85, batch 45 D Loss: 1.2273588180541992, G Loss: 0.9079025387763977\n",
      "Epoch 85, batch 46 D Loss: 1.2379239797592163, G Loss: 0.9416871666908264\n",
      "Epoch 85, batch 47 D Loss: 1.2278807163238525, G Loss: 0.8929870128631592\n",
      "Epoch 85, batch 48 D Loss: 1.293060541152954, G Loss: 0.9319060444831848\n",
      "Epoch 85, batch 49 D Loss: 1.2362704277038574, G Loss: 0.8871232867240906\n",
      "Epoch 85, batch 50 D Loss: 1.2016346454620361, G Loss: 0.9368425011634827\n",
      "Epoch 85, batch 51 D Loss: 1.3421717882156372, G Loss: 0.9290468096733093\n",
      "Epoch 85, batch 52 D Loss: 1.2087538242340088, G Loss: 0.8744822144508362\n",
      "Epoch 85, batch 53 D Loss: 1.143588900566101, G Loss: 0.922044575214386\n",
      "Epoch 85, batch 54 D Loss: 1.2772798538208008, G Loss: 0.8305444121360779\n",
      "Epoch 85, batch 55 D Loss: 1.2833954095840454, G Loss: 0.8748424053192139\n",
      "Epoch 85, batch 56 D Loss: 1.2025705575942993, G Loss: 0.9639017581939697\n",
      "Epoch 85, batch 57 D Loss: 1.3299161195755005, G Loss: 0.839730441570282\n",
      "Epoch 85, batch 58 D Loss: 1.2755694389343262, G Loss: 0.8408079743385315\n",
      "Epoch 85, batch 59 D Loss: 1.1678860187530518, G Loss: 0.8390902876853943\n",
      "Epoch 85, batch 60 D Loss: 1.229508399963379, G Loss: 0.8883171081542969\n",
      "Epoch 85, batch 61 D Loss: 1.3029861450195312, G Loss: 0.8812258839607239\n",
      "Epoch 85, batch 62 D Loss: 1.237391710281372, G Loss: 0.854233980178833\n",
      "Epoch 85, batch 63 D Loss: 1.230964183807373, G Loss: 0.847552478313446\n",
      "Epoch 85, batch 64 D Loss: 1.2328979969024658, G Loss: 0.9086137413978577\n",
      "Epoch 85, batch 65 D Loss: 1.2008075714111328, G Loss: 0.8754220008850098\n",
      "Epoch 85, batch 66 D Loss: 1.189313530921936, G Loss: 0.8422985672950745\n",
      "Epoch 85, batch 67 D Loss: 1.295346975326538, G Loss: 0.8261871337890625\n",
      "Epoch 85, batch 68 D Loss: 1.2309668064117432, G Loss: 0.9174355268478394\n",
      "Epoch 85, batch 69 D Loss: 1.170013189315796, G Loss: 0.820160984992981\n",
      "Epoch 85, batch 70 D Loss: 1.2191715240478516, G Loss: 0.9451876282691956\n",
      "Epoch 85, batch 71 D Loss: 1.2179927825927734, G Loss: 0.8201467394828796\n",
      "Epoch 85, batch 72 D Loss: 1.1534342765808105, G Loss: 0.8864386081695557\n",
      "Epoch 85, batch 73 D Loss: 1.3325461149215698, G Loss: 0.853760838508606\n",
      "Epoch 85, batch 74 D Loss: 1.171804428100586, G Loss: 0.8559194803237915\n",
      "Epoch 85, batch 75 D Loss: 1.3270807266235352, G Loss: 0.7860363125801086\n",
      "Epoch 85, batch 76 D Loss: 1.3498066663742065, G Loss: 0.814003050327301\n",
      "Epoch 85, batch 77 D Loss: 1.4274063110351562, G Loss: 0.7316053509712219\n",
      "Epoch 85, batch 78 D Loss: 1.2571296691894531, G Loss: 0.8317093849182129\n",
      "Epoch 85, batch 79 D Loss: 1.2577577829360962, G Loss: 0.8379648327827454\n",
      "Epoch 85, batch 80 D Loss: 1.4658708572387695, G Loss: 0.7925508618354797\n",
      "Epoch 85, batch 81 D Loss: 1.3332172632217407, G Loss: 0.781337559223175\n",
      "Epoch 85, batch 82 D Loss: 1.2889361381530762, G Loss: 0.7410755753517151\n",
      "Epoch 85, batch 83 D Loss: 1.285441517829895, G Loss: 0.7865164279937744\n",
      "Epoch 85, batch 84 D Loss: 1.280210018157959, G Loss: 0.7538536190986633\n",
      "Epoch 85, batch 85 D Loss: 1.3149709701538086, G Loss: 0.7751073241233826\n",
      "Epoch 85, batch 86 D Loss: 1.3407373428344727, G Loss: 0.7631620764732361\n",
      "Epoch 85, batch 87 D Loss: 1.3308980464935303, G Loss: 0.7242308259010315\n",
      "Epoch 85, batch 88 D Loss: 1.2077242136001587, G Loss: 0.7332119941711426\n",
      "Epoch 85, batch 89 D Loss: 1.3688867092132568, G Loss: 0.7327278852462769\n",
      "Epoch 85, batch 90 D Loss: 1.1848106384277344, G Loss: 0.788088858127594\n",
      "Epoch 85, batch 91 D Loss: 1.3769935369491577, G Loss: 0.7409438490867615\n",
      "Epoch 85, batch 92 D Loss: 1.3100292682647705, G Loss: 0.7051267027854919\n",
      "Epoch 85, batch 93 D Loss: 1.3573049306869507, G Loss: 0.680182695388794\n",
      "Epoch 85, batch 94 D Loss: 1.3742398023605347, G Loss: 0.6609140038490295\n",
      "Epoch 85, batch 95 D Loss: 1.3400764465332031, G Loss: 0.6584661602973938\n",
      "Epoch 85, batch 96 D Loss: 1.2637428045272827, G Loss: 0.7426894307136536\n",
      "Epoch 85, batch 97 D Loss: 1.2660664319992065, G Loss: 0.7162096500396729\n",
      "Epoch 85, batch 98 D Loss: 1.3902969360351562, G Loss: 0.6949450969696045\n",
      "Epoch 85, batch 99 D Loss: 1.3084765672683716, G Loss: 0.7211544513702393\n",
      "Epoch 85, batch 100 D Loss: 1.3478479385375977, G Loss: 0.6706716418266296\n",
      "Epoch 85, batch 101 D Loss: 1.39873206615448, G Loss: 0.6555586457252502\n",
      "Epoch 85, batch 102 D Loss: 1.414954662322998, G Loss: 0.6753728985786438\n",
      "Epoch 85, batch 103 D Loss: 1.3182076215744019, G Loss: 0.6864726543426514\n",
      "Epoch 85, batch 104 D Loss: 1.2658270597457886, G Loss: 0.6888226270675659\n",
      "Epoch 85, batch 105 D Loss: 1.4012490510940552, G Loss: 0.6540935039520264\n",
      "Epoch 85, batch 106 D Loss: 1.2890923023223877, G Loss: 0.6695862412452698\n",
      "Epoch 85, batch 107 D Loss: 1.3371667861938477, G Loss: 0.6456947326660156\n",
      "Epoch 85, batch 108 D Loss: 1.3695344924926758, G Loss: 0.641338586807251\n",
      "Epoch 85, batch 109 D Loss: 1.361891269683838, G Loss: 0.6603648662567139\n",
      "Epoch 85, batch 110 D Loss: 1.35965895652771, G Loss: 0.6904146671295166\n",
      "Epoch 85, batch 111 D Loss: 1.2985575199127197, G Loss: 0.6769721508026123\n",
      "Epoch 85, batch 112 D Loss: 1.3048298358917236, G Loss: 0.6819548010826111\n",
      "Epoch 85, batch 113 D Loss: 1.4141767024993896, G Loss: 0.637442946434021\n",
      "Epoch 85, batch 114 D Loss: 1.395645022392273, G Loss: 0.6664952635765076\n",
      "Epoch 85, batch 115 D Loss: 1.3770372867584229, G Loss: 0.6567960381507874\n",
      "Epoch 85, batch 116 D Loss: 1.2943873405456543, G Loss: 0.6878221035003662\n",
      "Epoch 85, batch 117 D Loss: 1.3007450103759766, G Loss: 0.685383141040802\n",
      "Epoch 85, batch 118 D Loss: 1.3878934383392334, G Loss: 0.6368810534477234\n",
      "Epoch 85, batch 119 D Loss: 1.2683463096618652, G Loss: 0.6864291429519653\n",
      "Epoch 85, batch 120 D Loss: 1.4691237211227417, G Loss: 0.6191480755805969\n",
      "Epoch 85, batch 121 D Loss: 1.41581392288208, G Loss: 0.5938675403594971\n",
      "Epoch 85, batch 122 D Loss: 1.3132731914520264, G Loss: 0.6749718189239502\n",
      "Epoch 85, batch 123 D Loss: 1.2724559307098389, G Loss: 0.6671943664550781\n",
      "Epoch 85, batch 124 D Loss: 1.333406925201416, G Loss: 0.6533905863761902\n",
      "Epoch 85, batch 125 D Loss: 1.28432297706604, G Loss: 0.7032150030136108\n",
      "Epoch 85, batch 126 D Loss: 1.285339117050171, G Loss: 0.6901928782463074\n",
      "Epoch 85, batch 127 D Loss: 1.3742880821228027, G Loss: 0.6573845148086548\n",
      "Epoch 85, batch 128 D Loss: 1.3473743200302124, G Loss: 0.6772725582122803\n",
      "Epoch 85, batch 129 D Loss: 1.3993291854858398, G Loss: 0.6642940640449524\n",
      "Epoch 85, batch 130 D Loss: 1.276747703552246, G Loss: 0.6891912221908569\n",
      "Epoch 85, batch 131 D Loss: 1.3703737258911133, G Loss: 0.693718671798706\n",
      "Epoch 85, batch 132 D Loss: 1.3664264678955078, G Loss: 0.6478700041770935\n",
      "Epoch 85, batch 133 D Loss: 1.2963907718658447, G Loss: 0.7046868205070496\n",
      "Epoch 85, batch 134 D Loss: 1.3904460668563843, G Loss: 0.6355143189430237\n",
      "Epoch 85, batch 135 D Loss: 1.3715139627456665, G Loss: 0.6697858572006226\n",
      "Epoch 85, batch 136 D Loss: 1.3417080640792847, G Loss: 0.6959850788116455\n",
      "Epoch 85, batch 137 D Loss: 1.2978646755218506, G Loss: 0.687491774559021\n",
      "Epoch 85, batch 138 D Loss: 1.3655850887298584, G Loss: 0.6443426012992859\n",
      "Epoch 85, batch 139 D Loss: 1.4058194160461426, G Loss: 0.6921218037605286\n",
      "Epoch 85, batch 140 D Loss: 1.3691487312316895, G Loss: 0.692104697227478\n",
      "Epoch 85, batch 141 D Loss: 1.3765454292297363, G Loss: 0.6607264876365662\n",
      "Epoch 85, batch 142 D Loss: 1.300419569015503, G Loss: 0.7242252826690674\n",
      "Epoch 85, batch 143 D Loss: 1.3889124393463135, G Loss: 0.7280157208442688\n",
      "Epoch 85, batch 144 D Loss: 1.334838628768921, G Loss: 0.7371190786361694\n",
      "Epoch 85, batch 145 D Loss: 1.3278672695159912, G Loss: 0.7384030222892761\n",
      "Epoch 85, batch 146 D Loss: 1.4449422359466553, G Loss: 0.6280269026756287\n",
      "Epoch 85, batch 147 D Loss: 1.3987650871276855, G Loss: 0.6671870350837708\n",
      "Epoch 85, batch 148 D Loss: 1.4354605674743652, G Loss: 0.6440140008926392\n",
      "Epoch 85, batch 149 D Loss: 1.3890407085418701, G Loss: 0.6738681197166443\n",
      "Epoch 85, batch 150 D Loss: 1.303156852722168, G Loss: 0.7148227691650391\n",
      "Epoch 85, batch 151 D Loss: 1.3891096115112305, G Loss: 0.6942565441131592\n",
      "Epoch 85, batch 152 D Loss: 1.4063098430633545, G Loss: 0.6721744537353516\n",
      "Epoch 85, batch 153 D Loss: 1.3616902828216553, G Loss: 0.7029653191566467\n",
      "Epoch 85, batch 154 D Loss: 1.3274434804916382, G Loss: 0.6963258981704712\n",
      "Epoch 85, batch 155 D Loss: 1.394096851348877, G Loss: 0.6087003350257874\n",
      "Epoch 85, batch 156 D Loss: 1.486013412475586, G Loss: 0.6052110195159912\n",
      "Epoch 85, batch 157 D Loss: 1.4484658241271973, G Loss: 0.6785414814949036\n",
      "Epoch 85, batch 158 D Loss: 1.3973057270050049, G Loss: 0.7197039723396301\n",
      "Epoch 85, batch 159 D Loss: 1.408881664276123, G Loss: 0.659677267074585\n",
      "Epoch 85, batch 160 D Loss: 1.4450631141662598, G Loss: 0.6694698333740234\n",
      "Epoch 85, batch 161 D Loss: 1.374497890472412, G Loss: 0.6831774711608887\n",
      "Epoch 85, batch 162 D Loss: 1.3109338283538818, G Loss: 0.7458834648132324\n",
      "Epoch 85, batch 163 D Loss: 1.3711955547332764, G Loss: 0.6780823469161987\n",
      "Epoch 85, batch 164 D Loss: 1.38507080078125, G Loss: 0.6819700002670288\n",
      "Epoch 85, batch 165 D Loss: 1.4171741008758545, G Loss: 0.6907699704170227\n",
      "Epoch 85, batch 166 D Loss: 1.44441556930542, G Loss: 0.6661999225616455\n",
      "Epoch 85, batch 167 D Loss: 1.4001061916351318, G Loss: 0.6957990527153015\n",
      "Epoch 85, batch 168 D Loss: 1.4089034795761108, G Loss: 0.688181459903717\n",
      "Epoch 85, batch 169 D Loss: 1.3322185277938843, G Loss: 0.7116113901138306\n",
      "Epoch 85, batch 170 D Loss: 1.3636316061019897, G Loss: 0.7143743634223938\n",
      "Epoch 85, batch 171 D Loss: 1.3516380786895752, G Loss: 0.6950982809066772\n",
      "Epoch 85, batch 172 D Loss: 1.3859775066375732, G Loss: 0.7293384671211243\n",
      "Epoch 85, batch 173 D Loss: 1.394181489944458, G Loss: 0.6971815228462219\n",
      "Epoch 85, batch 174 D Loss: 1.3691821098327637, G Loss: 0.7228153944015503\n",
      "Epoch 85, batch 175 D Loss: 1.4103398323059082, G Loss: 0.6730663180351257\n",
      "Epoch 85, batch 176 D Loss: 1.4227516651153564, G Loss: 0.6789184808731079\n",
      "Epoch 85, batch 177 D Loss: 1.2931175231933594, G Loss: 0.7170502543449402\n",
      "Epoch 85, batch 178 D Loss: 1.388087272644043, G Loss: 0.6960923671722412\n",
      "Epoch 85, batch 179 D Loss: 1.3633862733840942, G Loss: 0.7122271060943604\n",
      "Epoch 85, batch 180 D Loss: 1.4088356494903564, G Loss: 0.7332969903945923\n",
      "Epoch 85, batch 181 D Loss: 1.3240103721618652, G Loss: 0.7416152954101562\n",
      "Epoch 85, batch 182 D Loss: 1.2850828170776367, G Loss: 0.727885901927948\n",
      "Epoch 85, batch 183 D Loss: 1.4603949785232544, G Loss: 0.6926130056381226\n",
      "Epoch 85, batch 184 D Loss: 1.3203725814819336, G Loss: 0.7062976360321045\n",
      "Epoch 85, batch 185 D Loss: 1.3771021366119385, G Loss: 0.7355049848556519\n",
      "Epoch 85, batch 186 D Loss: 1.377976655960083, G Loss: 0.6999596953392029\n",
      "Epoch 85, batch 187 D Loss: 1.3332568407058716, G Loss: 0.744188666343689\n",
      "Epoch 85, batch 188 D Loss: 1.3028485774993896, G Loss: 0.7772496938705444\n",
      "Epoch 85, batch 189 D Loss: 1.3267109394073486, G Loss: 0.7160600423812866\n",
      "Epoch 85, batch 190 D Loss: 1.3673021793365479, G Loss: 0.7431431412696838\n",
      "Epoch 85, batch 191 D Loss: 1.4082130193710327, G Loss: 0.7277480959892273\n",
      "Epoch 85, batch 192 D Loss: 1.3661227226257324, G Loss: 0.7446126341819763\n",
      "Epoch 85, batch 193 D Loss: 1.3412435054779053, G Loss: 0.7396125197410583\n",
      "Epoch 85, batch 194 D Loss: 1.3307620286941528, G Loss: 0.7339722514152527\n",
      "Epoch 85, batch 195 D Loss: 1.3536629676818848, G Loss: 0.7521747350692749\n",
      "Epoch 85, batch 196 D Loss: 1.287142038345337, G Loss: 0.7597091197967529\n",
      "Epoch 85, batch 197 D Loss: 1.2968297004699707, G Loss: 0.7698217630386353\n",
      "Epoch 85, batch 198 D Loss: 1.3822124004364014, G Loss: 0.7464708685874939\n",
      "Epoch 85, batch 199 D Loss: 1.3780274391174316, G Loss: 0.7418034076690674\n",
      "Epoch 85, batch 200 D Loss: 1.3543397188186646, G Loss: 0.7217299938201904\n",
      "Epoch 86, batch 1 D Loss: 1.2960760593414307, G Loss: 0.7982084155082703\n",
      "Epoch 86, batch 2 D Loss: 1.2982680797576904, G Loss: 0.7337210178375244\n",
      "Epoch 86, batch 3 D Loss: 1.3518824577331543, G Loss: 0.7476752996444702\n",
      "Epoch 86, batch 4 D Loss: 1.321487307548523, G Loss: 0.7669206261634827\n",
      "Epoch 86, batch 5 D Loss: 1.4572088718414307, G Loss: 0.7049642205238342\n",
      "Epoch 86, batch 6 D Loss: 1.4244015216827393, G Loss: 0.7438545227050781\n",
      "Epoch 86, batch 7 D Loss: 1.3457460403442383, G Loss: 0.7670568823814392\n",
      "Epoch 86, batch 8 D Loss: 1.3137471675872803, G Loss: 0.8002986907958984\n",
      "Epoch 86, batch 9 D Loss: 1.303983211517334, G Loss: 0.789106011390686\n",
      "Epoch 86, batch 10 D Loss: 1.4179983139038086, G Loss: 0.7368243336677551\n",
      "Epoch 86, batch 11 D Loss: 1.3065593242645264, G Loss: 0.8429305553436279\n",
      "Epoch 86, batch 12 D Loss: 1.336797833442688, G Loss: 0.7734217643737793\n",
      "Epoch 86, batch 13 D Loss: 1.4182748794555664, G Loss: 0.7464817762374878\n",
      "Epoch 86, batch 14 D Loss: 1.251244306564331, G Loss: 0.821631669998169\n",
      "Epoch 86, batch 15 D Loss: 1.2986037731170654, G Loss: 0.8280936479568481\n",
      "Epoch 86, batch 16 D Loss: 1.3448469638824463, G Loss: 0.7783958315849304\n",
      "Epoch 86, batch 17 D Loss: 1.383332371711731, G Loss: 0.7496738433837891\n",
      "Epoch 86, batch 18 D Loss: 1.3790678977966309, G Loss: 0.7238969206809998\n",
      "Epoch 86, batch 19 D Loss: 1.3421865701675415, G Loss: 0.7716527581214905\n",
      "Epoch 86, batch 20 D Loss: 1.3508802652359009, G Loss: 0.7910659909248352\n",
      "Epoch 86, batch 21 D Loss: 1.313157558441162, G Loss: 0.8311614990234375\n",
      "Epoch 86, batch 22 D Loss: 1.3146562576293945, G Loss: 0.7712751626968384\n",
      "Epoch 86, batch 23 D Loss: 1.293315052986145, G Loss: 0.8256486654281616\n",
      "Epoch 86, batch 24 D Loss: 1.3286652565002441, G Loss: 0.7475131154060364\n",
      "Epoch 86, batch 25 D Loss: 1.435431718826294, G Loss: 0.8101068735122681\n",
      "Epoch 86, batch 26 D Loss: 1.3413059711456299, G Loss: 0.8135374188423157\n",
      "Epoch 86, batch 27 D Loss: 1.2997995615005493, G Loss: 0.8350193500518799\n",
      "Epoch 86, batch 28 D Loss: 1.3706073760986328, G Loss: 0.8153141140937805\n",
      "Epoch 86, batch 29 D Loss: 1.4008090496063232, G Loss: 0.8374711871147156\n",
      "Epoch 86, batch 30 D Loss: 1.3583310842514038, G Loss: 0.8000204563140869\n",
      "Epoch 86, batch 31 D Loss: 1.3422558307647705, G Loss: 0.7789280414581299\n",
      "Epoch 86, batch 32 D Loss: 1.3829166889190674, G Loss: 0.8164764642715454\n",
      "Epoch 86, batch 33 D Loss: 1.422349452972412, G Loss: 0.7259504199028015\n",
      "Epoch 86, batch 34 D Loss: 1.3326702117919922, G Loss: 0.8088425397872925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, batch 35 D Loss: 1.430718183517456, G Loss: 0.8334957957267761\n",
      "Epoch 86, batch 36 D Loss: 1.368233323097229, G Loss: 0.8304727673530579\n",
      "Epoch 86, batch 37 D Loss: 1.3656399250030518, G Loss: 0.7931696176528931\n",
      "Epoch 86, batch 38 D Loss: 1.3086544275283813, G Loss: 0.8572513461112976\n",
      "Epoch 86, batch 39 D Loss: 1.3531379699707031, G Loss: 0.8525083065032959\n",
      "Epoch 86, batch 40 D Loss: 1.4530699253082275, G Loss: 0.7765061855316162\n",
      "Epoch 86, batch 41 D Loss: 1.381974458694458, G Loss: 0.7908130884170532\n",
      "Epoch 86, batch 42 D Loss: 1.3840936422348022, G Loss: 0.7943671345710754\n",
      "Epoch 86, batch 43 D Loss: 1.4865278005599976, G Loss: 0.755452036857605\n",
      "Epoch 86, batch 44 D Loss: 1.3444745540618896, G Loss: 0.8280405402183533\n",
      "Epoch 86, batch 45 D Loss: 1.3548191785812378, G Loss: 0.7922361493110657\n",
      "Epoch 86, batch 46 D Loss: 1.3236443996429443, G Loss: 0.8163713216781616\n",
      "Epoch 86, batch 47 D Loss: 1.429173469543457, G Loss: 0.7644556164741516\n",
      "Epoch 86, batch 48 D Loss: 1.4024494886398315, G Loss: 0.8196008205413818\n",
      "Epoch 86, batch 49 D Loss: 1.3435134887695312, G Loss: 0.8352992534637451\n",
      "Epoch 86, batch 50 D Loss: 1.4206101894378662, G Loss: 0.7589157819747925\n",
      "Epoch 86, batch 51 D Loss: 1.5543620586395264, G Loss: 0.7597953677177429\n",
      "Epoch 86, batch 52 D Loss: 1.4453144073486328, G Loss: 0.7426311373710632\n",
      "Epoch 86, batch 53 D Loss: 1.4844025373458862, G Loss: 0.7459083795547485\n",
      "Epoch 86, batch 54 D Loss: 1.3688364028930664, G Loss: 0.8161657452583313\n",
      "Epoch 86, batch 55 D Loss: 1.429950475692749, G Loss: 0.7512457966804504\n",
      "Epoch 86, batch 56 D Loss: 1.3263447284698486, G Loss: 0.7877494692802429\n",
      "Epoch 86, batch 57 D Loss: 1.4444549083709717, G Loss: 0.7875453233718872\n",
      "Epoch 86, batch 58 D Loss: 1.3043395280838013, G Loss: 0.8425360918045044\n",
      "Epoch 86, batch 59 D Loss: 1.3885090351104736, G Loss: 0.8096708059310913\n",
      "Epoch 86, batch 60 D Loss: 1.404388189315796, G Loss: 0.7739099860191345\n",
      "Epoch 86, batch 61 D Loss: 1.3687493801116943, G Loss: 0.7734803557395935\n",
      "Epoch 86, batch 62 D Loss: 1.3759315013885498, G Loss: 0.7920259833335876\n",
      "Epoch 86, batch 63 D Loss: 1.3864667415618896, G Loss: 0.752757728099823\n",
      "Epoch 86, batch 64 D Loss: 1.3483322858810425, G Loss: 0.8243716955184937\n",
      "Epoch 86, batch 65 D Loss: 1.4457063674926758, G Loss: 0.7691792249679565\n",
      "Epoch 86, batch 66 D Loss: 1.42974054813385, G Loss: 0.7719014883041382\n",
      "Epoch 86, batch 67 D Loss: 1.4745774269104004, G Loss: 0.7370055913925171\n",
      "Epoch 86, batch 68 D Loss: 1.372719168663025, G Loss: 0.756346583366394\n",
      "Epoch 86, batch 69 D Loss: 1.4556589126586914, G Loss: 0.7293018102645874\n",
      "Epoch 86, batch 70 D Loss: 1.6279685497283936, G Loss: 0.6950252652168274\n",
      "Epoch 86, batch 71 D Loss: 1.4038671255111694, G Loss: 0.7543689012527466\n",
      "Epoch 86, batch 72 D Loss: 1.2925152778625488, G Loss: 0.8275713920593262\n",
      "Epoch 86, batch 73 D Loss: 1.4083032608032227, G Loss: 0.7592216730117798\n",
      "Epoch 86, batch 74 D Loss: 1.4109694957733154, G Loss: 0.759938657283783\n",
      "Epoch 86, batch 75 D Loss: 1.3944933414459229, G Loss: 0.7750256061553955\n",
      "Epoch 86, batch 76 D Loss: 1.3836944103240967, G Loss: 0.7480447888374329\n",
      "Epoch 86, batch 77 D Loss: 1.4093127250671387, G Loss: 0.7818545699119568\n",
      "Epoch 86, batch 78 D Loss: 1.3503923416137695, G Loss: 0.8317087292671204\n",
      "Epoch 86, batch 79 D Loss: 1.4188249111175537, G Loss: 0.8104273080825806\n",
      "Epoch 86, batch 80 D Loss: 1.3652443885803223, G Loss: 0.8279699683189392\n",
      "Epoch 86, batch 81 D Loss: 1.4478245973587036, G Loss: 0.7617558836936951\n",
      "Epoch 86, batch 82 D Loss: 1.5356354713439941, G Loss: 0.7186230421066284\n",
      "Epoch 86, batch 83 D Loss: 1.4289517402648926, G Loss: 0.7854889035224915\n",
      "Epoch 86, batch 84 D Loss: 1.3963885307312012, G Loss: 0.8218812346458435\n",
      "Epoch 86, batch 85 D Loss: 1.3028241395950317, G Loss: 0.8233584761619568\n",
      "Epoch 86, batch 86 D Loss: 1.5223534107208252, G Loss: 0.7446675896644592\n",
      "Epoch 86, batch 87 D Loss: 1.454432487487793, G Loss: 0.7380221486091614\n",
      "Epoch 86, batch 88 D Loss: 1.4105346202850342, G Loss: 0.7696449160575867\n",
      "Epoch 86, batch 89 D Loss: 1.4274559020996094, G Loss: 0.7601137757301331\n",
      "Epoch 86, batch 90 D Loss: 1.4561688899993896, G Loss: 0.7569579482078552\n",
      "Epoch 86, batch 91 D Loss: 1.4431536197662354, G Loss: 0.7652015686035156\n",
      "Epoch 86, batch 92 D Loss: 1.3641352653503418, G Loss: 0.7930981516838074\n",
      "Epoch 86, batch 93 D Loss: 1.487029790878296, G Loss: 0.7285349369049072\n",
      "Epoch 86, batch 94 D Loss: 1.4066314697265625, G Loss: 0.792094349861145\n",
      "Epoch 86, batch 95 D Loss: 1.4828410148620605, G Loss: 0.719902753829956\n",
      "Epoch 86, batch 96 D Loss: 1.4181784391403198, G Loss: 0.8042060732841492\n",
      "Epoch 86, batch 97 D Loss: 1.4287773370742798, G Loss: 0.7859703898429871\n",
      "Epoch 86, batch 98 D Loss: 1.4611871242523193, G Loss: 0.7587829828262329\n",
      "Epoch 86, batch 99 D Loss: 1.4383996725082397, G Loss: 0.792400598526001\n",
      "Epoch 86, batch 100 D Loss: 1.410055160522461, G Loss: 0.7618770003318787\n",
      "Epoch 86, batch 101 D Loss: 1.5108706951141357, G Loss: 0.7344126105308533\n",
      "Epoch 86, batch 102 D Loss: 1.4049323797225952, G Loss: 0.7392714619636536\n",
      "Epoch 86, batch 103 D Loss: 1.3707444667816162, G Loss: 0.7890378832817078\n",
      "Epoch 86, batch 104 D Loss: 1.3565089702606201, G Loss: 0.7954913377761841\n",
      "Epoch 86, batch 105 D Loss: 1.381068229675293, G Loss: 0.7816088795661926\n",
      "Epoch 86, batch 106 D Loss: 1.4214043617248535, G Loss: 0.7883695363998413\n",
      "Epoch 86, batch 107 D Loss: 1.4165992736816406, G Loss: 0.7632282972335815\n",
      "Epoch 86, batch 108 D Loss: 1.35624098777771, G Loss: 0.7840257287025452\n",
      "Epoch 86, batch 109 D Loss: 1.4448611736297607, G Loss: 0.7298358082771301\n",
      "Epoch 86, batch 110 D Loss: 1.4074015617370605, G Loss: 0.7716832160949707\n",
      "Epoch 86, batch 111 D Loss: 1.423478126525879, G Loss: 0.7876120805740356\n",
      "Epoch 86, batch 112 D Loss: 1.3776307106018066, G Loss: 0.790218710899353\n",
      "Epoch 86, batch 113 D Loss: 1.3983674049377441, G Loss: 0.7732521295547485\n",
      "Epoch 86, batch 114 D Loss: 1.401690125465393, G Loss: 0.8052577972412109\n",
      "Epoch 86, batch 115 D Loss: 1.40187406539917, G Loss: 0.7888204455375671\n",
      "Epoch 86, batch 116 D Loss: 1.4468729496002197, G Loss: 0.7255505323410034\n",
      "Epoch 86, batch 117 D Loss: 1.4662137031555176, G Loss: 0.7290641665458679\n",
      "Epoch 86, batch 118 D Loss: 1.3864165544509888, G Loss: 0.784740149974823\n",
      "Epoch 86, batch 119 D Loss: 1.392737627029419, G Loss: 0.7831710577011108\n",
      "Epoch 86, batch 120 D Loss: 1.4200419187545776, G Loss: 0.7513909935951233\n",
      "Epoch 86, batch 121 D Loss: 1.428248643875122, G Loss: 0.7677456736564636\n",
      "Epoch 86, batch 122 D Loss: 1.3513743877410889, G Loss: 0.7860555052757263\n",
      "Epoch 86, batch 123 D Loss: 1.4368038177490234, G Loss: 0.7566986680030823\n",
      "Epoch 86, batch 124 D Loss: 1.4460680484771729, G Loss: 0.7575475573539734\n",
      "Epoch 86, batch 125 D Loss: 1.3477609157562256, G Loss: 0.7581958174705505\n",
      "Epoch 86, batch 126 D Loss: 1.4156994819641113, G Loss: 0.7715133428573608\n",
      "Epoch 86, batch 127 D Loss: 1.3688966035842896, G Loss: 0.7822126746177673\n",
      "Epoch 86, batch 128 D Loss: 1.3619410991668701, G Loss: 0.7804129719734192\n",
      "Epoch 86, batch 129 D Loss: 1.3610378503799438, G Loss: 0.7813742756843567\n",
      "Epoch 86, batch 130 D Loss: 1.3846638202667236, G Loss: 0.7812970876693726\n",
      "Epoch 86, batch 131 D Loss: 1.3919023275375366, G Loss: 0.784583568572998\n",
      "Epoch 86, batch 132 D Loss: 1.3846222162246704, G Loss: 0.782756507396698\n",
      "Epoch 86, batch 133 D Loss: 1.3994922637939453, G Loss: 0.7644149661064148\n",
      "Epoch 86, batch 134 D Loss: 1.389878273010254, G Loss: 0.8065172433853149\n",
      "Epoch 86, batch 135 D Loss: 1.380710244178772, G Loss: 0.7748530507087708\n",
      "Epoch 86, batch 136 D Loss: 1.4194488525390625, G Loss: 0.78875732421875\n",
      "Epoch 86, batch 137 D Loss: 1.3250677585601807, G Loss: 0.81821608543396\n",
      "Epoch 86, batch 138 D Loss: 1.3978939056396484, G Loss: 0.777082085609436\n",
      "Epoch 86, batch 139 D Loss: 1.3622541427612305, G Loss: 0.7931908965110779\n",
      "Epoch 86, batch 140 D Loss: 1.4013272523880005, G Loss: 0.749357283115387\n",
      "Epoch 86, batch 141 D Loss: 1.3589673042297363, G Loss: 0.7976324558258057\n",
      "Epoch 86, batch 142 D Loss: 1.4077537059783936, G Loss: 0.7764085531234741\n",
      "Epoch 86, batch 143 D Loss: 1.4094901084899902, G Loss: 0.7534250020980835\n",
      "Epoch 86, batch 144 D Loss: 1.4208123683929443, G Loss: 0.7596735954284668\n",
      "Epoch 86, batch 145 D Loss: 1.3698959350585938, G Loss: 0.7514449954032898\n",
      "Epoch 86, batch 146 D Loss: 1.3834669589996338, G Loss: 0.7706058025360107\n",
      "Epoch 86, batch 147 D Loss: 1.4076144695281982, G Loss: 0.7481135725975037\n",
      "Epoch 86, batch 148 D Loss: 1.3726884126663208, G Loss: 0.7780897617340088\n",
      "Epoch 86, batch 149 D Loss: 1.3695411682128906, G Loss: 0.7668170928955078\n",
      "Epoch 86, batch 150 D Loss: 1.405489206314087, G Loss: 0.7398083209991455\n",
      "Epoch 86, batch 151 D Loss: 1.4505534172058105, G Loss: 0.7418275475502014\n",
      "Epoch 86, batch 152 D Loss: 1.4063358306884766, G Loss: 0.7568151950836182\n",
      "Epoch 86, batch 153 D Loss: 1.3751704692840576, G Loss: 0.7856546640396118\n",
      "Epoch 86, batch 154 D Loss: 1.393736720085144, G Loss: 0.7637315392494202\n",
      "Epoch 86, batch 155 D Loss: 1.3758405447006226, G Loss: 0.7812194228172302\n",
      "Epoch 86, batch 156 D Loss: 1.3943902254104614, G Loss: 0.7564257979393005\n",
      "Epoch 86, batch 157 D Loss: 1.3699302673339844, G Loss: 0.7410762310028076\n",
      "Epoch 86, batch 158 D Loss: 1.3657252788543701, G Loss: 0.8017544746398926\n",
      "Epoch 86, batch 159 D Loss: 1.4238579273223877, G Loss: 0.7193567752838135\n",
      "Epoch 86, batch 160 D Loss: 1.399951457977295, G Loss: 0.7368289828300476\n",
      "Epoch 86, batch 161 D Loss: 1.3431344032287598, G Loss: 0.763539731502533\n",
      "Epoch 86, batch 162 D Loss: 1.3994181156158447, G Loss: 0.7318512201309204\n",
      "Epoch 86, batch 163 D Loss: 1.4320602416992188, G Loss: 0.7219191193580627\n",
      "Epoch 86, batch 164 D Loss: 1.4404819011688232, G Loss: 0.7097162008285522\n",
      "Epoch 86, batch 165 D Loss: 1.4413490295410156, G Loss: 0.7026324272155762\n",
      "Epoch 86, batch 166 D Loss: 1.37982177734375, G Loss: 0.7453762888908386\n",
      "Epoch 86, batch 167 D Loss: 1.4035413265228271, G Loss: 0.7159374356269836\n",
      "Epoch 86, batch 168 D Loss: 1.4029169082641602, G Loss: 0.6996925473213196\n",
      "Epoch 86, batch 169 D Loss: 1.3449749946594238, G Loss: 0.7667125463485718\n",
      "Epoch 86, batch 170 D Loss: 1.3627971410751343, G Loss: 0.7451304793357849\n",
      "Epoch 86, batch 171 D Loss: 1.412822961807251, G Loss: 0.7244824171066284\n",
      "Epoch 86, batch 172 D Loss: 1.4232532978057861, G Loss: 0.733854353427887\n",
      "Epoch 86, batch 173 D Loss: 1.3735837936401367, G Loss: 0.750587522983551\n",
      "Epoch 86, batch 174 D Loss: 1.3459930419921875, G Loss: 0.7992695569992065\n",
      "Epoch 86, batch 175 D Loss: 1.4099996089935303, G Loss: 0.7130829691886902\n",
      "Epoch 86, batch 176 D Loss: 1.3788423538208008, G Loss: 0.7568561434745789\n",
      "Epoch 86, batch 177 D Loss: 1.421449065208435, G Loss: 0.6971052289009094\n",
      "Epoch 86, batch 178 D Loss: 1.4496002197265625, G Loss: 0.6909998059272766\n",
      "Epoch 86, batch 179 D Loss: 1.36708402633667, G Loss: 0.7660667896270752\n",
      "Epoch 86, batch 180 D Loss: 1.4159798622131348, G Loss: 0.7363138794898987\n",
      "Epoch 86, batch 181 D Loss: 1.375030755996704, G Loss: 0.7708075046539307\n",
      "Epoch 86, batch 182 D Loss: 1.4105255603790283, G Loss: 0.7309785485267639\n",
      "Epoch 86, batch 183 D Loss: 1.38936448097229, G Loss: 0.7390294075012207\n",
      "Epoch 86, batch 184 D Loss: 1.4419639110565186, G Loss: 0.6974994540214539\n",
      "Epoch 86, batch 185 D Loss: 1.3691699504852295, G Loss: 0.7274497747421265\n",
      "Epoch 86, batch 186 D Loss: 1.4281458854675293, G Loss: 0.699820339679718\n",
      "Epoch 86, batch 187 D Loss: 1.3719358444213867, G Loss: 0.7492173910140991\n",
      "Epoch 86, batch 188 D Loss: 1.4283020496368408, G Loss: 0.6966433525085449\n",
      "Epoch 86, batch 189 D Loss: 1.4068975448608398, G Loss: 0.7029160857200623\n",
      "Epoch 86, batch 190 D Loss: 1.3904571533203125, G Loss: 0.7194778323173523\n",
      "Epoch 86, batch 191 D Loss: 1.3907281160354614, G Loss: 0.7176461815834045\n",
      "Epoch 86, batch 192 D Loss: 1.444720983505249, G Loss: 0.6845689415931702\n",
      "Epoch 86, batch 193 D Loss: 1.3888269662857056, G Loss: 0.7234976291656494\n",
      "Epoch 86, batch 194 D Loss: 1.4166992902755737, G Loss: 0.7213042378425598\n",
      "Epoch 86, batch 195 D Loss: 1.407222867012024, G Loss: 0.7170619368553162\n",
      "Epoch 86, batch 196 D Loss: 1.4184398651123047, G Loss: 0.6903237700462341\n",
      "Epoch 86, batch 197 D Loss: 1.4001154899597168, G Loss: 0.7218735218048096\n",
      "Epoch 86, batch 198 D Loss: 1.4130938053131104, G Loss: 0.700631320476532\n",
      "Epoch 86, batch 199 D Loss: 1.407104253768921, G Loss: 0.7066851854324341\n",
      "Epoch 86, batch 200 D Loss: 1.3537700176239014, G Loss: 0.7292101979255676\n",
      "Epoch 87, batch 1 D Loss: 1.3911622762680054, G Loss: 0.7087416648864746\n",
      "Epoch 87, batch 2 D Loss: 1.3691613674163818, G Loss: 0.7281622290611267\n",
      "Epoch 87, batch 3 D Loss: 1.4165370464324951, G Loss: 0.7053990364074707\n",
      "Epoch 87, batch 4 D Loss: 1.411050796508789, G Loss: 0.6961867809295654\n",
      "Epoch 87, batch 5 D Loss: 1.4305267333984375, G Loss: 0.6907068490982056\n",
      "Epoch 87, batch 6 D Loss: 1.3972160816192627, G Loss: 0.6761260032653809\n",
      "Epoch 87, batch 7 D Loss: 1.4072151184082031, G Loss: 0.7060894966125488\n",
      "Epoch 87, batch 8 D Loss: 1.3785347938537598, G Loss: 0.6897955536842346\n",
      "Epoch 87, batch 9 D Loss: 1.3940014839172363, G Loss: 0.7134315967559814\n",
      "Epoch 87, batch 10 D Loss: 1.386352300643921, G Loss: 0.6976940035820007\n",
      "Epoch 87, batch 11 D Loss: 1.4157521724700928, G Loss: 0.6905975937843323\n",
      "Epoch 87, batch 12 D Loss: 1.4024333953857422, G Loss: 0.693448543548584\n",
      "Epoch 87, batch 13 D Loss: 1.384976863861084, G Loss: 0.7158918976783752\n",
      "Epoch 87, batch 14 D Loss: 1.3653101921081543, G Loss: 0.7225140333175659\n",
      "Epoch 87, batch 15 D Loss: 1.4008936882019043, G Loss: 0.6981555819511414\n",
      "Epoch 87, batch 16 D Loss: 1.3841657638549805, G Loss: 0.6792498826980591\n",
      "Epoch 87, batch 17 D Loss: 1.402524471282959, G Loss: 0.7237990498542786\n",
      "Epoch 87, batch 18 D Loss: 1.3636137247085571, G Loss: 0.7575274705886841\n",
      "Epoch 87, batch 19 D Loss: 1.4037859439849854, G Loss: 0.6979854106903076\n",
      "Epoch 87, batch 20 D Loss: 1.3649184703826904, G Loss: 0.7128309011459351\n",
      "Epoch 87, batch 21 D Loss: 1.4194610118865967, G Loss: 0.697007954120636\n",
      "Epoch 87, batch 22 D Loss: 1.3887680768966675, G Loss: 0.7086338996887207\n",
      "Epoch 87, batch 23 D Loss: 1.4050586223602295, G Loss: 0.6832482814788818\n",
      "Epoch 87, batch 24 D Loss: 1.3864378929138184, G Loss: 0.7206229567527771\n",
      "Epoch 87, batch 25 D Loss: 1.3661692142486572, G Loss: 0.7235708832740784\n",
      "Epoch 87, batch 26 D Loss: 1.3814232349395752, G Loss: 0.7186651825904846\n",
      "Epoch 87, batch 27 D Loss: 1.3804484605789185, G Loss: 0.7194569110870361\n",
      "Epoch 87, batch 28 D Loss: 1.402680516242981, G Loss: 0.7129765152931213\n",
      "Epoch 87, batch 29 D Loss: 1.3877484798431396, G Loss: 0.696841299533844\n",
      "Epoch 87, batch 30 D Loss: 1.3795924186706543, G Loss: 0.7249290347099304\n",
      "Epoch 87, batch 31 D Loss: 1.389047384262085, G Loss: 0.6888073682785034\n",
      "Epoch 87, batch 32 D Loss: 1.3534965515136719, G Loss: 0.741896390914917\n",
      "Epoch 87, batch 33 D Loss: 1.43162202835083, G Loss: 0.7113929986953735\n",
      "Epoch 87, batch 34 D Loss: 1.3927363157272339, G Loss: 0.7115892171859741\n",
      "Epoch 87, batch 35 D Loss: 1.4298670291900635, G Loss: 0.7051915526390076\n",
      "Epoch 87, batch 36 D Loss: 1.3999508619308472, G Loss: 0.7331874966621399\n",
      "Epoch 87, batch 37 D Loss: 1.377126693725586, G Loss: 0.7593692541122437\n",
      "Epoch 87, batch 38 D Loss: 1.3371329307556152, G Loss: 0.7303559184074402\n",
      "Epoch 87, batch 39 D Loss: 1.4615800380706787, G Loss: 0.6922245025634766\n",
      "Epoch 87, batch 40 D Loss: 1.3436816930770874, G Loss: 0.7516837120056152\n",
      "Epoch 87, batch 41 D Loss: 1.4074933528900146, G Loss: 0.7186413407325745\n",
      "Epoch 87, batch 42 D Loss: 1.4137316942214966, G Loss: 0.7000278234481812\n",
      "Epoch 87, batch 43 D Loss: 1.4424134492874146, G Loss: 0.6916555166244507\n",
      "Epoch 87, batch 44 D Loss: 1.3774454593658447, G Loss: 0.734062671661377\n",
      "Epoch 87, batch 45 D Loss: 1.376989483833313, G Loss: 0.7433411478996277\n",
      "Epoch 87, batch 46 D Loss: 1.3735358715057373, G Loss: 0.7299720048904419\n",
      "Epoch 87, batch 47 D Loss: 1.3784847259521484, G Loss: 0.7265289425849915\n",
      "Epoch 87, batch 48 D Loss: 1.3863105773925781, G Loss: 0.7126355171203613\n",
      "Epoch 87, batch 49 D Loss: 1.3843942880630493, G Loss: 0.706770658493042\n",
      "Epoch 87, batch 50 D Loss: 1.3984456062316895, G Loss: 0.7186315655708313\n",
      "Epoch 87, batch 51 D Loss: 1.3708218336105347, G Loss: 0.7399842143058777\n",
      "Epoch 87, batch 52 D Loss: 1.437849998474121, G Loss: 0.7002525329589844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, batch 53 D Loss: 1.3460593223571777, G Loss: 0.7387958765029907\n",
      "Epoch 87, batch 54 D Loss: 1.3817720413208008, G Loss: 0.730481743812561\n",
      "Epoch 87, batch 55 D Loss: 1.385909080505371, G Loss: 0.7358689308166504\n",
      "Epoch 87, batch 56 D Loss: 1.4024415016174316, G Loss: 0.7303539514541626\n",
      "Epoch 87, batch 57 D Loss: 1.400386095046997, G Loss: 0.7143726944923401\n",
      "Epoch 87, batch 58 D Loss: 1.3706927299499512, G Loss: 0.7321804761886597\n",
      "Epoch 87, batch 59 D Loss: 1.3603986501693726, G Loss: 0.7593564391136169\n",
      "Epoch 87, batch 60 D Loss: 1.4010095596313477, G Loss: 0.6949540972709656\n",
      "Epoch 87, batch 61 D Loss: 1.391410231590271, G Loss: 0.7265105247497559\n",
      "Epoch 87, batch 62 D Loss: 1.3689994812011719, G Loss: 0.71043860912323\n",
      "Epoch 87, batch 63 D Loss: 1.38883376121521, G Loss: 0.7320347428321838\n",
      "Epoch 87, batch 64 D Loss: 1.35536527633667, G Loss: 0.728465735912323\n",
      "Epoch 87, batch 65 D Loss: 1.4091477394104004, G Loss: 0.6852310299873352\n",
      "Epoch 87, batch 66 D Loss: 1.439237356185913, G Loss: 0.6768966913223267\n",
      "Epoch 87, batch 67 D Loss: 1.3808830976486206, G Loss: 0.7142528295516968\n",
      "Epoch 87, batch 68 D Loss: 1.377097487449646, G Loss: 0.7067404985427856\n",
      "Epoch 87, batch 69 D Loss: 1.407076358795166, G Loss: 0.7220932841300964\n",
      "Epoch 87, batch 70 D Loss: 1.3688138723373413, G Loss: 0.7028680443763733\n",
      "Epoch 87, batch 71 D Loss: 1.3904184103012085, G Loss: 0.7088353633880615\n",
      "Epoch 87, batch 72 D Loss: 1.3745932579040527, G Loss: 0.711597740650177\n",
      "Epoch 87, batch 73 D Loss: 1.4211726188659668, G Loss: 0.68080735206604\n",
      "Epoch 87, batch 74 D Loss: 1.4089624881744385, G Loss: 0.7226079702377319\n",
      "Epoch 87, batch 75 D Loss: 1.4133625030517578, G Loss: 0.7044141292572021\n",
      "Epoch 87, batch 76 D Loss: 1.3781862258911133, G Loss: 0.7121050953865051\n",
      "Epoch 87, batch 77 D Loss: 1.3647651672363281, G Loss: 0.7011083960533142\n",
      "Epoch 87, batch 78 D Loss: 1.3898159265518188, G Loss: 0.6901185512542725\n",
      "Epoch 87, batch 79 D Loss: 1.3825268745422363, G Loss: 0.7148894667625427\n",
      "Epoch 87, batch 80 D Loss: 1.3954651355743408, G Loss: 0.6966601014137268\n",
      "Epoch 87, batch 81 D Loss: 1.3652974367141724, G Loss: 0.7130083441734314\n",
      "Epoch 87, batch 82 D Loss: 1.3931413888931274, G Loss: 0.7032493352890015\n",
      "Epoch 87, batch 83 D Loss: 1.3835173845291138, G Loss: 0.7116660475730896\n",
      "Epoch 87, batch 84 D Loss: 1.3994724750518799, G Loss: 0.703217089176178\n",
      "Epoch 87, batch 85 D Loss: 1.3991811275482178, G Loss: 0.6948484778404236\n",
      "Epoch 87, batch 86 D Loss: 1.3749953508377075, G Loss: 0.6924891471862793\n",
      "Epoch 87, batch 87 D Loss: 1.381706953048706, G Loss: 0.7087442874908447\n",
      "Epoch 87, batch 88 D Loss: 1.3806977272033691, G Loss: 0.6984356641769409\n",
      "Epoch 87, batch 89 D Loss: 1.3754900693893433, G Loss: 0.710662305355072\n",
      "Epoch 87, batch 90 D Loss: 1.3807849884033203, G Loss: 0.7085833549499512\n",
      "Epoch 87, batch 91 D Loss: 1.3861048221588135, G Loss: 0.7205819487571716\n",
      "Epoch 87, batch 92 D Loss: 1.4001280069351196, G Loss: 0.6926125884056091\n",
      "Epoch 87, batch 93 D Loss: 1.4166648387908936, G Loss: 0.6811456084251404\n",
      "Epoch 87, batch 94 D Loss: 1.381121277809143, G Loss: 0.7007615566253662\n",
      "Epoch 87, batch 95 D Loss: 1.4051661491394043, G Loss: 0.6871278882026672\n",
      "Epoch 87, batch 96 D Loss: 1.3730086088180542, G Loss: 0.6954583525657654\n",
      "Epoch 87, batch 97 D Loss: 1.379595160484314, G Loss: 0.7066437005996704\n",
      "Epoch 87, batch 98 D Loss: 1.3843629360198975, G Loss: 0.699344277381897\n",
      "Epoch 87, batch 99 D Loss: 1.3790552616119385, G Loss: 0.7072150707244873\n",
      "Epoch 87, batch 100 D Loss: 1.4066214561462402, G Loss: 0.6983863711357117\n",
      "Epoch 87, batch 101 D Loss: 1.3678157329559326, G Loss: 0.7040212154388428\n",
      "Epoch 87, batch 102 D Loss: 1.3843226432800293, G Loss: 0.7147513031959534\n",
      "Epoch 87, batch 103 D Loss: 1.3875575065612793, G Loss: 0.687803328037262\n",
      "Epoch 87, batch 104 D Loss: 1.4333300590515137, G Loss: 0.6892394423484802\n",
      "Epoch 87, batch 105 D Loss: 1.4008784294128418, G Loss: 0.6877493858337402\n",
      "Epoch 87, batch 106 D Loss: 1.3932452201843262, G Loss: 0.6976699233055115\n",
      "Epoch 87, batch 107 D Loss: 1.4056662321090698, G Loss: 0.6953489184379578\n",
      "Epoch 87, batch 108 D Loss: 1.3669832944869995, G Loss: 0.7121480703353882\n",
      "Epoch 87, batch 109 D Loss: 1.3689262866973877, G Loss: 0.6894239783287048\n",
      "Epoch 87, batch 110 D Loss: 1.3788540363311768, G Loss: 0.693713366985321\n",
      "Epoch 87, batch 111 D Loss: 1.3910348415374756, G Loss: 0.6937737464904785\n",
      "Epoch 87, batch 112 D Loss: 1.4007604122161865, G Loss: 0.6893860101699829\n",
      "Epoch 87, batch 113 D Loss: 1.3919304609298706, G Loss: 0.6971648931503296\n",
      "Epoch 87, batch 114 D Loss: 1.3811280727386475, G Loss: 0.6956723928451538\n",
      "Epoch 87, batch 115 D Loss: 1.3996996879577637, G Loss: 0.7093181610107422\n",
      "Epoch 87, batch 116 D Loss: 1.383896827697754, G Loss: 0.6821662187576294\n",
      "Epoch 87, batch 117 D Loss: 1.38335120677948, G Loss: 0.6873675584793091\n",
      "Epoch 87, batch 118 D Loss: 1.4031932353973389, G Loss: 0.7002636194229126\n",
      "Epoch 87, batch 119 D Loss: 1.369800090789795, G Loss: 0.6801832318305969\n",
      "Epoch 87, batch 120 D Loss: 1.377195119857788, G Loss: 0.7116363048553467\n",
      "Epoch 87, batch 121 D Loss: 1.3894915580749512, G Loss: 0.694244921207428\n",
      "Epoch 87, batch 122 D Loss: 1.4280550479888916, G Loss: 0.6753575205802917\n",
      "Epoch 87, batch 123 D Loss: 1.3808590173721313, G Loss: 0.6949682831764221\n",
      "Epoch 87, batch 124 D Loss: 1.3946843147277832, G Loss: 0.694577157497406\n",
      "Epoch 87, batch 125 D Loss: 1.4070439338684082, G Loss: 0.6835911273956299\n",
      "Epoch 87, batch 126 D Loss: 1.362440586090088, G Loss: 0.7006228566169739\n",
      "Epoch 87, batch 127 D Loss: 1.4092357158660889, G Loss: 0.6741149425506592\n",
      "Epoch 87, batch 128 D Loss: 1.403510570526123, G Loss: 0.6872779130935669\n",
      "Epoch 87, batch 129 D Loss: 1.3821845054626465, G Loss: 0.6937345266342163\n",
      "Epoch 87, batch 130 D Loss: 1.38814377784729, G Loss: 0.6996979713439941\n",
      "Epoch 87, batch 131 D Loss: 1.4155949354171753, G Loss: 0.6816259026527405\n",
      "Epoch 87, batch 132 D Loss: 1.3785111904144287, G Loss: 0.6909831166267395\n",
      "Epoch 87, batch 133 D Loss: 1.4205365180969238, G Loss: 0.6882899403572083\n",
      "Epoch 87, batch 134 D Loss: 1.3806198835372925, G Loss: 0.709475576877594\n",
      "Epoch 87, batch 135 D Loss: 1.3907111883163452, G Loss: 0.7099878787994385\n",
      "Epoch 87, batch 136 D Loss: 1.3430345058441162, G Loss: 0.7139542102813721\n",
      "Epoch 87, batch 137 D Loss: 1.4011650085449219, G Loss: 0.6930274963378906\n",
      "Epoch 87, batch 138 D Loss: 1.4072095155715942, G Loss: 0.6877026557922363\n",
      "Epoch 87, batch 139 D Loss: 1.3833961486816406, G Loss: 0.6948654055595398\n",
      "Epoch 87, batch 140 D Loss: 1.3919724225997925, G Loss: 0.7006228566169739\n",
      "Epoch 87, batch 141 D Loss: 1.3859870433807373, G Loss: 0.700050950050354\n",
      "Epoch 87, batch 142 D Loss: 1.4039630889892578, G Loss: 0.6892982721328735\n",
      "Epoch 87, batch 143 D Loss: 1.3980321884155273, G Loss: 0.6897476315498352\n",
      "Epoch 87, batch 144 D Loss: 1.3931689262390137, G Loss: 0.6897867321968079\n",
      "Epoch 87, batch 145 D Loss: 1.4110655784606934, G Loss: 0.700843334197998\n",
      "Epoch 87, batch 146 D Loss: 1.3930509090423584, G Loss: 0.6885392665863037\n",
      "Epoch 87, batch 147 D Loss: 1.3587052822113037, G Loss: 0.7070224285125732\n",
      "Epoch 87, batch 148 D Loss: 1.420731544494629, G Loss: 0.6819248199462891\n",
      "Epoch 87, batch 149 D Loss: 1.3605914115905762, G Loss: 0.6990798711776733\n",
      "Epoch 87, batch 150 D Loss: 1.432765245437622, G Loss: 0.6772819757461548\n",
      "Epoch 87, batch 151 D Loss: 1.382089376449585, G Loss: 0.7148017883300781\n",
      "Epoch 87, batch 152 D Loss: 1.4040931463241577, G Loss: 0.6893380880355835\n",
      "Epoch 87, batch 153 D Loss: 1.3910305500030518, G Loss: 0.686201810836792\n",
      "Epoch 87, batch 154 D Loss: 1.3983759880065918, G Loss: 0.6875439286231995\n",
      "Epoch 87, batch 155 D Loss: 1.4080355167388916, G Loss: 0.6951999068260193\n",
      "Epoch 87, batch 156 D Loss: 1.3980371952056885, G Loss: 0.6932939291000366\n",
      "Epoch 87, batch 157 D Loss: 1.383150577545166, G Loss: 0.6978765726089478\n",
      "Epoch 87, batch 158 D Loss: 1.3948895931243896, G Loss: 0.6809559464454651\n",
      "Epoch 87, batch 159 D Loss: 1.3830852508544922, G Loss: 0.6925040483474731\n",
      "Epoch 87, batch 160 D Loss: 1.4033827781677246, G Loss: 0.6853231191635132\n",
      "Epoch 87, batch 161 D Loss: 1.3807487487792969, G Loss: 0.7020416855812073\n",
      "Epoch 87, batch 162 D Loss: 1.4049559831619263, G Loss: 0.6802393198013306\n",
      "Epoch 87, batch 163 D Loss: 1.405580759048462, G Loss: 0.685355007648468\n",
      "Epoch 87, batch 164 D Loss: 1.3868871927261353, G Loss: 0.6916201710700989\n",
      "Epoch 87, batch 165 D Loss: 1.3633869886398315, G Loss: 0.7136169672012329\n",
      "Epoch 87, batch 166 D Loss: 1.3869969844818115, G Loss: 0.6958988308906555\n",
      "Epoch 87, batch 167 D Loss: 1.3809365034103394, G Loss: 0.6805875897407532\n",
      "Epoch 87, batch 168 D Loss: 1.4177184104919434, G Loss: 0.6770815253257751\n",
      "Epoch 87, batch 169 D Loss: 1.3983876705169678, G Loss: 0.6887128949165344\n",
      "Epoch 87, batch 170 D Loss: 1.4233852624893188, G Loss: 0.6733039617538452\n",
      "Epoch 87, batch 171 D Loss: 1.403648853302002, G Loss: 0.6811089515686035\n",
      "Epoch 87, batch 172 D Loss: 1.4071485996246338, G Loss: 0.6847336292266846\n",
      "Epoch 87, batch 173 D Loss: 1.3905794620513916, G Loss: 0.694865345954895\n",
      "Epoch 87, batch 174 D Loss: 1.396850347518921, G Loss: 0.6900462508201599\n",
      "Epoch 87, batch 175 D Loss: 1.373985767364502, G Loss: 0.6978737711906433\n",
      "Epoch 87, batch 176 D Loss: 1.4071589708328247, G Loss: 0.6790772080421448\n",
      "Epoch 87, batch 177 D Loss: 1.3711886405944824, G Loss: 0.6892308592796326\n",
      "Epoch 87, batch 178 D Loss: 1.4057862758636475, G Loss: 0.6820458173751831\n",
      "Epoch 87, batch 179 D Loss: 1.3820098638534546, G Loss: 0.6808961629867554\n",
      "Epoch 87, batch 180 D Loss: 1.3989074230194092, G Loss: 0.6853672862052917\n",
      "Epoch 87, batch 181 D Loss: 1.4014073610305786, G Loss: 0.6917242407798767\n",
      "Epoch 87, batch 182 D Loss: 1.4150328636169434, G Loss: 0.6932541131973267\n",
      "Epoch 87, batch 183 D Loss: 1.3976773023605347, G Loss: 0.6740521788597107\n",
      "Epoch 87, batch 184 D Loss: 1.3952794075012207, G Loss: 0.6915096044540405\n",
      "Epoch 87, batch 185 D Loss: 1.4132540225982666, G Loss: 0.678927481174469\n",
      "Epoch 87, batch 186 D Loss: 1.3742815256118774, G Loss: 0.7110329270362854\n",
      "Epoch 87, batch 187 D Loss: 1.3814895153045654, G Loss: 0.6915116310119629\n",
      "Epoch 87, batch 188 D Loss: 1.3629601001739502, G Loss: 0.688636302947998\n",
      "Epoch 87, batch 189 D Loss: 1.396646499633789, G Loss: 0.6872094869613647\n",
      "Epoch 87, batch 190 D Loss: 1.4091088771820068, G Loss: 0.6697208285331726\n",
      "Epoch 87, batch 191 D Loss: 1.3931705951690674, G Loss: 0.6859025359153748\n",
      "Epoch 87, batch 192 D Loss: 1.3718547821044922, G Loss: 0.6936139464378357\n",
      "Epoch 87, batch 193 D Loss: 1.3790076971054077, G Loss: 0.6925361752510071\n",
      "Epoch 87, batch 194 D Loss: 1.4067684412002563, G Loss: 0.6805384755134583\n",
      "Epoch 87, batch 195 D Loss: 1.3775442838668823, G Loss: 0.6887948513031006\n",
      "Epoch 87, batch 196 D Loss: 1.384080171585083, G Loss: 0.6872718334197998\n",
      "Epoch 87, batch 197 D Loss: 1.3639826774597168, G Loss: 0.6824344396591187\n",
      "Epoch 87, batch 198 D Loss: 1.395763635635376, G Loss: 0.6849862933158875\n",
      "Epoch 87, batch 199 D Loss: 1.4058640003204346, G Loss: 0.6788076162338257\n",
      "Epoch 87, batch 200 D Loss: 1.378529667854309, G Loss: 0.6902963519096375\n",
      "Epoch 88, batch 1 D Loss: 1.4165167808532715, G Loss: 0.6734482645988464\n",
      "Epoch 88, batch 2 D Loss: 1.3738864660263062, G Loss: 0.6908095479011536\n",
      "Epoch 88, batch 3 D Loss: 1.3949840068817139, G Loss: 0.6727397441864014\n",
      "Epoch 88, batch 4 D Loss: 1.3992247581481934, G Loss: 0.6825639605522156\n",
      "Epoch 88, batch 5 D Loss: 1.410109281539917, G Loss: 0.6704527139663696\n",
      "Epoch 88, batch 6 D Loss: 1.3893686532974243, G Loss: 0.697033166885376\n",
      "Epoch 88, batch 7 D Loss: 1.3931503295898438, G Loss: 0.6828097701072693\n",
      "Epoch 88, batch 8 D Loss: 1.3801069259643555, G Loss: 0.6807246208190918\n",
      "Epoch 88, batch 9 D Loss: 1.3891680240631104, G Loss: 0.6817098259925842\n",
      "Epoch 88, batch 10 D Loss: 1.3972129821777344, G Loss: 0.6823521256446838\n",
      "Epoch 88, batch 11 D Loss: 1.3881018161773682, G Loss: 0.6797294020652771\n",
      "Epoch 88, batch 12 D Loss: 1.4123256206512451, G Loss: 0.679982602596283\n",
      "Epoch 88, batch 13 D Loss: 1.388754963874817, G Loss: 0.6880677938461304\n",
      "Epoch 88, batch 14 D Loss: 1.403404951095581, G Loss: 0.6851340532302856\n",
      "Epoch 88, batch 15 D Loss: 1.4070160388946533, G Loss: 0.6863375902175903\n",
      "Epoch 88, batch 16 D Loss: 1.4003276824951172, G Loss: 0.6855172514915466\n",
      "Epoch 88, batch 17 D Loss: 1.4155750274658203, G Loss: 0.6740192174911499\n",
      "Epoch 88, batch 18 D Loss: 1.3857356309890747, G Loss: 0.6826714277267456\n",
      "Epoch 88, batch 19 D Loss: 1.381819486618042, G Loss: 0.7005580067634583\n",
      "Epoch 88, batch 20 D Loss: 1.403393030166626, G Loss: 0.6764698624610901\n",
      "Epoch 88, batch 21 D Loss: 1.3832738399505615, G Loss: 0.6935926079750061\n",
      "Epoch 88, batch 22 D Loss: 1.3914618492126465, G Loss: 0.6879625916481018\n",
      "Epoch 88, batch 23 D Loss: 1.4022107124328613, G Loss: 0.6757175326347351\n",
      "Epoch 88, batch 24 D Loss: 1.4204940795898438, G Loss: 0.6680312156677246\n",
      "Epoch 88, batch 25 D Loss: 1.3964706659317017, G Loss: 0.6818376779556274\n",
      "Epoch 88, batch 26 D Loss: 1.3881134986877441, G Loss: 0.6839780211448669\n",
      "Epoch 88, batch 27 D Loss: 1.3919780254364014, G Loss: 0.6834084987640381\n",
      "Epoch 88, batch 28 D Loss: 1.4056864976882935, G Loss: 0.6729398369789124\n",
      "Epoch 88, batch 29 D Loss: 1.3989999294281006, G Loss: 0.6889808773994446\n",
      "Epoch 88, batch 30 D Loss: 1.386643886566162, G Loss: 0.6950885653495789\n",
      "Epoch 88, batch 31 D Loss: 1.4002504348754883, G Loss: 0.6867788434028625\n",
      "Epoch 88, batch 32 D Loss: 1.3916785717010498, G Loss: 0.6897580623626709\n",
      "Epoch 88, batch 33 D Loss: 1.3785145282745361, G Loss: 0.6947348117828369\n",
      "Epoch 88, batch 34 D Loss: 1.3691247701644897, G Loss: 0.7048662304878235\n",
      "Epoch 88, batch 35 D Loss: 1.389312744140625, G Loss: 0.686438798904419\n",
      "Epoch 88, batch 36 D Loss: 1.4050123691558838, G Loss: 0.6876196265220642\n",
      "Epoch 88, batch 37 D Loss: 1.4114148616790771, G Loss: 0.6775257587432861\n",
      "Epoch 88, batch 38 D Loss: 1.4022854566574097, G Loss: 0.6828292012214661\n",
      "Epoch 88, batch 39 D Loss: 1.3793202638626099, G Loss: 0.6860738396644592\n",
      "Epoch 88, batch 40 D Loss: 1.3836030960083008, G Loss: 0.6952861547470093\n",
      "Epoch 88, batch 41 D Loss: 1.3937342166900635, G Loss: 0.6898500919342041\n",
      "Epoch 88, batch 42 D Loss: 1.3913240432739258, G Loss: 0.6909856200218201\n",
      "Epoch 88, batch 43 D Loss: 1.3832018375396729, G Loss: 0.7054421901702881\n",
      "Epoch 88, batch 44 D Loss: 1.4080604314804077, G Loss: 0.6824327111244202\n",
      "Epoch 88, batch 45 D Loss: 1.3833438158035278, G Loss: 0.7034873962402344\n",
      "Epoch 88, batch 46 D Loss: 1.381649374961853, G Loss: 0.7081907391548157\n",
      "Epoch 88, batch 47 D Loss: 1.3874558210372925, G Loss: 0.6952109336853027\n",
      "Epoch 88, batch 48 D Loss: 1.4022619724273682, G Loss: 0.6839117407798767\n",
      "Epoch 88, batch 49 D Loss: 1.3857791423797607, G Loss: 0.7043735980987549\n",
      "Epoch 88, batch 50 D Loss: 1.3987066745758057, G Loss: 0.6957923173904419\n",
      "Epoch 88, batch 51 D Loss: 1.3858191967010498, G Loss: 0.694781482219696\n",
      "Epoch 88, batch 52 D Loss: 1.3824747800827026, G Loss: 0.7022507190704346\n",
      "Epoch 88, batch 53 D Loss: 1.396675705909729, G Loss: 0.6879406571388245\n",
      "Epoch 88, batch 54 D Loss: 1.3879663944244385, G Loss: 0.6900659203529358\n",
      "Epoch 88, batch 55 D Loss: 1.3992831707000732, G Loss: 0.686672031879425\n",
      "Epoch 88, batch 56 D Loss: 1.3910043239593506, G Loss: 0.6900045871734619\n",
      "Epoch 88, batch 57 D Loss: 1.3938672542572021, G Loss: 0.6937541365623474\n",
      "Epoch 88, batch 58 D Loss: 1.38338041305542, G Loss: 0.7059817314147949\n",
      "Epoch 88, batch 59 D Loss: 1.3935624361038208, G Loss: 0.6832236647605896\n",
      "Epoch 88, batch 60 D Loss: 1.3923356533050537, G Loss: 0.691231369972229\n",
      "Epoch 88, batch 61 D Loss: 1.3818427324295044, G Loss: 0.6962353587150574\n",
      "Epoch 88, batch 62 D Loss: 1.3806613683700562, G Loss: 0.7023018598556519\n",
      "Epoch 88, batch 63 D Loss: 1.3951553106307983, G Loss: 0.6985286474227905\n",
      "Epoch 88, batch 64 D Loss: 1.365910530090332, G Loss: 0.7037813067436218\n",
      "Epoch 88, batch 65 D Loss: 1.3886996507644653, G Loss: 0.6953414678573608\n",
      "Epoch 88, batch 66 D Loss: 1.4009099006652832, G Loss: 0.6899634003639221\n",
      "Epoch 88, batch 67 D Loss: 1.370556354522705, G Loss: 0.7111960649490356\n",
      "Epoch 88, batch 68 D Loss: 1.4097812175750732, G Loss: 0.7016887068748474\n",
      "Epoch 88, batch 69 D Loss: 1.3905978202819824, G Loss: 0.6973695158958435\n",
      "Epoch 88, batch 70 D Loss: 1.3834376335144043, G Loss: 0.6931045055389404\n",
      "Epoch 88, batch 71 D Loss: 1.4052984714508057, G Loss: 0.6878552436828613\n",
      "Epoch 88, batch 72 D Loss: 1.4100773334503174, G Loss: 0.6869266033172607\n",
      "Epoch 88, batch 73 D Loss: 1.3945953845977783, G Loss: 0.6896542906761169\n",
      "Epoch 88, batch 74 D Loss: 1.3838024139404297, G Loss: 0.7023655772209167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, batch 75 D Loss: 1.3866961002349854, G Loss: 0.6979379057884216\n",
      "Epoch 88, batch 76 D Loss: 1.37391996383667, G Loss: 0.6979501843452454\n",
      "Epoch 88, batch 77 D Loss: 1.3787319660186768, G Loss: 0.6986930966377258\n",
      "Epoch 88, batch 78 D Loss: 1.3771079778671265, G Loss: 0.7029818892478943\n",
      "Epoch 88, batch 79 D Loss: 1.4007391929626465, G Loss: 0.6931895613670349\n",
      "Epoch 88, batch 80 D Loss: 1.3845596313476562, G Loss: 0.7098167538642883\n",
      "Epoch 88, batch 81 D Loss: 1.370434284210205, G Loss: 0.7129031419754028\n",
      "Epoch 88, batch 82 D Loss: 1.3957029581069946, G Loss: 0.6910215616226196\n",
      "Epoch 88, batch 83 D Loss: 1.3912664651870728, G Loss: 0.711926281452179\n",
      "Epoch 88, batch 84 D Loss: 1.3763591051101685, G Loss: 0.708037257194519\n",
      "Epoch 88, batch 85 D Loss: 1.3872971534729004, G Loss: 0.7033963203430176\n",
      "Epoch 88, batch 86 D Loss: 1.3925013542175293, G Loss: 0.6948405504226685\n",
      "Epoch 88, batch 87 D Loss: 1.387967824935913, G Loss: 0.709486722946167\n",
      "Epoch 88, batch 88 D Loss: 1.3846452236175537, G Loss: 0.6952775716781616\n",
      "Epoch 88, batch 89 D Loss: 1.386863350868225, G Loss: 0.6990486979484558\n",
      "Epoch 88, batch 90 D Loss: 1.3979673385620117, G Loss: 0.6933451890945435\n",
      "Epoch 88, batch 91 D Loss: 1.4073328971862793, G Loss: 0.7006198167800903\n",
      "Epoch 88, batch 92 D Loss: 1.388085961341858, G Loss: 0.70530104637146\n",
      "Epoch 88, batch 93 D Loss: 1.3887344598770142, G Loss: 0.6983299851417542\n",
      "Epoch 88, batch 94 D Loss: 1.3880070447921753, G Loss: 0.7198312878608704\n",
      "Epoch 88, batch 95 D Loss: 1.4152864217758179, G Loss: 0.6949504017829895\n",
      "Epoch 88, batch 96 D Loss: 1.3886387348175049, G Loss: 0.7075799703598022\n",
      "Epoch 88, batch 97 D Loss: 1.3917961120605469, G Loss: 0.7049473524093628\n",
      "Epoch 88, batch 98 D Loss: 1.3850312232971191, G Loss: 0.7094855308532715\n",
      "Epoch 88, batch 99 D Loss: 1.385528564453125, G Loss: 0.7063915729522705\n",
      "Epoch 88, batch 100 D Loss: 1.3980712890625, G Loss: 0.6865277290344238\n",
      "Epoch 88, batch 101 D Loss: 1.3806843757629395, G Loss: 0.7076598405838013\n",
      "Epoch 88, batch 102 D Loss: 1.3904528617858887, G Loss: 0.7093610167503357\n",
      "Epoch 88, batch 103 D Loss: 1.3982245922088623, G Loss: 0.6970831155776978\n",
      "Epoch 88, batch 104 D Loss: 1.404066562652588, G Loss: 0.7097377181053162\n",
      "Epoch 88, batch 105 D Loss: 1.3930151462554932, G Loss: 0.7018583416938782\n",
      "Epoch 88, batch 106 D Loss: 1.39469575881958, G Loss: 0.6992290616035461\n",
      "Epoch 88, batch 107 D Loss: 1.3721932172775269, G Loss: 0.7251503467559814\n",
      "Epoch 88, batch 108 D Loss: 1.387824296951294, G Loss: 0.7005490660667419\n",
      "Epoch 88, batch 109 D Loss: 1.3836169242858887, G Loss: 0.7042227387428284\n",
      "Epoch 88, batch 110 D Loss: 1.408065676689148, G Loss: 0.692808985710144\n",
      "Epoch 88, batch 111 D Loss: 1.3895328044891357, G Loss: 0.7112178802490234\n",
      "Epoch 88, batch 112 D Loss: 1.3928446769714355, G Loss: 0.6928238868713379\n",
      "Epoch 88, batch 113 D Loss: 1.3840067386627197, G Loss: 0.6966055035591125\n",
      "Epoch 88, batch 114 D Loss: 1.3771929740905762, G Loss: 0.7076965570449829\n",
      "Epoch 88, batch 115 D Loss: 1.3902596235275269, G Loss: 0.7115037441253662\n",
      "Epoch 88, batch 116 D Loss: 1.3936052322387695, G Loss: 0.7114843726158142\n",
      "Epoch 88, batch 117 D Loss: 1.4276080131530762, G Loss: 0.7008550763130188\n",
      "Epoch 88, batch 118 D Loss: 1.405137062072754, G Loss: 0.6957007050514221\n",
      "Epoch 88, batch 119 D Loss: 1.3929774761199951, G Loss: 0.6919615268707275\n",
      "Epoch 88, batch 120 D Loss: 1.3916611671447754, G Loss: 0.7000864148139954\n",
      "Epoch 88, batch 121 D Loss: 1.3833603858947754, G Loss: 0.7141097187995911\n",
      "Epoch 88, batch 122 D Loss: 1.3839335441589355, G Loss: 0.6943342089653015\n",
      "Epoch 88, batch 123 D Loss: 1.403336524963379, G Loss: 0.6913263201713562\n",
      "Epoch 88, batch 124 D Loss: 1.3704278469085693, G Loss: 0.7144941687583923\n",
      "Epoch 88, batch 125 D Loss: 1.4022170305252075, G Loss: 0.7056827545166016\n",
      "Epoch 88, batch 126 D Loss: 1.3947174549102783, G Loss: 0.7083254456520081\n",
      "Epoch 88, batch 127 D Loss: 1.3881971836090088, G Loss: 0.7006233930587769\n",
      "Epoch 88, batch 128 D Loss: 1.3949964046478271, G Loss: 0.7012389302253723\n",
      "Epoch 88, batch 129 D Loss: 1.3851289749145508, G Loss: 0.7029207348823547\n",
      "Epoch 88, batch 130 D Loss: 1.412871241569519, G Loss: 0.6851718425750732\n",
      "Epoch 88, batch 131 D Loss: 1.4007408618927002, G Loss: 0.683796763420105\n",
      "Epoch 88, batch 132 D Loss: 1.3932420015335083, G Loss: 0.6918630003929138\n",
      "Epoch 88, batch 133 D Loss: 1.3652305603027344, G Loss: 0.7165841460227966\n",
      "Epoch 88, batch 134 D Loss: 1.3902530670166016, G Loss: 0.6874434947967529\n",
      "Epoch 88, batch 135 D Loss: 1.3938038349151611, G Loss: 0.6943713426589966\n",
      "Epoch 88, batch 136 D Loss: 1.3839938640594482, G Loss: 0.7010762095451355\n",
      "Epoch 88, batch 137 D Loss: 1.3976185321807861, G Loss: 0.6915918588638306\n",
      "Epoch 88, batch 138 D Loss: 1.404801607131958, G Loss: 0.7000964879989624\n",
      "Epoch 88, batch 139 D Loss: 1.3990869522094727, G Loss: 0.6976776719093323\n",
      "Epoch 88, batch 140 D Loss: 1.3907393217086792, G Loss: 0.7116415500640869\n",
      "Epoch 88, batch 141 D Loss: 1.4107918739318848, G Loss: 0.6872575879096985\n",
      "Epoch 88, batch 142 D Loss: 1.401318073272705, G Loss: 0.6868301630020142\n",
      "Epoch 88, batch 143 D Loss: 1.400496244430542, G Loss: 0.6971549987792969\n",
      "Epoch 88, batch 144 D Loss: 1.3925440311431885, G Loss: 0.7121124267578125\n",
      "Epoch 88, batch 145 D Loss: 1.385571837425232, G Loss: 0.7065129280090332\n",
      "Epoch 88, batch 146 D Loss: 1.393233299255371, G Loss: 0.7039576172828674\n",
      "Epoch 88, batch 147 D Loss: 1.4127336740493774, G Loss: 0.6893846392631531\n",
      "Epoch 88, batch 148 D Loss: 1.3989876508712769, G Loss: 0.686989963054657\n",
      "Epoch 88, batch 149 D Loss: 1.4090311527252197, G Loss: 0.6892133951187134\n",
      "Epoch 88, batch 150 D Loss: 1.3915419578552246, G Loss: 0.7079228758811951\n",
      "Epoch 88, batch 151 D Loss: 1.3997552394866943, G Loss: 0.6924365758895874\n",
      "Epoch 88, batch 152 D Loss: 1.3972618579864502, G Loss: 0.7062217593193054\n",
      "Epoch 88, batch 153 D Loss: 1.3971940279006958, G Loss: 0.7117094397544861\n",
      "Epoch 88, batch 154 D Loss: 1.3961750268936157, G Loss: 0.7047162652015686\n",
      "Epoch 88, batch 155 D Loss: 1.4011595249176025, G Loss: 0.7036312818527222\n",
      "Epoch 88, batch 156 D Loss: 1.3920738697052002, G Loss: 0.700588047504425\n",
      "Epoch 88, batch 157 D Loss: 1.391261100769043, G Loss: 0.695513904094696\n",
      "Epoch 88, batch 158 D Loss: 1.381247878074646, G Loss: 0.7097758650779724\n",
      "Epoch 88, batch 159 D Loss: 1.4056711196899414, G Loss: 0.6935983300209045\n",
      "Epoch 88, batch 160 D Loss: 1.3866246938705444, G Loss: 0.6972280144691467\n",
      "Epoch 88, batch 161 D Loss: 1.4092833995819092, G Loss: 0.6917861104011536\n",
      "Epoch 88, batch 162 D Loss: 1.38733971118927, G Loss: 0.6978054642677307\n",
      "Epoch 88, batch 163 D Loss: 1.3843860626220703, G Loss: 0.7008731961250305\n",
      "Epoch 88, batch 164 D Loss: 1.3956279754638672, G Loss: 0.6967567205429077\n",
      "Epoch 88, batch 165 D Loss: 1.4075391292572021, G Loss: 0.691270649433136\n",
      "Epoch 88, batch 166 D Loss: 1.390961766242981, G Loss: 0.6964809894561768\n",
      "Epoch 88, batch 167 D Loss: 1.413482666015625, G Loss: 0.6937465071678162\n",
      "Epoch 88, batch 168 D Loss: 1.40152907371521, G Loss: 0.7066662311553955\n",
      "Epoch 88, batch 169 D Loss: 1.393214464187622, G Loss: 0.6991645097732544\n",
      "Epoch 88, batch 170 D Loss: 1.393791913986206, G Loss: 0.6954169273376465\n",
      "Epoch 88, batch 171 D Loss: 1.3955509662628174, G Loss: 0.6924154758453369\n",
      "Epoch 88, batch 172 D Loss: 1.4019200801849365, G Loss: 0.6984171271324158\n",
      "Epoch 88, batch 173 D Loss: 1.3955103158950806, G Loss: 0.6976602673530579\n",
      "Epoch 88, batch 174 D Loss: 1.3916254043579102, G Loss: 0.703996479511261\n",
      "Epoch 88, batch 175 D Loss: 1.3806918859481812, G Loss: 0.7086993455886841\n",
      "Epoch 88, batch 176 D Loss: 1.3863943815231323, G Loss: 0.6961624026298523\n",
      "Epoch 88, batch 177 D Loss: 1.3760302066802979, G Loss: 0.7073849439620972\n",
      "Epoch 88, batch 178 D Loss: 1.3972804546356201, G Loss: 0.7131892442703247\n",
      "Epoch 88, batch 179 D Loss: 1.3895618915557861, G Loss: 0.7003508806228638\n",
      "Epoch 88, batch 180 D Loss: 1.388444185256958, G Loss: 0.7042601704597473\n",
      "Epoch 88, batch 181 D Loss: 1.377020001411438, G Loss: 0.7207823395729065\n",
      "Epoch 88, batch 182 D Loss: 1.3900279998779297, G Loss: 0.6989331841468811\n",
      "Epoch 88, batch 183 D Loss: 1.3783233165740967, G Loss: 0.7208508849143982\n",
      "Epoch 88, batch 184 D Loss: 1.396644115447998, G Loss: 0.7050372958183289\n",
      "Epoch 88, batch 185 D Loss: 1.3808544874191284, G Loss: 0.7014340162277222\n",
      "Epoch 88, batch 186 D Loss: 1.409147024154663, G Loss: 0.6870056390762329\n",
      "Epoch 88, batch 187 D Loss: 1.4010307788848877, G Loss: 0.7009398937225342\n",
      "Epoch 88, batch 188 D Loss: 1.3911455869674683, G Loss: 0.7046270966529846\n",
      "Epoch 88, batch 189 D Loss: 1.393495798110962, G Loss: 0.7102786302566528\n",
      "Epoch 88, batch 190 D Loss: 1.4059021472930908, G Loss: 0.7089841961860657\n",
      "Epoch 88, batch 191 D Loss: 1.3879997730255127, G Loss: 0.706868052482605\n",
      "Epoch 88, batch 192 D Loss: 1.3564426898956299, G Loss: 0.7211987376213074\n",
      "Epoch 88, batch 193 D Loss: 1.3711867332458496, G Loss: 0.723635196685791\n",
      "Epoch 88, batch 194 D Loss: 1.4080703258514404, G Loss: 0.6998802423477173\n",
      "Epoch 88, batch 195 D Loss: 1.4008891582489014, G Loss: 0.6957418322563171\n",
      "Epoch 88, batch 196 D Loss: 1.3994863033294678, G Loss: 0.7003828287124634\n",
      "Epoch 88, batch 197 D Loss: 1.4225215911865234, G Loss: 0.7029359340667725\n",
      "Epoch 88, batch 198 D Loss: 1.3884742259979248, G Loss: 0.7161360383033752\n",
      "Epoch 88, batch 199 D Loss: 1.4035953283309937, G Loss: 0.7061684131622314\n",
      "Epoch 88, batch 200 D Loss: 1.3806266784667969, G Loss: 0.7147972583770752\n",
      "Epoch 89, batch 1 D Loss: 1.367684245109558, G Loss: 0.7025573253631592\n",
      "Epoch 89, batch 2 D Loss: 1.3856412172317505, G Loss: 0.6981967687606812\n",
      "Epoch 89, batch 3 D Loss: 1.380829095840454, G Loss: 0.6954558491706848\n",
      "Epoch 89, batch 4 D Loss: 1.4127802848815918, G Loss: 0.686820924282074\n",
      "Epoch 89, batch 5 D Loss: 1.4186584949493408, G Loss: 0.6892768740653992\n",
      "Epoch 89, batch 6 D Loss: 1.3807940483093262, G Loss: 0.6967201232910156\n",
      "Epoch 89, batch 7 D Loss: 1.3934047222137451, G Loss: 0.695412278175354\n",
      "Epoch 89, batch 8 D Loss: 1.414302110671997, G Loss: 0.6795015931129456\n",
      "Epoch 89, batch 9 D Loss: 1.395033359527588, G Loss: 0.6798588037490845\n",
      "Epoch 89, batch 10 D Loss: 1.4010870456695557, G Loss: 0.6871544122695923\n",
      "Epoch 89, batch 11 D Loss: 1.3817200660705566, G Loss: 0.6859988570213318\n",
      "Epoch 89, batch 12 D Loss: 1.3755743503570557, G Loss: 0.6891099810600281\n",
      "Epoch 89, batch 13 D Loss: 1.4069374799728394, G Loss: 0.6695382595062256\n",
      "Epoch 89, batch 14 D Loss: 1.3978042602539062, G Loss: 0.6824592351913452\n",
      "Epoch 89, batch 15 D Loss: 1.396564245223999, G Loss: 0.6850640177726746\n",
      "Epoch 89, batch 16 D Loss: 1.3856499195098877, G Loss: 0.6842010021209717\n",
      "Epoch 89, batch 17 D Loss: 1.3988155126571655, G Loss: 0.6829684376716614\n",
      "Epoch 89, batch 18 D Loss: 1.374603271484375, G Loss: 0.6805225610733032\n",
      "Epoch 89, batch 19 D Loss: 1.3848152160644531, G Loss: 0.690642237663269\n",
      "Epoch 89, batch 20 D Loss: 1.3972482681274414, G Loss: 0.6870269179344177\n",
      "Epoch 89, batch 21 D Loss: 1.392897605895996, G Loss: 0.6972213983535767\n",
      "Epoch 89, batch 22 D Loss: 1.4164438247680664, G Loss: 0.6929144859313965\n",
      "Epoch 89, batch 23 D Loss: 1.384238839149475, G Loss: 0.6938708424568176\n",
      "Epoch 89, batch 24 D Loss: 1.3854042291641235, G Loss: 0.6921289563179016\n",
      "Epoch 89, batch 25 D Loss: 1.3891565799713135, G Loss: 0.696388840675354\n",
      "Epoch 89, batch 26 D Loss: 1.3864935636520386, G Loss: 0.6934385895729065\n",
      "Epoch 89, batch 27 D Loss: 1.378585934638977, G Loss: 0.6974114179611206\n",
      "Epoch 89, batch 28 D Loss: 1.3880963325500488, G Loss: 0.6943538188934326\n",
      "Epoch 89, batch 29 D Loss: 1.3814833164215088, G Loss: 0.6914816498756409\n",
      "Epoch 89, batch 30 D Loss: 1.4018969535827637, G Loss: 0.6809534430503845\n",
      "Epoch 89, batch 31 D Loss: 1.3869531154632568, G Loss: 0.6913313269615173\n",
      "Epoch 89, batch 32 D Loss: 1.3996162414550781, G Loss: 0.6884191632270813\n",
      "Epoch 89, batch 33 D Loss: 1.391679048538208, G Loss: 0.6961986422538757\n",
      "Epoch 89, batch 34 D Loss: 1.3802392482757568, G Loss: 0.7102778553962708\n",
      "Epoch 89, batch 35 D Loss: 1.4097225666046143, G Loss: 0.6905288100242615\n",
      "Epoch 89, batch 36 D Loss: 1.379448652267456, G Loss: 0.6947432160377502\n",
      "Epoch 89, batch 37 D Loss: 1.3878287076950073, G Loss: 0.6965242624282837\n",
      "Epoch 89, batch 38 D Loss: 1.404327154159546, G Loss: 0.6928815245628357\n",
      "Epoch 89, batch 39 D Loss: 1.391991138458252, G Loss: 0.6962793469429016\n",
      "Epoch 89, batch 40 D Loss: 1.391728162765503, G Loss: 0.6899237632751465\n",
      "Epoch 89, batch 41 D Loss: 1.3888179063796997, G Loss: 0.698870837688446\n",
      "Epoch 89, batch 42 D Loss: 1.408048391342163, G Loss: 0.6934715509414673\n",
      "Epoch 89, batch 43 D Loss: 1.3822259902954102, G Loss: 0.7049410939216614\n",
      "Epoch 89, batch 44 D Loss: 1.4020931720733643, G Loss: 0.6833330392837524\n",
      "Epoch 89, batch 45 D Loss: 1.3938114643096924, G Loss: 0.6980573534965515\n",
      "Epoch 89, batch 46 D Loss: 1.3915116786956787, G Loss: 0.7031490802764893\n",
      "Epoch 89, batch 47 D Loss: 1.389375925064087, G Loss: 0.697852373123169\n",
      "Epoch 89, batch 48 D Loss: 1.3984179496765137, G Loss: 0.6918970346450806\n",
      "Epoch 89, batch 49 D Loss: 1.383493185043335, G Loss: 0.7010669708251953\n",
      "Epoch 89, batch 50 D Loss: 1.3871781826019287, G Loss: 0.7016069889068604\n",
      "Epoch 89, batch 51 D Loss: 1.3849666118621826, G Loss: 0.7072363495826721\n",
      "Epoch 89, batch 52 D Loss: 1.379694938659668, G Loss: 0.7097164392471313\n",
      "Epoch 89, batch 53 D Loss: 1.3908261060714722, G Loss: 0.7002662420272827\n",
      "Epoch 89, batch 54 D Loss: 1.3859959840774536, G Loss: 0.6989178657531738\n",
      "Epoch 89, batch 55 D Loss: 1.3806641101837158, G Loss: 0.7097589373588562\n",
      "Epoch 89, batch 56 D Loss: 1.3790732622146606, G Loss: 0.7144894599914551\n",
      "Epoch 89, batch 57 D Loss: 1.3827162981033325, G Loss: 0.7000985741615295\n",
      "Epoch 89, batch 58 D Loss: 1.3871387243270874, G Loss: 0.6994366645812988\n",
      "Epoch 89, batch 59 D Loss: 1.378443956375122, G Loss: 0.7083902955055237\n",
      "Epoch 89, batch 60 D Loss: 1.3987441062927246, G Loss: 0.6990735530853271\n",
      "Epoch 89, batch 61 D Loss: 1.3754764795303345, G Loss: 0.6963357329368591\n",
      "Epoch 89, batch 62 D Loss: 1.3899140357971191, G Loss: 0.6986187696456909\n",
      "Epoch 89, batch 63 D Loss: 1.4036784172058105, G Loss: 0.6915380358695984\n",
      "Epoch 89, batch 64 D Loss: 1.3832037448883057, G Loss: 0.6988416910171509\n",
      "Epoch 89, batch 65 D Loss: 1.3757619857788086, G Loss: 0.7114724516868591\n",
      "Epoch 89, batch 66 D Loss: 1.3705718517303467, G Loss: 0.7078707814216614\n",
      "Epoch 89, batch 67 D Loss: 1.4186904430389404, G Loss: 0.6899492740631104\n",
      "Epoch 89, batch 68 D Loss: 1.392528772354126, G Loss: 0.7043601870536804\n",
      "Epoch 89, batch 69 D Loss: 1.389773964881897, G Loss: 0.7154920697212219\n",
      "Epoch 89, batch 70 D Loss: 1.391711711883545, G Loss: 0.6907100677490234\n",
      "Epoch 89, batch 71 D Loss: 1.3765100240707397, G Loss: 0.6927533149719238\n",
      "Epoch 89, batch 72 D Loss: 1.4026633501052856, G Loss: 0.6884214282035828\n",
      "Epoch 89, batch 73 D Loss: 1.3867422342300415, G Loss: 0.6963069438934326\n",
      "Epoch 89, batch 74 D Loss: 1.3850810527801514, G Loss: 0.6948091983795166\n",
      "Epoch 89, batch 75 D Loss: 1.4052009582519531, G Loss: 0.6829084157943726\n",
      "Epoch 89, batch 76 D Loss: 1.4036892652511597, G Loss: 0.6908978819847107\n",
      "Epoch 89, batch 77 D Loss: 1.3652844429016113, G Loss: 0.7026419639587402\n",
      "Epoch 89, batch 78 D Loss: 1.4024438858032227, G Loss: 0.6914655566215515\n",
      "Epoch 89, batch 79 D Loss: 1.3937832117080688, G Loss: 0.6916245222091675\n",
      "Epoch 89, batch 80 D Loss: 1.394831895828247, G Loss: 0.6903637647628784\n",
      "Epoch 89, batch 81 D Loss: 1.380141258239746, G Loss: 0.6988236904144287\n",
      "Epoch 89, batch 82 D Loss: 1.3814812898635864, G Loss: 0.7033339738845825\n",
      "Epoch 89, batch 83 D Loss: 1.3834013938903809, G Loss: 0.692638099193573\n",
      "Epoch 89, batch 84 D Loss: 1.4027857780456543, G Loss: 0.6894714832305908\n",
      "Epoch 89, batch 85 D Loss: 1.381805658340454, G Loss: 0.6885714530944824\n",
      "Epoch 89, batch 86 D Loss: 1.3886888027191162, G Loss: 0.6907541155815125\n",
      "Epoch 89, batch 87 D Loss: 1.4025262594223022, G Loss: 0.6831278800964355\n",
      "Epoch 89, batch 88 D Loss: 1.3668062686920166, G Loss: 0.6953128576278687\n",
      "Epoch 89, batch 89 D Loss: 1.3861818313598633, G Loss: 0.6956908702850342\n",
      "Epoch 89, batch 90 D Loss: 1.4012629985809326, G Loss: 0.683871865272522\n",
      "Epoch 89, batch 91 D Loss: 1.3948445320129395, G Loss: 0.686133623123169\n",
      "Epoch 89, batch 92 D Loss: 1.3895150423049927, G Loss: 0.6791172623634338\n",
      "Epoch 89, batch 93 D Loss: 1.3974254131317139, G Loss: 0.682024359703064\n",
      "Epoch 89, batch 94 D Loss: 1.396489143371582, G Loss: 0.6909970045089722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, batch 95 D Loss: 1.406140923500061, G Loss: 0.6724403500556946\n",
      "Epoch 89, batch 96 D Loss: 1.394956350326538, G Loss: 0.6842555999755859\n",
      "Epoch 89, batch 97 D Loss: 1.3844552040100098, G Loss: 0.6838893294334412\n",
      "Epoch 89, batch 98 D Loss: 1.376955509185791, G Loss: 0.6921998858451843\n",
      "Epoch 89, batch 99 D Loss: 1.414613962173462, G Loss: 0.6731967329978943\n",
      "Epoch 89, batch 100 D Loss: 1.4087865352630615, G Loss: 0.6792178153991699\n",
      "Epoch 89, batch 101 D Loss: 1.3821884393692017, G Loss: 0.6907427906990051\n",
      "Epoch 89, batch 102 D Loss: 1.387425184249878, G Loss: 0.6923103928565979\n",
      "Epoch 89, batch 103 D Loss: 1.4024016857147217, G Loss: 0.6787129044532776\n",
      "Epoch 89, batch 104 D Loss: 1.384421467781067, G Loss: 0.6883634328842163\n",
      "Epoch 89, batch 105 D Loss: 1.392657995223999, G Loss: 0.6862457394599915\n",
      "Epoch 89, batch 106 D Loss: 1.388603687286377, G Loss: 0.6913057565689087\n",
      "Epoch 89, batch 107 D Loss: 1.4051392078399658, G Loss: 0.6776098608970642\n",
      "Epoch 89, batch 108 D Loss: 1.3831920623779297, G Loss: 0.6906365156173706\n",
      "Epoch 89, batch 109 D Loss: 1.4009029865264893, G Loss: 0.6771370768547058\n",
      "Epoch 89, batch 110 D Loss: 1.3888381719589233, G Loss: 0.6809751391410828\n",
      "Epoch 89, batch 111 D Loss: 1.4053659439086914, G Loss: 0.6810138821601868\n",
      "Epoch 89, batch 112 D Loss: 1.388005256652832, G Loss: 0.6860209703445435\n",
      "Epoch 89, batch 113 D Loss: 1.3819618225097656, G Loss: 0.6918665170669556\n",
      "Epoch 89, batch 114 D Loss: 1.3907849788665771, G Loss: 0.6877234578132629\n",
      "Epoch 89, batch 115 D Loss: 1.3975672721862793, G Loss: 0.6846888065338135\n",
      "Epoch 89, batch 116 D Loss: 1.3966418504714966, G Loss: 0.6865187883377075\n",
      "Epoch 89, batch 117 D Loss: 1.395879864692688, G Loss: 0.6886051893234253\n",
      "Epoch 89, batch 118 D Loss: 1.3908627033233643, G Loss: 0.6916278600692749\n",
      "Epoch 89, batch 119 D Loss: 1.3784911632537842, G Loss: 0.6931150555610657\n",
      "Epoch 89, batch 120 D Loss: 1.3787055015563965, G Loss: 0.6893343925476074\n",
      "Epoch 89, batch 121 D Loss: 1.3868062496185303, G Loss: 0.6880801320075989\n",
      "Epoch 89, batch 122 D Loss: 1.401780128479004, G Loss: 0.6832788586616516\n",
      "Epoch 89, batch 123 D Loss: 1.391912579536438, G Loss: 0.6877255439758301\n",
      "Epoch 89, batch 124 D Loss: 1.3822965621948242, G Loss: 0.6917129755020142\n",
      "Epoch 89, batch 125 D Loss: 1.3901350498199463, G Loss: 0.6817387938499451\n",
      "Epoch 89, batch 126 D Loss: 1.390634536743164, G Loss: 0.6812297105789185\n",
      "Epoch 89, batch 127 D Loss: 1.3945465087890625, G Loss: 0.6833158135414124\n",
      "Epoch 89, batch 128 D Loss: 1.3849756717681885, G Loss: 0.687135636806488\n",
      "Epoch 89, batch 129 D Loss: 1.4013826847076416, G Loss: 0.6837342977523804\n",
      "Epoch 89, batch 130 D Loss: 1.381696343421936, G Loss: 0.6885070204734802\n",
      "Epoch 89, batch 131 D Loss: 1.3941609859466553, G Loss: 0.6793899536132812\n",
      "Epoch 89, batch 132 D Loss: 1.389557123184204, G Loss: 0.6836878061294556\n",
      "Epoch 89, batch 133 D Loss: 1.3807812929153442, G Loss: 0.6883021593093872\n",
      "Epoch 89, batch 134 D Loss: 1.4027085304260254, G Loss: 0.6848605871200562\n",
      "Epoch 89, batch 135 D Loss: 1.3829184770584106, G Loss: 0.6921863555908203\n",
      "Epoch 89, batch 136 D Loss: 1.374297857284546, G Loss: 0.6892980337142944\n",
      "Epoch 89, batch 137 D Loss: 1.3964241743087769, G Loss: 0.6836476922035217\n",
      "Epoch 89, batch 138 D Loss: 1.4029145240783691, G Loss: 0.6848245859146118\n",
      "Epoch 89, batch 139 D Loss: 1.3846657276153564, G Loss: 0.691009521484375\n",
      "Epoch 89, batch 140 D Loss: 1.3913989067077637, G Loss: 0.6855236291885376\n",
      "Epoch 89, batch 141 D Loss: 1.401795744895935, G Loss: 0.6873266100883484\n",
      "Epoch 89, batch 142 D Loss: 1.4061627388000488, G Loss: 0.6727069020271301\n",
      "Epoch 89, batch 143 D Loss: 1.3811531066894531, G Loss: 0.6890593767166138\n",
      "Epoch 89, batch 144 D Loss: 1.399585247039795, G Loss: 0.6829807758331299\n",
      "Epoch 89, batch 145 D Loss: 1.383871078491211, G Loss: 0.688758909702301\n",
      "Epoch 89, batch 146 D Loss: 1.3732906579971313, G Loss: 0.6853653192520142\n",
      "Epoch 89, batch 147 D Loss: 1.3907456398010254, G Loss: 0.682414710521698\n",
      "Epoch 89, batch 148 D Loss: 1.3881218433380127, G Loss: 0.6914193034172058\n",
      "Epoch 89, batch 149 D Loss: 1.4000649452209473, G Loss: 0.6784968376159668\n",
      "Epoch 89, batch 150 D Loss: 1.3880891799926758, G Loss: 0.6850123405456543\n",
      "Epoch 89, batch 151 D Loss: 1.3945393562316895, G Loss: 0.6794801354408264\n",
      "Epoch 89, batch 152 D Loss: 1.3919637203216553, G Loss: 0.6891446709632874\n",
      "Epoch 89, batch 153 D Loss: 1.3899511098861694, G Loss: 0.6867514848709106\n",
      "Epoch 89, batch 154 D Loss: 1.386825680732727, G Loss: 0.691235363483429\n",
      "Epoch 89, batch 155 D Loss: 1.3773186206817627, G Loss: 0.6949614882469177\n",
      "Epoch 89, batch 156 D Loss: 1.397482991218567, G Loss: 0.6826834678649902\n",
      "Epoch 89, batch 157 D Loss: 1.3988595008850098, G Loss: 0.6837447881698608\n",
      "Epoch 89, batch 158 D Loss: 1.3916785717010498, G Loss: 0.6943536400794983\n",
      "Epoch 89, batch 159 D Loss: 1.3886011838912964, G Loss: 0.6977888345718384\n",
      "Epoch 89, batch 160 D Loss: 1.36747145652771, G Loss: 0.7066113352775574\n",
      "Epoch 89, batch 161 D Loss: 1.3992810249328613, G Loss: 0.6952854990959167\n",
      "Epoch 89, batch 162 D Loss: 1.3906114101409912, G Loss: 0.6905120015144348\n",
      "Epoch 89, batch 163 D Loss: 1.3948707580566406, G Loss: 0.6847131252288818\n",
      "Epoch 89, batch 164 D Loss: 1.3931528329849243, G Loss: 0.6885954141616821\n",
      "Epoch 89, batch 165 D Loss: 1.3685786724090576, G Loss: 0.7102383971214294\n",
      "Epoch 89, batch 166 D Loss: 1.3917932510375977, G Loss: 0.6939279437065125\n",
      "Epoch 89, batch 167 D Loss: 1.3893308639526367, G Loss: 0.6965075731277466\n",
      "Epoch 89, batch 168 D Loss: 1.3801746368408203, G Loss: 0.6944229006767273\n",
      "Epoch 89, batch 169 D Loss: 1.3966705799102783, G Loss: 0.6986207365989685\n",
      "Epoch 89, batch 170 D Loss: 1.3876855373382568, G Loss: 0.7052770256996155\n",
      "Epoch 89, batch 171 D Loss: 1.3912934064865112, G Loss: 0.704392671585083\n",
      "Epoch 89, batch 172 D Loss: 1.3918004035949707, G Loss: 0.6995776891708374\n",
      "Epoch 89, batch 173 D Loss: 1.377913236618042, G Loss: 0.7077788710594177\n",
      "Epoch 89, batch 174 D Loss: 1.395772099494934, G Loss: 0.7034313678741455\n",
      "Epoch 89, batch 175 D Loss: 1.3933184146881104, G Loss: 0.6989181637763977\n",
      "Epoch 89, batch 176 D Loss: 1.3950315713882446, G Loss: 0.6943542957305908\n",
      "Epoch 89, batch 177 D Loss: 1.3901951313018799, G Loss: 0.702765703201294\n",
      "Epoch 89, batch 178 D Loss: 1.3889873027801514, G Loss: 0.7043775320053101\n",
      "Epoch 89, batch 179 D Loss: 1.3901357650756836, G Loss: 0.7067490220069885\n",
      "Epoch 89, batch 180 D Loss: 1.394540548324585, G Loss: 0.705894947052002\n",
      "Epoch 89, batch 181 D Loss: 1.3841023445129395, G Loss: 0.7050354480743408\n",
      "Epoch 89, batch 182 D Loss: 1.3832435607910156, G Loss: 0.7163400053977966\n",
      "Epoch 89, batch 183 D Loss: 1.387345314025879, G Loss: 0.7137607336044312\n",
      "Epoch 89, batch 184 D Loss: 1.4000389575958252, G Loss: 0.7070953249931335\n",
      "Epoch 89, batch 185 D Loss: 1.3845701217651367, G Loss: 0.706668496131897\n",
      "Epoch 89, batch 186 D Loss: 1.3965282440185547, G Loss: 0.7040941119194031\n",
      "Epoch 89, batch 187 D Loss: 1.3821909427642822, G Loss: 0.7104623913764954\n",
      "Epoch 89, batch 188 D Loss: 1.3894236087799072, G Loss: 0.7135814428329468\n",
      "Epoch 89, batch 189 D Loss: 1.3807659149169922, G Loss: 0.7191315293312073\n",
      "Epoch 89, batch 190 D Loss: 1.3935911655426025, G Loss: 0.703380286693573\n",
      "Epoch 89, batch 191 D Loss: 1.3894288539886475, G Loss: 0.703362226486206\n",
      "Epoch 89, batch 192 D Loss: 1.3984744548797607, G Loss: 0.7033911347389221\n",
      "Epoch 89, batch 193 D Loss: 1.391542673110962, G Loss: 0.7039625644683838\n",
      "Epoch 89, batch 194 D Loss: 1.394136667251587, G Loss: 0.7036855816841125\n",
      "Epoch 89, batch 195 D Loss: 1.3900394439697266, G Loss: 0.7092406749725342\n",
      "Epoch 89, batch 196 D Loss: 1.3846008777618408, G Loss: 0.7065262794494629\n",
      "Epoch 89, batch 197 D Loss: 1.393669843673706, G Loss: 0.7081239223480225\n",
      "Epoch 89, batch 198 D Loss: 1.389617919921875, G Loss: 0.7070575952529907\n",
      "Epoch 89, batch 199 D Loss: 1.380260944366455, G Loss: 0.7015009522438049\n",
      "Epoch 89, batch 200 D Loss: 1.3929815292358398, G Loss: 0.7007070779800415\n",
      "Epoch 90, batch 1 D Loss: 1.3932034969329834, G Loss: 0.6952811479568481\n",
      "Epoch 90, batch 2 D Loss: 1.3822054862976074, G Loss: 0.707481861114502\n",
      "Epoch 90, batch 3 D Loss: 1.3778624534606934, G Loss: 0.7023314833641052\n",
      "Epoch 90, batch 4 D Loss: 1.394267201423645, G Loss: 0.6981585025787354\n",
      "Epoch 90, batch 5 D Loss: 1.3915627002716064, G Loss: 0.7047926187515259\n",
      "Epoch 90, batch 6 D Loss: 1.3825780153274536, G Loss: 0.6972399353981018\n",
      "Epoch 90, batch 7 D Loss: 1.3880302906036377, G Loss: 0.7005009651184082\n",
      "Epoch 90, batch 8 D Loss: 1.3896894454956055, G Loss: 0.6943415999412537\n",
      "Epoch 90, batch 9 D Loss: 1.388840913772583, G Loss: 0.7000375390052795\n",
      "Epoch 90, batch 10 D Loss: 1.3993549346923828, G Loss: 0.6870399713516235\n",
      "Epoch 90, batch 11 D Loss: 1.3917490243911743, G Loss: 0.698383629322052\n",
      "Epoch 90, batch 12 D Loss: 1.3834657669067383, G Loss: 0.7018795013427734\n",
      "Epoch 90, batch 13 D Loss: 1.3842962980270386, G Loss: 0.6988663673400879\n",
      "Epoch 90, batch 14 D Loss: 1.3888745307922363, G Loss: 0.6920327544212341\n",
      "Epoch 90, batch 15 D Loss: 1.3926177024841309, G Loss: 0.6964234113693237\n",
      "Epoch 90, batch 16 D Loss: 1.3760974407196045, G Loss: 0.6986737251281738\n",
      "Epoch 90, batch 17 D Loss: 1.3987703323364258, G Loss: 0.6938133239746094\n",
      "Epoch 90, batch 18 D Loss: 1.3876681327819824, G Loss: 0.7081745266914368\n",
      "Epoch 90, batch 19 D Loss: 1.3783766031265259, G Loss: 0.7054800391197205\n",
      "Epoch 90, batch 20 D Loss: 1.3789989948272705, G Loss: 0.7128424644470215\n",
      "Epoch 90, batch 21 D Loss: 1.3829739093780518, G Loss: 0.6962217092514038\n",
      "Epoch 90, batch 22 D Loss: 1.3915135860443115, G Loss: 0.7052544355392456\n",
      "Epoch 90, batch 23 D Loss: 1.401929259300232, G Loss: 0.7008753418922424\n",
      "Epoch 90, batch 24 D Loss: 1.3851826190948486, G Loss: 0.7007564306259155\n",
      "Epoch 90, batch 25 D Loss: 1.3834301233291626, G Loss: 0.7187564969062805\n",
      "Epoch 90, batch 26 D Loss: 1.383843183517456, G Loss: 0.7182319760322571\n",
      "Epoch 90, batch 27 D Loss: 1.3859667778015137, G Loss: 0.7049722075462341\n",
      "Epoch 90, batch 28 D Loss: 1.3780056238174438, G Loss: 0.7177929878234863\n",
      "Epoch 90, batch 29 D Loss: 1.3902199268341064, G Loss: 0.7071383595466614\n",
      "Epoch 90, batch 30 D Loss: 1.3987610340118408, G Loss: 0.7067121863365173\n",
      "Epoch 90, batch 31 D Loss: 1.3976974487304688, G Loss: 0.7070615887641907\n",
      "Epoch 90, batch 32 D Loss: 1.3866534233093262, G Loss: 0.7230108380317688\n",
      "Epoch 90, batch 33 D Loss: 1.3868408203125, G Loss: 0.7114986181259155\n",
      "Epoch 90, batch 34 D Loss: 1.4009894132614136, G Loss: 0.7146657705307007\n",
      "Epoch 90, batch 35 D Loss: 1.396803617477417, G Loss: 0.6995287537574768\n",
      "Epoch 90, batch 36 D Loss: 1.3959126472473145, G Loss: 0.7063282132148743\n",
      "Epoch 90, batch 37 D Loss: 1.3913238048553467, G Loss: 0.7126634120941162\n",
      "Epoch 90, batch 38 D Loss: 1.3859902620315552, G Loss: 0.709378719329834\n",
      "Epoch 90, batch 39 D Loss: 1.39542818069458, G Loss: 0.705596923828125\n",
      "Epoch 90, batch 40 D Loss: 1.4017692804336548, G Loss: 0.716655969619751\n",
      "Epoch 90, batch 41 D Loss: 1.3963444232940674, G Loss: 0.7123528122901917\n",
      "Epoch 90, batch 42 D Loss: 1.3791812658309937, G Loss: 0.7213388681411743\n",
      "Epoch 90, batch 43 D Loss: 1.391670823097229, G Loss: 0.7106655240058899\n",
      "Epoch 90, batch 44 D Loss: 1.386305332183838, G Loss: 0.7287346720695496\n",
      "Epoch 90, batch 45 D Loss: 1.3935725688934326, G Loss: 0.7117069363594055\n",
      "Epoch 90, batch 46 D Loss: 1.3874492645263672, G Loss: 0.7090662121772766\n",
      "Epoch 90, batch 47 D Loss: 1.3838446140289307, G Loss: 0.7113189101219177\n",
      "Epoch 90, batch 48 D Loss: 1.3891432285308838, G Loss: 0.701316773891449\n",
      "Epoch 90, batch 49 D Loss: 1.3834865093231201, G Loss: 0.7097923159599304\n",
      "Epoch 90, batch 50 D Loss: 1.3790504932403564, G Loss: 0.7070416808128357\n",
      "Epoch 90, batch 51 D Loss: 1.397933006286621, G Loss: 0.7067599296569824\n",
      "Epoch 90, batch 52 D Loss: 1.3897523880004883, G Loss: 0.7064333558082581\n",
      "Epoch 90, batch 53 D Loss: 1.3737480640411377, G Loss: 0.7131317257881165\n",
      "Epoch 90, batch 54 D Loss: 1.3875842094421387, G Loss: 0.7078258395195007\n",
      "Epoch 90, batch 55 D Loss: 1.3874688148498535, G Loss: 0.7030159831047058\n",
      "Epoch 90, batch 56 D Loss: 1.3812272548675537, G Loss: 0.709027886390686\n",
      "Epoch 90, batch 57 D Loss: 1.3807986974716187, G Loss: 0.7147075533866882\n",
      "Epoch 90, batch 58 D Loss: 1.3879764080047607, G Loss: 0.7015263438224792\n",
      "Epoch 90, batch 59 D Loss: 1.3889985084533691, G Loss: 0.7137300372123718\n",
      "Epoch 90, batch 60 D Loss: 1.3854488134384155, G Loss: 0.7117581963539124\n",
      "Epoch 90, batch 61 D Loss: 1.3726799488067627, G Loss: 0.7063201665878296\n",
      "Epoch 90, batch 62 D Loss: 1.3771131038665771, G Loss: 0.7103271484375\n",
      "Epoch 90, batch 63 D Loss: 1.3913631439208984, G Loss: 0.7053321003913879\n",
      "Epoch 90, batch 64 D Loss: 1.3856881856918335, G Loss: 0.7061802744865417\n",
      "Epoch 90, batch 65 D Loss: 1.393715500831604, G Loss: 0.7032843232154846\n",
      "Epoch 90, batch 66 D Loss: 1.379507303237915, G Loss: 0.6964929103851318\n",
      "Epoch 90, batch 67 D Loss: 1.3891743421554565, G Loss: 0.7012490034103394\n",
      "Epoch 90, batch 68 D Loss: 1.3806848526000977, G Loss: 0.7122228145599365\n",
      "Epoch 90, batch 69 D Loss: 1.3665109872817993, G Loss: 0.7116506099700928\n",
      "Epoch 90, batch 70 D Loss: 1.3939404487609863, G Loss: 0.7044429779052734\n",
      "Epoch 90, batch 71 D Loss: 1.3892133235931396, G Loss: 0.7009157538414001\n",
      "Epoch 90, batch 72 D Loss: 1.3869673013687134, G Loss: 0.7105644345283508\n",
      "Epoch 90, batch 73 D Loss: 1.399427890777588, G Loss: 0.6935014128684998\n",
      "Epoch 90, batch 74 D Loss: 1.3896582126617432, G Loss: 0.7018784880638123\n",
      "Epoch 90, batch 75 D Loss: 1.391904592514038, G Loss: 0.7007635235786438\n",
      "Epoch 90, batch 76 D Loss: 1.3923897743225098, G Loss: 0.699948787689209\n",
      "Epoch 90, batch 77 D Loss: 1.3871359825134277, G Loss: 0.7105622887611389\n",
      "Epoch 90, batch 78 D Loss: 1.373185157775879, G Loss: 0.7087327837944031\n",
      "Epoch 90, batch 79 D Loss: 1.405057430267334, G Loss: 0.6859472393989563\n",
      "Epoch 90, batch 80 D Loss: 1.4104108810424805, G Loss: 0.7011855244636536\n",
      "Epoch 90, batch 81 D Loss: 1.377495527267456, G Loss: 0.7138209342956543\n",
      "Epoch 90, batch 82 D Loss: 1.3963873386383057, G Loss: 0.6900043487548828\n",
      "Epoch 90, batch 83 D Loss: 1.3790565729141235, G Loss: 0.7063234448432922\n",
      "Epoch 90, batch 84 D Loss: 1.39280366897583, G Loss: 0.7061630487442017\n",
      "Epoch 90, batch 85 D Loss: 1.3802294731140137, G Loss: 0.7135984897613525\n",
      "Epoch 90, batch 86 D Loss: 1.3914152383804321, G Loss: 0.7063893675804138\n",
      "Epoch 90, batch 87 D Loss: 1.370619535446167, G Loss: 0.7087015509605408\n",
      "Epoch 90, batch 88 D Loss: 1.3758771419525146, G Loss: 0.7029092311859131\n",
      "Epoch 90, batch 89 D Loss: 1.3816957473754883, G Loss: 0.708361029624939\n",
      "Epoch 90, batch 90 D Loss: 1.3896777629852295, G Loss: 0.7009451985359192\n",
      "Epoch 90, batch 91 D Loss: 1.35734224319458, G Loss: 0.7071963548660278\n",
      "Epoch 90, batch 92 D Loss: 1.3869709968566895, G Loss: 0.6977735757827759\n",
      "Epoch 90, batch 93 D Loss: 1.391075611114502, G Loss: 0.697638988494873\n",
      "Epoch 90, batch 94 D Loss: 1.3784692287445068, G Loss: 0.7021410465240479\n",
      "Epoch 90, batch 95 D Loss: 1.3979445695877075, G Loss: 0.6865729689598083\n",
      "Epoch 90, batch 96 D Loss: 1.3805022239685059, G Loss: 0.6929647922515869\n",
      "Epoch 90, batch 97 D Loss: 1.3841795921325684, G Loss: 0.7002477049827576\n",
      "Epoch 90, batch 98 D Loss: 1.3958306312561035, G Loss: 0.6896947622299194\n",
      "Epoch 90, batch 99 D Loss: 1.3812650442123413, G Loss: 0.6995955109596252\n",
      "Epoch 90, batch 100 D Loss: 1.4066874980926514, G Loss: 0.6816643476486206\n",
      "Epoch 90, batch 101 D Loss: 1.3770179748535156, G Loss: 0.6973361372947693\n",
      "Epoch 90, batch 102 D Loss: 1.3987996578216553, G Loss: 0.689856767654419\n",
      "Epoch 90, batch 103 D Loss: 1.4105215072631836, G Loss: 0.6694240570068359\n",
      "Epoch 90, batch 104 D Loss: 1.3757357597351074, G Loss: 0.6919348835945129\n",
      "Epoch 90, batch 105 D Loss: 1.3973984718322754, G Loss: 0.6802254319190979\n",
      "Epoch 90, batch 106 D Loss: 1.3828204870224, G Loss: 0.6946437954902649\n",
      "Epoch 90, batch 107 D Loss: 1.3937419652938843, G Loss: 0.6922854781150818\n",
      "Epoch 90, batch 108 D Loss: 1.3973902463912964, G Loss: 0.685931384563446\n",
      "Epoch 90, batch 109 D Loss: 1.3771705627441406, G Loss: 0.6883716583251953\n",
      "Epoch 90, batch 110 D Loss: 1.3914380073547363, G Loss: 0.6792210936546326\n",
      "Epoch 90, batch 111 D Loss: 1.3760197162628174, G Loss: 0.6829909682273865\n",
      "Epoch 90, batch 112 D Loss: 1.38417387008667, G Loss: 0.7009339332580566\n",
      "Epoch 90, batch 113 D Loss: 1.3702905178070068, G Loss: 0.6962809562683105\n",
      "Epoch 90, batch 114 D Loss: 1.3710157871246338, G Loss: 0.6928021907806396\n",
      "Epoch 90, batch 115 D Loss: 1.3969311714172363, G Loss: 0.6749401092529297\n",
      "Epoch 90, batch 116 D Loss: 1.3903909921646118, G Loss: 0.6972470283508301\n",
      "Epoch 90, batch 117 D Loss: 1.3861669301986694, G Loss: 0.6893262267112732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, batch 118 D Loss: 1.3972392082214355, G Loss: 0.6890940070152283\n",
      "Epoch 90, batch 119 D Loss: 1.3835020065307617, G Loss: 0.6862025260925293\n",
      "Epoch 90, batch 120 D Loss: 1.3947968482971191, G Loss: 0.6887001991271973\n",
      "Epoch 90, batch 121 D Loss: 1.3846848011016846, G Loss: 0.6934468150138855\n",
      "Epoch 90, batch 122 D Loss: 1.3966530561447144, G Loss: 0.691977858543396\n",
      "Epoch 90, batch 123 D Loss: 1.4004647731781006, G Loss: 0.6816493272781372\n",
      "Epoch 90, batch 124 D Loss: 1.4084558486938477, G Loss: 0.6793575882911682\n",
      "Epoch 90, batch 125 D Loss: 1.382192850112915, G Loss: 0.6953151226043701\n",
      "Epoch 90, batch 126 D Loss: 1.3770776987075806, G Loss: 0.7079485058784485\n",
      "Epoch 90, batch 127 D Loss: 1.3958969116210938, G Loss: 0.7014899253845215\n",
      "Epoch 90, batch 128 D Loss: 1.3674547672271729, G Loss: 0.7030403017997742\n",
      "Epoch 90, batch 129 D Loss: 1.3734188079833984, G Loss: 0.7117156982421875\n",
      "Epoch 90, batch 130 D Loss: 1.3845763206481934, G Loss: 0.6923800706863403\n",
      "Epoch 90, batch 131 D Loss: 1.3866724967956543, G Loss: 0.7071161866188049\n",
      "Epoch 90, batch 132 D Loss: 1.3806452751159668, G Loss: 0.692748486995697\n",
      "Epoch 90, batch 133 D Loss: 1.3712749481201172, G Loss: 0.7037311792373657\n",
      "Epoch 90, batch 134 D Loss: 1.4018547534942627, G Loss: 0.6921944618225098\n",
      "Epoch 90, batch 135 D Loss: 1.3926286697387695, G Loss: 0.6899372935295105\n",
      "Epoch 90, batch 136 D Loss: 1.3749645948410034, G Loss: 0.709627628326416\n",
      "Epoch 90, batch 137 D Loss: 1.3771865367889404, G Loss: 0.7081152200698853\n",
      "Epoch 90, batch 138 D Loss: 1.38541841506958, G Loss: 0.6975391507148743\n",
      "Epoch 90, batch 139 D Loss: 1.3924353122711182, G Loss: 0.6893340945243835\n",
      "Epoch 90, batch 140 D Loss: 1.3829424381256104, G Loss: 0.7043808102607727\n",
      "Epoch 90, batch 141 D Loss: 1.3729524612426758, G Loss: 0.7093772292137146\n",
      "Epoch 90, batch 142 D Loss: 1.3998143672943115, G Loss: 0.6925867199897766\n",
      "Epoch 90, batch 143 D Loss: 1.394296407699585, G Loss: 0.6995555758476257\n",
      "Epoch 90, batch 144 D Loss: 1.3869839906692505, G Loss: 0.7048132419586182\n",
      "Epoch 90, batch 145 D Loss: 1.3820796012878418, G Loss: 0.70662522315979\n",
      "Epoch 90, batch 146 D Loss: 1.3739161491394043, G Loss: 0.7054169178009033\n",
      "Epoch 90, batch 147 D Loss: 1.3975770473480225, G Loss: 0.7037401795387268\n",
      "Epoch 90, batch 148 D Loss: 1.3881458044052124, G Loss: 0.7045021057128906\n",
      "Epoch 90, batch 149 D Loss: 1.3854892253875732, G Loss: 0.701672375202179\n",
      "Epoch 90, batch 150 D Loss: 1.418108344078064, G Loss: 0.6857397556304932\n",
      "Epoch 90, batch 151 D Loss: 1.3867952823638916, G Loss: 0.7030485272407532\n",
      "Epoch 90, batch 152 D Loss: 1.3748204708099365, G Loss: 0.708141565322876\n",
      "Epoch 90, batch 153 D Loss: 1.3772141933441162, G Loss: 0.7002865076065063\n",
      "Epoch 90, batch 154 D Loss: 1.3951880931854248, G Loss: 0.6987525224685669\n",
      "Epoch 90, batch 155 D Loss: 1.3936773538589478, G Loss: 0.695671021938324\n",
      "Epoch 90, batch 156 D Loss: 1.3951804637908936, G Loss: 0.6889639496803284\n",
      "Epoch 90, batch 157 D Loss: 1.3841547966003418, G Loss: 0.694327712059021\n",
      "Epoch 90, batch 158 D Loss: 1.3762259483337402, G Loss: 0.7013143301010132\n",
      "Epoch 90, batch 159 D Loss: 1.395677089691162, G Loss: 0.6914020776748657\n",
      "Epoch 90, batch 160 D Loss: 1.3771542310714722, G Loss: 0.6947833895683289\n",
      "Epoch 90, batch 161 D Loss: 1.3865599632263184, G Loss: 0.6919217705726624\n",
      "Epoch 90, batch 162 D Loss: 1.3811135292053223, G Loss: 0.6909552812576294\n",
      "Epoch 90, batch 163 D Loss: 1.4163551330566406, G Loss: 0.6697123050689697\n",
      "Epoch 90, batch 164 D Loss: 1.3844332695007324, G Loss: 0.6916092038154602\n",
      "Epoch 90, batch 165 D Loss: 1.3989391326904297, G Loss: 0.6841253042221069\n",
      "Epoch 90, batch 166 D Loss: 1.3936519622802734, G Loss: 0.6914623379707336\n",
      "Epoch 90, batch 167 D Loss: 1.398273229598999, G Loss: 0.6826809644699097\n",
      "Epoch 90, batch 168 D Loss: 1.3894367218017578, G Loss: 0.6890302300453186\n",
      "Epoch 90, batch 169 D Loss: 1.3836355209350586, G Loss: 0.690782904624939\n",
      "Epoch 90, batch 170 D Loss: 1.3919365406036377, G Loss: 0.6838231086730957\n",
      "Epoch 90, batch 171 D Loss: 1.4042785167694092, G Loss: 0.6956544518470764\n",
      "Epoch 90, batch 172 D Loss: 1.3894273042678833, G Loss: 0.695268988609314\n",
      "Epoch 90, batch 173 D Loss: 1.400038480758667, G Loss: 0.6867010593414307\n",
      "Epoch 90, batch 174 D Loss: 1.3797252178192139, G Loss: 0.6925227642059326\n",
      "Epoch 90, batch 175 D Loss: 1.364793300628662, G Loss: 0.7095683217048645\n",
      "Epoch 90, batch 176 D Loss: 1.3928872346878052, G Loss: 0.6911405920982361\n",
      "Epoch 90, batch 177 D Loss: 1.3868255615234375, G Loss: 0.6918583512306213\n",
      "Epoch 90, batch 178 D Loss: 1.3954370021820068, G Loss: 0.6885489821434021\n",
      "Epoch 90, batch 179 D Loss: 1.381191611289978, G Loss: 0.7016840577125549\n",
      "Epoch 90, batch 180 D Loss: 1.380394697189331, G Loss: 0.6945287585258484\n",
      "Epoch 90, batch 181 D Loss: 1.3891925811767578, G Loss: 0.6965855956077576\n",
      "Epoch 90, batch 182 D Loss: 1.37392258644104, G Loss: 0.6921730041503906\n",
      "Epoch 90, batch 183 D Loss: 1.3926365375518799, G Loss: 0.6876454949378967\n",
      "Epoch 90, batch 184 D Loss: 1.39774751663208, G Loss: 0.6830508708953857\n",
      "Epoch 90, batch 185 D Loss: 1.3852133750915527, G Loss: 0.6995919942855835\n",
      "Epoch 90, batch 186 D Loss: 1.3766837120056152, G Loss: 0.6956478357315063\n",
      "Epoch 90, batch 187 D Loss: 1.3842452764511108, G Loss: 0.6945472955703735\n",
      "Epoch 90, batch 188 D Loss: 1.389951229095459, G Loss: 0.6897594928741455\n",
      "Epoch 90, batch 189 D Loss: 1.38229238986969, G Loss: 0.6926796436309814\n",
      "Epoch 90, batch 190 D Loss: 1.3910601139068604, G Loss: 0.6976508498191833\n",
      "Epoch 90, batch 191 D Loss: 1.4009075164794922, G Loss: 0.6819026470184326\n",
      "Epoch 90, batch 192 D Loss: 1.3715357780456543, G Loss: 0.705018162727356\n",
      "Epoch 90, batch 193 D Loss: 1.371689796447754, G Loss: 0.6991000175476074\n",
      "Epoch 90, batch 194 D Loss: 1.3875256776809692, G Loss: 0.6972621083259583\n",
      "Epoch 90, batch 195 D Loss: 1.373178482055664, G Loss: 0.7085800170898438\n",
      "Epoch 90, batch 196 D Loss: 1.3933104276657104, G Loss: 0.6936399340629578\n",
      "Epoch 90, batch 197 D Loss: 1.373826265335083, G Loss: 0.7077167630195618\n",
      "Epoch 90, batch 198 D Loss: 1.3674776554107666, G Loss: 0.710095226764679\n",
      "Epoch 90, batch 199 D Loss: 1.3921501636505127, G Loss: 0.6969993114471436\n",
      "Epoch 90, batch 200 D Loss: 1.3630914688110352, G Loss: 0.7087523937225342\n",
      "Epoch 91, batch 1 D Loss: 1.391669750213623, G Loss: 0.7091020941734314\n",
      "Epoch 91, batch 2 D Loss: 1.3759410381317139, G Loss: 0.7170935869216919\n",
      "Epoch 91, batch 3 D Loss: 1.3690211772918701, G Loss: 0.7239871025085449\n",
      "Epoch 91, batch 4 D Loss: 1.3809292316436768, G Loss: 0.7090969681739807\n",
      "Epoch 91, batch 5 D Loss: 1.3872522115707397, G Loss: 0.7074012756347656\n",
      "Epoch 91, batch 6 D Loss: 1.3883304595947266, G Loss: 0.7070223093032837\n",
      "Epoch 91, batch 7 D Loss: 1.3869318962097168, G Loss: 0.7061489820480347\n",
      "Epoch 91, batch 8 D Loss: 1.3770251274108887, G Loss: 0.7143281698226929\n",
      "Epoch 91, batch 9 D Loss: 1.3935797214508057, G Loss: 0.6948890089988708\n",
      "Epoch 91, batch 10 D Loss: 1.384141206741333, G Loss: 0.7039874196052551\n",
      "Epoch 91, batch 11 D Loss: 1.4051107168197632, G Loss: 0.6854779720306396\n",
      "Epoch 91, batch 12 D Loss: 1.398603916168213, G Loss: 0.6986233592033386\n",
      "Epoch 91, batch 13 D Loss: 1.378707766532898, G Loss: 0.6972147226333618\n",
      "Epoch 91, batch 14 D Loss: 1.375356674194336, G Loss: 0.7062009572982788\n",
      "Epoch 91, batch 15 D Loss: 1.397624135017395, G Loss: 0.6997482776641846\n",
      "Epoch 91, batch 16 D Loss: 1.3950579166412354, G Loss: 0.694959819316864\n",
      "Epoch 91, batch 17 D Loss: 1.3997111320495605, G Loss: 0.6948808431625366\n",
      "Epoch 91, batch 18 D Loss: 1.4011521339416504, G Loss: 0.6911488175392151\n",
      "Epoch 91, batch 19 D Loss: 1.3761953115463257, G Loss: 0.7033233046531677\n",
      "Epoch 91, batch 20 D Loss: 1.3975927829742432, G Loss: 0.685373842716217\n",
      "Epoch 91, batch 21 D Loss: 1.3867220878601074, G Loss: 0.6990824341773987\n",
      "Epoch 91, batch 22 D Loss: 1.3858001232147217, G Loss: 0.6870044469833374\n",
      "Epoch 91, batch 23 D Loss: 1.3897385597229004, G Loss: 0.6897823810577393\n",
      "Epoch 91, batch 24 D Loss: 1.387554407119751, G Loss: 0.6959936618804932\n",
      "Epoch 91, batch 25 D Loss: 1.3918378353118896, G Loss: 0.698590874671936\n",
      "Epoch 91, batch 26 D Loss: 1.376901388168335, G Loss: 0.7028618454933167\n",
      "Epoch 91, batch 27 D Loss: 1.3747457265853882, G Loss: 0.6988322734832764\n",
      "Epoch 91, batch 28 D Loss: 1.385429859161377, G Loss: 0.695942223072052\n",
      "Epoch 91, batch 29 D Loss: 1.3882243633270264, G Loss: 0.6977212429046631\n",
      "Epoch 91, batch 30 D Loss: 1.3819220066070557, G Loss: 0.7013251781463623\n",
      "Epoch 91, batch 31 D Loss: 1.3786544799804688, G Loss: 0.6989538669586182\n",
      "Epoch 91, batch 32 D Loss: 1.3916566371917725, G Loss: 0.6911461353302002\n",
      "Epoch 91, batch 33 D Loss: 1.3861489295959473, G Loss: 0.697903037071228\n",
      "Epoch 91, batch 34 D Loss: 1.3914707899093628, G Loss: 0.6978282928466797\n",
      "Epoch 91, batch 35 D Loss: 1.3861618041992188, G Loss: 0.6873860359191895\n",
      "Epoch 91, batch 36 D Loss: 1.3823800086975098, G Loss: 0.695524275302887\n",
      "Epoch 91, batch 37 D Loss: 1.398624062538147, G Loss: 0.6848596930503845\n",
      "Epoch 91, batch 38 D Loss: 1.3905118703842163, G Loss: 0.6939907073974609\n",
      "Epoch 91, batch 39 D Loss: 1.380596399307251, G Loss: 0.6999285817146301\n",
      "Epoch 91, batch 40 D Loss: 1.3931323289871216, G Loss: 0.6825204491615295\n",
      "Epoch 91, batch 41 D Loss: 1.3759498596191406, G Loss: 0.689531147480011\n",
      "Epoch 91, batch 42 D Loss: 1.3837988376617432, G Loss: 0.6899282932281494\n",
      "Epoch 91, batch 43 D Loss: 1.3938759565353394, G Loss: 0.6890703439712524\n",
      "Epoch 91, batch 44 D Loss: 1.3843796253204346, G Loss: 0.685824990272522\n",
      "Epoch 91, batch 45 D Loss: 1.3854124546051025, G Loss: 0.6991369128227234\n",
      "Epoch 91, batch 46 D Loss: 1.414320468902588, G Loss: 0.6727226972579956\n",
      "Epoch 91, batch 47 D Loss: 1.3947972059249878, G Loss: 0.6904764771461487\n",
      "Epoch 91, batch 48 D Loss: 1.3877671957015991, G Loss: 0.6805870532989502\n",
      "Epoch 91, batch 49 D Loss: 1.3831360340118408, G Loss: 0.6821098923683167\n",
      "Epoch 91, batch 50 D Loss: 1.3843727111816406, G Loss: 0.6957256197929382\n",
      "Epoch 91, batch 51 D Loss: 1.3737578392028809, G Loss: 0.6949308514595032\n",
      "Epoch 91, batch 52 D Loss: 1.3886737823486328, G Loss: 0.6897746920585632\n",
      "Epoch 91, batch 53 D Loss: 1.3821601867675781, G Loss: 0.6970733404159546\n",
      "Epoch 91, batch 54 D Loss: 1.3971195220947266, G Loss: 0.690773069858551\n",
      "Epoch 91, batch 55 D Loss: 1.3859550952911377, G Loss: 0.6832984685897827\n",
      "Epoch 91, batch 56 D Loss: 1.3683172464370728, G Loss: 0.7014982104301453\n",
      "Epoch 91, batch 57 D Loss: 1.3903214931488037, G Loss: 0.6883754134178162\n",
      "Epoch 91, batch 58 D Loss: 1.3945858478546143, G Loss: 0.682320237159729\n",
      "Epoch 91, batch 59 D Loss: 1.37089204788208, G Loss: 0.6981910467147827\n",
      "Epoch 91, batch 60 D Loss: 1.3841450214385986, G Loss: 0.6887714266777039\n",
      "Epoch 91, batch 61 D Loss: 1.3832855224609375, G Loss: 0.6875787377357483\n",
      "Epoch 91, batch 62 D Loss: 1.3737987279891968, G Loss: 0.6950975656509399\n",
      "Epoch 91, batch 63 D Loss: 1.3809998035430908, G Loss: 0.6977444291114807\n",
      "Epoch 91, batch 64 D Loss: 1.383253574371338, G Loss: 0.7022925019264221\n",
      "Epoch 91, batch 65 D Loss: 1.3760981559753418, G Loss: 0.6925289630889893\n",
      "Epoch 91, batch 66 D Loss: 1.3886632919311523, G Loss: 0.6937257647514343\n",
      "Epoch 91, batch 67 D Loss: 1.3742797374725342, G Loss: 0.6979716420173645\n",
      "Epoch 91, batch 68 D Loss: 1.3889378309249878, G Loss: 0.6900864243507385\n",
      "Epoch 91, batch 69 D Loss: 1.3808112144470215, G Loss: 0.6807182431221008\n",
      "Epoch 91, batch 70 D Loss: 1.371896743774414, G Loss: 0.6974453926086426\n",
      "Epoch 91, batch 71 D Loss: 1.3984918594360352, G Loss: 0.6938884854316711\n",
      "Epoch 91, batch 72 D Loss: 1.368224859237671, G Loss: 0.6967391967773438\n",
      "Epoch 91, batch 73 D Loss: 1.373176097869873, G Loss: 0.7022770643234253\n",
      "Epoch 91, batch 74 D Loss: 1.3802292346954346, G Loss: 0.7020084857940674\n",
      "Epoch 91, batch 75 D Loss: 1.3835086822509766, G Loss: 0.696251392364502\n",
      "Epoch 91, batch 76 D Loss: 1.4186594486236572, G Loss: 0.6761324405670166\n",
      "Epoch 91, batch 77 D Loss: 1.408862829208374, G Loss: 0.6786362528800964\n",
      "Epoch 91, batch 78 D Loss: 1.3653559684753418, G Loss: 0.7051516175270081\n",
      "Epoch 91, batch 79 D Loss: 1.4052962064743042, G Loss: 0.690460205078125\n",
      "Epoch 91, batch 80 D Loss: 1.3698111772537231, G Loss: 0.7056779265403748\n",
      "Epoch 91, batch 81 D Loss: 1.3744511604309082, G Loss: 0.7002969980239868\n",
      "Epoch 91, batch 82 D Loss: 1.3884193897247314, G Loss: 0.684678316116333\n",
      "Epoch 91, batch 83 D Loss: 1.3912363052368164, G Loss: 0.692578136920929\n",
      "Epoch 91, batch 84 D Loss: 1.3969172239303589, G Loss: 0.6897337436676025\n",
      "Epoch 91, batch 85 D Loss: 1.3944206237792969, G Loss: 0.691215455532074\n",
      "Epoch 91, batch 86 D Loss: 1.3729768991470337, G Loss: 0.7284365296363831\n",
      "Epoch 91, batch 87 D Loss: 1.3737108707427979, G Loss: 0.7129811644554138\n",
      "Epoch 91, batch 88 D Loss: 1.3620891571044922, G Loss: 0.7060989141464233\n",
      "Epoch 91, batch 89 D Loss: 1.3893773555755615, G Loss: 0.6972879767417908\n",
      "Epoch 91, batch 90 D Loss: 1.3889415264129639, G Loss: 0.7066876888275146\n",
      "Epoch 91, batch 91 D Loss: 1.3898375034332275, G Loss: 0.6912215948104858\n",
      "Epoch 91, batch 92 D Loss: 1.3828999996185303, G Loss: 0.7037107944488525\n",
      "Epoch 91, batch 93 D Loss: 1.394416093826294, G Loss: 0.6945312023162842\n",
      "Epoch 91, batch 94 D Loss: 1.389469861984253, G Loss: 0.6947484016418457\n",
      "Epoch 91, batch 95 D Loss: 1.3802087306976318, G Loss: 0.6998375058174133\n",
      "Epoch 91, batch 96 D Loss: 1.3817970752716064, G Loss: 0.6927382946014404\n",
      "Epoch 91, batch 97 D Loss: 1.3767426013946533, G Loss: 0.7065225839614868\n",
      "Epoch 91, batch 98 D Loss: 1.3972433805465698, G Loss: 0.698215663433075\n",
      "Epoch 91, batch 99 D Loss: 1.3901336193084717, G Loss: 0.7080709934234619\n",
      "Epoch 91, batch 100 D Loss: 1.3958078622817993, G Loss: 0.7068940997123718\n",
      "Epoch 91, batch 101 D Loss: 1.4134175777435303, G Loss: 0.6855049729347229\n",
      "Epoch 91, batch 102 D Loss: 1.3956736326217651, G Loss: 0.7071208953857422\n",
      "Epoch 91, batch 103 D Loss: 1.395801305770874, G Loss: 0.7028704881668091\n",
      "Epoch 91, batch 104 D Loss: 1.3809822797775269, G Loss: 0.7035260200500488\n",
      "Epoch 91, batch 105 D Loss: 1.392770767211914, G Loss: 0.6910595893859863\n",
      "Epoch 91, batch 106 D Loss: 1.3849984407424927, G Loss: 0.7177064418792725\n",
      "Epoch 91, batch 107 D Loss: 1.385493278503418, G Loss: 0.6973524689674377\n",
      "Epoch 91, batch 108 D Loss: 1.3909058570861816, G Loss: 0.7039909958839417\n",
      "Epoch 91, batch 109 D Loss: 1.3982828855514526, G Loss: 0.698305606842041\n",
      "Epoch 91, batch 110 D Loss: 1.383652925491333, G Loss: 0.708162248134613\n",
      "Epoch 91, batch 111 D Loss: 1.401442289352417, G Loss: 0.6983402967453003\n",
      "Epoch 91, batch 112 D Loss: 1.3775792121887207, G Loss: 0.7143004536628723\n",
      "Epoch 91, batch 113 D Loss: 1.3910250663757324, G Loss: 0.7005822658538818\n",
      "Epoch 91, batch 114 D Loss: 1.3905375003814697, G Loss: 0.6984149813652039\n",
      "Epoch 91, batch 115 D Loss: 1.3836634159088135, G Loss: 0.7070249915122986\n",
      "Epoch 91, batch 116 D Loss: 1.4036589860916138, G Loss: 0.6956019401550293\n",
      "Epoch 91, batch 117 D Loss: 1.3835854530334473, G Loss: 0.7111013531684875\n",
      "Epoch 91, batch 118 D Loss: 1.3977601528167725, G Loss: 0.7087348699569702\n",
      "Epoch 91, batch 119 D Loss: 1.39835786819458, G Loss: 0.7067394256591797\n",
      "Epoch 91, batch 120 D Loss: 1.3810689449310303, G Loss: 0.7109603881835938\n",
      "Epoch 91, batch 121 D Loss: 1.3997341394424438, G Loss: 0.698209822177887\n",
      "Epoch 91, batch 122 D Loss: 1.3840878009796143, G Loss: 0.7059056758880615\n",
      "Epoch 91, batch 123 D Loss: 1.3927662372589111, G Loss: 0.7017002701759338\n",
      "Epoch 91, batch 124 D Loss: 1.4048645496368408, G Loss: 0.7068710327148438\n",
      "Epoch 91, batch 125 D Loss: 1.3922996520996094, G Loss: 0.6973893642425537\n",
      "Epoch 91, batch 126 D Loss: 1.3853546380996704, G Loss: 0.7093612551689148\n",
      "Epoch 91, batch 127 D Loss: 1.386767864227295, G Loss: 0.709909200668335\n",
      "Epoch 91, batch 128 D Loss: 1.3924837112426758, G Loss: 0.7041762471199036\n",
      "Epoch 91, batch 129 D Loss: 1.386197566986084, G Loss: 0.7075070738792419\n",
      "Epoch 91, batch 130 D Loss: 1.3808890581130981, G Loss: 0.7067469954490662\n",
      "Epoch 91, batch 131 D Loss: 1.3887054920196533, G Loss: 0.7059739828109741\n",
      "Epoch 91, batch 132 D Loss: 1.374428391456604, G Loss: 0.7008814811706543\n",
      "Epoch 91, batch 133 D Loss: 1.3936078548431396, G Loss: 0.6912767291069031\n",
      "Epoch 91, batch 134 D Loss: 1.3697689771652222, G Loss: 0.695009708404541\n",
      "Epoch 91, batch 135 D Loss: 1.3664791584014893, G Loss: 0.7169836163520813\n",
      "Epoch 91, batch 136 D Loss: 1.3918206691741943, G Loss: 0.7126055955886841\n",
      "Epoch 91, batch 137 D Loss: 1.4006085395812988, G Loss: 0.6897300481796265\n",
      "Epoch 91, batch 138 D Loss: 1.3951902389526367, G Loss: 0.704662024974823\n",
      "Epoch 91, batch 139 D Loss: 1.3845806121826172, G Loss: 0.7050307393074036\n",
      "Epoch 91, batch 140 D Loss: 1.3974354267120361, G Loss: 0.6971945762634277\n",
      "Epoch 91, batch 141 D Loss: 1.388378381729126, G Loss: 0.7019704580307007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, batch 142 D Loss: 1.4062827825546265, G Loss: 0.6992328763008118\n",
      "Epoch 91, batch 143 D Loss: 1.3771960735321045, G Loss: 0.7070093750953674\n",
      "Epoch 91, batch 144 D Loss: 1.4109599590301514, G Loss: 0.6944897174835205\n",
      "Epoch 91, batch 145 D Loss: 1.3747397661209106, G Loss: 0.7063355445861816\n",
      "Epoch 91, batch 146 D Loss: 1.3892638683319092, G Loss: 0.6827678084373474\n",
      "Epoch 91, batch 147 D Loss: 1.389282464981079, G Loss: 0.6924545168876648\n",
      "Epoch 91, batch 148 D Loss: 1.353532314300537, G Loss: 0.7171908617019653\n",
      "Epoch 91, batch 149 D Loss: 1.3724331855773926, G Loss: 0.7019718885421753\n",
      "Epoch 91, batch 150 D Loss: 1.4026124477386475, G Loss: 0.7015188336372375\n",
      "Epoch 91, batch 151 D Loss: 1.388719081878662, G Loss: 0.7002205848693848\n",
      "Epoch 91, batch 152 D Loss: 1.38214910030365, G Loss: 0.6897858381271362\n",
      "Epoch 91, batch 153 D Loss: 1.3880963325500488, G Loss: 0.7073678374290466\n",
      "Epoch 91, batch 154 D Loss: 1.3921022415161133, G Loss: 0.6877713799476624\n",
      "Epoch 91, batch 155 D Loss: 1.3685014247894287, G Loss: 0.700783908367157\n",
      "Epoch 91, batch 156 D Loss: 1.3686418533325195, G Loss: 0.6916859149932861\n",
      "Epoch 91, batch 157 D Loss: 1.3621269464492798, G Loss: 0.7001147270202637\n",
      "Epoch 91, batch 158 D Loss: 1.3907877206802368, G Loss: 0.6954295635223389\n",
      "Epoch 91, batch 159 D Loss: 1.4068708419799805, G Loss: 0.6898084878921509\n",
      "Epoch 91, batch 160 D Loss: 1.4103541374206543, G Loss: 0.6765783429145813\n",
      "Epoch 91, batch 161 D Loss: 1.4082837104797363, G Loss: 0.6728475689888\n",
      "Epoch 91, batch 162 D Loss: 1.4125010967254639, G Loss: 0.6812770962715149\n",
      "Epoch 91, batch 163 D Loss: 1.3940296173095703, G Loss: 0.7059298157691956\n",
      "Epoch 91, batch 164 D Loss: 1.394477128982544, G Loss: 0.6985381841659546\n",
      "Epoch 91, batch 165 D Loss: 1.3779611587524414, G Loss: 0.7043566703796387\n",
      "Epoch 91, batch 166 D Loss: 1.3733676671981812, G Loss: 0.687248945236206\n",
      "Epoch 91, batch 167 D Loss: 1.3949016332626343, G Loss: 0.6798022389411926\n",
      "Epoch 91, batch 168 D Loss: 1.3928042650222778, G Loss: 0.693077802658081\n",
      "Epoch 91, batch 169 D Loss: 1.3987144231796265, G Loss: 0.6896343231201172\n",
      "Epoch 91, batch 170 D Loss: 1.3801624774932861, G Loss: 0.685955822467804\n",
      "Epoch 91, batch 171 D Loss: 1.3954901695251465, G Loss: 0.6916362643241882\n",
      "Epoch 91, batch 172 D Loss: 1.3804728984832764, G Loss: 0.700227677822113\n",
      "Epoch 91, batch 173 D Loss: 1.376175880432129, G Loss: 0.6834185123443604\n",
      "Epoch 91, batch 174 D Loss: 1.3816256523132324, G Loss: 0.6801092624664307\n",
      "Epoch 91, batch 175 D Loss: 1.3822771310806274, G Loss: 0.6877220869064331\n",
      "Epoch 91, batch 176 D Loss: 1.383245825767517, G Loss: 0.6910571455955505\n",
      "Epoch 91, batch 177 D Loss: 1.3861241340637207, G Loss: 0.6925511956214905\n",
      "Epoch 91, batch 178 D Loss: 1.400505781173706, G Loss: 0.6756020188331604\n",
      "Epoch 91, batch 179 D Loss: 1.3907196521759033, G Loss: 0.6889545321464539\n",
      "Epoch 91, batch 180 D Loss: 1.3927679061889648, G Loss: 0.6801433563232422\n",
      "Epoch 91, batch 181 D Loss: 1.3873136043548584, G Loss: 0.6851604580879211\n",
      "Epoch 91, batch 182 D Loss: 1.412735939025879, G Loss: 0.667920708656311\n",
      "Epoch 91, batch 183 D Loss: 1.3950297832489014, G Loss: 0.6739956736564636\n",
      "Epoch 91, batch 184 D Loss: 1.4054765701293945, G Loss: 0.6792556643486023\n",
      "Epoch 91, batch 185 D Loss: 1.3921661376953125, G Loss: 0.67938631772995\n",
      "Epoch 91, batch 186 D Loss: 1.38370943069458, G Loss: 0.6806249022483826\n",
      "Epoch 91, batch 187 D Loss: 1.3934874534606934, G Loss: 0.670047402381897\n",
      "Epoch 91, batch 188 D Loss: 1.379045009613037, G Loss: 0.6793757081031799\n",
      "Epoch 91, batch 189 D Loss: 1.3784971237182617, G Loss: 0.6839964389801025\n",
      "Epoch 91, batch 190 D Loss: 1.3697941303253174, G Loss: 0.6828811764717102\n",
      "Epoch 91, batch 191 D Loss: 1.3786934614181519, G Loss: 0.6840755939483643\n",
      "Epoch 91, batch 192 D Loss: 1.379246473312378, G Loss: 0.6898800134658813\n",
      "Epoch 91, batch 193 D Loss: 1.3832194805145264, G Loss: 0.6847884654998779\n",
      "Epoch 91, batch 194 D Loss: 1.3895227909088135, G Loss: 0.6752329468727112\n",
      "Epoch 91, batch 195 D Loss: 1.39151930809021, G Loss: 0.6892294883728027\n",
      "Epoch 91, batch 196 D Loss: 1.3753166198730469, G Loss: 0.6951901316642761\n",
      "Epoch 91, batch 197 D Loss: 1.38930344581604, G Loss: 0.6843411922454834\n",
      "Epoch 91, batch 198 D Loss: 1.369945764541626, G Loss: 0.6842190623283386\n",
      "Epoch 91, batch 199 D Loss: 1.3784897327423096, G Loss: 0.6924961805343628\n",
      "Epoch 91, batch 200 D Loss: 1.3732069730758667, G Loss: 0.6962637901306152\n",
      "Epoch 92, batch 1 D Loss: 1.3868820667266846, G Loss: 0.6918699741363525\n",
      "Epoch 92, batch 2 D Loss: 1.4126396179199219, G Loss: 0.6673983931541443\n",
      "Epoch 92, batch 3 D Loss: 1.4341492652893066, G Loss: 0.6615417003631592\n",
      "Epoch 92, batch 4 D Loss: 1.3815569877624512, G Loss: 0.6999078989028931\n",
      "Epoch 92, batch 5 D Loss: 1.3932578563690186, G Loss: 0.6944958567619324\n",
      "Epoch 92, batch 6 D Loss: 1.3669028282165527, G Loss: 0.6988802552223206\n",
      "Epoch 92, batch 7 D Loss: 1.3836853504180908, G Loss: 0.6934691667556763\n",
      "Epoch 92, batch 8 D Loss: 1.3919621706008911, G Loss: 0.7059249877929688\n",
      "Epoch 92, batch 9 D Loss: 1.38910710811615, G Loss: 0.6908455491065979\n",
      "Epoch 92, batch 10 D Loss: 1.377558946609497, G Loss: 0.709248423576355\n",
      "Epoch 92, batch 11 D Loss: 1.4096653461456299, G Loss: 0.6838842630386353\n",
      "Epoch 92, batch 12 D Loss: 1.387746810913086, G Loss: 0.6926817893981934\n",
      "Epoch 92, batch 13 D Loss: 1.3891960382461548, G Loss: 0.6987372040748596\n",
      "Epoch 92, batch 14 D Loss: 1.394576907157898, G Loss: 0.6977424025535583\n",
      "Epoch 92, batch 15 D Loss: 1.4106590747833252, G Loss: 0.6987562775611877\n",
      "Epoch 92, batch 16 D Loss: 1.368778944015503, G Loss: 0.7152475118637085\n",
      "Epoch 92, batch 17 D Loss: 1.405656099319458, G Loss: 0.6840336322784424\n",
      "Epoch 92, batch 18 D Loss: 1.3882577419281006, G Loss: 0.6974856853485107\n",
      "Epoch 92, batch 19 D Loss: 1.3782434463500977, G Loss: 0.6967167854309082\n",
      "Epoch 92, batch 20 D Loss: 1.3978896141052246, G Loss: 0.6940228939056396\n",
      "Epoch 92, batch 21 D Loss: 1.4058496952056885, G Loss: 0.7068329453468323\n",
      "Epoch 92, batch 22 D Loss: 1.4031847715377808, G Loss: 0.700812816619873\n",
      "Epoch 92, batch 23 D Loss: 1.3895584344863892, G Loss: 0.7002617716789246\n",
      "Epoch 92, batch 24 D Loss: 1.393364667892456, G Loss: 0.6956321001052856\n",
      "Epoch 92, batch 25 D Loss: 1.392016887664795, G Loss: 0.6986588835716248\n",
      "Epoch 92, batch 26 D Loss: 1.4007160663604736, G Loss: 0.6964952349662781\n",
      "Epoch 92, batch 27 D Loss: 1.377665400505066, G Loss: 0.7160138487815857\n",
      "Epoch 92, batch 28 D Loss: 1.389784574508667, G Loss: 0.7023003101348877\n",
      "Epoch 92, batch 29 D Loss: 1.3919869661331177, G Loss: 0.703442394733429\n",
      "Epoch 92, batch 30 D Loss: 1.3808708190917969, G Loss: 0.7122538089752197\n",
      "Epoch 92, batch 31 D Loss: 1.3863658905029297, G Loss: 0.7033753991127014\n",
      "Epoch 92, batch 32 D Loss: 1.4106132984161377, G Loss: 0.6927137970924377\n",
      "Epoch 92, batch 33 D Loss: 1.3919427394866943, G Loss: 0.7052221894264221\n",
      "Epoch 92, batch 34 D Loss: 1.3893862962722778, G Loss: 0.7069222331047058\n",
      "Epoch 92, batch 35 D Loss: 1.3836419582366943, G Loss: 0.7128639221191406\n",
      "Epoch 92, batch 36 D Loss: 1.3954932689666748, G Loss: 0.6908042430877686\n",
      "Epoch 92, batch 37 D Loss: 1.373178482055664, G Loss: 0.7123834490776062\n",
      "Epoch 92, batch 38 D Loss: 1.384168028831482, G Loss: 0.7130096554756165\n",
      "Epoch 92, batch 39 D Loss: 1.3868770599365234, G Loss: 0.7046898007392883\n",
      "Epoch 92, batch 40 D Loss: 1.3834435939788818, G Loss: 0.7053979635238647\n",
      "Epoch 92, batch 41 D Loss: 1.3856559991836548, G Loss: 0.7009312510490417\n",
      "Epoch 92, batch 42 D Loss: 1.3830416202545166, G Loss: 0.7084438800811768\n",
      "Epoch 92, batch 43 D Loss: 1.4054062366485596, G Loss: 0.6943953633308411\n",
      "Epoch 92, batch 44 D Loss: 1.396047830581665, G Loss: 0.7005981206893921\n",
      "Epoch 92, batch 45 D Loss: 1.3793851137161255, G Loss: 0.7084596157073975\n",
      "Epoch 92, batch 46 D Loss: 1.3893306255340576, G Loss: 0.7008394002914429\n",
      "Epoch 92, batch 47 D Loss: 1.377604365348816, G Loss: 0.7061339020729065\n",
      "Epoch 92, batch 48 D Loss: 1.3819308280944824, G Loss: 0.7084559798240662\n",
      "Epoch 92, batch 49 D Loss: 1.3983266353607178, G Loss: 0.704295814037323\n",
      "Epoch 92, batch 50 D Loss: 1.3988125324249268, G Loss: 0.7133672833442688\n",
      "Epoch 92, batch 51 D Loss: 1.3862226009368896, G Loss: 0.7032185196876526\n",
      "Epoch 92, batch 52 D Loss: 1.3910720348358154, G Loss: 0.7001528739929199\n",
      "Epoch 92, batch 53 D Loss: 1.4043235778808594, G Loss: 0.7006621360778809\n",
      "Epoch 92, batch 54 D Loss: 1.378326654434204, G Loss: 0.7170576453208923\n",
      "Epoch 92, batch 55 D Loss: 1.395695447921753, G Loss: 0.699301540851593\n",
      "Epoch 92, batch 56 D Loss: 1.3857983350753784, G Loss: 0.6960266828536987\n",
      "Epoch 92, batch 57 D Loss: 1.3796346187591553, G Loss: 0.7070184350013733\n",
      "Epoch 92, batch 58 D Loss: 1.3910613059997559, G Loss: 0.6998473405838013\n",
      "Epoch 92, batch 59 D Loss: 1.37747061252594, G Loss: 0.7089937329292297\n",
      "Epoch 92, batch 60 D Loss: 1.3966248035430908, G Loss: 0.6959484219551086\n",
      "Epoch 92, batch 61 D Loss: 1.404026985168457, G Loss: 0.6907327771186829\n",
      "Epoch 92, batch 62 D Loss: 1.3806166648864746, G Loss: 0.7068898677825928\n",
      "Epoch 92, batch 63 D Loss: 1.396312952041626, G Loss: 0.7016152739524841\n",
      "Epoch 92, batch 64 D Loss: 1.389472246170044, G Loss: 0.7084320187568665\n",
      "Epoch 92, batch 65 D Loss: 1.3726696968078613, G Loss: 0.7201629877090454\n",
      "Epoch 92, batch 66 D Loss: 1.3984427452087402, G Loss: 0.6908013224601746\n",
      "Epoch 92, batch 67 D Loss: 1.3832800388336182, G Loss: 0.702955961227417\n",
      "Epoch 92, batch 68 D Loss: 1.385618805885315, G Loss: 0.7166562080383301\n",
      "Epoch 92, batch 69 D Loss: 1.3702828884124756, G Loss: 0.7111331820487976\n",
      "Epoch 92, batch 70 D Loss: 1.398789405822754, G Loss: 0.7016909718513489\n",
      "Epoch 92, batch 71 D Loss: 1.400814175605774, G Loss: 0.6934704780578613\n",
      "Epoch 92, batch 72 D Loss: 1.3933014869689941, G Loss: 0.7008429169654846\n",
      "Epoch 92, batch 73 D Loss: 1.396165370941162, G Loss: 0.6988956928253174\n",
      "Epoch 92, batch 74 D Loss: 1.3912529945373535, G Loss: 0.6908782720565796\n",
      "Epoch 92, batch 75 D Loss: 1.369554042816162, G Loss: 0.7099883556365967\n",
      "Epoch 92, batch 76 D Loss: 1.3886078596115112, G Loss: 0.6903315782546997\n",
      "Epoch 92, batch 77 D Loss: 1.3828892707824707, G Loss: 0.7063701748847961\n",
      "Epoch 92, batch 78 D Loss: 1.3914381265640259, G Loss: 0.6906471252441406\n",
      "Epoch 92, batch 79 D Loss: 1.3802236318588257, G Loss: 0.7045282125473022\n",
      "Epoch 92, batch 80 D Loss: 1.3912907838821411, G Loss: 0.6827086806297302\n",
      "Epoch 92, batch 81 D Loss: 1.3756036758422852, G Loss: 0.7020484209060669\n",
      "Epoch 92, batch 82 D Loss: 1.3977718353271484, G Loss: 0.691810667514801\n",
      "Epoch 92, batch 83 D Loss: 1.397174596786499, G Loss: 0.6894372701644897\n",
      "Epoch 92, batch 84 D Loss: 1.3873915672302246, G Loss: 0.6770994067192078\n",
      "Epoch 92, batch 85 D Loss: 1.391362190246582, G Loss: 0.6779749989509583\n",
      "Epoch 92, batch 86 D Loss: 1.3738030195236206, G Loss: 0.6767305135726929\n",
      "Epoch 92, batch 87 D Loss: 1.388489007949829, G Loss: 0.6796910166740417\n",
      "Epoch 92, batch 88 D Loss: 1.3872580528259277, G Loss: 0.6755025386810303\n",
      "Epoch 92, batch 89 D Loss: 1.3937875032424927, G Loss: 0.6715824604034424\n",
      "Epoch 92, batch 90 D Loss: 1.3787918090820312, G Loss: 0.6733402013778687\n",
      "Epoch 92, batch 91 D Loss: 1.397765040397644, G Loss: 0.6724469661712646\n",
      "Epoch 92, batch 92 D Loss: 1.3849811553955078, G Loss: 0.6762790679931641\n",
      "Epoch 92, batch 93 D Loss: 1.3909430503845215, G Loss: 0.6811655163764954\n",
      "Epoch 92, batch 94 D Loss: 1.3885488510131836, G Loss: 0.6826476454734802\n",
      "Epoch 92, batch 95 D Loss: 1.3879822492599487, G Loss: 0.6805728077888489\n",
      "Epoch 92, batch 96 D Loss: 1.3703818321228027, G Loss: 0.6887415051460266\n",
      "Epoch 92, batch 97 D Loss: 1.3885630369186401, G Loss: 0.6690374612808228\n",
      "Epoch 92, batch 98 D Loss: 1.3923555612564087, G Loss: 0.6774236559867859\n",
      "Epoch 92, batch 99 D Loss: 1.3972008228302002, G Loss: 0.6791840195655823\n",
      "Epoch 92, batch 100 D Loss: 1.3876726627349854, G Loss: 0.6744815707206726\n",
      "Epoch 92, batch 101 D Loss: 1.3693342208862305, G Loss: 0.6820180416107178\n",
      "Epoch 92, batch 102 D Loss: 1.398153305053711, G Loss: 0.6755531430244446\n",
      "Epoch 92, batch 103 D Loss: 1.3840268850326538, G Loss: 0.6816348433494568\n",
      "Epoch 92, batch 104 D Loss: 1.3762078285217285, G Loss: 0.6788269877433777\n",
      "Epoch 92, batch 105 D Loss: 1.3834893703460693, G Loss: 0.6757774949073792\n",
      "Epoch 92, batch 106 D Loss: 1.3725147247314453, G Loss: 0.6859988570213318\n",
      "Epoch 92, batch 107 D Loss: 1.3819270133972168, G Loss: 0.6816143989562988\n",
      "Epoch 92, batch 108 D Loss: 1.3976354598999023, G Loss: 0.6790262460708618\n",
      "Epoch 92, batch 109 D Loss: 1.3729424476623535, G Loss: 0.688483476638794\n",
      "Epoch 92, batch 110 D Loss: 1.3920419216156006, G Loss: 0.6847679018974304\n",
      "Epoch 92, batch 111 D Loss: 1.391143560409546, G Loss: 0.6817538738250732\n",
      "Epoch 92, batch 112 D Loss: 1.3796577453613281, G Loss: 0.6849187612533569\n",
      "Epoch 92, batch 113 D Loss: 1.3881175518035889, G Loss: 0.6804807782173157\n",
      "Epoch 92, batch 114 D Loss: 1.375670313835144, G Loss: 0.6940585374832153\n",
      "Epoch 92, batch 115 D Loss: 1.367973804473877, G Loss: 0.6918031573295593\n",
      "Epoch 92, batch 116 D Loss: 1.3924272060394287, G Loss: 0.682925283908844\n",
      "Epoch 92, batch 117 D Loss: 1.3705347776412964, G Loss: 0.6950541734695435\n",
      "Epoch 92, batch 118 D Loss: 1.373147964477539, G Loss: 0.6958092451095581\n",
      "Epoch 92, batch 119 D Loss: 1.383231282234192, G Loss: 0.6800060272216797\n",
      "Epoch 92, batch 120 D Loss: 1.3914170265197754, G Loss: 0.6943013072013855\n",
      "Epoch 92, batch 121 D Loss: 1.371762990951538, G Loss: 0.6899220943450928\n",
      "Epoch 92, batch 122 D Loss: 1.3878576755523682, G Loss: 0.694969117641449\n",
      "Epoch 92, batch 123 D Loss: 1.376882553100586, G Loss: 0.6969516277313232\n",
      "Epoch 92, batch 124 D Loss: 1.3936307430267334, G Loss: 0.6812323927879333\n",
      "Epoch 92, batch 125 D Loss: 1.3865045309066772, G Loss: 0.6879645586013794\n",
      "Epoch 92, batch 126 D Loss: 1.3791629076004028, G Loss: 0.6959989666938782\n",
      "Epoch 92, batch 127 D Loss: 1.3849060535430908, G Loss: 0.6914299130439758\n",
      "Epoch 92, batch 128 D Loss: 1.4059706926345825, G Loss: 0.6862890124320984\n",
      "Epoch 92, batch 129 D Loss: 1.3869324922561646, G Loss: 0.6954101324081421\n",
      "Epoch 92, batch 130 D Loss: 1.3764166831970215, G Loss: 0.699557363986969\n",
      "Epoch 92, batch 131 D Loss: 1.370234489440918, G Loss: 0.6956371068954468\n",
      "Epoch 92, batch 132 D Loss: 1.3710603713989258, G Loss: 0.6941125392913818\n",
      "Epoch 92, batch 133 D Loss: 1.3742036819458008, G Loss: 0.707324206829071\n",
      "Epoch 92, batch 134 D Loss: 1.3902864456176758, G Loss: 0.6868800520896912\n",
      "Epoch 92, batch 135 D Loss: 1.3695697784423828, G Loss: 0.6982784867286682\n",
      "Epoch 92, batch 136 D Loss: 1.3842825889587402, G Loss: 0.6796743273735046\n",
      "Epoch 92, batch 137 D Loss: 1.3874576091766357, G Loss: 0.7036339044570923\n",
      "Epoch 92, batch 138 D Loss: 1.3991540670394897, G Loss: 0.68613600730896\n",
      "Epoch 92, batch 139 D Loss: 1.3889344930648804, G Loss: 0.6944928765296936\n",
      "Epoch 92, batch 140 D Loss: 1.3922088146209717, G Loss: 0.7050198316574097\n",
      "Epoch 92, batch 141 D Loss: 1.3781867027282715, G Loss: 0.6887394785881042\n",
      "Epoch 92, batch 142 D Loss: 1.393811821937561, G Loss: 0.7000468969345093\n",
      "Epoch 92, batch 143 D Loss: 1.3939262628555298, G Loss: 0.6831753253936768\n",
      "Epoch 92, batch 144 D Loss: 1.4039700031280518, G Loss: 0.6886339783668518\n",
      "Epoch 92, batch 145 D Loss: 1.3979337215423584, G Loss: 0.7029925584793091\n",
      "Epoch 92, batch 146 D Loss: 1.394760012626648, G Loss: 0.6941980123519897\n",
      "Epoch 92, batch 147 D Loss: 1.3731694221496582, G Loss: 0.6996552348136902\n",
      "Epoch 92, batch 148 D Loss: 1.374865174293518, G Loss: 0.714134931564331\n",
      "Epoch 92, batch 149 D Loss: 1.381056785583496, G Loss: 0.7025617361068726\n",
      "Epoch 92, batch 150 D Loss: 1.3768469095230103, G Loss: 0.7190958261489868\n",
      "Epoch 92, batch 151 D Loss: 1.3945603370666504, G Loss: 0.7055566906929016\n",
      "Epoch 92, batch 152 D Loss: 1.3829078674316406, G Loss: 0.7127604484558105\n",
      "Epoch 92, batch 153 D Loss: 1.3958239555358887, G Loss: 0.7049678564071655\n",
      "Epoch 92, batch 154 D Loss: 1.3873882293701172, G Loss: 0.7074097394943237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, batch 155 D Loss: 1.3875820636749268, G Loss: 0.7132450938224792\n",
      "Epoch 92, batch 156 D Loss: 1.4000039100646973, G Loss: 0.7062939405441284\n",
      "Epoch 92, batch 157 D Loss: 1.3930621147155762, G Loss: 0.7076558470726013\n",
      "Epoch 92, batch 158 D Loss: 1.388209342956543, G Loss: 0.7034110426902771\n",
      "Epoch 92, batch 159 D Loss: 1.3855578899383545, G Loss: 0.7067659497261047\n",
      "Epoch 92, batch 160 D Loss: 1.3884307146072388, G Loss: 0.6969401836395264\n",
      "Epoch 92, batch 161 D Loss: 1.4108399152755737, G Loss: 0.7062088251113892\n",
      "Epoch 92, batch 162 D Loss: 1.3677632808685303, G Loss: 0.7273237109184265\n",
      "Epoch 92, batch 163 D Loss: 1.4064230918884277, G Loss: 0.7058565020561218\n",
      "Epoch 92, batch 164 D Loss: 1.3590959310531616, G Loss: 0.7197970747947693\n",
      "Epoch 92, batch 165 D Loss: 1.4035905599594116, G Loss: 0.698890209197998\n",
      "Epoch 92, batch 166 D Loss: 1.4016209840774536, G Loss: 0.7084303498268127\n",
      "Epoch 92, batch 167 D Loss: 1.3975510597229004, G Loss: 0.7145191431045532\n",
      "Epoch 92, batch 168 D Loss: 1.3849575519561768, G Loss: 0.7063825726509094\n",
      "Epoch 92, batch 169 D Loss: 1.3788468837738037, G Loss: 0.7193979024887085\n",
      "Epoch 92, batch 170 D Loss: 1.3738102912902832, G Loss: 0.7259376049041748\n",
      "Epoch 92, batch 171 D Loss: 1.3858822584152222, G Loss: 0.7187854647636414\n",
      "Epoch 92, batch 172 D Loss: 1.4036312103271484, G Loss: 0.7019047737121582\n",
      "Epoch 92, batch 173 D Loss: 1.3958508968353271, G Loss: 0.7106596231460571\n",
      "Epoch 92, batch 174 D Loss: 1.385636806488037, G Loss: 0.7142451405525208\n",
      "Epoch 92, batch 175 D Loss: 1.3886301517486572, G Loss: 0.7204071283340454\n",
      "Epoch 92, batch 176 D Loss: 1.383700966835022, G Loss: 0.7210137844085693\n",
      "Epoch 92, batch 177 D Loss: 1.385810375213623, G Loss: 0.7060459852218628\n",
      "Epoch 92, batch 178 D Loss: 1.3935233354568481, G Loss: 0.7096621990203857\n",
      "Epoch 92, batch 179 D Loss: 1.3950281143188477, G Loss: 0.7126432061195374\n",
      "Epoch 92, batch 180 D Loss: 1.3882575035095215, G Loss: 0.7229709029197693\n",
      "Epoch 92, batch 181 D Loss: 1.383862018585205, G Loss: 0.7050018310546875\n",
      "Epoch 92, batch 182 D Loss: 1.3916364908218384, G Loss: 0.703752338886261\n",
      "Epoch 92, batch 183 D Loss: 1.3792866468429565, G Loss: 0.7136324048042297\n",
      "Epoch 92, batch 184 D Loss: 1.3989980220794678, G Loss: 0.7027263641357422\n",
      "Epoch 92, batch 185 D Loss: 1.3843765258789062, G Loss: 0.7147239446640015\n",
      "Epoch 92, batch 186 D Loss: 1.3886618614196777, G Loss: 0.717002272605896\n",
      "Epoch 92, batch 187 D Loss: 1.382983922958374, G Loss: 0.7100769877433777\n",
      "Epoch 92, batch 188 D Loss: 1.3799235820770264, G Loss: 0.7189267873764038\n",
      "Epoch 92, batch 189 D Loss: 1.3975911140441895, G Loss: 0.7042567729949951\n",
      "Epoch 92, batch 190 D Loss: 1.400284767150879, G Loss: 0.6893558502197266\n",
      "Epoch 92, batch 191 D Loss: 1.390763759613037, G Loss: 0.7079829573631287\n",
      "Epoch 92, batch 192 D Loss: 1.3966511487960815, G Loss: 0.7092974185943604\n",
      "Epoch 92, batch 193 D Loss: 1.4017748832702637, G Loss: 0.6980666518211365\n",
      "Epoch 92, batch 194 D Loss: 1.38657546043396, G Loss: 0.7120126485824585\n",
      "Epoch 92, batch 195 D Loss: 1.3972952365875244, G Loss: 0.6925361156463623\n",
      "Epoch 92, batch 196 D Loss: 1.3902873992919922, G Loss: 0.7058689594268799\n",
      "Epoch 92, batch 197 D Loss: 1.3939098119735718, G Loss: 0.6970168948173523\n",
      "Epoch 92, batch 198 D Loss: 1.4030027389526367, G Loss: 0.6992406249046326\n",
      "Epoch 92, batch 199 D Loss: 1.3912559747695923, G Loss: 0.7049441337585449\n",
      "Epoch 92, batch 200 D Loss: 1.3815027475357056, G Loss: 0.7158516645431519\n",
      "Epoch 93, batch 1 D Loss: 1.3896877765655518, G Loss: 0.7062874436378479\n",
      "Epoch 93, batch 2 D Loss: 1.3802106380462646, G Loss: 0.7166505455970764\n",
      "Epoch 93, batch 3 D Loss: 1.3960235118865967, G Loss: 0.7023753523826599\n",
      "Epoch 93, batch 4 D Loss: 1.3877233266830444, G Loss: 0.7108112573623657\n",
      "Epoch 93, batch 5 D Loss: 1.3936660289764404, G Loss: 0.7059872150421143\n",
      "Epoch 93, batch 6 D Loss: 1.3819091320037842, G Loss: 0.7079306244850159\n",
      "Epoch 93, batch 7 D Loss: 1.3684378862380981, G Loss: 0.7182274460792542\n",
      "Epoch 93, batch 8 D Loss: 1.3871617317199707, G Loss: 0.707801878452301\n",
      "Epoch 93, batch 9 D Loss: 1.3822382688522339, G Loss: 0.7149751782417297\n",
      "Epoch 93, batch 10 D Loss: 1.3796050548553467, G Loss: 0.7090420722961426\n",
      "Epoch 93, batch 11 D Loss: 1.3722336292266846, G Loss: 0.7164077758789062\n",
      "Epoch 93, batch 12 D Loss: 1.369747281074524, G Loss: 0.7230292558670044\n",
      "Epoch 93, batch 13 D Loss: 1.378817081451416, G Loss: 0.7227200269699097\n",
      "Epoch 93, batch 14 D Loss: 1.389143943786621, G Loss: 0.7156938314437866\n",
      "Epoch 93, batch 15 D Loss: 1.388826847076416, G Loss: 0.7157957553863525\n",
      "Epoch 93, batch 16 D Loss: 1.391595721244812, G Loss: 0.7100958228111267\n",
      "Epoch 93, batch 17 D Loss: 1.3704404830932617, G Loss: 0.7243786454200745\n",
      "Epoch 93, batch 18 D Loss: 1.366593360900879, G Loss: 0.7262387871742249\n",
      "Epoch 93, batch 19 D Loss: 1.379225254058838, G Loss: 0.7253267168998718\n",
      "Epoch 93, batch 20 D Loss: 1.3754104375839233, G Loss: 0.7112707495689392\n",
      "Epoch 93, batch 21 D Loss: 1.3892139196395874, G Loss: 0.7101773619651794\n",
      "Epoch 93, batch 22 D Loss: 1.3587138652801514, G Loss: 0.7259616851806641\n",
      "Epoch 93, batch 23 D Loss: 1.3705148696899414, G Loss: 0.7235586047172546\n",
      "Epoch 93, batch 24 D Loss: 1.381301999092102, G Loss: 0.7269753813743591\n",
      "Epoch 93, batch 25 D Loss: 1.3698484897613525, G Loss: 0.7349424958229065\n",
      "Epoch 93, batch 26 D Loss: 1.3906924724578857, G Loss: 0.7121912240982056\n",
      "Epoch 93, batch 27 D Loss: 1.375325083732605, G Loss: 0.7260480523109436\n",
      "Epoch 93, batch 28 D Loss: 1.3731629848480225, G Loss: 0.7241840958595276\n",
      "Epoch 93, batch 29 D Loss: 1.3615456819534302, G Loss: 0.7238462567329407\n",
      "Epoch 93, batch 30 D Loss: 1.3617029190063477, G Loss: 0.7308733463287354\n",
      "Epoch 93, batch 31 D Loss: 1.3854970932006836, G Loss: 0.7193344831466675\n",
      "Epoch 93, batch 32 D Loss: 1.381663203239441, G Loss: 0.7192140817642212\n",
      "Epoch 93, batch 33 D Loss: 1.3833057880401611, G Loss: 0.7128468155860901\n",
      "Epoch 93, batch 34 D Loss: 1.3654167652130127, G Loss: 0.728004515171051\n",
      "Epoch 93, batch 35 D Loss: 1.3961079120635986, G Loss: 0.7165212035179138\n",
      "Epoch 93, batch 36 D Loss: 1.381995439529419, G Loss: 0.7215703725814819\n",
      "Epoch 93, batch 37 D Loss: 1.3689801692962646, G Loss: 0.7234649062156677\n",
      "Epoch 93, batch 38 D Loss: 1.390913724899292, G Loss: 0.7024729251861572\n",
      "Epoch 93, batch 39 D Loss: 1.3860715627670288, G Loss: 0.7105444073677063\n",
      "Epoch 93, batch 40 D Loss: 1.3754346370697021, G Loss: 0.7176074385643005\n",
      "Epoch 93, batch 41 D Loss: 1.4062097072601318, G Loss: 0.7058355808258057\n",
      "Epoch 93, batch 42 D Loss: 1.3951013088226318, G Loss: 0.696067750453949\n",
      "Epoch 93, batch 43 D Loss: 1.4027976989746094, G Loss: 0.7010640501976013\n",
      "Epoch 93, batch 44 D Loss: 1.3794702291488647, G Loss: 0.7101568579673767\n",
      "Epoch 93, batch 45 D Loss: 1.3631539344787598, G Loss: 0.7156737446784973\n",
      "Epoch 93, batch 46 D Loss: 1.395294427871704, G Loss: 0.6970329880714417\n",
      "Epoch 93, batch 47 D Loss: 1.4037209749221802, G Loss: 0.6914689540863037\n",
      "Epoch 93, batch 48 D Loss: 1.3837482929229736, G Loss: 0.702826976776123\n",
      "Epoch 93, batch 49 D Loss: 1.4041780233383179, G Loss: 0.6954029202461243\n",
      "Epoch 93, batch 50 D Loss: 1.383290410041809, G Loss: 0.6955223083496094\n",
      "Epoch 93, batch 51 D Loss: 1.419008731842041, G Loss: 0.6696109771728516\n",
      "Epoch 93, batch 52 D Loss: 1.3927282094955444, G Loss: 0.6829432845115662\n",
      "Epoch 93, batch 53 D Loss: 1.3915059566497803, G Loss: 0.6872657537460327\n",
      "Epoch 93, batch 54 D Loss: 1.38955557346344, G Loss: 0.6815140247344971\n",
      "Epoch 93, batch 55 D Loss: 1.394567608833313, G Loss: 0.6763735413551331\n",
      "Epoch 93, batch 56 D Loss: 1.4098212718963623, G Loss: 0.6733022928237915\n",
      "Epoch 93, batch 57 D Loss: 1.3833544254302979, G Loss: 0.676689863204956\n",
      "Epoch 93, batch 58 D Loss: 1.3586984872817993, G Loss: 0.703032910823822\n",
      "Epoch 93, batch 59 D Loss: 1.3858144283294678, G Loss: 0.6915690898895264\n",
      "Epoch 93, batch 60 D Loss: 1.3995370864868164, G Loss: 0.6739012002944946\n",
      "Epoch 93, batch 61 D Loss: 1.3794379234313965, G Loss: 0.6768304705619812\n",
      "Epoch 93, batch 62 D Loss: 1.390331745147705, G Loss: 0.666745126247406\n",
      "Epoch 93, batch 63 D Loss: 1.393061637878418, G Loss: 0.6816379427909851\n",
      "Epoch 93, batch 64 D Loss: 1.3994243144989014, G Loss: 0.670656144618988\n",
      "Epoch 93, batch 65 D Loss: 1.396600604057312, G Loss: 0.6756834387779236\n",
      "Epoch 93, batch 66 D Loss: 1.4125428199768066, G Loss: 0.6718854308128357\n",
      "Epoch 93, batch 67 D Loss: 1.381615161895752, G Loss: 0.6745482087135315\n",
      "Epoch 93, batch 68 D Loss: 1.3801651000976562, G Loss: 0.6838229298591614\n",
      "Epoch 93, batch 69 D Loss: 1.385123610496521, G Loss: 0.6745633482933044\n",
      "Epoch 93, batch 70 D Loss: 1.4053441286087036, G Loss: 0.6763147115707397\n",
      "Epoch 93, batch 71 D Loss: 1.389174461364746, G Loss: 0.6802266836166382\n",
      "Epoch 93, batch 72 D Loss: 1.405080795288086, G Loss: 0.6693339347839355\n",
      "Epoch 93, batch 73 D Loss: 1.4006708860397339, G Loss: 0.6679193377494812\n",
      "Epoch 93, batch 74 D Loss: 1.3894600868225098, G Loss: 0.6818803548812866\n",
      "Epoch 93, batch 75 D Loss: 1.3899240493774414, G Loss: 0.679073691368103\n",
      "Epoch 93, batch 76 D Loss: 1.398252010345459, G Loss: 0.6698483228683472\n",
      "Epoch 93, batch 77 D Loss: 1.3792688846588135, G Loss: 0.678752064704895\n",
      "Epoch 93, batch 78 D Loss: 1.3831236362457275, G Loss: 0.6825607419013977\n",
      "Epoch 93, batch 79 D Loss: 1.394106388092041, G Loss: 0.6733266711235046\n",
      "Epoch 93, batch 80 D Loss: 1.3836266994476318, G Loss: 0.6810558438301086\n",
      "Epoch 93, batch 81 D Loss: 1.4032546281814575, G Loss: 0.6687393188476562\n",
      "Epoch 93, batch 82 D Loss: 1.3879492282867432, G Loss: 0.6768491268157959\n",
      "Epoch 93, batch 83 D Loss: 1.378333568572998, G Loss: 0.6854334473609924\n",
      "Epoch 93, batch 84 D Loss: 1.4051120281219482, G Loss: 0.6659918427467346\n",
      "Epoch 93, batch 85 D Loss: 1.3812904357910156, G Loss: 0.6787997484207153\n",
      "Epoch 93, batch 86 D Loss: 1.393929362297058, G Loss: 0.6770310997962952\n",
      "Epoch 93, batch 87 D Loss: 1.385671615600586, G Loss: 0.6777147054672241\n",
      "Epoch 93, batch 88 D Loss: 1.4066531658172607, G Loss: 0.6760991811752319\n",
      "Epoch 93, batch 89 D Loss: 1.4034366607666016, G Loss: 0.6682485342025757\n",
      "Epoch 93, batch 90 D Loss: 1.3849884271621704, G Loss: 0.6779676675796509\n",
      "Epoch 93, batch 91 D Loss: 1.4015015363693237, G Loss: 0.6667778491973877\n",
      "Epoch 93, batch 92 D Loss: 1.3826327323913574, G Loss: 0.6749362349510193\n",
      "Epoch 93, batch 93 D Loss: 1.3681752681732178, G Loss: 0.6774980425834656\n",
      "Epoch 93, batch 94 D Loss: 1.3763129711151123, G Loss: 0.6802727580070496\n",
      "Epoch 93, batch 95 D Loss: 1.4020659923553467, G Loss: 0.664664626121521\n",
      "Epoch 93, batch 96 D Loss: 1.3960349559783936, G Loss: 0.6641464233398438\n",
      "Epoch 93, batch 97 D Loss: 1.3896536827087402, G Loss: 0.6564063429832458\n",
      "Epoch 93, batch 98 D Loss: 1.3791675567626953, G Loss: 0.6663472056388855\n",
      "Epoch 93, batch 99 D Loss: 1.3746577501296997, G Loss: 0.6686623096466064\n",
      "Epoch 93, batch 100 D Loss: 1.3905174732208252, G Loss: 0.6698419451713562\n",
      "Epoch 93, batch 101 D Loss: 1.3802433013916016, G Loss: 0.6663644313812256\n",
      "Epoch 93, batch 102 D Loss: 1.398637056350708, G Loss: 0.661560595035553\n",
      "Epoch 93, batch 103 D Loss: 1.3915013074874878, G Loss: 0.667244553565979\n",
      "Epoch 93, batch 104 D Loss: 1.3854509592056274, G Loss: 0.6743344068527222\n",
      "Epoch 93, batch 105 D Loss: 1.4042761325836182, G Loss: 0.6596021056175232\n",
      "Epoch 93, batch 106 D Loss: 1.3801493644714355, G Loss: 0.6740105152130127\n",
      "Epoch 93, batch 107 D Loss: 1.3677641153335571, G Loss: 0.6871710419654846\n",
      "Epoch 93, batch 108 D Loss: 1.3817827701568604, G Loss: 0.6744348406791687\n",
      "Epoch 93, batch 109 D Loss: 1.3973784446716309, G Loss: 0.675494372844696\n",
      "Epoch 93, batch 110 D Loss: 1.3922688961029053, G Loss: 0.6745534539222717\n",
      "Epoch 93, batch 111 D Loss: 1.3869757652282715, G Loss: 0.676004946231842\n",
      "Epoch 93, batch 112 D Loss: 1.378746509552002, G Loss: 0.6838911175727844\n",
      "Epoch 93, batch 113 D Loss: 1.3682622909545898, G Loss: 0.6823596954345703\n",
      "Epoch 93, batch 114 D Loss: 1.379969596862793, G Loss: 0.6790496706962585\n",
      "Epoch 93, batch 115 D Loss: 1.3684436082839966, G Loss: 0.6814900040626526\n",
      "Epoch 93, batch 116 D Loss: 1.375611662864685, G Loss: 0.6751381754875183\n",
      "Epoch 93, batch 117 D Loss: 1.3799967765808105, G Loss: 0.6854406595230103\n",
      "Epoch 93, batch 118 D Loss: 1.39512300491333, G Loss: 0.6899858713150024\n",
      "Epoch 93, batch 119 D Loss: 1.3795114755630493, G Loss: 0.6864411234855652\n",
      "Epoch 93, batch 120 D Loss: 1.392317771911621, G Loss: 0.6651552319526672\n",
      "Epoch 93, batch 121 D Loss: 1.3867619037628174, G Loss: 0.6926896572113037\n",
      "Epoch 93, batch 122 D Loss: 1.385419249534607, G Loss: 0.687334418296814\n",
      "Epoch 93, batch 123 D Loss: 1.3756803274154663, G Loss: 0.6900315284729004\n",
      "Epoch 93, batch 124 D Loss: 1.3784483671188354, G Loss: 0.6949985027313232\n",
      "Epoch 93, batch 125 D Loss: 1.4046945571899414, G Loss: 0.6799975037574768\n",
      "Epoch 93, batch 126 D Loss: 1.3913969993591309, G Loss: 0.6860795617103577\n",
      "Epoch 93, batch 127 D Loss: 1.3839776515960693, G Loss: 0.6881605386734009\n",
      "Epoch 93, batch 128 D Loss: 1.412163496017456, G Loss: 0.6692306399345398\n",
      "Epoch 93, batch 129 D Loss: 1.371946096420288, G Loss: 0.7011024951934814\n",
      "Epoch 93, batch 130 D Loss: 1.3971014022827148, G Loss: 0.6906103491783142\n",
      "Epoch 93, batch 131 D Loss: 1.4025615453720093, G Loss: 0.6750752925872803\n",
      "Epoch 93, batch 132 D Loss: 1.3717169761657715, G Loss: 0.6883071660995483\n",
      "Epoch 93, batch 133 D Loss: 1.3852870464324951, G Loss: 0.6758572459220886\n",
      "Epoch 93, batch 134 D Loss: 1.366999626159668, G Loss: 0.6866819858551025\n",
      "Epoch 93, batch 135 D Loss: 1.4045641422271729, G Loss: 0.6717978715896606\n",
      "Epoch 93, batch 136 D Loss: 1.3958293199539185, G Loss: 0.6727856993675232\n",
      "Epoch 93, batch 137 D Loss: 1.3806571960449219, G Loss: 0.6841943860054016\n",
      "Epoch 93, batch 138 D Loss: 1.3853439092636108, G Loss: 0.679514467716217\n",
      "Epoch 93, batch 139 D Loss: 1.39473295211792, G Loss: 0.6632934808731079\n",
      "Epoch 93, batch 140 D Loss: 1.3875993490219116, G Loss: 0.6848166584968567\n",
      "Epoch 93, batch 141 D Loss: 1.3976935148239136, G Loss: 0.6754611134529114\n",
      "Epoch 93, batch 142 D Loss: 1.3836779594421387, G Loss: 0.6801303625106812\n",
      "Epoch 93, batch 143 D Loss: 1.3945128917694092, G Loss: 0.6735286712646484\n",
      "Epoch 93, batch 144 D Loss: 1.3826991319656372, G Loss: 0.6788589358329773\n",
      "Epoch 93, batch 145 D Loss: 1.3724370002746582, G Loss: 0.6922100782394409\n",
      "Epoch 93, batch 146 D Loss: 1.3843994140625, G Loss: 0.6797093152999878\n",
      "Epoch 93, batch 147 D Loss: 1.4030656814575195, G Loss: 0.6758744120597839\n",
      "Epoch 93, batch 148 D Loss: 1.3848724365234375, G Loss: 0.693087637424469\n",
      "Epoch 93, batch 149 D Loss: 1.3857972621917725, G Loss: 0.6927181482315063\n",
      "Epoch 93, batch 150 D Loss: 1.3911409378051758, G Loss: 0.6885360479354858\n",
      "Epoch 93, batch 151 D Loss: 1.3750672340393066, G Loss: 0.6974704265594482\n",
      "Epoch 93, batch 152 D Loss: 1.37831711769104, G Loss: 0.6927826404571533\n",
      "Epoch 93, batch 153 D Loss: 1.3854522705078125, G Loss: 0.6915909051895142\n",
      "Epoch 93, batch 154 D Loss: 1.394512414932251, G Loss: 0.6860491037368774\n",
      "Epoch 93, batch 155 D Loss: 1.3837220668792725, G Loss: 0.6918655633926392\n",
      "Epoch 93, batch 156 D Loss: 1.387282133102417, G Loss: 0.6962963342666626\n",
      "Epoch 93, batch 157 D Loss: 1.380124568939209, G Loss: 0.6899836659431458\n",
      "Epoch 93, batch 158 D Loss: 1.3672984838485718, G Loss: 0.7020858526229858\n",
      "Epoch 93, batch 159 D Loss: 1.3842682838439941, G Loss: 0.702075183391571\n",
      "Epoch 93, batch 160 D Loss: 1.3919790983200073, G Loss: 0.6925850510597229\n",
      "Epoch 93, batch 161 D Loss: 1.3894248008728027, G Loss: 0.7001156806945801\n",
      "Epoch 93, batch 162 D Loss: 1.399531364440918, G Loss: 0.6912999749183655\n",
      "Epoch 93, batch 163 D Loss: 1.3797204494476318, G Loss: 0.7050067186355591\n",
      "Epoch 93, batch 164 D Loss: 1.368774175643921, G Loss: 0.7123146057128906\n",
      "Epoch 93, batch 165 D Loss: 1.3896126747131348, G Loss: 0.6882460713386536\n",
      "Epoch 93, batch 166 D Loss: 1.394364356994629, G Loss: 0.6851187944412231\n",
      "Epoch 93, batch 167 D Loss: 1.3668937683105469, G Loss: 0.7047351002693176\n",
      "Epoch 93, batch 168 D Loss: 1.3743431568145752, G Loss: 0.697287380695343\n",
      "Epoch 93, batch 169 D Loss: 1.3802964687347412, G Loss: 0.6928266286849976\n",
      "Epoch 93, batch 170 D Loss: 1.388716459274292, G Loss: 0.6962182521820068\n",
      "Epoch 93, batch 171 D Loss: 1.3906776905059814, G Loss: 0.7051123976707458\n",
      "Epoch 93, batch 172 D Loss: 1.3966729640960693, G Loss: 0.6932418942451477\n",
      "Epoch 93, batch 173 D Loss: 1.3911781311035156, G Loss: 0.6928451657295227\n",
      "Epoch 93, batch 174 D Loss: 1.3984919786453247, G Loss: 0.6871868968009949\n",
      "Epoch 93, batch 175 D Loss: 1.3900742530822754, G Loss: 0.699939489364624\n",
      "Epoch 93, batch 176 D Loss: 1.400973916053772, G Loss: 0.6938011050224304\n",
      "Epoch 93, batch 177 D Loss: 1.3881882429122925, G Loss: 0.6904245615005493\n",
      "Epoch 93, batch 178 D Loss: 1.3964149951934814, G Loss: 0.682091474533081\n",
      "Epoch 93, batch 179 D Loss: 1.4017152786254883, G Loss: 0.688008725643158\n",
      "Epoch 93, batch 180 D Loss: 1.3993735313415527, G Loss: 0.6968473196029663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, batch 181 D Loss: 1.3861191272735596, G Loss: 0.6923000812530518\n",
      "Epoch 93, batch 182 D Loss: 1.3951469659805298, G Loss: 0.6929850578308105\n",
      "Epoch 93, batch 183 D Loss: 1.3892730474472046, G Loss: 0.6951463222503662\n",
      "Epoch 93, batch 184 D Loss: 1.3583654165267944, G Loss: 0.712496280670166\n",
      "Epoch 93, batch 185 D Loss: 1.3713794946670532, G Loss: 0.698411762714386\n",
      "Epoch 93, batch 186 D Loss: 1.3670809268951416, G Loss: 0.6999871134757996\n",
      "Epoch 93, batch 187 D Loss: 1.3889846801757812, G Loss: 0.6923936605453491\n",
      "Epoch 93, batch 188 D Loss: 1.392890453338623, G Loss: 0.6920012831687927\n",
      "Epoch 93, batch 189 D Loss: 1.3894213438034058, G Loss: 0.7001820206642151\n",
      "Epoch 93, batch 190 D Loss: 1.3865845203399658, G Loss: 0.6980315446853638\n",
      "Epoch 93, batch 191 D Loss: 1.3741803169250488, G Loss: 0.6887338757514954\n",
      "Epoch 93, batch 192 D Loss: 1.4153181314468384, G Loss: 0.6810625195503235\n",
      "Epoch 93, batch 193 D Loss: 1.3915514945983887, G Loss: 0.686962366104126\n",
      "Epoch 93, batch 194 D Loss: 1.3869996070861816, G Loss: 0.7033042907714844\n",
      "Epoch 93, batch 195 D Loss: 1.4174683094024658, G Loss: 0.6917444467544556\n",
      "Epoch 93, batch 196 D Loss: 1.3889238834381104, G Loss: 0.7012827396392822\n",
      "Epoch 93, batch 197 D Loss: 1.3828926086425781, G Loss: 0.701963484287262\n",
      "Epoch 93, batch 198 D Loss: 1.3795744180679321, G Loss: 0.7041438817977905\n",
      "Epoch 93, batch 199 D Loss: 1.3773976564407349, G Loss: 0.7062126398086548\n",
      "Epoch 93, batch 200 D Loss: 1.4020617008209229, G Loss: 0.6934996843338013\n",
      "Epoch 94, batch 1 D Loss: 1.4039385318756104, G Loss: 0.6930320262908936\n",
      "Epoch 94, batch 2 D Loss: 1.4005913734436035, G Loss: 0.6891996264457703\n",
      "Epoch 94, batch 3 D Loss: 1.4010725021362305, G Loss: 0.6895812749862671\n",
      "Epoch 94, batch 4 D Loss: 1.3898173570632935, G Loss: 0.6966354250907898\n",
      "Epoch 94, batch 5 D Loss: 1.3942937850952148, G Loss: 0.7037686109542847\n",
      "Epoch 94, batch 6 D Loss: 1.3975651264190674, G Loss: 0.699429452419281\n",
      "Epoch 94, batch 7 D Loss: 1.4022626876831055, G Loss: 0.6908654570579529\n",
      "Epoch 94, batch 8 D Loss: 1.3902226686477661, G Loss: 0.7069027423858643\n",
      "Epoch 94, batch 9 D Loss: 1.3987582921981812, G Loss: 0.7073243856430054\n",
      "Epoch 94, batch 10 D Loss: 1.3943486213684082, G Loss: 0.6994867920875549\n",
      "Epoch 94, batch 11 D Loss: 1.3925635814666748, G Loss: 0.7068485021591187\n",
      "Epoch 94, batch 12 D Loss: 1.3831441402435303, G Loss: 0.7114322781562805\n",
      "Epoch 94, batch 13 D Loss: 1.380037784576416, G Loss: 0.7154154181480408\n",
      "Epoch 94, batch 14 D Loss: 1.396249532699585, G Loss: 0.7156802415847778\n",
      "Epoch 94, batch 15 D Loss: 1.3846449851989746, G Loss: 0.7116400003433228\n",
      "Epoch 94, batch 16 D Loss: 1.3918371200561523, G Loss: 0.7132889032363892\n",
      "Epoch 94, batch 17 D Loss: 1.3988349437713623, G Loss: 0.7131686210632324\n",
      "Epoch 94, batch 18 D Loss: 1.3897051811218262, G Loss: 0.7229081988334656\n",
      "Epoch 94, batch 19 D Loss: 1.3877856731414795, G Loss: 0.7158315777778625\n",
      "Epoch 94, batch 20 D Loss: 1.3736448287963867, G Loss: 0.725484311580658\n",
      "Epoch 94, batch 21 D Loss: 1.374619483947754, G Loss: 0.7231252789497375\n",
      "Epoch 94, batch 22 D Loss: 1.3877629041671753, G Loss: 0.7151606678962708\n",
      "Epoch 94, batch 23 D Loss: 1.3965123891830444, G Loss: 0.713481068611145\n",
      "Epoch 94, batch 24 D Loss: 1.3878390789031982, G Loss: 0.7194317579269409\n",
      "Epoch 94, batch 25 D Loss: 1.390270471572876, G Loss: 0.7169857621192932\n",
      "Epoch 94, batch 26 D Loss: 1.3896815776824951, G Loss: 0.7157438397407532\n",
      "Epoch 94, batch 27 D Loss: 1.4106342792510986, G Loss: 0.7080274820327759\n",
      "Epoch 94, batch 28 D Loss: 1.3929486274719238, G Loss: 0.7192655205726624\n",
      "Epoch 94, batch 29 D Loss: 1.3933851718902588, G Loss: 0.714264988899231\n",
      "Epoch 94, batch 30 D Loss: 1.3834540843963623, G Loss: 0.7200301885604858\n",
      "Epoch 94, batch 31 D Loss: 1.3860507011413574, G Loss: 0.709883451461792\n",
      "Epoch 94, batch 32 D Loss: 1.386027455329895, G Loss: 0.7127290964126587\n",
      "Epoch 94, batch 33 D Loss: 1.4019739627838135, G Loss: 0.7063443660736084\n",
      "Epoch 94, batch 34 D Loss: 1.3910751342773438, G Loss: 0.7112093567848206\n",
      "Epoch 94, batch 35 D Loss: 1.3816863298416138, G Loss: 0.7103533744812012\n",
      "Epoch 94, batch 36 D Loss: 1.3883212804794312, G Loss: 0.7128405570983887\n",
      "Epoch 94, batch 37 D Loss: 1.3829196691513062, G Loss: 0.7171724438667297\n",
      "Epoch 94, batch 38 D Loss: 1.3924157619476318, G Loss: 0.7134437561035156\n",
      "Epoch 94, batch 39 D Loss: 1.3887572288513184, G Loss: 0.717183530330658\n",
      "Epoch 94, batch 40 D Loss: 1.3915451765060425, G Loss: 0.7106305956840515\n",
      "Epoch 94, batch 41 D Loss: 1.3828202486038208, G Loss: 0.7269855737686157\n",
      "Epoch 94, batch 42 D Loss: 1.3895337581634521, G Loss: 0.7187058329582214\n",
      "Epoch 94, batch 43 D Loss: 1.3803305625915527, G Loss: 0.7275694012641907\n",
      "Epoch 94, batch 44 D Loss: 1.390878677368164, G Loss: 0.7182537317276001\n",
      "Epoch 94, batch 45 D Loss: 1.3842012882232666, G Loss: 0.7168216109275818\n",
      "Epoch 94, batch 46 D Loss: 1.3804261684417725, G Loss: 0.7210331559181213\n",
      "Epoch 94, batch 47 D Loss: 1.377603530883789, G Loss: 0.7212942242622375\n",
      "Epoch 94, batch 48 D Loss: 1.3918659687042236, G Loss: 0.7148277163505554\n",
      "Epoch 94, batch 49 D Loss: 1.3849799633026123, G Loss: 0.7214024066925049\n",
      "Epoch 94, batch 50 D Loss: 1.3760595321655273, G Loss: 0.7330718040466309\n",
      "Epoch 94, batch 51 D Loss: 1.392030954360962, G Loss: 0.7212947607040405\n",
      "Epoch 94, batch 52 D Loss: 1.3939011096954346, G Loss: 0.7179587483406067\n",
      "Epoch 94, batch 53 D Loss: 1.3529577255249023, G Loss: 0.7406157851219177\n",
      "Epoch 94, batch 54 D Loss: 1.3764796257019043, G Loss: 0.7260695695877075\n",
      "Epoch 94, batch 55 D Loss: 1.3998980522155762, G Loss: 0.7096667289733887\n",
      "Epoch 94, batch 56 D Loss: 1.368900179862976, G Loss: 0.7423831820487976\n",
      "Epoch 94, batch 57 D Loss: 1.3892021179199219, G Loss: 0.7197802066802979\n",
      "Epoch 94, batch 58 D Loss: 1.3672541379928589, G Loss: 0.7403886914253235\n",
      "Epoch 94, batch 59 D Loss: 1.3737232685089111, G Loss: 0.7227846384048462\n",
      "Epoch 94, batch 60 D Loss: 1.385521411895752, G Loss: 0.7292277812957764\n",
      "Epoch 94, batch 61 D Loss: 1.3974854946136475, G Loss: 0.7191202640533447\n",
      "Epoch 94, batch 62 D Loss: 1.3770241737365723, G Loss: 0.7162273526191711\n",
      "Epoch 94, batch 63 D Loss: 1.4017868041992188, G Loss: 0.7145843505859375\n",
      "Epoch 94, batch 64 D Loss: 1.3945586681365967, G Loss: 0.7117748260498047\n",
      "Epoch 94, batch 65 D Loss: 1.38241446018219, G Loss: 0.7201189994812012\n",
      "Epoch 94, batch 66 D Loss: 1.3964264392852783, G Loss: 0.6995540857315063\n",
      "Epoch 94, batch 67 D Loss: 1.4014742374420166, G Loss: 0.6942015886306763\n",
      "Epoch 94, batch 68 D Loss: 1.3922946453094482, G Loss: 0.7011235952377319\n",
      "Epoch 94, batch 69 D Loss: 1.3929157257080078, G Loss: 0.713807225227356\n",
      "Epoch 94, batch 70 D Loss: 1.4022172689437866, G Loss: 0.702393114566803\n",
      "Epoch 94, batch 71 D Loss: 1.3822858333587646, G Loss: 0.708143949508667\n",
      "Epoch 94, batch 72 D Loss: 1.397423267364502, G Loss: 0.7144315838813782\n",
      "Epoch 94, batch 73 D Loss: 1.3707447052001953, G Loss: 0.723151445388794\n",
      "Epoch 94, batch 74 D Loss: 1.397756576538086, G Loss: 0.700614333152771\n",
      "Epoch 94, batch 75 D Loss: 1.381133794784546, G Loss: 0.7089750170707703\n",
      "Epoch 94, batch 76 D Loss: 1.3985595703125, G Loss: 0.6891866326332092\n",
      "Epoch 94, batch 77 D Loss: 1.3762803077697754, G Loss: 0.7032263875007629\n",
      "Epoch 94, batch 78 D Loss: 1.3989717960357666, G Loss: 0.6851742267608643\n",
      "Epoch 94, batch 79 D Loss: 1.3940343856811523, G Loss: 0.6859896779060364\n",
      "Epoch 94, batch 80 D Loss: 1.3928375244140625, G Loss: 0.6968744397163391\n",
      "Epoch 94, batch 81 D Loss: 1.385472059249878, G Loss: 0.6943547129631042\n",
      "Epoch 94, batch 82 D Loss: 1.3767006397247314, G Loss: 0.6960600018501282\n",
      "Epoch 94, batch 83 D Loss: 1.3846802711486816, G Loss: 0.6937715411186218\n",
      "Epoch 94, batch 84 D Loss: 1.3876595497131348, G Loss: 0.6753799915313721\n",
      "Epoch 94, batch 85 D Loss: 1.3862024545669556, G Loss: 0.6866466999053955\n",
      "Epoch 94, batch 86 D Loss: 1.393908977508545, G Loss: 0.674370288848877\n",
      "Epoch 94, batch 87 D Loss: 1.3849685192108154, G Loss: 0.6818435788154602\n",
      "Epoch 94, batch 88 D Loss: 1.3927063941955566, G Loss: 0.6787626147270203\n",
      "Epoch 94, batch 89 D Loss: 1.3889260292053223, G Loss: 0.6760042309761047\n",
      "Epoch 94, batch 90 D Loss: 1.3959217071533203, G Loss: 0.6747468709945679\n",
      "Epoch 94, batch 91 D Loss: 1.3917917013168335, G Loss: 0.6687564253807068\n",
      "Epoch 94, batch 92 D Loss: 1.3805992603302002, G Loss: 0.6745690703392029\n",
      "Epoch 94, batch 93 D Loss: 1.3819303512573242, G Loss: 0.6778301000595093\n",
      "Epoch 94, batch 94 D Loss: 1.3784751892089844, G Loss: 0.6754915118217468\n",
      "Epoch 94, batch 95 D Loss: 1.3868603706359863, G Loss: 0.6721087694168091\n",
      "Epoch 94, batch 96 D Loss: 1.3850343227386475, G Loss: 0.6694543957710266\n",
      "Epoch 94, batch 97 D Loss: 1.3958243131637573, G Loss: 0.6678324937820435\n",
      "Epoch 94, batch 98 D Loss: 1.3828821182250977, G Loss: 0.6780668497085571\n",
      "Epoch 94, batch 99 D Loss: 1.38922119140625, G Loss: 0.6688657402992249\n",
      "Epoch 94, batch 100 D Loss: 1.3809784650802612, G Loss: 0.6792741417884827\n",
      "Epoch 94, batch 101 D Loss: 1.3904260396957397, G Loss: 0.6787463426589966\n",
      "Epoch 94, batch 102 D Loss: 1.3753297328948975, G Loss: 0.6857509016990662\n",
      "Epoch 94, batch 103 D Loss: 1.3803496360778809, G Loss: 0.6786172389984131\n",
      "Epoch 94, batch 104 D Loss: 1.3917274475097656, G Loss: 0.6708738207817078\n",
      "Epoch 94, batch 105 D Loss: 1.3854764699935913, G Loss: 0.6783970594406128\n",
      "Epoch 94, batch 106 D Loss: 1.387650728225708, G Loss: 0.6829965114593506\n",
      "Epoch 94, batch 107 D Loss: 1.379774808883667, G Loss: 0.6823828816413879\n",
      "Epoch 94, batch 108 D Loss: 1.3924543857574463, G Loss: 0.6667004227638245\n",
      "Epoch 94, batch 109 D Loss: 1.3969296216964722, G Loss: 0.6762265563011169\n",
      "Epoch 94, batch 110 D Loss: 1.3891842365264893, G Loss: 0.6837248802185059\n",
      "Epoch 94, batch 111 D Loss: 1.3881875276565552, G Loss: 0.6857768297195435\n",
      "Epoch 94, batch 112 D Loss: 1.3806110620498657, G Loss: 0.6896059513092041\n",
      "Epoch 94, batch 113 D Loss: 1.3891271352767944, G Loss: 0.686506986618042\n",
      "Epoch 94, batch 114 D Loss: 1.387822151184082, G Loss: 0.6867972016334534\n",
      "Epoch 94, batch 115 D Loss: 1.397364854812622, G Loss: 0.6772424578666687\n",
      "Epoch 94, batch 116 D Loss: 1.3990488052368164, G Loss: 0.6854691505432129\n",
      "Epoch 94, batch 117 D Loss: 1.3801578283309937, G Loss: 0.6945526599884033\n",
      "Epoch 94, batch 118 D Loss: 1.3721431493759155, G Loss: 0.6952575445175171\n",
      "Epoch 94, batch 119 D Loss: 1.3917937278747559, G Loss: 0.6878397464752197\n",
      "Epoch 94, batch 120 D Loss: 1.379865050315857, G Loss: 0.7040955424308777\n",
      "Epoch 94, batch 121 D Loss: 1.379652976989746, G Loss: 0.694989800453186\n",
      "Epoch 94, batch 122 D Loss: 1.3862674236297607, G Loss: 0.6951716542243958\n",
      "Epoch 94, batch 123 D Loss: 1.3864715099334717, G Loss: 0.6983294486999512\n",
      "Epoch 94, batch 124 D Loss: 1.3888616561889648, G Loss: 0.6936240196228027\n",
      "Epoch 94, batch 125 D Loss: 1.3919051885604858, G Loss: 0.6924466490745544\n",
      "Epoch 94, batch 126 D Loss: 1.3831641674041748, G Loss: 0.7044679522514343\n",
      "Epoch 94, batch 127 D Loss: 1.389678955078125, G Loss: 0.6967096924781799\n",
      "Epoch 94, batch 128 D Loss: 1.387096643447876, G Loss: 0.7052819132804871\n",
      "Epoch 94, batch 129 D Loss: 1.3747209310531616, G Loss: 0.703818142414093\n",
      "Epoch 94, batch 130 D Loss: 1.3842082023620605, G Loss: 0.698814868927002\n",
      "Epoch 94, batch 131 D Loss: 1.387526512145996, G Loss: 0.6988300085067749\n",
      "Epoch 94, batch 132 D Loss: 1.3758107423782349, G Loss: 0.7000185251235962\n",
      "Epoch 94, batch 133 D Loss: 1.3838162422180176, G Loss: 0.6999024748802185\n",
      "Epoch 94, batch 134 D Loss: 1.4029686450958252, G Loss: 0.6903997659683228\n",
      "Epoch 94, batch 135 D Loss: 1.3759913444519043, G Loss: 0.7120620012283325\n",
      "Epoch 94, batch 136 D Loss: 1.379894733428955, G Loss: 0.6998258233070374\n",
      "Epoch 94, batch 137 D Loss: 1.3912346363067627, G Loss: 0.7034019231796265\n",
      "Epoch 94, batch 138 D Loss: 1.3835744857788086, G Loss: 0.7120649814605713\n",
      "Epoch 94, batch 139 D Loss: 1.3969011306762695, G Loss: 0.6972835659980774\n",
      "Epoch 94, batch 140 D Loss: 1.3841313123703003, G Loss: 0.6902523040771484\n",
      "Epoch 94, batch 141 D Loss: 1.390718936920166, G Loss: 0.7006732821464539\n",
      "Epoch 94, batch 142 D Loss: 1.4008398056030273, G Loss: 0.6928828954696655\n",
      "Epoch 94, batch 143 D Loss: 1.39043390750885, G Loss: 0.6999948024749756\n",
      "Epoch 94, batch 144 D Loss: 1.3858826160430908, G Loss: 0.7001895308494568\n",
      "Epoch 94, batch 145 D Loss: 1.408491849899292, G Loss: 0.6948696374893188\n",
      "Epoch 94, batch 146 D Loss: 1.3877947330474854, G Loss: 0.6977331042289734\n",
      "Epoch 94, batch 147 D Loss: 1.3900041580200195, G Loss: 0.6971753835678101\n",
      "Epoch 94, batch 148 D Loss: 1.3947811126708984, G Loss: 0.6909645795822144\n",
      "Epoch 94, batch 149 D Loss: 1.3938093185424805, G Loss: 0.690744161605835\n",
      "Epoch 94, batch 150 D Loss: 1.3935418128967285, G Loss: 0.6847848296165466\n",
      "Epoch 94, batch 151 D Loss: 1.3952898979187012, G Loss: 0.6908183097839355\n",
      "Epoch 94, batch 152 D Loss: 1.3872485160827637, G Loss: 0.6941227912902832\n",
      "Epoch 94, batch 153 D Loss: 1.3858680725097656, G Loss: 0.6910772919654846\n",
      "Epoch 94, batch 154 D Loss: 1.3927669525146484, G Loss: 0.6869519948959351\n",
      "Epoch 94, batch 155 D Loss: 1.395958423614502, G Loss: 0.6853995323181152\n",
      "Epoch 94, batch 156 D Loss: 1.3920389413833618, G Loss: 0.688209593296051\n",
      "Epoch 94, batch 157 D Loss: 1.3839386701583862, G Loss: 0.6930825114250183\n",
      "Epoch 94, batch 158 D Loss: 1.3776063919067383, G Loss: 0.6906147599220276\n",
      "Epoch 94, batch 159 D Loss: 1.389728307723999, G Loss: 0.6909196376800537\n",
      "Epoch 94, batch 160 D Loss: 1.3814703226089478, G Loss: 0.6942318081855774\n",
      "Epoch 94, batch 161 D Loss: 1.3834799528121948, G Loss: 0.6864662766456604\n",
      "Epoch 94, batch 162 D Loss: 1.3909120559692383, G Loss: 0.6779976487159729\n",
      "Epoch 94, batch 163 D Loss: 1.3855047225952148, G Loss: 0.686260998249054\n",
      "Epoch 94, batch 164 D Loss: 1.3767051696777344, G Loss: 0.6888280510902405\n",
      "Epoch 94, batch 165 D Loss: 1.3869600296020508, G Loss: 0.6881034970283508\n",
      "Epoch 94, batch 166 D Loss: 1.3916881084442139, G Loss: 0.677338182926178\n",
      "Epoch 94, batch 167 D Loss: 1.397733449935913, G Loss: 0.6788157820701599\n",
      "Epoch 94, batch 168 D Loss: 1.387859582901001, G Loss: 0.6898077130317688\n",
      "Epoch 94, batch 169 D Loss: 1.386070966720581, G Loss: 0.6850780248641968\n",
      "Epoch 94, batch 170 D Loss: 1.3778111934661865, G Loss: 0.6886871457099915\n",
      "Epoch 94, batch 171 D Loss: 1.375373363494873, G Loss: 0.6961297392845154\n",
      "Epoch 94, batch 172 D Loss: 1.3754076957702637, G Loss: 0.6913637518882751\n",
      "Epoch 94, batch 173 D Loss: 1.3814942836761475, G Loss: 0.689237654209137\n",
      "Epoch 94, batch 174 D Loss: 1.3946831226348877, G Loss: 0.686129093170166\n",
      "Epoch 94, batch 175 D Loss: 1.3800926208496094, G Loss: 0.68479323387146\n",
      "Epoch 94, batch 176 D Loss: 1.3913730382919312, G Loss: 0.6840999126434326\n",
      "Epoch 94, batch 177 D Loss: 1.3875130414962769, G Loss: 0.6815604567527771\n",
      "Epoch 94, batch 178 D Loss: 1.389502763748169, G Loss: 0.6863049864768982\n",
      "Epoch 94, batch 179 D Loss: 1.3878040313720703, G Loss: 0.6835115551948547\n",
      "Epoch 94, batch 180 D Loss: 1.3850650787353516, G Loss: 0.6894248723983765\n",
      "Epoch 94, batch 181 D Loss: 1.3787562847137451, G Loss: 0.7054994106292725\n",
      "Epoch 94, batch 182 D Loss: 1.389505386352539, G Loss: 0.6932214498519897\n",
      "Epoch 94, batch 183 D Loss: 1.3908915519714355, G Loss: 0.690194845199585\n",
      "Epoch 94, batch 184 D Loss: 1.3830406665802002, G Loss: 0.6940858364105225\n",
      "Epoch 94, batch 185 D Loss: 1.3741434812545776, G Loss: 0.7124089598655701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, batch 186 D Loss: 1.4064435958862305, G Loss: 0.6755330562591553\n",
      "Epoch 94, batch 187 D Loss: 1.3824758529663086, G Loss: 0.7015144228935242\n",
      "Epoch 94, batch 188 D Loss: 1.3877711296081543, G Loss: 0.6968234777450562\n",
      "Epoch 94, batch 189 D Loss: 1.4067225456237793, G Loss: 0.6769131422042847\n",
      "Epoch 94, batch 190 D Loss: 1.3814032077789307, G Loss: 0.6991149187088013\n",
      "Epoch 94, batch 191 D Loss: 1.4014028310775757, G Loss: 0.6760286092758179\n",
      "Epoch 94, batch 192 D Loss: 1.390900731086731, G Loss: 0.6980816125869751\n",
      "Epoch 94, batch 193 D Loss: 1.3900288343429565, G Loss: 0.687835156917572\n",
      "Epoch 94, batch 194 D Loss: 1.3807252645492554, G Loss: 0.6986792683601379\n",
      "Epoch 94, batch 195 D Loss: 1.3999521732330322, G Loss: 0.6947664022445679\n",
      "Epoch 94, batch 196 D Loss: 1.3881937265396118, G Loss: 0.6906923055648804\n",
      "Epoch 94, batch 197 D Loss: 1.371675968170166, G Loss: 0.7141554951667786\n",
      "Epoch 94, batch 198 D Loss: 1.3820888996124268, G Loss: 0.69777512550354\n",
      "Epoch 94, batch 199 D Loss: 1.3827767372131348, G Loss: 0.7012457251548767\n",
      "Epoch 94, batch 200 D Loss: 1.387744426727295, G Loss: 0.698213517665863\n",
      "Epoch 95, batch 1 D Loss: 1.3912737369537354, G Loss: 0.6949083805084229\n",
      "Epoch 95, batch 2 D Loss: 1.377112627029419, G Loss: 0.7114866375923157\n",
      "Epoch 95, batch 3 D Loss: 1.3811769485473633, G Loss: 0.7042068243026733\n",
      "Epoch 95, batch 4 D Loss: 1.397308588027954, G Loss: 0.7013334035873413\n",
      "Epoch 95, batch 5 D Loss: 1.3813819885253906, G Loss: 0.7075394988059998\n",
      "Epoch 95, batch 6 D Loss: 1.3961551189422607, G Loss: 0.6882339715957642\n",
      "Epoch 95, batch 7 D Loss: 1.3924707174301147, G Loss: 0.701755166053772\n",
      "Epoch 95, batch 8 D Loss: 1.3828599452972412, G Loss: 0.706397533416748\n",
      "Epoch 95, batch 9 D Loss: 1.3755757808685303, G Loss: 0.7130821347236633\n",
      "Epoch 95, batch 10 D Loss: 1.3850369453430176, G Loss: 0.6968440413475037\n",
      "Epoch 95, batch 11 D Loss: 1.3872432708740234, G Loss: 0.7082279324531555\n",
      "Epoch 95, batch 12 D Loss: 1.3841583728790283, G Loss: 0.7019665241241455\n",
      "Epoch 95, batch 13 D Loss: 1.3831301927566528, G Loss: 0.7018528580665588\n",
      "Epoch 95, batch 14 D Loss: 1.3926644325256348, G Loss: 0.6961718201637268\n",
      "Epoch 95, batch 15 D Loss: 1.3809454441070557, G Loss: 0.7063980102539062\n",
      "Epoch 95, batch 16 D Loss: 1.3831405639648438, G Loss: 0.6976608037948608\n",
      "Epoch 95, batch 17 D Loss: 1.393337607383728, G Loss: 0.694108247756958\n",
      "Epoch 95, batch 18 D Loss: 1.373868465423584, G Loss: 0.7011600732803345\n",
      "Epoch 95, batch 19 D Loss: 1.3850626945495605, G Loss: 0.6950305700302124\n",
      "Epoch 95, batch 20 D Loss: 1.3879585266113281, G Loss: 0.6944560408592224\n",
      "Epoch 95, batch 21 D Loss: 1.385085105895996, G Loss: 0.6997265815734863\n",
      "Epoch 95, batch 22 D Loss: 1.3826398849487305, G Loss: 0.6928627490997314\n",
      "Epoch 95, batch 23 D Loss: 1.3696521520614624, G Loss: 0.7031123638153076\n",
      "Epoch 95, batch 24 D Loss: 1.3930127620697021, G Loss: 0.6919432282447815\n",
      "Epoch 95, batch 25 D Loss: 1.3861351013183594, G Loss: 0.6956431865692139\n",
      "Epoch 95, batch 26 D Loss: 1.3861268758773804, G Loss: 0.6893054246902466\n",
      "Epoch 95, batch 27 D Loss: 1.3828322887420654, G Loss: 0.6870785355567932\n",
      "Epoch 95, batch 28 D Loss: 1.3959767818450928, G Loss: 0.6755356788635254\n",
      "Epoch 95, batch 29 D Loss: 1.3825969696044922, G Loss: 0.6837806701660156\n",
      "Epoch 95, batch 30 D Loss: 1.4043638706207275, G Loss: 0.6814211010932922\n",
      "Epoch 95, batch 31 D Loss: 1.381717324256897, G Loss: 0.6923936605453491\n",
      "Epoch 95, batch 32 D Loss: 1.3811070919036865, G Loss: 0.6926637291908264\n",
      "Epoch 95, batch 33 D Loss: 1.3807607889175415, G Loss: 0.6862713098526001\n",
      "Epoch 95, batch 34 D Loss: 1.3854107856750488, G Loss: 0.6891728043556213\n",
      "Epoch 95, batch 35 D Loss: 1.3700790405273438, G Loss: 0.7018657922744751\n",
      "Epoch 95, batch 36 D Loss: 1.3960164785385132, G Loss: 0.6760085821151733\n",
      "Epoch 95, batch 37 D Loss: 1.3700934648513794, G Loss: 0.6893441677093506\n",
      "Epoch 95, batch 38 D Loss: 1.3888475894927979, G Loss: 0.6767734289169312\n",
      "Epoch 95, batch 39 D Loss: 1.3894641399383545, G Loss: 0.6814002394676208\n",
      "Epoch 95, batch 40 D Loss: 1.3791611194610596, G Loss: 0.6739400625228882\n",
      "Epoch 95, batch 41 D Loss: 1.3897547721862793, G Loss: 0.682644248008728\n",
      "Epoch 95, batch 42 D Loss: 1.3750557899475098, G Loss: 0.6977211833000183\n",
      "Epoch 95, batch 43 D Loss: 1.3771820068359375, G Loss: 0.6774919629096985\n",
      "Epoch 95, batch 44 D Loss: 1.3898634910583496, G Loss: 0.6744012236595154\n",
      "Epoch 95, batch 45 D Loss: 1.3855571746826172, G Loss: 0.6793542504310608\n",
      "Epoch 95, batch 46 D Loss: 1.4013205766677856, G Loss: 0.6774404048919678\n",
      "Epoch 95, batch 47 D Loss: 1.3864901065826416, G Loss: 0.6965792179107666\n",
      "Epoch 95, batch 48 D Loss: 1.3861117362976074, G Loss: 0.6830319762229919\n",
      "Epoch 95, batch 49 D Loss: 1.3975582122802734, G Loss: 0.6730998158454895\n",
      "Epoch 95, batch 50 D Loss: 1.386549949645996, G Loss: 0.6848393082618713\n",
      "Epoch 95, batch 51 D Loss: 1.3709325790405273, G Loss: 0.6880226135253906\n",
      "Epoch 95, batch 52 D Loss: 1.3683847188949585, G Loss: 0.6978738903999329\n",
      "Epoch 95, batch 53 D Loss: 1.4032278060913086, G Loss: 0.6711812615394592\n",
      "Epoch 95, batch 54 D Loss: 1.4021605253219604, G Loss: 0.6867596507072449\n",
      "Epoch 95, batch 55 D Loss: 1.393660545349121, G Loss: 0.6766752600669861\n",
      "Epoch 95, batch 56 D Loss: 1.3789523839950562, G Loss: 0.6917872428894043\n",
      "Epoch 95, batch 57 D Loss: 1.364417552947998, G Loss: 0.6965494751930237\n",
      "Epoch 95, batch 58 D Loss: 1.3988275527954102, G Loss: 0.6842381358146667\n",
      "Epoch 95, batch 59 D Loss: 1.4083967208862305, G Loss: 0.6794607639312744\n",
      "Epoch 95, batch 60 D Loss: 1.3861637115478516, G Loss: 0.6840482354164124\n",
      "Epoch 95, batch 61 D Loss: 1.3854209184646606, G Loss: 0.6832166910171509\n",
      "Epoch 95, batch 62 D Loss: 1.3947789669036865, G Loss: 0.6864908337593079\n",
      "Epoch 95, batch 63 D Loss: 1.3968727588653564, G Loss: 0.6859477162361145\n",
      "Epoch 95, batch 64 D Loss: 1.393617033958435, G Loss: 0.6880510449409485\n",
      "Epoch 95, batch 65 D Loss: 1.3904781341552734, G Loss: 0.6906932592391968\n",
      "Epoch 95, batch 66 D Loss: 1.3919801712036133, G Loss: 0.694089949131012\n",
      "Epoch 95, batch 67 D Loss: 1.3749537467956543, G Loss: 0.7062755823135376\n",
      "Epoch 95, batch 68 D Loss: 1.385182499885559, G Loss: 0.7028040885925293\n",
      "Epoch 95, batch 69 D Loss: 1.371877908706665, G Loss: 0.7139451503753662\n",
      "Epoch 95, batch 70 D Loss: 1.3604716062545776, G Loss: 0.7213786244392395\n",
      "Epoch 95, batch 71 D Loss: 1.3828425407409668, G Loss: 0.7101413011550903\n",
      "Epoch 95, batch 72 D Loss: 1.401526927947998, G Loss: 0.6946936249732971\n",
      "Epoch 95, batch 73 D Loss: 1.3985161781311035, G Loss: 0.6977941989898682\n",
      "Epoch 95, batch 74 D Loss: 1.3985211849212646, G Loss: 0.7081974148750305\n",
      "Epoch 95, batch 75 D Loss: 1.3947227001190186, G Loss: 0.7179713249206543\n",
      "Epoch 95, batch 76 D Loss: 1.3865723609924316, G Loss: 0.7130391001701355\n",
      "Epoch 95, batch 77 D Loss: 1.3993124961853027, G Loss: 0.7166677117347717\n",
      "Epoch 95, batch 78 D Loss: 1.405766487121582, G Loss: 0.7104112505912781\n",
      "Epoch 95, batch 79 D Loss: 1.3899321556091309, G Loss: 0.7237230539321899\n",
      "Epoch 95, batch 80 D Loss: 1.3883378505706787, G Loss: 0.717936635017395\n",
      "Epoch 95, batch 81 D Loss: 1.3916411399841309, G Loss: 0.7284649610519409\n",
      "Epoch 95, batch 82 D Loss: 1.3913578987121582, G Loss: 0.7182153463363647\n",
      "Epoch 95, batch 83 D Loss: 1.395665168762207, G Loss: 0.7077407240867615\n",
      "Epoch 95, batch 84 D Loss: 1.3833770751953125, G Loss: 0.7174460887908936\n",
      "Epoch 95, batch 85 D Loss: 1.354637861251831, G Loss: 0.736630380153656\n",
      "Epoch 95, batch 86 D Loss: 1.3856472969055176, G Loss: 0.7166135907173157\n",
      "Epoch 95, batch 87 D Loss: 1.3901238441467285, G Loss: 0.7143669724464417\n",
      "Epoch 95, batch 88 D Loss: 1.371030330657959, G Loss: 0.7279368042945862\n",
      "Epoch 95, batch 89 D Loss: 1.3848118782043457, G Loss: 0.7156352400779724\n",
      "Epoch 95, batch 90 D Loss: 1.3897461891174316, G Loss: 0.7183185815811157\n",
      "Epoch 95, batch 91 D Loss: 1.3746631145477295, G Loss: 0.71402907371521\n",
      "Epoch 95, batch 92 D Loss: 1.37628173828125, G Loss: 0.7197883725166321\n",
      "Epoch 95, batch 93 D Loss: 1.3752793073654175, G Loss: 0.7225444316864014\n",
      "Epoch 95, batch 94 D Loss: 1.4156274795532227, G Loss: 0.6856972575187683\n",
      "Epoch 95, batch 95 D Loss: 1.3835840225219727, G Loss: 0.7180387377738953\n",
      "Epoch 95, batch 96 D Loss: 1.3825578689575195, G Loss: 0.7322472929954529\n",
      "Epoch 95, batch 97 D Loss: 1.4125099182128906, G Loss: 0.7001368999481201\n",
      "Epoch 95, batch 98 D Loss: 1.396313190460205, G Loss: 0.7041855454444885\n",
      "Epoch 95, batch 99 D Loss: 1.3950462341308594, G Loss: 0.7110206484794617\n",
      "Epoch 95, batch 100 D Loss: 1.3872605562210083, G Loss: 0.7135604023933411\n",
      "Epoch 95, batch 101 D Loss: 1.3815901279449463, G Loss: 0.7074958682060242\n",
      "Epoch 95, batch 102 D Loss: 1.4010385274887085, G Loss: 0.7000904083251953\n",
      "Epoch 95, batch 103 D Loss: 1.3914580345153809, G Loss: 0.6989395022392273\n",
      "Epoch 95, batch 104 D Loss: 1.3848615884780884, G Loss: 0.6939945816993713\n",
      "Epoch 95, batch 105 D Loss: 1.3797099590301514, G Loss: 0.7019179463386536\n",
      "Epoch 95, batch 106 D Loss: 1.3715236186981201, G Loss: 0.7039113640785217\n",
      "Epoch 95, batch 107 D Loss: 1.3918452262878418, G Loss: 0.7008370757102966\n",
      "Epoch 95, batch 108 D Loss: 1.3954761028289795, G Loss: 0.6934152841567993\n",
      "Epoch 95, batch 109 D Loss: 1.3894190788269043, G Loss: 0.7025188207626343\n",
      "Epoch 95, batch 110 D Loss: 1.3829985857009888, G Loss: 0.7049283385276794\n",
      "Epoch 95, batch 111 D Loss: 1.3910577297210693, G Loss: 0.6985081434249878\n",
      "Epoch 95, batch 112 D Loss: 1.3903645277023315, G Loss: 0.7002877593040466\n",
      "Epoch 95, batch 113 D Loss: 1.3848847150802612, G Loss: 0.7030050754547119\n",
      "Epoch 95, batch 114 D Loss: 1.3727192878723145, G Loss: 0.7067450881004333\n",
      "Epoch 95, batch 115 D Loss: 1.4039905071258545, G Loss: 0.7009129524230957\n",
      "Epoch 95, batch 116 D Loss: 1.3786728382110596, G Loss: 0.7035727500915527\n",
      "Epoch 95, batch 117 D Loss: 1.3798410892486572, G Loss: 0.7011898756027222\n",
      "Epoch 95, batch 118 D Loss: 1.367985725402832, G Loss: 0.7078941464424133\n",
      "Epoch 95, batch 119 D Loss: 1.3774460554122925, G Loss: 0.7100284099578857\n",
      "Epoch 95, batch 120 D Loss: 1.3915460109710693, G Loss: 0.6955398321151733\n",
      "Epoch 95, batch 121 D Loss: 1.3949179649353027, G Loss: 0.6946858167648315\n",
      "Epoch 95, batch 122 D Loss: 1.3863970041275024, G Loss: 0.6934024095535278\n",
      "Epoch 95, batch 123 D Loss: 1.3931653499603271, G Loss: 0.6937670707702637\n",
      "Epoch 95, batch 124 D Loss: 1.3899240493774414, G Loss: 0.6897833943367004\n",
      "Epoch 95, batch 125 D Loss: 1.3840739727020264, G Loss: 0.686097264289856\n",
      "Epoch 95, batch 126 D Loss: 1.383131504058838, G Loss: 0.6841903924942017\n",
      "Epoch 95, batch 127 D Loss: 1.385689377784729, G Loss: 0.6853778958320618\n",
      "Epoch 95, batch 128 D Loss: 1.3935869932174683, G Loss: 0.675751805305481\n",
      "Epoch 95, batch 129 D Loss: 1.3858435153961182, G Loss: 0.6869326829910278\n",
      "Epoch 95, batch 130 D Loss: 1.4013254642486572, G Loss: 0.6778561472892761\n",
      "Epoch 95, batch 131 D Loss: 1.3909249305725098, G Loss: 0.6801432967185974\n",
      "Epoch 95, batch 132 D Loss: 1.3847219944000244, G Loss: 0.6827247142791748\n",
      "Epoch 95, batch 133 D Loss: 1.3883509635925293, G Loss: 0.6807349324226379\n",
      "Epoch 95, batch 134 D Loss: 1.384531855583191, G Loss: 0.6750425696372986\n",
      "Epoch 95, batch 135 D Loss: 1.3888031244277954, G Loss: 0.6758567690849304\n",
      "Epoch 95, batch 136 D Loss: 1.371640920639038, G Loss: 0.6870660185813904\n",
      "Epoch 95, batch 137 D Loss: 1.3897193670272827, G Loss: 0.672029972076416\n",
      "Epoch 95, batch 138 D Loss: 1.371777057647705, G Loss: 0.6863075494766235\n",
      "Epoch 95, batch 139 D Loss: 1.3931187391281128, G Loss: 0.6820314526557922\n",
      "Epoch 95, batch 140 D Loss: 1.3933675289154053, G Loss: 0.6713094115257263\n",
      "Epoch 95, batch 141 D Loss: 1.3802993297576904, G Loss: 0.6803340315818787\n",
      "Epoch 95, batch 142 D Loss: 1.3851447105407715, G Loss: 0.6786725521087646\n",
      "Epoch 95, batch 143 D Loss: 1.372516393661499, G Loss: 0.6771877408027649\n",
      "Epoch 95, batch 144 D Loss: 1.3806676864624023, G Loss: 0.6865209341049194\n",
      "Epoch 95, batch 145 D Loss: 1.3870360851287842, G Loss: 0.679731011390686\n",
      "Epoch 95, batch 146 D Loss: 1.3892121315002441, G Loss: 0.6858358979225159\n",
      "Epoch 95, batch 147 D Loss: 1.404308557510376, G Loss: 0.6736956238746643\n",
      "Epoch 95, batch 148 D Loss: 1.3757715225219727, G Loss: 0.6865187287330627\n",
      "Epoch 95, batch 149 D Loss: 1.3850046396255493, G Loss: 0.6891821026802063\n",
      "Epoch 95, batch 150 D Loss: 1.3806456327438354, G Loss: 0.6870288252830505\n",
      "Epoch 95, batch 151 D Loss: 1.3831586837768555, G Loss: 0.6905744671821594\n",
      "Epoch 95, batch 152 D Loss: 1.3955373764038086, G Loss: 0.6872962117195129\n",
      "Epoch 95, batch 153 D Loss: 1.3877959251403809, G Loss: 0.6847034692764282\n",
      "Epoch 95, batch 154 D Loss: 1.3820888996124268, G Loss: 0.6951212882995605\n",
      "Epoch 95, batch 155 D Loss: 1.375209927558899, G Loss: 0.700522780418396\n",
      "Epoch 95, batch 156 D Loss: 1.4105587005615234, G Loss: 0.6793730854988098\n",
      "Epoch 95, batch 157 D Loss: 1.3878759145736694, G Loss: 0.6900553107261658\n",
      "Epoch 95, batch 158 D Loss: 1.3960418701171875, G Loss: 0.6886215806007385\n",
      "Epoch 95, batch 159 D Loss: 1.386521339416504, G Loss: 0.6898251175880432\n",
      "Epoch 95, batch 160 D Loss: 1.382333755493164, G Loss: 0.685360312461853\n",
      "Epoch 95, batch 161 D Loss: 1.3766039609909058, G Loss: 0.6892411708831787\n",
      "Epoch 95, batch 162 D Loss: 1.3821957111358643, G Loss: 0.6947351694107056\n",
      "Epoch 95, batch 163 D Loss: 1.408149242401123, G Loss: 0.6863867044448853\n",
      "Epoch 95, batch 164 D Loss: 1.3820209503173828, G Loss: 0.6968851685523987\n",
      "Epoch 95, batch 165 D Loss: 1.3880362510681152, G Loss: 0.6995866894721985\n",
      "Epoch 95, batch 166 D Loss: 1.385777473449707, G Loss: 0.6970307230949402\n",
      "Epoch 95, batch 167 D Loss: 1.392655611038208, G Loss: 0.689911961555481\n",
      "Epoch 95, batch 168 D Loss: 1.3823109865188599, G Loss: 0.691892147064209\n",
      "Epoch 95, batch 169 D Loss: 1.375409722328186, G Loss: 0.7021192908287048\n",
      "Epoch 95, batch 170 D Loss: 1.3853566646575928, G Loss: 0.6993924975395203\n",
      "Epoch 95, batch 171 D Loss: 1.3834762573242188, G Loss: 0.6967632174491882\n",
      "Epoch 95, batch 172 D Loss: 1.4047932624816895, G Loss: 0.6856441497802734\n",
      "Epoch 95, batch 173 D Loss: 1.39108407497406, G Loss: 0.6918738484382629\n",
      "Epoch 95, batch 174 D Loss: 1.3833913803100586, G Loss: 0.7039443254470825\n",
      "Epoch 95, batch 175 D Loss: 1.3955780267715454, G Loss: 0.6954203248023987\n",
      "Epoch 95, batch 176 D Loss: 1.3947120904922485, G Loss: 0.7034361362457275\n",
      "Epoch 95, batch 177 D Loss: 1.3927295207977295, G Loss: 0.6990599632263184\n",
      "Epoch 95, batch 178 D Loss: 1.396493673324585, G Loss: 0.703993558883667\n",
      "Epoch 95, batch 179 D Loss: 1.3914618492126465, G Loss: 0.7029386162757874\n",
      "Epoch 95, batch 180 D Loss: 1.3955233097076416, G Loss: 0.7036982774734497\n",
      "Epoch 95, batch 181 D Loss: 1.3944010734558105, G Loss: 0.7042198777198792\n",
      "Epoch 95, batch 182 D Loss: 1.3781342506408691, G Loss: 0.7068940997123718\n",
      "Epoch 95, batch 183 D Loss: 1.3899414539337158, G Loss: 0.7070431709289551\n",
      "Epoch 95, batch 184 D Loss: 1.3887379169464111, G Loss: 0.7055595517158508\n",
      "Epoch 95, batch 185 D Loss: 1.3866560459136963, G Loss: 0.7026100158691406\n",
      "Epoch 95, batch 186 D Loss: 1.3880751132965088, G Loss: 0.7022854685783386\n",
      "Epoch 95, batch 187 D Loss: 1.3787949085235596, G Loss: 0.701735258102417\n",
      "Epoch 95, batch 188 D Loss: 1.392679214477539, G Loss: 0.693462610244751\n",
      "Epoch 95, batch 189 D Loss: 1.3867557048797607, G Loss: 0.699953556060791\n",
      "Epoch 95, batch 190 D Loss: 1.3901922702789307, G Loss: 0.7009373307228088\n",
      "Epoch 95, batch 191 D Loss: 1.3905279636383057, G Loss: 0.6969605088233948\n",
      "Epoch 95, batch 192 D Loss: 1.3898593187332153, G Loss: 0.6940595507621765\n",
      "Epoch 95, batch 193 D Loss: 1.3881691694259644, G Loss: 0.6943606734275818\n",
      "Epoch 95, batch 194 D Loss: 1.3906300067901611, G Loss: 0.6941892504692078\n",
      "Epoch 95, batch 195 D Loss: 1.3873565196990967, G Loss: 0.6903390288352966\n",
      "Epoch 95, batch 196 D Loss: 1.382591962814331, G Loss: 0.6895481944084167\n",
      "Epoch 95, batch 197 D Loss: 1.3765714168548584, G Loss: 0.6993527412414551\n",
      "Epoch 95, batch 198 D Loss: 1.382526159286499, G Loss: 0.6926591396331787\n",
      "Epoch 95, batch 199 D Loss: 1.386475920677185, G Loss: 0.6910743713378906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, batch 200 D Loss: 1.384580373764038, G Loss: 0.6978998780250549\n",
      "Epoch 96, batch 1 D Loss: 1.3869893550872803, G Loss: 0.695924699306488\n",
      "Epoch 96, batch 2 D Loss: 1.3936400413513184, G Loss: 0.687049150466919\n",
      "Epoch 96, batch 3 D Loss: 1.38298499584198, G Loss: 0.6991584897041321\n",
      "Epoch 96, batch 4 D Loss: 1.3843154907226562, G Loss: 0.6946141123771667\n",
      "Epoch 96, batch 5 D Loss: 1.3845951557159424, G Loss: 0.6962587237358093\n",
      "Epoch 96, batch 6 D Loss: 1.3839638233184814, G Loss: 0.6971935033798218\n",
      "Epoch 96, batch 7 D Loss: 1.3844821453094482, G Loss: 0.694273054599762\n",
      "Epoch 96, batch 8 D Loss: 1.3931018114089966, G Loss: 0.6898439526557922\n",
      "Epoch 96, batch 9 D Loss: 1.3799591064453125, G Loss: 0.7012348771095276\n",
      "Epoch 96, batch 10 D Loss: 1.3880751132965088, G Loss: 0.7039029598236084\n",
      "Epoch 96, batch 11 D Loss: 1.3783645629882812, G Loss: 0.6965903639793396\n",
      "Epoch 96, batch 12 D Loss: 1.3816981315612793, G Loss: 0.6979640126228333\n",
      "Epoch 96, batch 13 D Loss: 1.387818455696106, G Loss: 0.6962968707084656\n",
      "Epoch 96, batch 14 D Loss: 1.3919107913970947, G Loss: 0.6955748796463013\n",
      "Epoch 96, batch 15 D Loss: 1.388091802597046, G Loss: 0.6896263360977173\n",
      "Epoch 96, batch 16 D Loss: 1.3943527936935425, G Loss: 0.6918296217918396\n",
      "Epoch 96, batch 17 D Loss: 1.3852664232254028, G Loss: 0.6954449415206909\n",
      "Epoch 96, batch 18 D Loss: 1.3901996612548828, G Loss: 0.6933944821357727\n",
      "Epoch 96, batch 19 D Loss: 1.400329351425171, G Loss: 0.6840166449546814\n",
      "Epoch 96, batch 20 D Loss: 1.399658441543579, G Loss: 0.6857698559761047\n",
      "Epoch 96, batch 21 D Loss: 1.3836488723754883, G Loss: 0.6971920728683472\n",
      "Epoch 96, batch 22 D Loss: 1.3826848268508911, G Loss: 0.6982061862945557\n",
      "Epoch 96, batch 23 D Loss: 1.3775367736816406, G Loss: 0.6957399845123291\n",
      "Epoch 96, batch 24 D Loss: 1.3915293216705322, G Loss: 0.6927593946456909\n",
      "Epoch 96, batch 25 D Loss: 1.3833441734313965, G Loss: 0.7082569003105164\n",
      "Epoch 96, batch 26 D Loss: 1.3873121738433838, G Loss: 0.6918652057647705\n",
      "Epoch 96, batch 27 D Loss: 1.386681318283081, G Loss: 0.6958909630775452\n",
      "Epoch 96, batch 28 D Loss: 1.3827435970306396, G Loss: 0.7007571458816528\n",
      "Epoch 96, batch 29 D Loss: 1.3798930644989014, G Loss: 0.6961283087730408\n",
      "Epoch 96, batch 30 D Loss: 1.3793950080871582, G Loss: 0.6995184421539307\n",
      "Epoch 96, batch 31 D Loss: 1.3952782154083252, G Loss: 0.6959552764892578\n",
      "Epoch 96, batch 32 D Loss: 1.382235050201416, G Loss: 0.6904988288879395\n",
      "Epoch 96, batch 33 D Loss: 1.381604790687561, G Loss: 0.6895585656166077\n",
      "Epoch 96, batch 34 D Loss: 1.375896692276001, G Loss: 0.6951486468315125\n",
      "Epoch 96, batch 35 D Loss: 1.3654439449310303, G Loss: 0.7060344815254211\n",
      "Epoch 96, batch 36 D Loss: 1.3901093006134033, G Loss: 0.6934966444969177\n",
      "Epoch 96, batch 37 D Loss: 1.390449047088623, G Loss: 0.6840293407440186\n",
      "Epoch 96, batch 38 D Loss: 1.3922953605651855, G Loss: 0.6814649701118469\n",
      "Epoch 96, batch 39 D Loss: 1.3882761001586914, G Loss: 0.6846470832824707\n",
      "Epoch 96, batch 40 D Loss: 1.3886146545410156, G Loss: 0.6857871413230896\n",
      "Epoch 96, batch 41 D Loss: 1.3863165378570557, G Loss: 0.6882902383804321\n",
      "Epoch 96, batch 42 D Loss: 1.3776880502700806, G Loss: 0.6964983344078064\n",
      "Epoch 96, batch 43 D Loss: 1.3886785507202148, G Loss: 0.6896973252296448\n",
      "Epoch 96, batch 44 D Loss: 1.3872003555297852, G Loss: 0.6942102909088135\n",
      "Epoch 96, batch 45 D Loss: 1.3882473707199097, G Loss: 0.6907604336738586\n",
      "Epoch 96, batch 46 D Loss: 1.3914874792099, G Loss: 0.6892307996749878\n",
      "Epoch 96, batch 47 D Loss: 1.384749174118042, G Loss: 0.6974201798439026\n",
      "Epoch 96, batch 48 D Loss: 1.381733775138855, G Loss: 0.6982318162918091\n",
      "Epoch 96, batch 49 D Loss: 1.3838868141174316, G Loss: 0.6948068737983704\n",
      "Epoch 96, batch 50 D Loss: 1.398149013519287, G Loss: 0.6828056573867798\n",
      "Epoch 96, batch 51 D Loss: 1.3879776000976562, G Loss: 0.6918895244598389\n",
      "Epoch 96, batch 52 D Loss: 1.385867953300476, G Loss: 0.6970958113670349\n",
      "Epoch 96, batch 53 D Loss: 1.375036597251892, G Loss: 0.7022235989570618\n",
      "Epoch 96, batch 54 D Loss: 1.399075984954834, G Loss: 0.6917767524719238\n",
      "Epoch 96, batch 55 D Loss: 1.3907663822174072, G Loss: 0.6995567083358765\n",
      "Epoch 96, batch 56 D Loss: 1.384056568145752, G Loss: 0.7041388154029846\n",
      "Epoch 96, batch 57 D Loss: 1.3971998691558838, G Loss: 0.6896522045135498\n",
      "Epoch 96, batch 58 D Loss: 1.3936076164245605, G Loss: 0.7039682865142822\n",
      "Epoch 96, batch 59 D Loss: 1.3887662887573242, G Loss: 0.7021544575691223\n",
      "Epoch 96, batch 60 D Loss: 1.3898475170135498, G Loss: 0.7037555575370789\n",
      "Epoch 96, batch 61 D Loss: 1.3827452659606934, G Loss: 0.7023059129714966\n",
      "Epoch 96, batch 62 D Loss: 1.384859323501587, G Loss: 0.7023184895515442\n",
      "Epoch 96, batch 63 D Loss: 1.3860728740692139, G Loss: 0.7014740109443665\n",
      "Epoch 96, batch 64 D Loss: 1.386195182800293, G Loss: 0.6973670721054077\n",
      "Epoch 96, batch 65 D Loss: 1.396479606628418, G Loss: 0.7035784721374512\n",
      "Epoch 96, batch 66 D Loss: 1.3839573860168457, G Loss: 0.697602391242981\n",
      "Epoch 96, batch 67 D Loss: 1.384937047958374, G Loss: 0.7015097737312317\n",
      "Epoch 96, batch 68 D Loss: 1.3899869918823242, G Loss: 0.6978796124458313\n",
      "Epoch 96, batch 69 D Loss: 1.3870415687561035, G Loss: 0.7012896537780762\n",
      "Epoch 96, batch 70 D Loss: 1.380444049835205, G Loss: 0.703722357749939\n",
      "Epoch 96, batch 71 D Loss: 1.3930459022521973, G Loss: 0.6976844072341919\n",
      "Epoch 96, batch 72 D Loss: 1.3979668617248535, G Loss: 0.6918748617172241\n",
      "Epoch 96, batch 73 D Loss: 1.381331443786621, G Loss: 0.6957600116729736\n",
      "Epoch 96, batch 74 D Loss: 1.3865795135498047, G Loss: 0.6894104480743408\n",
      "Epoch 96, batch 75 D Loss: 1.3812830448150635, G Loss: 0.7008665204048157\n",
      "Epoch 96, batch 76 D Loss: 1.3849618434906006, G Loss: 0.6856529116630554\n",
      "Epoch 96, batch 77 D Loss: 1.3956446647644043, G Loss: 0.6831359267234802\n",
      "Epoch 96, batch 78 D Loss: 1.4014501571655273, G Loss: 0.6707720756530762\n",
      "Epoch 96, batch 79 D Loss: 1.389549970626831, G Loss: 0.6907566785812378\n",
      "Epoch 96, batch 80 D Loss: 1.3758502006530762, G Loss: 0.6840286254882812\n",
      "Epoch 96, batch 81 D Loss: 1.3980021476745605, G Loss: 0.6739978194236755\n",
      "Epoch 96, batch 82 D Loss: 1.368360996246338, G Loss: 0.7097582817077637\n",
      "Epoch 96, batch 83 D Loss: 1.3931597471237183, G Loss: 0.6796107292175293\n",
      "Epoch 96, batch 84 D Loss: 1.391625165939331, G Loss: 0.6885534524917603\n",
      "Epoch 96, batch 85 D Loss: 1.3973286151885986, G Loss: 0.6708258986473083\n",
      "Epoch 96, batch 86 D Loss: 1.3931365013122559, G Loss: 0.6766000986099243\n",
      "Epoch 96, batch 87 D Loss: 1.3811339139938354, G Loss: 0.6825193762779236\n",
      "Epoch 96, batch 88 D Loss: 1.3966177701950073, G Loss: 0.6777998208999634\n",
      "Epoch 96, batch 89 D Loss: 1.3772547245025635, G Loss: 0.6782093644142151\n",
      "Epoch 96, batch 90 D Loss: 1.3883578777313232, G Loss: 0.6708641648292542\n",
      "Epoch 96, batch 91 D Loss: 1.3806564807891846, G Loss: 0.6750370860099792\n",
      "Epoch 96, batch 92 D Loss: 1.406029224395752, G Loss: 0.6646124124526978\n",
      "Epoch 96, batch 93 D Loss: 1.3845947980880737, G Loss: 0.6705542206764221\n",
      "Epoch 96, batch 94 D Loss: 1.3796218633651733, G Loss: 0.6717877984046936\n",
      "Epoch 96, batch 95 D Loss: 1.380824089050293, G Loss: 0.6782168745994568\n",
      "Epoch 96, batch 96 D Loss: 1.3829689025878906, G Loss: 0.6800228953361511\n",
      "Epoch 96, batch 97 D Loss: 1.3949337005615234, G Loss: 0.6729066371917725\n",
      "Epoch 96, batch 98 D Loss: 1.394177794456482, G Loss: 0.6799126267433167\n",
      "Epoch 96, batch 99 D Loss: 1.3963961601257324, G Loss: 0.6770061254501343\n",
      "Epoch 96, batch 100 D Loss: 1.3863391876220703, G Loss: 0.6755955219268799\n",
      "Epoch 96, batch 101 D Loss: 1.3897528648376465, G Loss: 0.6767793297767639\n",
      "Epoch 96, batch 102 D Loss: 1.4004541635513306, G Loss: 0.6856474876403809\n",
      "Epoch 96, batch 103 D Loss: 1.4120702743530273, G Loss: 0.656517744064331\n",
      "Epoch 96, batch 104 D Loss: 1.4030547142028809, G Loss: 0.670331597328186\n",
      "Epoch 96, batch 105 D Loss: 1.382324457168579, G Loss: 0.6852654814720154\n",
      "Epoch 96, batch 106 D Loss: 1.3889012336730957, G Loss: 0.674641489982605\n",
      "Epoch 96, batch 107 D Loss: 1.3855092525482178, G Loss: 0.6802561283111572\n",
      "Epoch 96, batch 108 D Loss: 1.3919548988342285, G Loss: 0.6870937347412109\n",
      "Epoch 96, batch 109 D Loss: 1.3961483240127563, G Loss: 0.6847135424613953\n",
      "Epoch 96, batch 110 D Loss: 1.3765548467636108, G Loss: 0.6861393451690674\n",
      "Epoch 96, batch 111 D Loss: 1.387596607208252, G Loss: 0.6845185160636902\n",
      "Epoch 96, batch 112 D Loss: 1.4035186767578125, G Loss: 0.6751551628112793\n",
      "Epoch 96, batch 113 D Loss: 1.3741071224212646, G Loss: 0.6946477293968201\n",
      "Epoch 96, batch 114 D Loss: 1.38871431350708, G Loss: 0.6754990220069885\n",
      "Epoch 96, batch 115 D Loss: 1.3859097957611084, G Loss: 0.6785009503364563\n",
      "Epoch 96, batch 116 D Loss: 1.389648675918579, G Loss: 0.6852114200592041\n",
      "Epoch 96, batch 117 D Loss: 1.386739730834961, G Loss: 0.690532386302948\n",
      "Epoch 96, batch 118 D Loss: 1.3915948867797852, G Loss: 0.6869901418685913\n",
      "Epoch 96, batch 119 D Loss: 1.3814005851745605, G Loss: 0.6903653740882874\n",
      "Epoch 96, batch 120 D Loss: 1.405766487121582, G Loss: 0.6710888147354126\n",
      "Epoch 96, batch 121 D Loss: 1.3850892782211304, G Loss: 0.6914108991622925\n",
      "Epoch 96, batch 122 D Loss: 1.3663296699523926, G Loss: 0.7132807374000549\n",
      "Epoch 96, batch 123 D Loss: 1.3897606134414673, G Loss: 0.6923321485519409\n",
      "Epoch 96, batch 124 D Loss: 1.4003307819366455, G Loss: 0.6848411560058594\n",
      "Epoch 96, batch 125 D Loss: 1.3723645210266113, G Loss: 0.69966721534729\n",
      "Epoch 96, batch 126 D Loss: 1.394167423248291, G Loss: 0.6855471134185791\n",
      "Epoch 96, batch 127 D Loss: 1.403698444366455, G Loss: 0.697604238986969\n",
      "Epoch 96, batch 128 D Loss: 1.3852922916412354, G Loss: 0.7029722332954407\n",
      "Epoch 96, batch 129 D Loss: 1.3840478658676147, G Loss: 0.6974704265594482\n",
      "Epoch 96, batch 130 D Loss: 1.374861240386963, G Loss: 0.7060138583183289\n",
      "Epoch 96, batch 131 D Loss: 1.4066311120986938, G Loss: 0.6878957152366638\n",
      "Epoch 96, batch 132 D Loss: 1.3856775760650635, G Loss: 0.7111020684242249\n",
      "Epoch 96, batch 133 D Loss: 1.3814650774002075, G Loss: 0.7004002332687378\n",
      "Epoch 96, batch 134 D Loss: 1.3813092708587646, G Loss: 0.7001831531524658\n",
      "Epoch 96, batch 135 D Loss: 1.3870124816894531, G Loss: 0.6970972418785095\n",
      "Epoch 96, batch 136 D Loss: 1.3791048526763916, G Loss: 0.6988228559494019\n",
      "Epoch 96, batch 137 D Loss: 1.3854961395263672, G Loss: 0.7065341472625732\n",
      "Epoch 96, batch 138 D Loss: 1.3767333030700684, G Loss: 0.7088907361030579\n",
      "Epoch 96, batch 139 D Loss: 1.382226824760437, G Loss: 0.6977022290229797\n",
      "Epoch 96, batch 140 D Loss: 1.3654849529266357, G Loss: 0.7060144543647766\n",
      "Epoch 96, batch 141 D Loss: 1.3903474807739258, G Loss: 0.7023669481277466\n",
      "Epoch 96, batch 142 D Loss: 1.3883366584777832, G Loss: 0.6959424018859863\n",
      "Epoch 96, batch 143 D Loss: 1.386093258857727, G Loss: 0.7067725658416748\n",
      "Epoch 96, batch 144 D Loss: 1.3911046981811523, G Loss: 0.6926669478416443\n",
      "Epoch 96, batch 145 D Loss: 1.378436803817749, G Loss: 0.7074794769287109\n",
      "Epoch 96, batch 146 D Loss: 1.3921302556991577, G Loss: 0.6938978433609009\n",
      "Epoch 96, batch 147 D Loss: 1.3851325511932373, G Loss: 0.6995375752449036\n",
      "Epoch 96, batch 148 D Loss: 1.374650001525879, G Loss: 0.707124650478363\n",
      "Epoch 96, batch 149 D Loss: 1.3775181770324707, G Loss: 0.7074699997901917\n",
      "Epoch 96, batch 150 D Loss: 1.385223627090454, G Loss: 0.6985939741134644\n",
      "Epoch 96, batch 151 D Loss: 1.37652587890625, G Loss: 0.706768810749054\n",
      "Epoch 96, batch 152 D Loss: 1.4087228775024414, G Loss: 0.681338369846344\n",
      "Epoch 96, batch 153 D Loss: 1.396669626235962, G Loss: 0.6964927911758423\n",
      "Epoch 96, batch 154 D Loss: 1.3893134593963623, G Loss: 0.6953253746032715\n",
      "Epoch 96, batch 155 D Loss: 1.3776681423187256, G Loss: 0.7009184956550598\n",
      "Epoch 96, batch 156 D Loss: 1.3848278522491455, G Loss: 0.6990423798561096\n",
      "Epoch 96, batch 157 D Loss: 1.3864802122116089, G Loss: 0.7036604881286621\n",
      "Epoch 96, batch 158 D Loss: 1.4107179641723633, G Loss: 0.6936382055282593\n",
      "Epoch 96, batch 159 D Loss: 1.3927888870239258, G Loss: 0.7041077613830566\n",
      "Epoch 96, batch 160 D Loss: 1.3945727348327637, G Loss: 0.7005922794342041\n",
      "Epoch 96, batch 161 D Loss: 1.4085536003112793, G Loss: 0.7049323916435242\n",
      "Epoch 96, batch 162 D Loss: 1.3915621042251587, G Loss: 0.7132490277290344\n",
      "Epoch 96, batch 163 D Loss: 1.381864309310913, G Loss: 0.7082026600837708\n",
      "Epoch 96, batch 164 D Loss: 1.4041112661361694, G Loss: 0.7009296417236328\n",
      "Epoch 96, batch 165 D Loss: 1.3868757486343384, G Loss: 0.7196545600891113\n",
      "Epoch 96, batch 166 D Loss: 1.382299542427063, G Loss: 0.7158047556877136\n",
      "Epoch 96, batch 167 D Loss: 1.386096477508545, G Loss: 0.7106296420097351\n",
      "Epoch 96, batch 168 D Loss: 1.3807083368301392, G Loss: 0.721217930316925\n",
      "Epoch 96, batch 169 D Loss: 1.3879766464233398, G Loss: 0.7141037583351135\n",
      "Epoch 96, batch 170 D Loss: 1.377992868423462, G Loss: 0.7191584706306458\n",
      "Epoch 96, batch 171 D Loss: 1.3805757761001587, G Loss: 0.7317028641700745\n",
      "Epoch 96, batch 172 D Loss: 1.3949302434921265, G Loss: 0.7156401872634888\n",
      "Epoch 96, batch 173 D Loss: 1.3814356327056885, G Loss: 0.7133729457855225\n",
      "Epoch 96, batch 174 D Loss: 1.3790779113769531, G Loss: 0.7279123663902283\n",
      "Epoch 96, batch 175 D Loss: 1.3765395879745483, G Loss: 0.7248949408531189\n",
      "Epoch 96, batch 176 D Loss: 1.3806860446929932, G Loss: 0.7176692485809326\n",
      "Epoch 96, batch 177 D Loss: 1.391148328781128, G Loss: 0.7157198190689087\n",
      "Epoch 96, batch 178 D Loss: 1.372645378112793, G Loss: 0.7340999841690063\n",
      "Epoch 96, batch 179 D Loss: 1.3760614395141602, G Loss: 0.7271719574928284\n",
      "Epoch 96, batch 180 D Loss: 1.384462833404541, G Loss: 0.7244079113006592\n",
      "Epoch 96, batch 181 D Loss: 1.3849518299102783, G Loss: 0.717836856842041\n",
      "Epoch 96, batch 182 D Loss: 1.379197597503662, G Loss: 0.713533341884613\n",
      "Epoch 96, batch 183 D Loss: 1.3741765022277832, G Loss: 0.7308619022369385\n",
      "Epoch 96, batch 184 D Loss: 1.3928797245025635, G Loss: 0.7088574767112732\n",
      "Epoch 96, batch 185 D Loss: 1.3817150592803955, G Loss: 0.7173216342926025\n",
      "Epoch 96, batch 186 D Loss: 1.3677643537521362, G Loss: 0.7301687002182007\n",
      "Epoch 96, batch 187 D Loss: 1.385079264640808, G Loss: 0.7157747149467468\n",
      "Epoch 96, batch 188 D Loss: 1.3855597972869873, G Loss: 0.7125450372695923\n",
      "Epoch 96, batch 189 D Loss: 1.382806420326233, G Loss: 0.7088862657546997\n",
      "Epoch 96, batch 190 D Loss: 1.3618285655975342, G Loss: 0.7341291308403015\n",
      "Epoch 96, batch 191 D Loss: 1.3962743282318115, G Loss: 0.7060688138008118\n",
      "Epoch 96, batch 192 D Loss: 1.382926106452942, G Loss: 0.7258648872375488\n",
      "Epoch 96, batch 193 D Loss: 1.3964710235595703, G Loss: 0.7078116536140442\n",
      "Epoch 96, batch 194 D Loss: 1.4057176113128662, G Loss: 0.7149038910865784\n",
      "Epoch 96, batch 195 D Loss: 1.3762140274047852, G Loss: 0.7200079560279846\n",
      "Epoch 96, batch 196 D Loss: 1.3874419927597046, G Loss: 0.7099658846855164\n",
      "Epoch 96, batch 197 D Loss: 1.4054784774780273, G Loss: 0.7037568688392639\n",
      "Epoch 96, batch 198 D Loss: 1.3848965167999268, G Loss: 0.7081844806671143\n",
      "Epoch 96, batch 199 D Loss: 1.3871625661849976, G Loss: 0.7028491497039795\n",
      "Epoch 96, batch 200 D Loss: 1.4031875133514404, G Loss: 0.7058963775634766\n",
      "Epoch 97, batch 1 D Loss: 1.399642825126648, G Loss: 0.6821932792663574\n",
      "Epoch 97, batch 2 D Loss: 1.4045324325561523, G Loss: 0.6966896653175354\n",
      "Epoch 97, batch 3 D Loss: 1.370120882987976, G Loss: 0.7159839868545532\n",
      "Epoch 97, batch 4 D Loss: 1.3869545459747314, G Loss: 0.7051917910575867\n",
      "Epoch 97, batch 5 D Loss: 1.3916749954223633, G Loss: 0.694383978843689\n",
      "Epoch 97, batch 6 D Loss: 1.3755794763565063, G Loss: 0.7065032124519348\n",
      "Epoch 97, batch 7 D Loss: 1.394217610359192, G Loss: 0.6947153210639954\n",
      "Epoch 97, batch 8 D Loss: 1.3837370872497559, G Loss: 0.6862581372261047\n",
      "Epoch 97, batch 9 D Loss: 1.3826301097869873, G Loss: 0.6906453967094421\n",
      "Epoch 97, batch 10 D Loss: 1.4063464403152466, G Loss: 0.6791345477104187\n",
      "Epoch 97, batch 11 D Loss: 1.3815701007843018, G Loss: 0.6850457191467285\n",
      "Epoch 97, batch 12 D Loss: 1.364196538925171, G Loss: 0.6987786293029785\n",
      "Epoch 97, batch 13 D Loss: 1.3861948251724243, G Loss: 0.6844402551651001\n",
      "Epoch 97, batch 14 D Loss: 1.400301456451416, G Loss: 0.6667364239692688\n",
      "Epoch 97, batch 15 D Loss: 1.415346384048462, G Loss: 0.6695035696029663\n",
      "Epoch 97, batch 16 D Loss: 1.408036231994629, G Loss: 0.6813432574272156\n",
      "Epoch 97, batch 17 D Loss: 1.396512746810913, G Loss: 0.6885767579078674\n",
      "Epoch 97, batch 18 D Loss: 1.3780303001403809, G Loss: 0.6884185671806335\n",
      "Epoch 97, batch 19 D Loss: 1.3811441659927368, G Loss: 0.679334819316864\n",
      "Epoch 97, batch 20 D Loss: 1.362830638885498, G Loss: 0.6971771717071533\n",
      "Epoch 97, batch 21 D Loss: 1.3901751041412354, G Loss: 0.6778544783592224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, batch 22 D Loss: 1.3956851959228516, G Loss: 0.6834062337875366\n",
      "Epoch 97, batch 23 D Loss: 1.3910067081451416, G Loss: 0.6813844442367554\n",
      "Epoch 97, batch 24 D Loss: 1.3958332538604736, G Loss: 0.6848882436752319\n",
      "Epoch 97, batch 25 D Loss: 1.3793424367904663, G Loss: 0.6918010115623474\n",
      "Epoch 97, batch 26 D Loss: 1.3750449419021606, G Loss: 0.6951026320457458\n",
      "Epoch 97, batch 27 D Loss: 1.3877699375152588, G Loss: 0.703754723072052\n",
      "Epoch 97, batch 28 D Loss: 1.389772891998291, G Loss: 0.6905826330184937\n",
      "Epoch 97, batch 29 D Loss: 1.3984851837158203, G Loss: 0.6833887696266174\n",
      "Epoch 97, batch 30 D Loss: 1.3734605312347412, G Loss: 0.7097082734107971\n",
      "Epoch 97, batch 31 D Loss: 1.39041006565094, G Loss: 0.6949863433837891\n",
      "Epoch 97, batch 32 D Loss: 1.382181167602539, G Loss: 0.7025525569915771\n",
      "Epoch 97, batch 33 D Loss: 1.3964991569519043, G Loss: 0.6923753619194031\n",
      "Epoch 97, batch 34 D Loss: 1.3750518560409546, G Loss: 0.6936387419700623\n",
      "Epoch 97, batch 35 D Loss: 1.380920171737671, G Loss: 0.7040103673934937\n",
      "Epoch 97, batch 36 D Loss: 1.3855302333831787, G Loss: 0.6915146708488464\n",
      "Epoch 97, batch 37 D Loss: 1.3810288906097412, G Loss: 0.6980252861976624\n",
      "Epoch 97, batch 38 D Loss: 1.3856432437896729, G Loss: 0.6914713978767395\n",
      "Epoch 97, batch 39 D Loss: 1.3934962749481201, G Loss: 0.6858049631118774\n",
      "Epoch 97, batch 40 D Loss: 1.400395154953003, G Loss: 0.6953955888748169\n",
      "Epoch 97, batch 41 D Loss: 1.400107979774475, G Loss: 0.684251070022583\n",
      "Epoch 97, batch 42 D Loss: 1.375615119934082, G Loss: 0.701629638671875\n",
      "Epoch 97, batch 43 D Loss: 1.3837530612945557, G Loss: 0.6964446306228638\n",
      "Epoch 97, batch 44 D Loss: 1.38658607006073, G Loss: 0.6961222887039185\n",
      "Epoch 97, batch 45 D Loss: 1.378358244895935, G Loss: 0.6994144916534424\n",
      "Epoch 97, batch 46 D Loss: 1.396120548248291, G Loss: 0.6879973411560059\n",
      "Epoch 97, batch 47 D Loss: 1.394291639328003, G Loss: 0.6991059184074402\n",
      "Epoch 97, batch 48 D Loss: 1.3914575576782227, G Loss: 0.6932070255279541\n",
      "Epoch 97, batch 49 D Loss: 1.3813292980194092, G Loss: 0.6917951703071594\n",
      "Epoch 97, batch 50 D Loss: 1.3703852891921997, G Loss: 0.7050697803497314\n",
      "Epoch 97, batch 51 D Loss: 1.400222659111023, G Loss: 0.691312849521637\n",
      "Epoch 97, batch 52 D Loss: 1.378780722618103, G Loss: 0.7024112939834595\n",
      "Epoch 97, batch 53 D Loss: 1.3845257759094238, G Loss: 0.6877424120903015\n",
      "Epoch 97, batch 54 D Loss: 1.3822710514068604, G Loss: 0.6917967200279236\n",
      "Epoch 97, batch 55 D Loss: 1.3717116117477417, G Loss: 0.7001228332519531\n",
      "Epoch 97, batch 56 D Loss: 1.3778254985809326, G Loss: 0.6923462748527527\n",
      "Epoch 97, batch 57 D Loss: 1.3815927505493164, G Loss: 0.6896504759788513\n",
      "Epoch 97, batch 58 D Loss: 1.3609888553619385, G Loss: 0.6982889771461487\n",
      "Epoch 97, batch 59 D Loss: 1.4083423614501953, G Loss: 0.6725388169288635\n",
      "Epoch 97, batch 60 D Loss: 1.3941962718963623, G Loss: 0.6731107831001282\n",
      "Epoch 97, batch 61 D Loss: 1.376888632774353, G Loss: 0.6934604048728943\n",
      "Epoch 97, batch 62 D Loss: 1.383756399154663, G Loss: 0.6923336982727051\n",
      "Epoch 97, batch 63 D Loss: 1.3761628866195679, G Loss: 0.693291425704956\n",
      "Epoch 97, batch 64 D Loss: 1.369227409362793, G Loss: 0.6939138174057007\n",
      "Epoch 97, batch 65 D Loss: 1.376833200454712, G Loss: 0.6822622418403625\n",
      "Epoch 97, batch 66 D Loss: 1.3859584331512451, G Loss: 0.6886305809020996\n",
      "Epoch 97, batch 67 D Loss: 1.3836276531219482, G Loss: 0.6923744082450867\n",
      "Epoch 97, batch 68 D Loss: 1.3671118021011353, G Loss: 0.7006726861000061\n",
      "Epoch 97, batch 69 D Loss: 1.386608362197876, G Loss: 0.6863940358161926\n",
      "Epoch 97, batch 70 D Loss: 1.393399715423584, G Loss: 0.6874775886535645\n",
      "Epoch 97, batch 71 D Loss: 1.4052212238311768, G Loss: 0.6898263692855835\n",
      "Epoch 97, batch 72 D Loss: 1.3758504390716553, G Loss: 0.6988053917884827\n",
      "Epoch 97, batch 73 D Loss: 1.3976017236709595, G Loss: 0.6830741167068481\n",
      "Epoch 97, batch 74 D Loss: 1.4104217290878296, G Loss: 0.6690417528152466\n",
      "Epoch 97, batch 75 D Loss: 1.3922998905181885, G Loss: 0.6883342862129211\n",
      "Epoch 97, batch 76 D Loss: 1.3847968578338623, G Loss: 0.6906812191009521\n",
      "Epoch 97, batch 77 D Loss: 1.372223138809204, G Loss: 0.6904890537261963\n",
      "Epoch 97, batch 78 D Loss: 1.4039196968078613, G Loss: 0.6788339018821716\n",
      "Epoch 97, batch 79 D Loss: 1.401137351989746, G Loss: 0.6778714656829834\n",
      "Epoch 97, batch 80 D Loss: 1.364382266998291, G Loss: 0.6977447271347046\n",
      "Epoch 97, batch 81 D Loss: 1.37270188331604, G Loss: 0.6890521049499512\n",
      "Epoch 97, batch 82 D Loss: 1.3845481872558594, G Loss: 0.683778703212738\n",
      "Epoch 97, batch 83 D Loss: 1.3876333236694336, G Loss: 0.6844055652618408\n",
      "Epoch 97, batch 84 D Loss: 1.3816876411437988, G Loss: 0.6856215000152588\n",
      "Epoch 97, batch 85 D Loss: 1.366718053817749, G Loss: 0.6986836791038513\n",
      "Epoch 97, batch 86 D Loss: 1.3838070631027222, G Loss: 0.6897380948066711\n",
      "Epoch 97, batch 87 D Loss: 1.3754756450653076, G Loss: 0.6929642558097839\n",
      "Epoch 97, batch 88 D Loss: 1.388728141784668, G Loss: 0.6920896172523499\n",
      "Epoch 97, batch 89 D Loss: 1.3731156587600708, G Loss: 0.6883928775787354\n",
      "Epoch 97, batch 90 D Loss: 1.379323959350586, G Loss: 0.6848100423812866\n",
      "Epoch 97, batch 91 D Loss: 1.4078234434127808, G Loss: 0.6765509247779846\n",
      "Epoch 97, batch 92 D Loss: 1.3865859508514404, G Loss: 0.6785890460014343\n",
      "Epoch 97, batch 93 D Loss: 1.3812659978866577, G Loss: 0.6850913166999817\n",
      "Epoch 97, batch 94 D Loss: 1.3926177024841309, G Loss: 0.67896568775177\n",
      "Epoch 97, batch 95 D Loss: 1.4041494131088257, G Loss: 0.6687378883361816\n",
      "Epoch 97, batch 96 D Loss: 1.3964252471923828, G Loss: 0.6844210624694824\n",
      "Epoch 97, batch 97 D Loss: 1.4015251398086548, G Loss: 0.6749809384346008\n",
      "Epoch 97, batch 98 D Loss: 1.4037753343582153, G Loss: 0.666105329990387\n",
      "Epoch 97, batch 99 D Loss: 1.3964594602584839, G Loss: 0.6816920638084412\n",
      "Epoch 97, batch 100 D Loss: 1.409554123878479, G Loss: 0.6739414930343628\n",
      "Epoch 97, batch 101 D Loss: 1.3924927711486816, G Loss: 0.6726443767547607\n",
      "Epoch 97, batch 102 D Loss: 1.3822460174560547, G Loss: 0.6786858439445496\n",
      "Epoch 97, batch 103 D Loss: 1.4153757095336914, G Loss: 0.6575848460197449\n",
      "Epoch 97, batch 104 D Loss: 1.3730676174163818, G Loss: 0.6775424480438232\n",
      "Epoch 97, batch 105 D Loss: 1.4160221815109253, G Loss: 0.6601278781890869\n",
      "Epoch 97, batch 106 D Loss: 1.3998568058013916, G Loss: 0.6758583784103394\n",
      "Epoch 97, batch 107 D Loss: 1.414534091949463, G Loss: 0.6695687174797058\n",
      "Epoch 97, batch 108 D Loss: 1.3718866109848022, G Loss: 0.6853699684143066\n",
      "Epoch 97, batch 109 D Loss: 1.383904218673706, G Loss: 0.6842343807220459\n",
      "Epoch 97, batch 110 D Loss: 1.4036414623260498, G Loss: 0.6722456216812134\n",
      "Epoch 97, batch 111 D Loss: 1.3688322305679321, G Loss: 0.6974565982818604\n",
      "Epoch 97, batch 112 D Loss: 1.392042875289917, G Loss: 0.6786920428276062\n",
      "Epoch 97, batch 113 D Loss: 1.3990225791931152, G Loss: 0.6750889420509338\n",
      "Epoch 97, batch 114 D Loss: 1.3802428245544434, G Loss: 0.6832812428474426\n",
      "Epoch 97, batch 115 D Loss: 1.400758147239685, G Loss: 0.6751478314399719\n",
      "Epoch 97, batch 116 D Loss: 1.3794946670532227, G Loss: 0.6889286637306213\n",
      "Epoch 97, batch 117 D Loss: 1.3919308185577393, G Loss: 0.6835817098617554\n",
      "Epoch 97, batch 118 D Loss: 1.3944960832595825, G Loss: 0.6898053884506226\n",
      "Epoch 97, batch 119 D Loss: 1.374708890914917, G Loss: 0.699806809425354\n",
      "Epoch 97, batch 120 D Loss: 1.3920981884002686, G Loss: 0.6845556497573853\n",
      "Epoch 97, batch 121 D Loss: 1.3719487190246582, G Loss: 0.6942201256752014\n",
      "Epoch 97, batch 122 D Loss: 1.3950109481811523, G Loss: 0.6882243156433105\n",
      "Epoch 97, batch 123 D Loss: 1.3924314975738525, G Loss: 0.6915811896324158\n",
      "Epoch 97, batch 124 D Loss: 1.3849480152130127, G Loss: 0.7055374383926392\n",
      "Epoch 97, batch 125 D Loss: 1.3901008367538452, G Loss: 0.7001306414604187\n",
      "Epoch 97, batch 126 D Loss: 1.403059720993042, G Loss: 0.6842675805091858\n",
      "Epoch 97, batch 127 D Loss: 1.3983736038208008, G Loss: 0.6892271637916565\n",
      "Epoch 97, batch 128 D Loss: 1.3781816959381104, G Loss: 0.6971368193626404\n",
      "Epoch 97, batch 129 D Loss: 1.3871183395385742, G Loss: 0.6925055980682373\n",
      "Epoch 97, batch 130 D Loss: 1.3792563676834106, G Loss: 0.7041964530944824\n",
      "Epoch 97, batch 131 D Loss: 1.3769819736480713, G Loss: 0.6952627301216125\n",
      "Epoch 97, batch 132 D Loss: 1.3808400630950928, G Loss: 0.7010955810546875\n",
      "Epoch 97, batch 133 D Loss: 1.411704659461975, G Loss: 0.6803771257400513\n",
      "Epoch 97, batch 134 D Loss: 1.4068223237991333, G Loss: 0.6799114942550659\n",
      "Epoch 97, batch 135 D Loss: 1.3716742992401123, G Loss: 0.696056604385376\n",
      "Epoch 97, batch 136 D Loss: 1.3808395862579346, G Loss: 0.697676420211792\n",
      "Epoch 97, batch 137 D Loss: 1.3885325193405151, G Loss: 0.6926547288894653\n",
      "Epoch 97, batch 138 D Loss: 1.3864266872406006, G Loss: 0.6919869184494019\n",
      "Epoch 97, batch 139 D Loss: 1.3839256763458252, G Loss: 0.6978242993354797\n",
      "Epoch 97, batch 140 D Loss: 1.3836894035339355, G Loss: 0.6979005336761475\n",
      "Epoch 97, batch 141 D Loss: 1.3885531425476074, G Loss: 0.6900428533554077\n",
      "Epoch 97, batch 142 D Loss: 1.38225519657135, G Loss: 0.6931517124176025\n",
      "Epoch 97, batch 143 D Loss: 1.4113833904266357, G Loss: 0.6734540462493896\n",
      "Epoch 97, batch 144 D Loss: 1.3898146152496338, G Loss: 0.6793423295021057\n",
      "Epoch 97, batch 145 D Loss: 1.3794822692871094, G Loss: 0.6887853145599365\n",
      "Epoch 97, batch 146 D Loss: 1.3817979097366333, G Loss: 0.6856831908226013\n",
      "Epoch 97, batch 147 D Loss: 1.3755877017974854, G Loss: 0.6835793852806091\n",
      "Epoch 97, batch 148 D Loss: 1.3813937902450562, G Loss: 0.6853030920028687\n",
      "Epoch 97, batch 149 D Loss: 1.3808187246322632, G Loss: 0.6886128783226013\n",
      "Epoch 97, batch 150 D Loss: 1.378422498703003, G Loss: 0.682267427444458\n",
      "Epoch 97, batch 151 D Loss: 1.3937350511550903, G Loss: 0.6756747364997864\n",
      "Epoch 97, batch 152 D Loss: 1.3944497108459473, G Loss: 0.6876000761985779\n",
      "Epoch 97, batch 153 D Loss: 1.3749738931655884, G Loss: 0.6900871396064758\n",
      "Epoch 97, batch 154 D Loss: 1.386210322380066, G Loss: 0.6927180290222168\n",
      "Epoch 97, batch 155 D Loss: 1.3808376789093018, G Loss: 0.6914903521537781\n",
      "Epoch 97, batch 156 D Loss: 1.404167652130127, G Loss: 0.6757245659828186\n",
      "Epoch 97, batch 157 D Loss: 1.4118577241897583, G Loss: 0.6741965413093567\n",
      "Epoch 97, batch 158 D Loss: 1.3810505867004395, G Loss: 0.6867039203643799\n",
      "Epoch 97, batch 159 D Loss: 1.3778809309005737, G Loss: 0.7025127410888672\n",
      "Epoch 97, batch 160 D Loss: 1.3848648071289062, G Loss: 0.7097541689872742\n",
      "Epoch 97, batch 161 D Loss: 1.3736121654510498, G Loss: 0.7102581858634949\n",
      "Epoch 97, batch 162 D Loss: 1.3978300094604492, G Loss: 0.7141830325126648\n",
      "Epoch 97, batch 163 D Loss: 1.376485824584961, G Loss: 0.707912266254425\n",
      "Epoch 97, batch 164 D Loss: 1.3704586029052734, G Loss: 0.7076548933982849\n",
      "Epoch 97, batch 165 D Loss: 1.3942456245422363, G Loss: 0.7065764665603638\n",
      "Epoch 97, batch 166 D Loss: 1.3857128620147705, G Loss: 0.7164753675460815\n",
      "Epoch 97, batch 167 D Loss: 1.3744802474975586, G Loss: 0.7100791335105896\n",
      "Epoch 97, batch 168 D Loss: 1.3986386060714722, G Loss: 0.6992989182472229\n",
      "Epoch 97, batch 169 D Loss: 1.3626441955566406, G Loss: 0.7187283039093018\n",
      "Epoch 97, batch 170 D Loss: 1.3772780895233154, G Loss: 0.7117173075675964\n",
      "Epoch 97, batch 171 D Loss: 1.3604867458343506, G Loss: 0.7223525047302246\n",
      "Epoch 97, batch 172 D Loss: 1.3934929370880127, G Loss: 0.7043254375457764\n",
      "Epoch 97, batch 173 D Loss: 1.3813400268554688, G Loss: 0.7171995043754578\n",
      "Epoch 97, batch 174 D Loss: 1.3562266826629639, G Loss: 0.7234575152397156\n",
      "Epoch 97, batch 175 D Loss: 1.3830900192260742, G Loss: 0.7180418968200684\n",
      "Epoch 97, batch 176 D Loss: 1.3766512870788574, G Loss: 0.7189682126045227\n",
      "Epoch 97, batch 177 D Loss: 1.3725438117980957, G Loss: 0.722243070602417\n",
      "Epoch 97, batch 178 D Loss: 1.3959970474243164, G Loss: 0.7058053612709045\n",
      "Epoch 97, batch 179 D Loss: 1.3729465007781982, G Loss: 0.7212100028991699\n",
      "Epoch 97, batch 180 D Loss: 1.3826892375946045, G Loss: 0.7229300141334534\n",
      "Epoch 97, batch 181 D Loss: 1.3824143409729004, G Loss: 0.7238722443580627\n",
      "Epoch 97, batch 182 D Loss: 1.3847193717956543, G Loss: 0.7142157554626465\n",
      "Epoch 97, batch 183 D Loss: 1.360715389251709, G Loss: 0.7090486288070679\n",
      "Epoch 97, batch 184 D Loss: 1.3869110345840454, G Loss: 0.7108610272407532\n",
      "Epoch 97, batch 185 D Loss: 1.347634196281433, G Loss: 0.7340081930160522\n",
      "Epoch 97, batch 186 D Loss: 1.3542160987854004, G Loss: 0.7196890115737915\n",
      "Epoch 97, batch 187 D Loss: 1.3639978170394897, G Loss: 0.70952308177948\n",
      "Epoch 97, batch 188 D Loss: 1.3755810260772705, G Loss: 0.7199123501777649\n",
      "Epoch 97, batch 189 D Loss: 1.3888801336288452, G Loss: 0.7052757740020752\n",
      "Epoch 97, batch 190 D Loss: 1.3675620555877686, G Loss: 0.7242608070373535\n",
      "Epoch 97, batch 191 D Loss: 1.365190029144287, G Loss: 0.7250739932060242\n",
      "Epoch 97, batch 192 D Loss: 1.3705304861068726, G Loss: 0.688435971736908\n",
      "Epoch 97, batch 193 D Loss: 1.378312587738037, G Loss: 0.7021297216415405\n",
      "Epoch 97, batch 194 D Loss: 1.3652153015136719, G Loss: 0.7133122086524963\n",
      "Epoch 97, batch 195 D Loss: 1.3977810144424438, G Loss: 0.6942824721336365\n",
      "Epoch 97, batch 196 D Loss: 1.4008841514587402, G Loss: 0.7040799856185913\n",
      "Epoch 97, batch 197 D Loss: 1.3868627548217773, G Loss: 0.7062572240829468\n",
      "Epoch 97, batch 198 D Loss: 1.4090440273284912, G Loss: 0.6873676180839539\n",
      "Epoch 97, batch 199 D Loss: 1.3883264064788818, G Loss: 0.6918923258781433\n",
      "Epoch 97, batch 200 D Loss: 1.412900447845459, G Loss: 0.6868741512298584\n",
      "Epoch 98, batch 1 D Loss: 1.4256114959716797, G Loss: 0.6818721294403076\n",
      "Epoch 98, batch 2 D Loss: 1.3849725723266602, G Loss: 0.6869329810142517\n",
      "Epoch 98, batch 3 D Loss: 1.4205784797668457, G Loss: 0.6793160438537598\n",
      "Epoch 98, batch 4 D Loss: 1.4170666933059692, G Loss: 0.6829016804695129\n",
      "Epoch 98, batch 5 D Loss: 1.3875343799591064, G Loss: 0.7011879086494446\n",
      "Epoch 98, batch 6 D Loss: 1.3869445323944092, G Loss: 0.7025945782661438\n",
      "Epoch 98, batch 7 D Loss: 1.4105015993118286, G Loss: 0.6945238709449768\n",
      "Epoch 98, batch 8 D Loss: 1.4093108177185059, G Loss: 0.6987819075584412\n",
      "Epoch 98, batch 9 D Loss: 1.388267993927002, G Loss: 0.7100532054901123\n",
      "Epoch 98, batch 10 D Loss: 1.3946192264556885, G Loss: 0.7042611837387085\n",
      "Epoch 98, batch 11 D Loss: 1.3990304470062256, G Loss: 0.6924194097518921\n",
      "Epoch 98, batch 12 D Loss: 1.39266037940979, G Loss: 0.691413938999176\n",
      "Epoch 98, batch 13 D Loss: 1.4327735900878906, G Loss: 0.6678022146224976\n",
      "Epoch 98, batch 14 D Loss: 1.3996567726135254, G Loss: 0.7061868906021118\n",
      "Epoch 98, batch 15 D Loss: 1.389486312866211, G Loss: 0.7078502178192139\n",
      "Epoch 98, batch 16 D Loss: 1.4079840183258057, G Loss: 0.6986833214759827\n",
      "Epoch 98, batch 17 D Loss: 1.399049162864685, G Loss: 0.7022163271903992\n",
      "Epoch 98, batch 18 D Loss: 1.4016592502593994, G Loss: 0.7064209580421448\n",
      "Epoch 98, batch 19 D Loss: 1.3814131021499634, G Loss: 0.7086971998214722\n",
      "Epoch 98, batch 20 D Loss: 1.3984825611114502, G Loss: 0.7011696100234985\n",
      "Epoch 98, batch 21 D Loss: 1.4069693088531494, G Loss: 0.6956835985183716\n",
      "Epoch 98, batch 22 D Loss: 1.3924946784973145, G Loss: 0.7126544117927551\n",
      "Epoch 98, batch 23 D Loss: 1.370821237564087, G Loss: 0.7210723161697388\n",
      "Epoch 98, batch 24 D Loss: 1.3868227005004883, G Loss: 0.6987597942352295\n",
      "Epoch 98, batch 25 D Loss: 1.3927297592163086, G Loss: 0.715232253074646\n",
      "Epoch 98, batch 26 D Loss: 1.3809847831726074, G Loss: 0.7135653495788574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, batch 27 D Loss: 1.3835248947143555, G Loss: 0.7112261056900024\n",
      "Epoch 98, batch 28 D Loss: 1.414483904838562, G Loss: 0.7016953825950623\n",
      "Epoch 98, batch 29 D Loss: 1.4239802360534668, G Loss: 0.6919097900390625\n",
      "Epoch 98, batch 30 D Loss: 1.3768588304519653, G Loss: 0.7179889678955078\n",
      "Epoch 98, batch 31 D Loss: 1.3869097232818604, G Loss: 0.7096879482269287\n",
      "Epoch 98, batch 32 D Loss: 1.396236777305603, G Loss: 0.7081292867660522\n",
      "Epoch 98, batch 33 D Loss: 1.3854944705963135, G Loss: 0.7084892392158508\n",
      "Epoch 98, batch 34 D Loss: 1.390378475189209, G Loss: 0.7049139142036438\n",
      "Epoch 98, batch 35 D Loss: 1.394860029220581, G Loss: 0.7048198580741882\n",
      "Epoch 98, batch 36 D Loss: 1.3899732828140259, G Loss: 0.7045086622238159\n",
      "Epoch 98, batch 37 D Loss: 1.4040451049804688, G Loss: 0.6955049633979797\n",
      "Epoch 98, batch 38 D Loss: 1.3891427516937256, G Loss: 0.696498692035675\n",
      "Epoch 98, batch 39 D Loss: 1.393982172012329, G Loss: 0.696503758430481\n",
      "Epoch 98, batch 40 D Loss: 1.3714828491210938, G Loss: 0.7075124979019165\n",
      "Epoch 98, batch 41 D Loss: 1.4008898735046387, G Loss: 0.6933075189590454\n",
      "Epoch 98, batch 42 D Loss: 1.4047062397003174, G Loss: 0.681050181388855\n",
      "Epoch 98, batch 43 D Loss: 1.3989050388336182, G Loss: 0.6948767304420471\n",
      "Epoch 98, batch 44 D Loss: 1.408509373664856, G Loss: 0.6938290596008301\n",
      "Epoch 98, batch 45 D Loss: 1.408710241317749, G Loss: 0.694537341594696\n",
      "Epoch 98, batch 46 D Loss: 1.3919823169708252, G Loss: 0.6851140856742859\n",
      "Epoch 98, batch 47 D Loss: 1.411348819732666, G Loss: 0.6895320415496826\n",
      "Epoch 98, batch 48 D Loss: 1.3784890174865723, G Loss: 0.7047955393791199\n",
      "Epoch 98, batch 49 D Loss: 1.3894681930541992, G Loss: 0.6890395879745483\n",
      "Epoch 98, batch 50 D Loss: 1.37937331199646, G Loss: 0.6952440142631531\n",
      "Epoch 98, batch 51 D Loss: 1.3715206384658813, G Loss: 0.7001753449440002\n",
      "Epoch 98, batch 52 D Loss: 1.3759361505508423, G Loss: 0.7032628655433655\n",
      "Epoch 98, batch 53 D Loss: 1.3930261135101318, G Loss: 0.693769097328186\n",
      "Epoch 98, batch 54 D Loss: 1.3817317485809326, G Loss: 0.6978301405906677\n",
      "Epoch 98, batch 55 D Loss: 1.4089868068695068, G Loss: 0.6840174794197083\n",
      "Epoch 98, batch 56 D Loss: 1.3811416625976562, G Loss: 0.6936310529708862\n",
      "Epoch 98, batch 57 D Loss: 1.3861443996429443, G Loss: 0.6917358636856079\n",
      "Epoch 98, batch 58 D Loss: 1.376293420791626, G Loss: 0.6934345960617065\n",
      "Epoch 98, batch 59 D Loss: 1.3791687488555908, G Loss: 0.6928192973136902\n",
      "Epoch 98, batch 60 D Loss: 1.3810572624206543, G Loss: 0.6920427083969116\n",
      "Epoch 98, batch 61 D Loss: 1.3945395946502686, G Loss: 0.684088945388794\n",
      "Epoch 98, batch 62 D Loss: 1.3832252025604248, G Loss: 0.6965400576591492\n",
      "Epoch 98, batch 63 D Loss: 1.3794512748718262, G Loss: 0.6982384324073792\n",
      "Epoch 98, batch 64 D Loss: 1.3664923906326294, G Loss: 0.6992139220237732\n",
      "Epoch 98, batch 65 D Loss: 1.3695874214172363, G Loss: 0.7141718864440918\n",
      "Epoch 98, batch 66 D Loss: 1.3821871280670166, G Loss: 0.695676863193512\n",
      "Epoch 98, batch 67 D Loss: 1.3704254627227783, G Loss: 0.7038319110870361\n",
      "Epoch 98, batch 68 D Loss: 1.3693417310714722, G Loss: 0.708180844783783\n",
      "Epoch 98, batch 69 D Loss: 1.4000258445739746, G Loss: 0.6988837718963623\n",
      "Epoch 98, batch 70 D Loss: 1.3535284996032715, G Loss: 0.7141395807266235\n",
      "Epoch 98, batch 71 D Loss: 1.3840699195861816, G Loss: 0.6988520622253418\n",
      "Epoch 98, batch 72 D Loss: 1.386906623840332, G Loss: 0.7066071033477783\n",
      "Epoch 98, batch 73 D Loss: 1.3771772384643555, G Loss: 0.709078848361969\n",
      "Epoch 98, batch 74 D Loss: 1.376262903213501, G Loss: 0.6991124749183655\n",
      "Epoch 98, batch 75 D Loss: 1.3820725679397583, G Loss: 0.7041693329811096\n",
      "Epoch 98, batch 76 D Loss: 1.3566980361938477, G Loss: 0.7278739809989929\n",
      "Epoch 98, batch 77 D Loss: 1.361218810081482, G Loss: 0.7124026417732239\n",
      "Epoch 98, batch 78 D Loss: 1.3882224559783936, G Loss: 0.6908677816390991\n",
      "Epoch 98, batch 79 D Loss: 1.3754128217697144, G Loss: 0.7045646905899048\n",
      "Epoch 98, batch 80 D Loss: 1.3785934448242188, G Loss: 0.7034412622451782\n",
      "Epoch 98, batch 81 D Loss: 1.372727394104004, G Loss: 0.6981786489486694\n",
      "Epoch 98, batch 82 D Loss: 1.384766697883606, G Loss: 0.7085702419281006\n",
      "Epoch 98, batch 83 D Loss: 1.3705739974975586, G Loss: 0.70367431640625\n",
      "Epoch 98, batch 84 D Loss: 1.3715357780456543, G Loss: 0.6983100771903992\n",
      "Epoch 98, batch 85 D Loss: 1.3550360202789307, G Loss: 0.7134794592857361\n",
      "Epoch 98, batch 86 D Loss: 1.3700079917907715, G Loss: 0.7147946953773499\n",
      "Epoch 98, batch 87 D Loss: 1.3773484230041504, G Loss: 0.7068707346916199\n",
      "Epoch 98, batch 88 D Loss: 1.366257905960083, G Loss: 0.6940732002258301\n",
      "Epoch 98, batch 89 D Loss: 1.3808262348175049, G Loss: 0.7145473957061768\n",
      "Epoch 98, batch 90 D Loss: 1.3861544132232666, G Loss: 0.7165560722351074\n",
      "Epoch 98, batch 91 D Loss: 1.4081251621246338, G Loss: 0.6912363171577454\n",
      "Epoch 98, batch 92 D Loss: 1.3884445428848267, G Loss: 0.6872255206108093\n",
      "Epoch 98, batch 93 D Loss: 1.397456407546997, G Loss: 0.7015219330787659\n",
      "Epoch 98, batch 94 D Loss: 1.3884916305541992, G Loss: 0.6844990253448486\n",
      "Epoch 98, batch 95 D Loss: 1.3678884506225586, G Loss: 0.7105053663253784\n",
      "Epoch 98, batch 96 D Loss: 1.3771604299545288, G Loss: 0.6993345618247986\n",
      "Epoch 98, batch 97 D Loss: 1.381481647491455, G Loss: 0.7096260190010071\n",
      "Epoch 98, batch 98 D Loss: 1.4078073501586914, G Loss: 0.6964062452316284\n",
      "Epoch 98, batch 99 D Loss: 1.4323426485061646, G Loss: 0.6739231944084167\n",
      "Epoch 98, batch 100 D Loss: 1.4017330408096313, G Loss: 0.6938567161560059\n",
      "Epoch 98, batch 101 D Loss: 1.3978381156921387, G Loss: 0.6878637671470642\n",
      "Epoch 98, batch 102 D Loss: 1.3923814296722412, G Loss: 0.678697407245636\n",
      "Epoch 98, batch 103 D Loss: 1.3880767822265625, G Loss: 0.6865111589431763\n",
      "Epoch 98, batch 104 D Loss: 1.4028878211975098, G Loss: 0.6774548888206482\n",
      "Epoch 98, batch 105 D Loss: 1.3767504692077637, G Loss: 0.6916698217391968\n",
      "Epoch 98, batch 106 D Loss: 1.40516197681427, G Loss: 0.6788161396980286\n",
      "Epoch 98, batch 107 D Loss: 1.4086461067199707, G Loss: 0.6686803698539734\n",
      "Epoch 98, batch 108 D Loss: 1.3831720352172852, G Loss: 0.690581202507019\n",
      "Epoch 98, batch 109 D Loss: 1.3862468004226685, G Loss: 0.6828117966651917\n",
      "Epoch 98, batch 110 D Loss: 1.4068905115127563, G Loss: 0.6641646027565002\n",
      "Epoch 98, batch 111 D Loss: 1.3783862590789795, G Loss: 0.6987112164497375\n",
      "Epoch 98, batch 112 D Loss: 1.4095088243484497, G Loss: 0.6781471967697144\n",
      "Epoch 98, batch 113 D Loss: 1.3901331424713135, G Loss: 0.6781227588653564\n",
      "Epoch 98, batch 114 D Loss: 1.4052997827529907, G Loss: 0.6725812554359436\n",
      "Epoch 98, batch 115 D Loss: 1.4098761081695557, G Loss: 0.6766341924667358\n",
      "Epoch 98, batch 116 D Loss: 1.3887929916381836, G Loss: 0.6746634840965271\n",
      "Epoch 98, batch 117 D Loss: 1.398215889930725, G Loss: 0.6815566420555115\n",
      "Epoch 98, batch 118 D Loss: 1.3954687118530273, G Loss: 0.6758729815483093\n",
      "Epoch 98, batch 119 D Loss: 1.400422215461731, G Loss: 0.6948189735412598\n",
      "Epoch 98, batch 120 D Loss: 1.3913413286209106, G Loss: 0.6760254502296448\n",
      "Epoch 98, batch 121 D Loss: 1.4297831058502197, G Loss: 0.6548575758934021\n",
      "Epoch 98, batch 122 D Loss: 1.4284437894821167, G Loss: 0.6602003574371338\n",
      "Epoch 98, batch 123 D Loss: 1.3753890991210938, G Loss: 0.6955509781837463\n",
      "Epoch 98, batch 124 D Loss: 1.3924643993377686, G Loss: 0.6922822594642639\n",
      "Epoch 98, batch 125 D Loss: 1.3796403408050537, G Loss: 0.6979694962501526\n",
      "Epoch 98, batch 126 D Loss: 1.3964858055114746, G Loss: 0.6790866851806641\n",
      "Epoch 98, batch 127 D Loss: 1.4081741571426392, G Loss: 0.7007623910903931\n",
      "Epoch 98, batch 128 D Loss: 1.361746072769165, G Loss: 0.713451623916626\n",
      "Epoch 98, batch 129 D Loss: 1.3923671245574951, G Loss: 0.6968312859535217\n",
      "Epoch 98, batch 130 D Loss: 1.377037763595581, G Loss: 0.6991528272628784\n",
      "Epoch 98, batch 131 D Loss: 1.3715908527374268, G Loss: 0.6931101083755493\n",
      "Epoch 98, batch 132 D Loss: 1.3999998569488525, G Loss: 0.6809123754501343\n",
      "Epoch 98, batch 133 D Loss: 1.394951343536377, G Loss: 0.7059938311576843\n",
      "Epoch 98, batch 134 D Loss: 1.3633997440338135, G Loss: 0.7017825841903687\n",
      "Epoch 98, batch 135 D Loss: 1.403608798980713, G Loss: 0.6886069774627686\n",
      "Epoch 98, batch 136 D Loss: 1.3895699977874756, G Loss: 0.6899561882019043\n",
      "Epoch 98, batch 137 D Loss: 1.3831706047058105, G Loss: 0.7000064849853516\n",
      "Epoch 98, batch 138 D Loss: 1.4073196649551392, G Loss: 0.6879478693008423\n",
      "Epoch 98, batch 139 D Loss: 1.3766376972198486, G Loss: 0.694908082485199\n",
      "Epoch 98, batch 140 D Loss: 1.3797433376312256, G Loss: 0.7063549160957336\n",
      "Epoch 98, batch 141 D Loss: 1.3898168802261353, G Loss: 0.6906750202178955\n",
      "Epoch 98, batch 142 D Loss: 1.4177968502044678, G Loss: 0.691876232624054\n",
      "Epoch 98, batch 143 D Loss: 1.4020519256591797, G Loss: 0.6853858828544617\n",
      "Epoch 98, batch 144 D Loss: 1.381239652633667, G Loss: 0.6966228485107422\n",
      "Epoch 98, batch 145 D Loss: 1.3909575939178467, G Loss: 0.6925508379936218\n",
      "Epoch 98, batch 146 D Loss: 1.397188663482666, G Loss: 0.6979585886001587\n",
      "Epoch 98, batch 147 D Loss: 1.3728516101837158, G Loss: 0.7100609540939331\n",
      "Epoch 98, batch 148 D Loss: 1.430105447769165, G Loss: 0.6721085906028748\n",
      "Epoch 98, batch 149 D Loss: 1.3853836059570312, G Loss: 0.6907064914703369\n",
      "Epoch 98, batch 150 D Loss: 1.4021425247192383, G Loss: 0.6885406970977783\n",
      "Epoch 98, batch 151 D Loss: 1.3949100971221924, G Loss: 0.6924280524253845\n",
      "Epoch 98, batch 152 D Loss: 1.3779218196868896, G Loss: 0.6945182085037231\n",
      "Epoch 98, batch 153 D Loss: 1.388390302658081, G Loss: 0.695115327835083\n",
      "Epoch 98, batch 154 D Loss: 1.3977023363113403, G Loss: 0.6906754374504089\n",
      "Epoch 98, batch 155 D Loss: 1.3867833614349365, G Loss: 0.6946238875389099\n",
      "Epoch 98, batch 156 D Loss: 1.3900315761566162, G Loss: 0.6956605315208435\n",
      "Epoch 98, batch 157 D Loss: 1.3892583847045898, G Loss: 0.6940328478813171\n",
      "Epoch 98, batch 158 D Loss: 1.3857593536376953, G Loss: 0.6964677572250366\n",
      "Epoch 98, batch 159 D Loss: 1.3981990814208984, G Loss: 0.6892790198326111\n",
      "Epoch 98, batch 160 D Loss: 1.3785094022750854, G Loss: 0.7146037817001343\n",
      "Epoch 98, batch 161 D Loss: 1.3776161670684814, G Loss: 0.708423376083374\n",
      "Epoch 98, batch 162 D Loss: 1.3938831090927124, G Loss: 0.7020222544670105\n",
      "Epoch 98, batch 163 D Loss: 1.3818882703781128, G Loss: 0.706149160861969\n",
      "Epoch 98, batch 164 D Loss: 1.369031310081482, G Loss: 0.7163918018341064\n",
      "Epoch 98, batch 165 D Loss: 1.3868577480316162, G Loss: 0.7051247358322144\n",
      "Epoch 98, batch 166 D Loss: 1.3536837100982666, G Loss: 0.7232782244682312\n",
      "Epoch 98, batch 167 D Loss: 1.3762660026550293, G Loss: 0.7098610401153564\n",
      "Epoch 98, batch 168 D Loss: 1.387627124786377, G Loss: 0.7015854120254517\n",
      "Epoch 98, batch 169 D Loss: 1.3832485675811768, G Loss: 0.7161301374435425\n",
      "Epoch 98, batch 170 D Loss: 1.384577989578247, G Loss: 0.7096710801124573\n",
      "Epoch 98, batch 171 D Loss: 1.3710405826568604, G Loss: 0.7236685156822205\n",
      "Epoch 98, batch 172 D Loss: 1.3812334537506104, G Loss: 0.7145845890045166\n",
      "Epoch 98, batch 173 D Loss: 1.3763172626495361, G Loss: 0.7216129302978516\n",
      "Epoch 98, batch 174 D Loss: 1.3770546913146973, G Loss: 0.7119582295417786\n",
      "Epoch 98, batch 175 D Loss: 1.32108473777771, G Loss: 0.7582170367240906\n",
      "Epoch 98, batch 176 D Loss: 1.3752999305725098, G Loss: 0.7151502370834351\n",
      "Epoch 98, batch 177 D Loss: 1.3804662227630615, G Loss: 0.6963052153587341\n",
      "Epoch 98, batch 178 D Loss: 1.3745449781417847, G Loss: 0.7392807006835938\n",
      "Epoch 98, batch 179 D Loss: 1.3916902542114258, G Loss: 0.7328585982322693\n",
      "Epoch 98, batch 180 D Loss: 1.388174295425415, G Loss: 0.7003607153892517\n",
      "Epoch 98, batch 181 D Loss: 1.4071346521377563, G Loss: 0.709148645401001\n",
      "Epoch 98, batch 182 D Loss: 1.3492088317871094, G Loss: 0.7301754951477051\n",
      "Epoch 98, batch 183 D Loss: 1.3888479471206665, G Loss: 0.7060876488685608\n",
      "Epoch 98, batch 184 D Loss: 1.3654811382293701, G Loss: 0.7256819009780884\n",
      "Epoch 98, batch 185 D Loss: 1.363705039024353, G Loss: 0.7214203476905823\n",
      "Epoch 98, batch 186 D Loss: 1.413905382156372, G Loss: 0.7139539122581482\n",
      "Epoch 98, batch 187 D Loss: 1.3650221824645996, G Loss: 0.7210446000099182\n",
      "Epoch 98, batch 188 D Loss: 1.3393133878707886, G Loss: 0.7204268574714661\n",
      "Epoch 98, batch 189 D Loss: 1.3638684749603271, G Loss: 0.6974771022796631\n",
      "Epoch 98, batch 190 D Loss: 1.3915276527404785, G Loss: 0.7067203521728516\n",
      "Epoch 98, batch 191 D Loss: 1.374140977859497, G Loss: 0.7150200009346008\n",
      "Epoch 98, batch 192 D Loss: 1.392707347869873, G Loss: 0.6774406433105469\n",
      "Epoch 98, batch 193 D Loss: 1.3759334087371826, G Loss: 0.70506751537323\n",
      "Epoch 98, batch 194 D Loss: 1.38454008102417, G Loss: 0.6964707970619202\n",
      "Epoch 98, batch 195 D Loss: 1.399505615234375, G Loss: 0.686964213848114\n",
      "Epoch 98, batch 196 D Loss: 1.3604916334152222, G Loss: 0.7068946957588196\n",
      "Epoch 98, batch 197 D Loss: 1.3893303871154785, G Loss: 0.6886684894561768\n",
      "Epoch 98, batch 198 D Loss: 1.3901393413543701, G Loss: 0.6974031329154968\n",
      "Epoch 98, batch 199 D Loss: 1.3700907230377197, G Loss: 0.7031217217445374\n",
      "Epoch 98, batch 200 D Loss: 1.3924946784973145, G Loss: 0.7041583061218262\n",
      "Epoch 99, batch 1 D Loss: 1.3896987438201904, G Loss: 0.7068067193031311\n",
      "Epoch 99, batch 2 D Loss: 1.417496681213379, G Loss: 0.6866213083267212\n",
      "Epoch 99, batch 3 D Loss: 1.404681921005249, G Loss: 0.6854792833328247\n",
      "Epoch 99, batch 4 D Loss: 1.3836889266967773, G Loss: 0.6915158629417419\n",
      "Epoch 99, batch 5 D Loss: 1.410430669784546, G Loss: 0.6935160160064697\n",
      "Epoch 99, batch 6 D Loss: 1.4027752876281738, G Loss: 0.7017173767089844\n",
      "Epoch 99, batch 7 D Loss: 1.3845494985580444, G Loss: 0.6988283395767212\n",
      "Epoch 99, batch 8 D Loss: 1.3526853322982788, G Loss: 0.7201281785964966\n",
      "Epoch 99, batch 9 D Loss: 1.4109714031219482, G Loss: 0.6994177103042603\n",
      "Epoch 99, batch 10 D Loss: 1.3826632499694824, G Loss: 0.7018619775772095\n",
      "Epoch 99, batch 11 D Loss: 1.3831449747085571, G Loss: 0.7106887698173523\n",
      "Epoch 99, batch 12 D Loss: 1.3916172981262207, G Loss: 0.6949165463447571\n",
      "Epoch 99, batch 13 D Loss: 1.3934040069580078, G Loss: 0.7108890414237976\n",
      "Epoch 99, batch 14 D Loss: 1.4206154346466064, G Loss: 0.6761254072189331\n",
      "Epoch 99, batch 15 D Loss: 1.3726422786712646, G Loss: 0.699347734451294\n",
      "Epoch 99, batch 16 D Loss: 1.396484613418579, G Loss: 0.7013921141624451\n",
      "Epoch 99, batch 17 D Loss: 1.4127320051193237, G Loss: 0.6841980814933777\n",
      "Epoch 99, batch 18 D Loss: 1.3616176843643188, G Loss: 0.7058291435241699\n",
      "Epoch 99, batch 19 D Loss: 1.3866004943847656, G Loss: 0.7024588584899902\n",
      "Epoch 99, batch 20 D Loss: 1.3941103219985962, G Loss: 0.7125698924064636\n",
      "Epoch 99, batch 21 D Loss: 1.4023830890655518, G Loss: 0.6875988841056824\n",
      "Epoch 99, batch 22 D Loss: 1.3920649290084839, G Loss: 0.7090478539466858\n",
      "Epoch 99, batch 23 D Loss: 1.367180585861206, G Loss: 0.7151191830635071\n",
      "Epoch 99, batch 24 D Loss: 1.3647148609161377, G Loss: 0.7206175923347473\n",
      "Epoch 99, batch 25 D Loss: 1.4225128889083862, G Loss: 0.6863526701927185\n",
      "Epoch 99, batch 26 D Loss: 1.3952703475952148, G Loss: 0.6961400508880615\n",
      "Epoch 99, batch 27 D Loss: 1.3976011276245117, G Loss: 0.6842638254165649\n",
      "Epoch 99, batch 28 D Loss: 1.4008069038391113, G Loss: 0.687254011631012\n",
      "Epoch 99, batch 29 D Loss: 1.3983241319656372, G Loss: 0.7007582187652588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, batch 30 D Loss: 1.3824584484100342, G Loss: 0.6926795840263367\n",
      "Epoch 99, batch 31 D Loss: 1.3700796365737915, G Loss: 0.689717173576355\n",
      "Epoch 99, batch 32 D Loss: 1.3591136932373047, G Loss: 0.6948782205581665\n",
      "Epoch 99, batch 33 D Loss: 1.4041032791137695, G Loss: 0.6745498776435852\n",
      "Epoch 99, batch 34 D Loss: 1.3770332336425781, G Loss: 0.7185576558113098\n",
      "Epoch 99, batch 35 D Loss: 1.4039262533187866, G Loss: 0.6882462501525879\n",
      "Epoch 99, batch 36 D Loss: 1.396518349647522, G Loss: 0.6823772192001343\n",
      "Epoch 99, batch 37 D Loss: 1.3870787620544434, G Loss: 0.6976068019866943\n",
      "Epoch 99, batch 38 D Loss: 1.400280237197876, G Loss: 0.6805263757705688\n",
      "Epoch 99, batch 39 D Loss: 1.3884902000427246, G Loss: 0.7001078724861145\n",
      "Epoch 99, batch 40 D Loss: 1.3978967666625977, G Loss: 0.6861353516578674\n",
      "Epoch 99, batch 41 D Loss: 1.4037044048309326, G Loss: 0.6690787672996521\n",
      "Epoch 99, batch 42 D Loss: 1.4299702644348145, G Loss: 0.6790015697479248\n",
      "Epoch 99, batch 43 D Loss: 1.372991919517517, G Loss: 0.7021137475967407\n",
      "Epoch 99, batch 44 D Loss: 1.4161808490753174, G Loss: 0.6763632893562317\n",
      "Epoch 99, batch 45 D Loss: 1.397784948348999, G Loss: 0.6755226850509644\n",
      "Epoch 99, batch 46 D Loss: 1.3969330787658691, G Loss: 0.692638635635376\n",
      "Epoch 99, batch 47 D Loss: 1.3625034093856812, G Loss: 0.7056897878646851\n",
      "Epoch 99, batch 48 D Loss: 1.4249615669250488, G Loss: 0.670748770236969\n",
      "Epoch 99, batch 49 D Loss: 1.4064695835113525, G Loss: 0.6860400438308716\n",
      "Epoch 99, batch 50 D Loss: 1.3969295024871826, G Loss: 0.7019129395484924\n",
      "Epoch 99, batch 51 D Loss: 1.39188551902771, G Loss: 0.7069734334945679\n",
      "Epoch 99, batch 52 D Loss: 1.398462176322937, G Loss: 0.7050808668136597\n",
      "Epoch 99, batch 53 D Loss: 1.3949605226516724, G Loss: 0.6912617683410645\n",
      "Epoch 99, batch 54 D Loss: 1.4112811088562012, G Loss: 0.7116760015487671\n",
      "Epoch 99, batch 55 D Loss: 1.3831775188446045, G Loss: 0.707663893699646\n",
      "Epoch 99, batch 56 D Loss: 1.3748693466186523, G Loss: 0.7099672555923462\n",
      "Epoch 99, batch 57 D Loss: 1.3846285343170166, G Loss: 0.7111157178878784\n",
      "Epoch 99, batch 58 D Loss: 1.398441195487976, G Loss: 0.6968194842338562\n",
      "Epoch 99, batch 59 D Loss: 1.4128769636154175, G Loss: 0.6836308240890503\n",
      "Epoch 99, batch 60 D Loss: 1.3977293968200684, G Loss: 0.7147390842437744\n",
      "Epoch 99, batch 61 D Loss: 1.3748233318328857, G Loss: 0.7009851336479187\n",
      "Epoch 99, batch 62 D Loss: 1.3808598518371582, G Loss: 0.7163549661636353\n",
      "Epoch 99, batch 63 D Loss: 1.3886799812316895, G Loss: 0.7203361988067627\n",
      "Epoch 99, batch 64 D Loss: 1.3648200035095215, G Loss: 0.7303162217140198\n",
      "Epoch 99, batch 65 D Loss: 1.3781195878982544, G Loss: 0.7138305902481079\n",
      "Epoch 99, batch 66 D Loss: 1.3850481510162354, G Loss: 0.7147569060325623\n",
      "Epoch 99, batch 67 D Loss: 1.3911027908325195, G Loss: 0.7220911383628845\n",
      "Epoch 99, batch 68 D Loss: 1.3930459022521973, G Loss: 0.7245448231697083\n",
      "Epoch 99, batch 69 D Loss: 1.4016444683074951, G Loss: 0.725598156452179\n",
      "Epoch 99, batch 70 D Loss: 1.3811917304992676, G Loss: 0.7301961779594421\n",
      "Epoch 99, batch 71 D Loss: 1.3501081466674805, G Loss: 0.7408036589622498\n",
      "Epoch 99, batch 72 D Loss: 1.368265986442566, G Loss: 0.733542799949646\n",
      "Epoch 99, batch 73 D Loss: 1.405289888381958, G Loss: 0.716198742389679\n",
      "Epoch 99, batch 74 D Loss: 1.3778150081634521, G Loss: 0.7222654223442078\n",
      "Epoch 99, batch 75 D Loss: 1.3882880210876465, G Loss: 0.7257373332977295\n",
      "Epoch 99, batch 76 D Loss: 1.3639802932739258, G Loss: 0.7213104367256165\n",
      "Epoch 99, batch 77 D Loss: 1.3748517036437988, G Loss: 0.7293115854263306\n",
      "Epoch 99, batch 78 D Loss: 1.3632290363311768, G Loss: 0.7380374073982239\n",
      "Epoch 99, batch 79 D Loss: 1.3664416074752808, G Loss: 0.7291600704193115\n",
      "Epoch 99, batch 80 D Loss: 1.4148964881896973, G Loss: 0.7032321691513062\n",
      "Epoch 99, batch 81 D Loss: 1.351318359375, G Loss: 0.7324262857437134\n",
      "Epoch 99, batch 82 D Loss: 1.3684914112091064, G Loss: 0.7463727593421936\n",
      "Epoch 99, batch 83 D Loss: 1.3370763063430786, G Loss: 0.7568203806877136\n",
      "Epoch 99, batch 84 D Loss: 1.3940939903259277, G Loss: 0.7164203524589539\n",
      "Epoch 99, batch 85 D Loss: 1.3833906650543213, G Loss: 0.7087167501449585\n",
      "Epoch 99, batch 86 D Loss: 1.3739150762557983, G Loss: 0.7499803900718689\n",
      "Epoch 99, batch 87 D Loss: 1.4054820537567139, G Loss: 0.7087438106536865\n",
      "Epoch 99, batch 88 D Loss: 1.333575963973999, G Loss: 0.7784993648529053\n",
      "Epoch 99, batch 89 D Loss: 1.381350040435791, G Loss: 0.7557739019393921\n",
      "Epoch 99, batch 90 D Loss: 1.3604159355163574, G Loss: 0.7301243543624878\n",
      "Epoch 99, batch 91 D Loss: 1.3890538215637207, G Loss: 0.7478481531143188\n",
      "Epoch 99, batch 92 D Loss: 1.35310959815979, G Loss: 0.7398099303245544\n",
      "Epoch 99, batch 93 D Loss: 1.3438305854797363, G Loss: 0.7726492285728455\n",
      "Epoch 99, batch 94 D Loss: 1.3875892162322998, G Loss: 0.7392867803573608\n",
      "Epoch 99, batch 95 D Loss: 1.3877906799316406, G Loss: 0.733876645565033\n",
      "Epoch 99, batch 96 D Loss: 1.352470874786377, G Loss: 0.7503004670143127\n",
      "Epoch 99, batch 97 D Loss: 1.4004977941513062, G Loss: 0.7534171938896179\n",
      "Epoch 99, batch 98 D Loss: 1.4025397300720215, G Loss: 0.7362651824951172\n",
      "Epoch 99, batch 99 D Loss: 1.3954535722732544, G Loss: 0.7165146470069885\n",
      "Epoch 99, batch 100 D Loss: 1.3693605661392212, G Loss: 0.7201189994812012\n",
      "Epoch 99, batch 101 D Loss: 1.3926146030426025, G Loss: 0.7102006673812866\n",
      "Epoch 99, batch 102 D Loss: 1.4171873331069946, G Loss: 0.6692969799041748\n",
      "Epoch 99, batch 103 D Loss: 1.4415785074234009, G Loss: 0.6758401393890381\n",
      "Epoch 99, batch 104 D Loss: 1.3480290174484253, G Loss: 0.7390559911727905\n",
      "Epoch 99, batch 105 D Loss: 1.4060261249542236, G Loss: 0.6856349110603333\n",
      "Epoch 99, batch 106 D Loss: 1.4087717533111572, G Loss: 0.6965144276618958\n",
      "Epoch 99, batch 107 D Loss: 1.4368497133255005, G Loss: 0.6572916507720947\n",
      "Epoch 99, batch 108 D Loss: 1.3861041069030762, G Loss: 0.6675131320953369\n",
      "Epoch 99, batch 109 D Loss: 1.3972468376159668, G Loss: 0.6605399250984192\n",
      "Epoch 99, batch 110 D Loss: 1.3689100742340088, G Loss: 0.6911986470222473\n",
      "Epoch 99, batch 111 D Loss: 1.3981108665466309, G Loss: 0.6543376445770264\n",
      "Epoch 99, batch 112 D Loss: 1.388749361038208, G Loss: 0.6733564734458923\n",
      "Epoch 99, batch 113 D Loss: 1.3946086168289185, G Loss: 0.6705882549285889\n",
      "Epoch 99, batch 114 D Loss: 1.3996672630310059, G Loss: 0.6590047478675842\n",
      "Epoch 99, batch 115 D Loss: 1.3857145309448242, G Loss: 0.6587088108062744\n",
      "Epoch 99, batch 116 D Loss: 1.422660231590271, G Loss: 0.6320361495018005\n",
      "Epoch 99, batch 117 D Loss: 1.399024248123169, G Loss: 0.6742682456970215\n",
      "Epoch 99, batch 118 D Loss: 1.4103796482086182, G Loss: 0.6404369473457336\n",
      "Epoch 99, batch 119 D Loss: 1.396026611328125, G Loss: 0.653352677822113\n",
      "Epoch 99, batch 120 D Loss: 1.3722524642944336, G Loss: 0.6701644062995911\n",
      "Epoch 99, batch 121 D Loss: 1.3397976160049438, G Loss: 0.686237633228302\n",
      "Epoch 99, batch 122 D Loss: 1.3695929050445557, G Loss: 0.6762437224388123\n",
      "Epoch 99, batch 123 D Loss: 1.3718191385269165, G Loss: 0.6741724610328674\n",
      "Epoch 99, batch 124 D Loss: 1.404423475265503, G Loss: 0.6692458987236023\n",
      "Epoch 99, batch 125 D Loss: 1.3715683221817017, G Loss: 0.6798103451728821\n",
      "Epoch 99, batch 126 D Loss: 1.3891242742538452, G Loss: 0.6692628264427185\n",
      "Epoch 99, batch 127 D Loss: 1.38319730758667, G Loss: 0.6717602014541626\n",
      "Epoch 99, batch 128 D Loss: 1.3430274724960327, G Loss: 0.6755607724189758\n",
      "Epoch 99, batch 129 D Loss: 1.3684258460998535, G Loss: 0.6706474423408508\n",
      "Epoch 99, batch 130 D Loss: 1.4013701677322388, G Loss: 0.6603308320045471\n",
      "Epoch 99, batch 131 D Loss: 1.3653981685638428, G Loss: 0.6805598735809326\n",
      "Epoch 99, batch 132 D Loss: 1.3784656524658203, G Loss: 0.6770098805427551\n",
      "Epoch 99, batch 133 D Loss: 1.4010871648788452, G Loss: 0.6598584055900574\n",
      "Epoch 99, batch 134 D Loss: 1.3926029205322266, G Loss: 0.6563939452171326\n",
      "Epoch 99, batch 135 D Loss: 1.4013574123382568, G Loss: 0.6627398729324341\n",
      "Epoch 99, batch 136 D Loss: 1.4016022682189941, G Loss: 0.6681957840919495\n",
      "Epoch 99, batch 137 D Loss: 1.4075891971588135, G Loss: 0.6555047035217285\n",
      "Epoch 99, batch 138 D Loss: 1.4301986694335938, G Loss: 0.6623135209083557\n",
      "Epoch 99, batch 139 D Loss: 1.4196391105651855, G Loss: 0.6543412804603577\n",
      "Epoch 99, batch 140 D Loss: 1.3531208038330078, G Loss: 0.6847981214523315\n",
      "Epoch 99, batch 141 D Loss: 1.4756982326507568, G Loss: 0.6381667256355286\n",
      "Epoch 99, batch 142 D Loss: 1.353450059890747, G Loss: 0.678570568561554\n",
      "Epoch 99, batch 143 D Loss: 1.3374615907669067, G Loss: 0.7080709934234619\n",
      "Epoch 99, batch 144 D Loss: 1.4091503620147705, G Loss: 0.6842114925384521\n",
      "Epoch 99, batch 145 D Loss: 1.393513560295105, G Loss: 0.6719807386398315\n",
      "Epoch 99, batch 146 D Loss: 1.3920522928237915, G Loss: 0.6705717444419861\n",
      "Epoch 99, batch 147 D Loss: 1.378826379776001, G Loss: 0.6733240485191345\n",
      "Epoch 99, batch 148 D Loss: 1.3249362707138062, G Loss: 0.6858242154121399\n",
      "Epoch 99, batch 149 D Loss: 1.3998664617538452, G Loss: 0.6519500613212585\n",
      "Epoch 99, batch 150 D Loss: 1.3797825574874878, G Loss: 0.6688711047172546\n",
      "Epoch 99, batch 151 D Loss: 1.3502259254455566, G Loss: 0.6876760721206665\n",
      "Epoch 99, batch 152 D Loss: 1.3781938552856445, G Loss: 0.6739017963409424\n",
      "Epoch 99, batch 153 D Loss: 1.3861546516418457, G Loss: 0.6785455346107483\n",
      "Epoch 99, batch 154 D Loss: 1.3724114894866943, G Loss: 0.6914675831794739\n",
      "Epoch 99, batch 155 D Loss: 1.4189624786376953, G Loss: 0.6611019372940063\n",
      "Epoch 99, batch 156 D Loss: 1.3924486637115479, G Loss: 0.67377108335495\n",
      "Epoch 99, batch 157 D Loss: 1.3776072263717651, G Loss: 0.6597001552581787\n",
      "Epoch 99, batch 158 D Loss: 1.36283278465271, G Loss: 0.6834724545478821\n",
      "Epoch 99, batch 159 D Loss: 1.408247470855713, G Loss: 0.6792985796928406\n",
      "Epoch 99, batch 160 D Loss: 1.424619436264038, G Loss: 0.6687008738517761\n",
      "Epoch 99, batch 161 D Loss: 1.4098882675170898, G Loss: 0.6911648511886597\n",
      "Epoch 99, batch 162 D Loss: 1.4365113973617554, G Loss: 0.6792948842048645\n",
      "Epoch 99, batch 163 D Loss: 1.3528351783752441, G Loss: 0.7007982134819031\n",
      "Epoch 99, batch 164 D Loss: 1.397025465965271, G Loss: 0.6766477227210999\n",
      "Epoch 99, batch 165 D Loss: 1.411967396736145, G Loss: 0.6679744124412537\n",
      "Epoch 99, batch 166 D Loss: 1.409114956855774, G Loss: 0.6940222978591919\n",
      "Epoch 99, batch 167 D Loss: 1.3847203254699707, G Loss: 0.6863327622413635\n",
      "Epoch 99, batch 168 D Loss: 1.3876307010650635, G Loss: 0.6909158229827881\n",
      "Epoch 99, batch 169 D Loss: 1.408227801322937, G Loss: 0.6828258037567139\n",
      "Epoch 99, batch 170 D Loss: 1.3906135559082031, G Loss: 0.7002147436141968\n",
      "Epoch 99, batch 171 D Loss: 1.3860291242599487, G Loss: 0.7025731801986694\n",
      "Epoch 99, batch 172 D Loss: 1.4030834436416626, G Loss: 0.716245710849762\n",
      "Epoch 99, batch 173 D Loss: 1.416586995124817, G Loss: 0.7016546726226807\n",
      "Epoch 99, batch 174 D Loss: 1.406259298324585, G Loss: 0.7068053483963013\n",
      "Epoch 99, batch 175 D Loss: 1.3981659412384033, G Loss: 0.7233726382255554\n",
      "Epoch 99, batch 176 D Loss: 1.3934473991394043, G Loss: 0.7058655619621277\n",
      "Epoch 99, batch 177 D Loss: 1.4017107486724854, G Loss: 0.7049879431724548\n",
      "Epoch 99, batch 178 D Loss: 1.382223129272461, G Loss: 0.7343303561210632\n",
      "Epoch 99, batch 179 D Loss: 1.4206318855285645, G Loss: 0.7089287042617798\n",
      "Epoch 99, batch 180 D Loss: 1.361419677734375, G Loss: 0.7418455481529236\n",
      "Epoch 99, batch 181 D Loss: 1.3742146492004395, G Loss: 0.7342460751533508\n",
      "Epoch 99, batch 182 D Loss: 1.421398401260376, G Loss: 0.701789915561676\n",
      "Epoch 99, batch 183 D Loss: 1.3740715980529785, G Loss: 0.7349768280982971\n",
      "Epoch 99, batch 184 D Loss: 1.3433622121810913, G Loss: 0.772469162940979\n",
      "Epoch 99, batch 185 D Loss: 1.3841255903244019, G Loss: 0.7300906181335449\n",
      "Epoch 99, batch 186 D Loss: 1.3682312965393066, G Loss: 0.7524564862251282\n",
      "Epoch 99, batch 187 D Loss: 1.3653674125671387, G Loss: 0.7367240786552429\n",
      "Epoch 99, batch 188 D Loss: 1.3909904956817627, G Loss: 0.7267736792564392\n",
      "Epoch 99, batch 189 D Loss: 1.3743150234222412, G Loss: 0.7502774000167847\n",
      "Epoch 99, batch 190 D Loss: 1.3746459484100342, G Loss: 0.7563231587409973\n",
      "Epoch 99, batch 191 D Loss: 1.3708614110946655, G Loss: 0.726917028427124\n",
      "Epoch 99, batch 192 D Loss: 1.3245097398757935, G Loss: 0.7969169616699219\n",
      "Epoch 99, batch 193 D Loss: 1.3856581449508667, G Loss: 0.7416872382164001\n",
      "Epoch 99, batch 194 D Loss: 1.4068524837493896, G Loss: 0.7344502210617065\n",
      "Epoch 99, batch 195 D Loss: 1.3602850437164307, G Loss: 0.7431025505065918\n",
      "Epoch 99, batch 196 D Loss: 1.368453025817871, G Loss: 0.7640377283096313\n",
      "Epoch 99, batch 197 D Loss: 1.331478476524353, G Loss: 0.7614444494247437\n",
      "Epoch 99, batch 198 D Loss: 1.3364289999008179, G Loss: 0.7490187883377075\n",
      "Epoch 99, batch 199 D Loss: 1.4625933170318604, G Loss: 0.6982042789459229\n",
      "Epoch 99, batch 200 D Loss: 1.3950831890106201, G Loss: 0.7365226149559021\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "# Build the networks\n",
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "# Training the GAN\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "losses_d = []\n",
    "losses_g = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch = 1\n",
    "    running_d = 0.0\n",
    "    running_g = 0.0\n",
    "    for sample in samples_loader:\n",
    "        # Generate real and fake samples\n",
    "        real_samples = sample\n",
    "        fake_samples = generator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "        # Labels for real and fake samples\n",
    "        real_labels = torch.ones((batch_size, 1))\n",
    "        fake_labels = torch.zeros((batch_size, 1))\n",
    "\n",
    "        # Train discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss_real = criterion(discriminator(real_samples), real_labels)\n",
    "        d_loss_fake = criterion(discriminator(fake_samples.detach()), fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = criterion(discriminator(fake_samples), real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch}, batch {batch} D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "        running_d += d_loss.item()\n",
    "        running_g += g_loss.item()\n",
    "        batch += 1\n",
    "    losses_d.append(running_d / len(samples_loader))\n",
    "    losses_g.append(running_g / len(samples_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e55d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses_d)\n",
    "plt.title('D-Loss vs Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a348cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses_g)\n",
    "plt.title('G-Loss vs Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples using the trained generator\n",
    "generated_samples = generator(torch.randn(3000, latent_dim)).detach()\n",
    "gen_mean = torch.tensor([0, 0], dtype = torch.float32)\n",
    "gen_mean[0] = generated_samples[:,0].mean()\n",
    "gen_mean[1] = generated_samples[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655998a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated samples\n",
    "plt.hist2d(generated_samples[:, 0], generated_samples[:, 1])\n",
    "plt.colorbar()\n",
    "plt.plot(total_mean[0], total_mean[1],label='Real Mean', marker='x', color='red')\n",
    "plt.plot(gen_mean[0], gen_mean[1],label='Generated Mean', marker='x', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d59bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D histogram\n",
    "hist, x_edges, y_edges = np.histogram2d(generated_samples[:, 0], generated_samples[:, 1], bins=20)\n",
    "\n",
    "# Get bin centers for x and y\n",
    "x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "\n",
    "# Create a meshgrid from bin centers\n",
    "x_mesh, y_mesh = np.meshgrid(x_centers, y_centers)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the 3D surface\n",
    "ax.plot_surface(x_mesh, y_mesh, hist.T, cmap='viridis')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592cbcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
