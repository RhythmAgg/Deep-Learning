{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76f213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce59de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1138            6.3             0.510         0.13             2.3      0.076   \n",
       "1139            6.8             0.620         0.08             1.9      0.068   \n",
       "1140            6.2             0.600         0.08             2.0      0.090   \n",
       "1141            5.9             0.550         0.10             2.2      0.062   \n",
       "1142            5.9             0.645         0.12             2.0      0.075   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "\n",
       "      alcohol  quality    Id  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     1  \n",
       "2         9.8        5     2  \n",
       "3         9.8        6     3  \n",
       "4         9.4        5     4  \n",
       "...       ...      ...   ...  \n",
       "1138     11.0        6  1592  \n",
       "1139      9.5        6  1593  \n",
       "1140     10.5        5  1594  \n",
       "1141     11.2        6  1595  \n",
       "1142     10.2        5  1597  \n",
       "\n",
       "[1143 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/WineQT.csv')\n",
    "y = pd.get_dummies(df['quality'])\n",
    "categories,counts = np.unique(df['quality'],return_counts = True)\n",
    "y = np.where(y == True,1.0,0.0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b094de0",
   "metadata": {},
   "source": [
    "#  Dataset Analysis and Preprocessing\n",
    "- We have splitted the data into 70% train, 15% validation and 15% test\n",
    "- for normalising the data we have used standardscalar of the sklearn which transforms each attribute by so that the mean and variance for that feature in the data space are 0 and 1 respectively. This helps in convergence and bringing all features down to same scale so that they dont influence the gradient descent more than they should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a7faee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.311111</td>\n",
       "      <td>0.531339</td>\n",
       "      <td>0.268364</td>\n",
       "      <td>2.532152</td>\n",
       "      <td>0.086933</td>\n",
       "      <td>15.615486</td>\n",
       "      <td>45.914698</td>\n",
       "      <td>0.996730</td>\n",
       "      <td>3.311015</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>10.442111</td>\n",
       "      <td>5.657043</td>\n",
       "      <td>804.969379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.747595</td>\n",
       "      <td>0.179633</td>\n",
       "      <td>0.196686</td>\n",
       "      <td>1.355917</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>10.250486</td>\n",
       "      <td>32.782130</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.156664</td>\n",
       "      <td>0.170399</td>\n",
       "      <td>1.082196</td>\n",
       "      <td>0.805824</td>\n",
       "      <td>463.997116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1597.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "mean       8.311111          0.531339     0.268364        2.532152   0.086933   \n",
       "std        1.747595          0.179633     0.196686        1.355917   0.047267   \n",
       "min        4.600000          0.120000     0.000000        0.900000   0.012000   \n",
       "max       15.900000          1.580000     1.000000       15.500000   0.611000   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "mean            15.615486             45.914698  0.996730  3.311015   \n",
       "std             10.250486             32.782130  0.001925  0.156664   \n",
       "min              1.000000              6.000000  0.990070  2.740000   \n",
       "max             68.000000            289.000000  1.003690  4.010000   \n",
       "\n",
       "      sulphates    alcohol   quality           Id  \n",
       "mean   0.657708  10.442111  5.657043   804.969379  \n",
       "std    0.170399   1.082196  0.805824   463.997116  \n",
       "min    0.330000   8.400000  3.000000     0.000000  \n",
       "max    2.000000  14.900000  8.000000  1597.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats= (df.describe()).loc[[\"mean\",\"std\",\"min\",\"max\"]]\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056d4806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA170lEQVR4nO3dfVgVdf7/8dcR5AgIqKggK96GWaBWUqZmYoitN+iqramtqWlbeVOsmmXurthXoTRvUlfbWlPLlP3+TMpyNTVvyjU3vCu1Vs07NEFSERAJEOb3R5fn2xFMOR6dw/R8XNdc185nPjPznln34rWf+cwcm2EYhgAAACyqitkFAAAA3EyEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHcADLF68WDabzbFUq1ZNoaGh6tSpk5KTk5WVlVVmn8TERNlstgqd5+LFi0pMTNTmzZsrtF9552rUqJF69OhRoeNcy7JlyzR79uxyt9lsNiUmJrr1fO726aefKjo6Wv7+/rLZbPrggw/K7Xfs2DHZbDa99tprbjlvTEyMoqKi3HKsnx8zJibGrccEzOJtdgEA/s+iRYvUvHlzFRcXKysrS1u3btWrr76q1157Tf/85z/VuXNnR9/hw4frt7/9bYWOf/HiRU2ePFmSKvSHzJVzuWLZsmXat2+fEhISymz74osvVL9+/Zteg6sMw1C/fv3UrFkzrVq1Sv7+/rr99tvNLguACDuAR4mKilJ0dLRjvW/fvvrTn/6kBx54QH369NGhQ4cUEhIiSapfv/5N/+N/8eJF+fn53ZJzXcv9999v6vmv5dSpUzp37px69+6t2NhYs8sB8DM8xgI8XIMGDTRjxgzl5eXp73//u6O9vEdLGzduVExMjIKDg+Xr66sGDRqob9++unjxoo4dO6Y6depIkiZPnux4ZDZkyBCn4+3atUuPPPKIatasqaZNm171XJelpqaqZcuWqlatmpo0aaI5c+Y4bb/8iO7YsWNO7Zs3b5bNZnM8UouJidHq1at1/Phxp0d6l5X3GGvfvn3q1auXatasqWrVqumuu+7SkiVLyj3P8uXLNXHiRIWFhSkwMFCdO3fWgQMHrn7jf2br1q2KjY1VQECA/Pz81K5dO61evdqxPTEx0REGX3jhBdlsNjVq1Oi6jv1L/va3v+nBBx9U3bp15e/vrxYtWmjatGkqLi4ut//nn3+u+++/X76+vvrNb36jv/zlLyopKXHqU1RUpClTpqh58+ay2+2qU6eOhg4dqh9++OGa9SxYsECtWrVS9erVFRAQoObNm+ull1664esEbjZGdoBKoFu3bvLy8tJnn3121T7Hjh1T9+7d1aFDB7399tuqUaOGvv/+e61du1ZFRUWqV6+e1q5dq9/+9rcaNmyYhg8fLkmOAHRZnz591L9/fz399NPKz8//xbr27NmjhIQEJSYmKjQ0VO+9956ee+45FRUVady4cRW6xvnz5+uPf/yjDh8+rNTU1Gv2P3DggNq1a6e6detqzpw5Cg4O1tKlSzVkyBCdPn1a48ePd+r/0ksvqX379vrHP/6h3NxcvfDCC4qPj9e3334rLy+vq55ny5YtiouLU8uWLbVw4ULZ7XbNnz9f8fHxWr58uR599FENHz5crVq1Up8+fTR69GgNHDhQdru9QtdfnsOHD2vgwIFq3LixfHx89NVXX2nq1Kn673//q7ffftupb2Zmpvr3768XX3xRL7/8slavXq0pU6YoOztb8+bNkySVlpaqV69e+vzzzzV+/Hi1a9dOx48f16RJkxQTE6MdO3bI19e33FpSUlI0YsQIjR49Wq+99pqqVKmi7777Tt98880NXydw0xkATLdo0SJDkpGWlnbVPiEhIcYdd9zhWJ80aZLx8/8Jr1ixwpBk7Nmz56rH+OGHHwxJxqRJk8psu3y8v/71r1fd9nMNGzY0bDZbmfPFxcUZgYGBRn5+vtO1HT161Knfpk2bDEnGpk2bHG3du3c3GjZsWG7tV9bdv39/w263G+np6U79unbtavj5+Rnnz593Ok+3bt2c+v3v//6vIcn44osvyj3fZffff79Rt25dIy8vz9F26dIlIyoqyqhfv75RWlpqGIZhHD161JBkTJ8+/RePV9G+l5WUlBjFxcXGO++8Y3h5eRnnzp1zbOvYsaMhyfjwww+d9nnyySeNKlWqGMePHzcMwzCWL19uSDLef/99p35paWmGJGP+/PlOx+zYsaNjfdSoUUaNGjWuu17Ak/AYC6gkDMP4xe133XWXfHx89Mc//lFLlizRkSNHXDpP3759r7tvZGSkWrVq5dQ2cOBA5ebmateuXS6d/3pt3LhRsbGxCg8Pd2ofMmSILl68qC+++MKpvWfPnk7rLVu2lCQdP378qufIz8/Xf/7zHz3yyCOqXr26o93Ly0uDBg3SyZMnr/tRmCt2796tnj17Kjg4WF5eXqpataoef/xxlZSU6ODBg059AwICylzjwIEDVVpa6hgR/Pjjj1WjRg3Fx8fr0qVLjuWuu+5SaGjoL76ld9999+n8+fMaMGCAPvzwQ505c8bt1wvcLIQdoBLIz8/X2bNnFRYWdtU+TZs21YYNG1S3bl2NHDlSTZs2VdOmTfX6669X6Fz16tW77r6hoaFXbTt79myFzltRZ8+eLbfWy/foyvMHBwc7rV9+zFRQUHDVc2RnZ8swjAqdx13S09PVoUMHff/993r99df1+eefKy0tTX/729/KrfvyxPWfu/K/i9OnT+v8+fPy8fFR1apVnZbMzMxfDDCDBg3S22+/rePHj6tv376qW7eu2rRpo/Xr17vrkoGbhjk7QCWwevVqlZSUXPN18Q4dOqhDhw4qKSnRjh07NHfuXCUkJCgkJET9+/e/rnNV5Ns9mZmZV227HC6qVasmSSosLHTqd6MjA8HBwcrIyCjTfurUKUlS7dq1b+j4klSzZk1VqVLlpp+nPB988IHy8/O1cuVKNWzY0NG+Z8+ecvufPn26TNuV/13Url1bwcHBWrt2bbnHCAgI+MWahg4dqqFDhyo/P1+fffaZJk2apB49eujgwYNONQKehpEdwMOlp6dr3LhxCgoK0lNPPXVd+3h5ealNmzaOUYDLj5SuZzSjIvbv36+vvvrKqW3ZsmUKCAjQPffcI0mOt5K+/vprp36rVq0qczy73X7dtcXGxmrjxo2O0HHZO++8Iz8/P7e8qu7v7682bdpo5cqVTnWVlpZq6dKlql+/vpo1a3bD5ynP5dD584nOhmHorbfeKrd/Xl5emXu6bNkyValSRQ8++KAkqUePHjp79qxKSkoUHR1dZrne7wL5+/ura9eumjhxooqKirR//35XLhG4ZRjZATzIvn37HPMosrKy9Pnnn2vRokXy8vJSampqmTenfu6NN97Qxo0b1b17dzVo0EA//vij442dyx8jDAgIUMOGDfXhhx8qNjZWtWrVUu3atV1+TTosLEw9e/ZUYmKi6tWrp6VLl2r9+vV69dVX5efnJ0m69957dfvtt2vcuHG6dOmSatasqdTUVG3durXM8Vq0aKGVK1dqwYIFat26tapUqeL03aGfmzRpkj7++GN16tRJf/3rX1WrVi299957Wr16taZNm6agoCCXrulKycnJiouLU6dOnTRu3Dj5+Pho/vz52rdvn5YvX17hr1j/3N69e7VixYoy7ffee6/i4uLk4+OjAQMGaPz48frxxx+1YMECZWdnl3us4OBgPfPMM0pPT1ezZs30r3/9S2+99ZaeeeYZNWjQQJLUv39/vffee+rWrZuee+453XfffapatapOnjypTZs2qVevXurdu3e5x3/yySfl6+ur9u3bq169esrMzFRycrKCgoJ07733unwPgFvC5AnSAIz/e2Pp8uLj42PUrVvX6Nixo5GUlGRkZWWV2efKN6S++OILo3fv3kbDhg0Nu91uBAcHGx07djRWrVrltN+GDRuMu+++27Db7YYkY/DgwU7H++GHH655LsP46W2s7t27GytWrDAiIyMNHx8fo1GjRsbMmTPL7H/w4EGjS5cuRmBgoFGnTh1j9OjRxurVq8u8jXXu3DnjkUceMWrUqGHYbDanc6qct8j27t1rxMfHG0FBQYaPj4/RqlUrY9GiRU59Lr+N9f/+3/9zar/8RtSV/cvz+eefGw899JDh7+9v+Pr6Gvfff7/x0UcflXu8iryNdbXlck0fffSR0apVK6NatWrGb37zG+P555831qxZU+a+dezY0YiMjDQ2b95sREdHG3a73ahXr57x0ksvGcXFxU7nLi4uNl577TXHcatXr240b97ceOqpp4xDhw45HfPnb2MtWbLE6NSpkxESEmL4+PgYYWFhRr9+/Yyvv/76mtcLmM1mGNd4xQMAAKASY84OAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwND4qqJ++hnrq1CkFBATc0AfCAADArWMYhvLy8hQWFqYqVa4+fkPY0U+/cXPlLycDAIDK4cSJE6pfv/5Vt5sadhITEzV58mSntpCQEMeP1xmGocmTJ+vNN99Udna247d+IiMjHf0LCws1btw4LV++XAUFBYqNjdX8+fN/8aKvdPnH706cOKHAwEA3XBkAALjZcnNzFR4efs0fsTV9ZCcyMlIbNmxwrHt5eTn+87Rp0zRz5kwtXrxYzZo105QpUxQXF6cDBw44LiwhIUEfffSRUlJSFBwcrLFjx6pHjx7auXOn07F+yeVHV4GBgYQdAAAqmWtNQTE97Hh7eys0NLRMu2EYmj17tiZOnKg+ffpIkpYsWaKQkBAtW7ZMTz31lHJycrRw4UK9++67jh86XLp0qcLDw7VhwwY9/PDDt/RaAACA5zH9baxDhw4pLCxMjRs3Vv/+/XXkyBFJ0tGjR5WZmakuXbo4+trtdnXs2FHbtm2TJO3cuVPFxcVOfcLCwhQVFeXoU57CwkLl5uY6LQAAwJpMDTtt2rTRO++8o08++URvvfWWMjMz1a5dO509e9YxbyckJMRpn5/P6cnMzJSPj49q1qx51T7lSU5OVlBQkGNhcjIAANZlatjp2rWr+vbtqxYtWqhz585avXq1pJ8eV1125XM4wzCu+WzuWn0mTJignJwcx3LixIkbuAoAAODJTH+M9XP+/v5q0aKFDh065JjHc+UITVZWlmO0JzQ0VEVFRcrOzr5qn/LY7XbHZGQmJQMAYG0eFXYKCwv17bffql69emrcuLFCQ0O1fv16x/aioiJt2bJF7dq1kyS1bt1aVatWdeqTkZGhffv2OfoAAIBfN1Pfxho3bpzi4+PVoEEDZWVlacqUKcrNzdXgwYNls9mUkJCgpKQkRUREKCIiQklJSfLz89PAgQMlSUFBQRo2bJjGjh2r4OBg1apVS+PGjXM8FgMAADA17Jw8eVIDBgzQmTNnVKdOHd1///3avn27GjZsKEkaP368CgoKNGLECMdHBdetW+f08aBZs2bJ29tb/fr1c3xUcPHixdf9jR0AAGBtNsMwDLOLMFtubq6CgoKUk5PD/B0AACqJ6/377VFzdgAAANyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACzN1O/sAHCvRi+uNrsEUxx7pbvZJQDwYIzsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS/M2uwAAMFujF1ebXYIpjr3S3ewSgFuCkR0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBpHhN2kpOTZbPZlJCQ4GgzDEOJiYkKCwuTr6+vYmJitH//fqf9CgsLNXr0aNWuXVv+/v7q2bOnTp48eYurBwAAnsojwk5aWprefPNNtWzZ0ql92rRpmjlzpubNm6e0tDSFhoYqLi5OeXl5jj4JCQlKTU1VSkqKtm7dqgsXLqhHjx4qKSm51ZcBAAA8kOlh58KFC3rsscf01ltvqWbNmo52wzA0e/ZsTZw4UX369FFUVJSWLFmiixcvatmyZZKknJwcLVy4UDNmzFDnzp119913a+nSpdq7d682bNhg1iUBAAAPYnrYGTlypLp3767OnTs7tR89elSZmZnq0qWLo81ut6tjx47atm2bJGnnzp0qLi526hMWFqaoqChHn/IUFhYqNzfXaQEAANbkbebJU1JStGvXLqWlpZXZlpmZKUkKCQlxag8JCdHx48cdfXx8fJxGhC73ubx/eZKTkzV58uQbLR8AAFQCpo3snDhxQs8995yWLl2qatWqXbWfzWZzWjcMo0zbla7VZ8KECcrJyXEsJ06cqFjxAACg0jAt7OzcuVNZWVlq3bq1vL295e3trS1btmjOnDny9vZ2jOhcOUKTlZXl2BYaGqqioiJlZ2dftU957Ha7AgMDnRYAAGBNpoWd2NhY7d27V3v27HEs0dHReuyxx7Rnzx41adJEoaGhWr9+vWOfoqIibdmyRe3atZMktW7dWlWrVnXqk5GRoX379jn6AACAXzfT5uwEBAQoKirKqc3f31/BwcGO9oSEBCUlJSkiIkIRERFKSkqSn5+fBg4cKEkKCgrSsGHDNHbsWAUHB6tWrVoaN26cWrRoUWbCMwAA+HUydYLytYwfP14FBQUaMWKEsrOz1aZNG61bt04BAQGOPrNmzZK3t7f69eungoICxcbGavHixfLy8jKxcgAA4ClshmEYZhdhttzcXAUFBSknJ4f5O6jUGr242uwSTHHsle43tD/3Daicrvfvt+nf2QEAALiZCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSTA07CxYsUMuWLRUYGKjAwEC1bdtWa9ascWw3DEOJiYkKCwuTr6+vYmJitH//fqdjFBYWavTo0apdu7b8/f3Vs2dPnTx58lZfCgAA8FCmhp369evrlVde0Y4dO7Rjxw499NBD6tWrlyPQTJs2TTNnztS8efOUlpam0NBQxcXFKS8vz3GMhIQEpaamKiUlRVu3btWFCxfUo0cPlZSUmHVZAADAg5gaduLj49WtWzc1a9ZMzZo109SpU1W9enVt375dhmFo9uzZmjhxovr06aOoqCgtWbJEFy9e1LJlyyRJOTk5WrhwoWbMmKHOnTvr7rvv1tKlS7V3715t2LDBzEsDAAAewmPm7JSUlCglJUX5+flq27atjh49qszMTHXp0sXRx263q2PHjtq2bZskaefOnSouLnbqExYWpqioKEef8hQWFio3N9dpAQAA1mR62Nm7d6+qV68uu92up59+WqmpqbrzzjuVmZkpSQoJCXHqHxIS4tiWmZkpHx8f1axZ86p9ypOcnKygoCDHEh4e7uarAgAAnsL0sHP77bdrz5492r59u5555hkNHjxY33zzjWO7zWZz6m8YRpm2K12rz4QJE5STk+NYTpw4cWMXAQAAPJbpYcfHx0e33XaboqOjlZycrFatWun1119XaGioJJUZocnKynKM9oSGhqqoqEjZ2dlX7VMeu93ueAPs8gIAAKzJ9LBzJcMwVFhYqMaNGys0NFTr1693bCsqKtKWLVvUrl07SVLr1q1VtWpVpz4ZGRnat2+fow8AAPh18zbz5C+99JK6du2q8PBw5eXlKSUlRZs3b9batWtls9mUkJCgpKQkRUREKCIiQklJSfLz89PAgQMlSUFBQRo2bJjGjh2r4OBg1apVS+PGjVOLFi3UuXNnMy8NAAB4CFPDzunTpzVo0CBlZGQoKChILVu21Nq1axUXFydJGj9+vAoKCjRixAhlZ2erTZs2WrdunQICAhzHmDVrlry9vdWvXz8VFBQoNjZWixcvlpeXl1mXBQAAPIjNMAyjojstXrxY/fr1k5+f382o6ZbLzc1VUFCQcnJymL+DSq3Ri6vNLsEUx17pfkP7c9+Ayul6/367NGdnwoQJCg0N1bBhw37xezYAAABmcynsnDx5UkuXLlV2drY6deqk5s2b69VXX/3Fb9sAAACYwaWw4+XlpZ49e2rlypU6ceKE/vjHP+q9995TgwYN1LNnT3344YcqLS11d60AAAAVdsOvntetW1ft27dX27ZtVaVKFe3du1dDhgxR06ZNtXnzZjeUCAAA4DqXw87p06f12muvKTIyUjExMcrNzdXHH3+so0eP6tSpU+rTp48GDx7szloBAAAqzKVXz+Pj4/XJJ5+oWbNmevLJJ/X444+rVq1aju2+vr4aO3asZs2a5bZCAQAAXOFS2Klbt662bNmitm3bXrVPvXr1dPToUZcLAwAAcAeXws7ChQuv2cdms6lhw4auHB4AAMBtXJqz8+yzz2rOnDll2ufNm6eEhIQbrQkAAMBtXAo777//vtq3b1+mvV27dlqxYsUNFwUAAOAuLoWds2fPKigoqEx7YGCgzpw5c8NFAQAAuItLYee2227T2rVry7SvWbNGTZo0ueGiAAAA3MWlCcpjxozRqFGj9MMPP+ihhx6SJH366aeaMWOGZs+e7c76AAAAbohLYeeJJ55QYWGhpk6dqv/5n/+RJDVq1EgLFizQ448/7tYCAQAAboRLYUeSnnnmGT3zzDP64Ycf5Ovrq+rVq7uzLgAAALdw+eciLl26pA0bNmjlypUyDEOSdOrUKV24cMFtxQEAANwol0Z2jh8/rt/+9rdKT09XYWGh4uLiFBAQoGnTpunHH3/UG2+84e46AQAAXOLSyM5zzz2n6OhoZWdny9fX19Heu3dvffrpp24rDgAA4Ea5NLKzdetW/fvf/5aPj49Te8OGDfX999+7pTAAAAB3cGlkp7S0VCUlJWXaT548qYCAgBsuCgAAwF1cCjtxcXFO39Ox2Wy6cOGCJk2apG7durmrNgAAgBvm0mOsWbNmqVOnTrrzzjv1448/auDAgTp06JBq166t5cuXu7tGAAAAl7kUdsLCwrRnzx4tX75cu3btUmlpqYYNG6bHHnvMacIyAACA2Vz+qKCvr6+eeOIJPfHEE+6sBwAAwK1cCjvvvPPOL27nJyMAAICncCnsPPfcc07rxcXFunjxonx8fOTn50fYAQAAHsOlt7Gys7OdlgsXLujAgQN64IEHmKAMAAA8isu/jXWliIgIvfLKK2VGfQAAAMzktrAjSV5eXjp16pQ7DwkAAHBDXJqzs2rVKqd1wzCUkZGhefPmqX379m4pDAAAwB1cCju/+93vnNZtNpvq1Kmjhx56SDNmzHBHXQAAAG7hUtgpLS11dx0AAAA3hVvn7AAAAHgal0Z2xowZc919Z86c6copAAAA3MKlsLN7927t2rVLly5d0u233y5JOnjwoLy8vHTPPfc4+tlsNvdUCQAA4CKXwk58fLwCAgK0ZMkS1axZU9JPHxocOnSoOnTooLFjx7q1SAAAAFe5NGdnxowZSk5OdgQdSapZs6amTJnC21gAAMCjuBR2cnNzdfr06TLtWVlZysvLu+GiAAAA3MWlsNO7d28NHTpUK1as0MmTJ3Xy5EmtWLFCw4YNU58+fdxdIwAAgMtcmrPzxhtvaNy4cfrDH/6g4uLinw7k7a1hw4Zp+vTpbi0QAADgRrgUdvz8/DR//nxNnz5dhw8flmEYuu222+Tv7+/u+gAAAG7IDX1UMCMjQxkZGWrWrJn8/f1lGIa76gIAAHALl8LO2bNnFRsbq2bNmqlbt27KyMiQJA0fPpzXzgEAgEdxKez86U9/UtWqVZWeni4/Pz9H+6OPPqq1a9e6rTgAAIAb5dKcnXXr1umTTz5R/fr1ndojIiJ0/PhxtxQGAADgDi6N7OTn5zuN6Fx25swZ2e32Gy4KAADAXVwKOw8++KDeeecdx7rNZlNpaammT5+uTp06ua04AACAG+XSY6zp06crJiZGO3bsUFFRkcaPH6/9+/fr3Llz+ve//+3uGgEAAFzm0sjOnXfeqa+//lr33Xef4uLilJ+frz59+mj37t1q2rSpu2sEAABwWYVHdoqLi9WlSxf9/e9/1+TJk29GTQAAAG5T4ZGdqlWrat++fbLZbDejHgAAALdy6THW448/roULF7q7FgAAALdzaYJyUVGR/vGPf2j9+vWKjo4u85tYM2fOdEtxAAAAN6pCYefIkSNq1KiR9u3bp3vuuUeSdPDgQac+PN4CAACepEJhJyIiQhkZGdq0aZOkn34eYs6cOQoJCbkpxQEAANyoCs3ZufJXzdesWaP8/Hy3FgQAAOBOLk1QvuzK8AMAAOBpKhR2bDZbmTk5zNEBAACerEJzdgzD0JAhQxw/9vnjjz/q6aefLvM21sqVK91XIQAAwA2oUNgZPHiw0/of/vAHtxYDAADgbhUKO4sWLbpZdQAAANwUNzRBGQAAwNOZGnaSk5N17733KiAgQHXr1tXvfvc7HThwwKmPYRhKTExUWFiYfH19FRMTo/379zv1KSws1OjRo1W7dm35+/urZ8+eOnny5K28FAAA4KFMDTtbtmzRyJEjtX37dq1fv16XLl1Sly5dnL7dM23aNM2cOVPz5s1TWlqaQkNDFRcXp7y8PEefhIQEpaamKiUlRVu3btWFCxfUo0cPlZSUmHFZAADAg7j021jusnbtWqf1RYsWqW7dutq5c6cefPBBGYah2bNna+LEierTp48kacmSJQoJCdGyZcv01FNPKScnRwsXLtS7776rzp07S5KWLl2q8PBwbdiwQQ8//PAtvy4AAOA5PGrOTk5OjiSpVq1akqSjR48qMzNTXbp0cfSx2+3q2LGjtm3bJknauXOniouLnfqEhYUpKirK0edKhYWFys3NdVoAAIA1eUzYMQxDY8aM0QMPPKCoqChJUmZmpiSV+e2tkJAQx7bMzEz5+PioZs2aV+1zpeTkZAUFBTmW8PBwd18OAADwEB4TdkaNGqWvv/5ay5cvL7Ptyq80G4ZxzS83/1KfCRMmKCcnx7GcOHHC9cIBAIBH84iwM3r0aK1atUqbNm1S/fr1He2hoaGSVGaEJisryzHaExoaqqKiImVnZ1+1z5XsdrsCAwOdFgAAYE2mhh3DMDRq1CitXLlSGzduVOPGjZ22N27cWKGhoVq/fr2jraioSFu2bFG7du0kSa1bt1bVqlWd+mRkZGjfvn2OPgAA4NfL1LexRo4cqWXLlunDDz9UQECAYwQnKChIvr6+stlsSkhIUFJSkiIiIhQREaGkpCT5+flp4MCBjr7Dhg3T2LFjFRwcrFq1amncuHFq0aKF4+0sAADw62Vq2FmwYIEkKSYmxql90aJFGjJkiCRp/PjxKigo0IgRI5Sdna02bdpo3bp1CggIcPSfNWuWvL291a9fPxUUFCg2NlaLFy+Wl5fXrboUAADgoWyGYRhmF2G23NxcBQUFKScnh/k7qNQavbja7BJMceyV7je0P/cNqJyu9++3R0xQBgAAuFkIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNK8zS4AAFD5NHpxtdklmObYK93NLgEVxMgOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNFPDzmeffab4+HiFhYXJZrPpgw8+cNpuGIYSExMVFhYmX19fxcTEaP/+/U59CgsLNXr0aNWuXVv+/v7q2bOnTp48eQuvAgAAeDJTw05+fr5atWqlefPmlbt92rRpmjlzpubNm6e0tDSFhoYqLi5OeXl5jj4JCQlKTU1VSkqKtm7dqgsXLqhHjx4qKSm5VZcBAAA8mLeZJ+/atau6du1a7jbDMDR79mxNnDhRffr0kSQtWbJEISEhWrZsmZ566inl5ORo4cKFevfdd9W5c2dJ0tKlSxUeHq4NGzbo4YcfvmXXAgAAPJPHztk5evSoMjMz1aVLF0eb3W5Xx44dtW3bNknSzp07VVxc7NQnLCxMUVFRjj7lKSwsVG5urtMCAACsyWPDTmZmpiQpJCTEqT0kJMSxLTMzUz4+PqpZs+ZV+5QnOTlZQUFBjiU8PNzN1QMAAE/hsWHnMpvN5rRuGEaZtitdq8+ECROUk5PjWE6cOOGWWgEAgOfx2LATGhoqSWVGaLKyshyjPaGhoSoqKlJ2dvZV+5THbrcrMDDQaQEAANbksWGncePGCg0N1fr16x1tRUVF2rJli9q1aydJat26tapWrerUJyMjQ/v27XP0AQAAv26mvo114cIFfffdd471o0ePas+ePapVq5YaNGighIQEJSUlKSIiQhEREUpKSpKfn58GDhwoSQoKCtKwYcM0duxYBQcHq1atWho3bpxatGjheDsLAAD8upkadnbs2KFOnTo51seMGSNJGjx4sBYvXqzx48eroKBAI0aMUHZ2ttq0aaN169YpICDAsc+sWbPk7e2tfv36qaCgQLGxsVq8eLG8vLxu+fUAAADPY2rYiYmJkWEYV91us9mUmJioxMTEq/apVq2a5s6dq7lz596ECgEAQGXnsXN2AAAA3IGwAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALM3b7AKA8jR6cbXZJZji2CvdzS4BACyHkR0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBp/DYWAAC3CL/7Zw5GdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKVZ5tXz+fPna/r06crIyFBkZKRmz56tDh06mF3Wr/Y1Q8n8Vw0BAJAsMrLzz3/+UwkJCZo4caJ2796tDh06qGvXrkpPTze7NAAAYDJLhJ2ZM2dq2LBhGj58uO644w7Nnj1b4eHhWrBggdmlAQAAk1X6sFNUVKSdO3eqS5cuTu1dunTRtm3bTKoKAAB4iko/Z+fMmTMqKSlRSEiIU3tISIgyMzPL3aewsFCFhYWO9ZycHElSbm6u2+srLbzo9mNWFjdyP3+t9+1G/w1y31zDfau4X+s9k7hvrrgZf19/flzDMH6xX6UPO5fZbDandcMwyrRdlpycrMmTJ5dpDw8Pvym1/VoFzTa7gsqHe+Ya7ptruG+u4b5V3M2+Z3l5eQoKCrrq9kofdmrXri0vL68yozhZWVllRnsumzBhgsaMGeNYLy0t1blz5xQcHHzVgFQZ5ebmKjw8XCdOnFBgYKDZ5VQK3DPXcN9cw31zDfet4qx6zwzDUF5ensLCwn6xX6UPOz4+PmrdurXWr1+v3r17O9rXr1+vXr16lbuP3W6X3W53aqtRo8bNLNNUgYGBlvrHfStwz1zDfXMN98013LeKs+I9+6URncsqfdiRpDFjxmjQoEGKjo5W27Zt9eabbyo9PV1PP/202aUBAACTWSLsPProozp79qxefvllZWRkKCoqSv/617/UsGFDs0sDAAAms0TYkaQRI0ZoxIgRZpfhUex2uyZNmlTmkR2ujnvmGu6ba7hvruG+Vdyv/Z7ZjGu9rwUAAFCJVfqPCgIAAPwSwg4AALA0wg4AALA0wg4AALA0wo7FLFiwQC1btnR8OKpt27Zas2aN2WVVOsnJybLZbEpISDC7FI+WmJgom83mtISGhppdlsf7/vvv9Yc//EHBwcHy8/PTXXfdpZ07d5pdlkdr1KhRmX9rNptNI0eONLs0j3bp0iX9+c9/VuPGjeXr66smTZro5ZdfVmlpqdml3VKWefUcP6lfv75eeeUV3XbbbZKkJUuWqFevXtq9e7ciIyNNrq5ySEtL05tvvqmWLVuaXUqlEBkZqQ0bNjjWvby8TKzG82VnZ6t9+/bq1KmT1qxZo7p16+rw4cOW/oq7O6SlpamkpMSxvm/fPsXFxen3v/+9iVV5vldffVVvvPGGlixZosjISO3YsUNDhw5VUFCQnnvuObPLu2UIOxYTHx/vtD516lQtWLBA27dvJ+xchwsXLuixxx7TW2+9pSlTpphdTqXg7e3NaE4FvPrqqwoPD9eiRYscbY0aNTKvoEqiTp06TuuvvPKKmjZtqo4dO5pUUeXwxRdfqFevXurevbukn/6tLV++XDt27DC5sluLx1gWVlJSopSUFOXn56tt27Zml1MpjBw5Ut27d1fnzp3NLqXSOHTokMLCwtS4cWP1799fR44cMbskj7Zq1SpFR0fr97//verWrau7775bb731ltllVSpFRUVaunSpnnjiCUv9ePPN8MADD+jTTz/VwYMHJUlfffWVtm7dqm7duplc2a3FyI4F7d27V23bttWPP/6o6tWrKzU1VXfeeafZZXm8lJQU7dq1S2lpaWaXUmm0adNG77zzjpo1a6bTp09rypQpateunfbv36/g4GCzy/NIR44c0YIFCzRmzBi99NJL+vLLL/Xss8/Kbrfr8ccfN7u8SuGDDz7Q+fPnNWTIELNL8XgvvPCCcnJy1Lx5c3l5eamkpERTp07VgAEDzC7tluILyhZUVFSk9PR0nT9/Xu+//77+8Y9/aMuWLQSeX3DixAlFR0dr3bp1atWqlSQpJiZGd911l2bPnm1ucZVIfn6+mjZtqvHjx2vMmDFml+ORfHx8FB0drW3btjnann32WaWlpemLL74wsbLK4+GHH5aPj48++ugjs0vxeCkpKXr++ec1ffp0RUZGas+ePUpISNDMmTM1ePBgs8u7ZRjZsSAfHx/HBOXo6GilpaXp9ddf19///neTK/NcO3fuVFZWllq3bu1oKykp0WeffaZ58+apsLCQibfXwd/fXy1atNChQ4fMLsVj1atXr8z/8bjjjjv0/vvvm1RR5XL8+HFt2LBBK1euNLuUSuH555/Xiy++qP79+0uSWrRooePHjys5OZmwA2sxDEOFhYVml+HRYmNjtXfvXqe2oUOHqnnz5nrhhRcIOtepsLBQ3377rTp06GB2KR6rffv2OnDggFPbwYMH1bBhQ5MqqlwWLVqkunXrOibc4pddvHhRVao4T8/18vLi1XNUbi+99JK6du2q8PBw5eXlKSUlRZs3b9batWvNLs2jBQQEKCoqyqnN399fwcHBZdrxf8aNG6f4+Hg1aNBAWVlZmjJlinJzc39V/4+xov70pz+pXbt2SkpKUr9+/fTll1/qzTff1Jtvvml2aR6vtLRUixYt0uDBg+XtzZ+v6xEfH6+pU6eqQYMGioyM1O7duzVz5kw98cQTZpd2S/GvxWJOnz6tQYMGKSMjQ0FBQWrZsqXWrl2ruLg4s0uDBZ08eVIDBgzQmTNnVKdOHd1///3avn07oxS/4N5771VqaqomTJigl19+WY0bN9bs2bP12GOPmV2ax9uwYYPS09N/dX+ob8TcuXP1l7/8RSNGjFBWVpbCwsL01FNP6a9//avZpd1STFAGAACWxnd2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AFQKNptNH3zwgdllAKiECDsAPEJmZqZGjx6tJk2ayG63Kzw8XPHx8fr000/NLg1AJcfPRQAw3bFjx9S+fXvVqFFD06ZNU8uWLVVcXKxPPvlEI0eO1H//+1+zSwRQiTGyA8B0I0aMkM1m05dffqlHHnlEzZo1U2RkpMaMGaPt27eXu88LL7ygZs2ayc/PT02aNNFf/vIXFRcXO7Z/9dVX6tSpkwICAhQYGKjWrVtrx44dkqTjx48rPj5eNWvWlL+/vyIjI/Wvf/3Lse8333yjbt26qXr16goJCdGgQYN05swZx/YVK1aoRYsW8vX1VXBwsDp37qz8/PybdHcA3ChGdgCY6ty5c1q7dq2mTp0qf3//Mttr1KhR7n4BAQFavHixwsLCtHfvXj355JMKCAjQ+PHjJUmPPfaY7r77bi1YsEBeXl7as2ePqlatKkkaOXKkioqK9Nlnn8nf31/ffPONqlevLknKyMhQx44d9eSTT2rmzJkqKCjQCy+8oH79+mnjxo3KyMjQgAEDNG3aNPXu3Vt5eXn6/PPPxc8MAp6LsAPAVN99950Mw1Dz5s0rtN+f//xnx39u1KiRxo4dq3/+85+OsJOenq7nn3/ecdyIiAhH//T0dPXt21ctWrSQJDVp0sSxbcGCBbrnnnuUlJTkaHv77bcVHh6ugwcP6sKFC7p06ZL69Onj+HX3y8cB4JkIOwBMdXlExGazVWi/FStWaPbs2fruu+8cASQwMNCxfcyYMRo+fLjeffddde7cWb///e/VtGlTSdKzzz6rZ555RuvWrVPnzp3Vt29ftWzZUpK0c+dObdq0yTHS83OHDx9Wly5dFBsbqxYtWujhhx9Wly5d9Mgjj6hmzZqu3gIANxlzdgCYKiIiQjabTd9+++1177N9+3b1799fXbt21ccff6zdu3dr4sSJKioqcvRJTEzU/v371b17d23cuFF33nmnUlNTJUnDhw/XkSNHNGjQIO3du1fR0dGaO3euJKm0tFTx8fHas2eP03Lo0CE9+OCD8vLy0vr167VmzRrdeeedmjt3rm6//XYdPXrUvTcGgNvYDB40AzBZ165dtXfvXh04cKDMvJ3z58+rRo0astlsSk1N1e9+9zvNmDFD8+fP1+HDhx39hg8frhUrVuj8+fPlnmPAgAHKz8/XqlWrymybMGGCVq9era+//loTJ07U+++/r3379snb+9qD3yUlJWrYsKHGjBmjMWPGVOzCAdwSjOwAMN38+fNVUlKi++67T++//74OHTqkb7/9VnPmzFHbtm3L9L/tttuUnp6ulJQUHT58WHPmzHGM2khSQUGBRo0apc2bN+v48eP697//rbS0NN1xxx2SpISEBH3yySc6evSodu3apY0bNzq2jRw5UufOndOAAQP05Zdf6siRI1q3bp2eeOIJlZSU6D//+Y+SkpK0Y8cOpaena+XKlfrhhx8c+wPwQAYAeIBTp04ZI0eONBo2bGj4+PgYv/nNb4yePXsamzZtMgzDMCQZqampjv7PP/+8ERwcbFSvXt149NFHjVmzZhlBQUGGYRhGYWGh0b9/fyM8PNzw8fExwsLCjFGjRhkFBQWGYRjGqFGjjKZNmxp2u92oU6eOMWjQIOPMmTOOYx88eNDo3bu3UaNGDcPX19do3ry5kZCQYJSWlhrffPON8fDDDxt16tQx7Ha70axZM2Pu3Lm36jYBcAGPsQAAgKXxGAsAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFja/wfY57PDt7P3IwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(categories,counts)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequecy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a0f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_rest, y_train, y_rest = train_test_split(df.iloc[:,:-2], y, test_size=0.3, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_rest, y_rest, test_size=0.5, random_state=42)\n",
    "scalar = StandardScaler()\n",
    "scalar.fit(x_train.values)\n",
    "x_train = scalar.transform(x_train.values)\n",
    "x_val = scalar.transform(x_val.values)\n",
    "x_test = scalar.transform(x_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eeeb32",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102394c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xent(y_true,y_pred):\n",
    "    eps = 1e-08\n",
    "    return -np.sum(y_true*np.log(y_pred)) / len(y_true)\n",
    "\n",
    "def tanh(y):\n",
    "    return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n",
    "\n",
    "def relu(y):\n",
    "    return np.where(y >= 0,y,0.0)\n",
    "\n",
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "def Softmax(y):\n",
    "    total = np.exp(y)\n",
    "    return total / np.sum(total,axis =1,keepdims = True)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,input_size,output_size,num_layers,layer_sizes,activations,optimiser = 'batch',lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.optimiser = optimiser\n",
    "#         Initialising weights and biases\n",
    "        self.w_and_b = []\n",
    "        prev_size = input_size\n",
    "        for layer in range(num_layers):\n",
    "            self.w_and_b.append(0.1*np.random.rand(layer_sizes[layer],prev_size+1))\n",
    "            prev_size = layer_sizes[layer]\n",
    "        self.w_and_b.append(np.random.rand(output_size,prev_size+1))\n",
    "                            \n",
    "        self.layer_inputs = []    \n",
    "        self.layer_outputs = []\n",
    "        self.out = np.zeros(output_size)\n",
    "    \n",
    "    def tanh_grad(self,z):\n",
    "        return 1 - tanh(z)**2\n",
    "    \n",
    "    def relu_grad(self,z):\n",
    "        return np.where(z > 0,1.0,0.0)\n",
    "                            \n",
    "    def sigmoid_grad(self,z):\n",
    "        return sigmoid(z)*(1 - sigmoid(z))\n",
    "    \n",
    "    def XentGrad(self,y_true,y_pred):\n",
    "        eps = 1e-15\n",
    "        return -(1/len(y_true))*(y_true/(y_pred+eps))                       \n",
    "    \n",
    "    def softmax_jacobian(self,p):\n",
    "        n = p.shape[1]\n",
    "        jacobian = np.zeros((len(p),n, n))\n",
    "        \n",
    "        for sample in range(len(p)):\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i == j:\n",
    "                        jacobian[sample,i, j] = p[sample,i] * (1 - p[sample,i])\n",
    "                    else:\n",
    "                        jacobian[sample,i, j] = -p[sample,i] * p[sample,j]\n",
    "\n",
    "        return jacobian\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = np.append(x,np.ones((x.shape[0],1)),axis=1)\n",
    "        for layer in range(self.num_layers):    \n",
    "            self.layer_inputs.append(inp) # (914,12)\n",
    "            z = inp @ self.w_and_b[layer].T\n",
    "            y = (self.activations[layer])(z)\n",
    "            self.layer_outputs.append(z)\n",
    "            inp = np.append(y,np.ones((y.shape[0],1)),axis=1)\n",
    "\n",
    "        self.layer_inputs.append(inp) # (914,12)\n",
    "        z = inp @ self.w_and_b[self.num_layers].T\n",
    "        self.layer_outputs.append(z)\n",
    "        y_prob = Softmax(z) \n",
    "        self.out = y_prob\n",
    "        return y_prob\n",
    "    \n",
    "    def backward(self,y_true):\n",
    "        w_and_b_gradients = []\n",
    "        grad_y_prob = self.XentGrad(y_true,self.out) # (914,7)\n",
    "        grad_y_linear = np.vectorize(lambda jac,grad: np.dot(jac,grad),signature='(n,n),(n)->(n)')(self.softmax_jacobian(self.out),grad_y_prob)  # (batch,output)\n",
    "        grad_w_and_b = grad_y_linear.T @ self.layer_inputs[-1]\n",
    "        w_and_b_gradients.append(grad_w_and_b)\n",
    "        grad_y = grad_y_linear@self.w_and_b[-1][:,:-1]\n",
    "        for layer in range(self.num_layers-1,-1,-1):\n",
    "            grad_z = []\n",
    "            if(self.activations[layer] == tanh):\n",
    "                grad_z = grad_y*self.tanh_grad(self.layer_outputs[layer])\n",
    "            elif(self.activations[layer] == relu):\n",
    "                grad_z = grad_y*self.relu_grad(self.layer_outputs[layer])\n",
    "            elif(self.activations[layer] == sigmoid):\n",
    "                grad_z = grad_y*self.sigmoid_grad(self.layer_outputs[layer])\n",
    "            \n",
    "            grad_w_and_b = grad_z.T @ self.layer_inputs[layer] \n",
    "            w_and_b_gradients.append(grad_w_and_b)\n",
    "            grad_y = grad_z@self.w_and_b[layer][:,:-1]\n",
    "            \n",
    "        for layer in range(self.num_layers,-1,-1):\n",
    "            self.w_and_b[layer] -= self.lr*w_and_b_gradients[self.num_layers-layer]\n",
    "    \n",
    "    def training(self,x,y,epochs):\n",
    "        num_samples = len(x)\n",
    "        batch_size = 0\n",
    "        if(self.optimiser == 'batch'):\n",
    "            batch_size = x.shape[0]\n",
    "        elif(self.optimiser == 'sgd'):\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = 200\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(len(x))\n",
    "            x_shuffled = x[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            total_batches = math.ceil(num_samples / batch_size)\n",
    "            for i in range(0,total_batches,batch_size):\n",
    "                out = self.forward(x_shuffled[i:batch_size+i])\n",
    "                self.backward(y_shuffled[i:batch_size+i])\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38114d7c",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687908cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhushi1703\u001b[0m (\u001b[33msmai-khushi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b288e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361c1fa",
   "metadata": {},
   "source": [
    "## Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d7ffbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l9bdja78) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120ed7f8551043798b0bf1b6d8c8360d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-terrain-16</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/l9bdja78' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/l9bdja78</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_010747-l9bdja78/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l9bdja78). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da749208db941738c6a69375a2d0cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168009721829245, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_010811-t477x2w7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/t477x2w7' target=\"_blank\">lucky-butterfly-17</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/t477x2w7' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/t477x2w7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Training => Accuracy : 0.5425\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Training => Loss : 1.0601\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => Accuracy : 0.5465\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => f1 : 0.5200\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => precision : 0.5048\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Training => Accuracy : 0.5587\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Training => Loss : 1.0383\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => Accuracy : 0.5407\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => f1 : 0.5293\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => precision : 0.5403\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Training => Accuracy : 0.5938\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Training => Loss : 1.0146\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => f1 : 0.5741\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => precision : 0.5649\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Training => Accuracy : 0.5112\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Training => Loss : 1.1071\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => Accuracy : 0.5000\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => f1 : 0.4807\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => precision : 0.5091\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Training => Accuracy : 0.5925\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Training => Loss : 0.9861\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => Accuracy : 0.5407\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => f1 : 0.5216\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => precision : 0.5144\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Training => Accuracy : 0.5962\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Training => Loss : 0.9697\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => Accuracy : 0.5814\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => f1 : 0.5645\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => precision : 0.5565\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Training => Accuracy : 0.5238\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Training => Loss : 1.0675\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => f1 : 0.5228\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => precision : 0.5509\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Training => Accuracy : 0.5962\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Training => Loss : 0.9782\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => f1 : 0.5390\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => precision : 0.5263\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Training => Accuracy : 0.6100\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Training => Loss : 0.9270\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => Accuracy : 0.6163\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => f1 : 0.5973\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => precision : 0.5840\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => recall : 0.6163\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Training => Accuracy : 0.5212\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Training => Loss : 1.0835\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => f1 : 0.5203\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => precision : 0.5140\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Training => Accuracy : 0.5950\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Training => Loss : 0.9541\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => Accuracy : 0.5930\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => f1 : 0.5696\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => precision : 0.5573\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Training => Accuracy : 0.5675\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Training => Loss : 1.0266\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => Accuracy : 0.5407\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => f1 : 0.5065\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => precision : 0.5291\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Training => Accuracy : 0.5475\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Training => Loss : 0.9904\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => f1 : 0.5559\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => precision : 0.5572\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Training => Accuracy : 0.5775\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Training => Loss : 0.9756\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => Accuracy : 0.6105\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => f1 : 0.5852\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => precision : 0.5776\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Training => Accuracy : 0.6100\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Training => Loss : 0.8906\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => f1 : 0.5552\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => precision : 0.5431\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Training => Accuracy : 0.5212\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Training => Loss : 1.0533\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => f1 : 0.4977\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => precision : 0.4873\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => recall : 0.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Training => Accuracy : 0.6250\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Training => Loss : 0.9185\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => Accuracy : 0.5988\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => f1 : 0.5813\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => precision : 0.5711\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Training => Accuracy : 0.6062\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Training => Loss : 0.8517\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => f1 : 0.5619\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => precision : 0.5532\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Training => Accuracy : 0.5725\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Training => Loss : 1.0283\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => f1 : 0.5195\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => precision : 0.5083\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Training => Accuracy : 0.5737\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Training => Loss : 1.0118\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => f1 : 0.5356\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => precision : 0.5232\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Training => Accuracy : 0.5737\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Training => Loss : 0.9713\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => Accuracy : 0.5698\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => f1 : 0.5554\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => precision : 0.5551\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Training => Accuracy : 0.5663\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Training => Loss : 1.0018\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => f1 : 0.5310\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => precision : 0.5281\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Training => Accuracy : 0.5775\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Training => Loss : 0.9604\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => Accuracy : 0.5523\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => f1 : 0.5367\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => precision : 0.5272\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Training => Accuracy : 0.5250\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Training => Loss : 1.2869\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => Accuracy : 0.5174\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => f1 : 0.4280\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => precision : 0.4474\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => recall : 0.5174\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Training => Accuracy : 0.5125\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Training => Loss : 1.0956\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => f1 : 0.4427\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => precision : 0.4384\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => recall : 0.4709\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Training => Accuracy : 0.5663\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Training => Loss : 0.9556\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => f1 : 0.5643\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => precision : 0.5547\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Training => Accuracy : 0.5975\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Training => Loss : 0.8980\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => f1 : 0.5438\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => precision : 0.5309\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Training => Accuracy : 0.5550\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Training => Loss : 1.0541\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => f1 : 0.5343\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => precision : 0.5378\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Training => Accuracy : 0.5363\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Training => Loss : 0.9847\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => Accuracy : 0.5000\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => f1 : 0.4838\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => precision : 0.4713\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Training => Accuracy : 0.5500\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Training => Loss : 1.0322\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => Accuracy : 0.5349\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => f1 : 0.5040\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => precision : 0.4909\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Training => Accuracy : 0.5450\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Training => Loss : 1.0159\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => Accuracy : 0.5407\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => f1 : 0.5162\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => precision : 0.5303\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Training => Accuracy : 0.5763\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Training => Loss : 0.9626\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => Accuracy : 0.5291\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => f1 : 0.5085\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => precision : 0.5270\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => recall : 0.5291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Training => Accuracy : 0.6088\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Training => Loss : 0.9099\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => f1 : 0.5585\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => precision : 0.5565\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Training => Accuracy : 0.4600\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Training => Loss : 1.1370\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => Accuracy : 0.5058\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => f1 : 0.4632\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => precision : 0.4781\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => recall : 0.5058\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Training => Accuracy : 0.5875\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Training => Loss : 0.9593\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => f1 : 0.5779\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => precision : 0.5778\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Training => Accuracy : 0.6088\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Training => Loss : 0.9076\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => Accuracy : 0.5291\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => f1 : 0.5069\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => precision : 0.5241\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Training => Accuracy : 0.4713\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Training => Loss : 1.1440\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => Accuracy : 0.4302\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => f1 : 0.4061\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => precision : 0.3939\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => recall : 0.4302\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Training => Accuracy : 0.5563\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Training => Loss : 0.9809\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => f1 : 0.5344\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => precision : 0.5249\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Training => Accuracy : 0.4863\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Training => Loss : 1.0724\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => Accuracy : 0.5058\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => f1 : 0.4957\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => precision : 0.5133\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => recall : 0.5058\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Training => Accuracy : 0.4788\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Training => Loss : 1.1122\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => f1 : 0.4166\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => precision : 0.4149\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => recall : 0.4709\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Training => Accuracy : 0.5613\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Training => Loss : 1.2637\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => Accuracy : 0.5407\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => f1 : 0.5024\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => precision : 0.4712\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Training => Accuracy : 0.6038\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Training => Loss : 0.9303\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => Accuracy : 0.5698\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => f1 : 0.5497\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => precision : 0.5497\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Training => Accuracy : 0.5463\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Training => Loss : 1.0336\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => f1 : 0.4984\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => precision : 0.4853\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => recall : 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_17616/3102918169.py:50: RuntimeWarning: invalid value encountered in divide\n",
      "  return -(1/len(y_true))*(y_true/y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Training => Accuracy : 0.0075\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Training => Loss : nan\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => Accuracy : 0.0000\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => f1 : 0.0000\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => precision : 0.0000\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => recall : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_17616/3102918169.py:50: RuntimeWarning: invalid value encountered in divide\n",
      "  return -(1/len(y_true))*(y_true/y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Training => Accuracy : 0.0075\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Training => Loss : nan\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => Accuracy : 0.0000\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => f1 : 0.0000\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => precision : 0.0000\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => recall : 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69e93b21cd54f23a56522ac990c59a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▄▄▃▅▃▂▄▃▄▂▃▃▃▁▄▁▃▃▂▃▂█▅▂▄▃▄▃▂▁▅▂▅▃▄▅█▂▄ </td></tr><tr><td>Validation accuracy</td><td>▇▇█▇▇█▇▇▇█▇███▇█▇▇█▇▇▇▆█▇▇▇▇▇▇▇█▆▇▇▆▇█▇▁</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>nan</td></tr><tr><td>Validation accuracy</td><td>0.0</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-butterfly-17</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/t477x2w7' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/t477x2w7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_010811-t477x2w7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4723d4a056724e27b14f56eb94686448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168066666383918, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_011427-9lc1frhg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/9lc1frhg' target=\"_blank\">silvery-paper-18</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/9lc1frhg' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/9lc1frhg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5375\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.1213\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.5756\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.5171\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.4853\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5800\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Training => Loss : 1.0443\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5058\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.4854\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.4789\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5058\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.5750\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Training => Loss : 1.0433\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5527\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5529\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5238\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.0786\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.5328\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.5872\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5775\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Training => Loss : 1.0513\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.5255\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.5064\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.5813\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.9977\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5695\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5516\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5212\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.0904\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.4826\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.4541\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.4418\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.4826\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5825\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Training => Loss : 0.9981\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5872\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.5666\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.5505\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.5962\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.9611\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5402\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5251\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5238\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0399\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5465\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.5294\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.5251\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5613\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Training => Loss : 0.9945\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.5756\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5479\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.5339\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.5950\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.9090\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5436\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.5266\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5663\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0266\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.5171\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.5049\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5600\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Training => Loss : 0.9655\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.5523\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5314\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.5212\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.6175\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.8981\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5536\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.5459\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5962\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0034\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.5604\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.5473\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5787\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Loss : 0.9633\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.5756\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5553\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.5417\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.6225\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.8734\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5610\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.5449\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Training => Accuracy : 0.5787\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Training => Loss : 0.9941\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => f1 : 0.5529\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => precision : 0.5361\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Training => Accuracy : 0.5775\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Training => Loss : 0.9733\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => f1 : 0.5402\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => precision : 0.5240\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Training => Accuracy : 0.6125\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Training => Loss : 0.8856\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => f1 : 0.5550\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => precision : 0.5508\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Training => Accuracy : 0.5225\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Training => Loss : 1.0975\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => Accuracy : 0.4651\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => f1 : 0.4255\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => precision : 0.4072\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => recall : 0.4651\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Training => Accuracy : 0.5775\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Training => Loss : 0.9724\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => Accuracy : 0.5756\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => f1 : 0.5517\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => precision : 0.5332\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Training => Accuracy : 0.6325\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Training => Loss : 0.8643\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => Accuracy : 0.6395\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => f1 : 0.6243\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => precision : 0.6195\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => recall : 0.6395\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Training => Accuracy : 0.5262\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Training => Loss : 1.0474\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => f1 : 0.5030\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => precision : 0.4896\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Training => Accuracy : 0.5875\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Training => Loss : 0.9567\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => Accuracy : 0.5116\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => f1 : 0.4916\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => precision : 0.4775\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Training => Accuracy : 0.6075\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Training => Loss : 0.8742\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => Accuracy : 0.5174\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => f1 : 0.5021\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => precision : 0.4888\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => recall : 0.5174\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Training => Accuracy : 0.4713\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Training => Loss : 1.0994\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => f1 : 0.4523\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => precision : 0.4469\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => recall : 0.4709\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Training => Accuracy : 0.5913\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Training => Loss : 0.9443\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => Accuracy : 0.5233\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => f1 : 0.5072\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => precision : 0.4942\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Training => Accuracy : 0.5225\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Training => Loss : 1.1051\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => Accuracy : 0.5291\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => f1 : 0.5072\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => precision : 0.5017\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Training => Accuracy : 0.4988\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Training => Loss : 1.0506\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => f1 : 0.5333\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => precision : 0.5209\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Training => Accuracy : 0.5913\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Training => Loss : 0.9302\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => f1 : 0.5685\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => precision : 0.5568\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => recall : 0.5872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Training => Accuracy : 0.6050\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Training => Loss : 0.8911\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => f1 : 0.5591\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => precision : 0.5546\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Training => Accuracy : 0.5188\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Training => Loss : 1.0651\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => f1 : 0.5243\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => precision : 0.5160\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Training => Accuracy : 0.6025\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Training => Loss : 0.9206\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => f1 : 0.5482\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => precision : 0.5417\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Training => Accuracy : 0.5663\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Training => Loss : 0.9977\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => Accuracy : 0.5465\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => f1 : 0.5276\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => precision : 0.5152\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Training => Accuracy : 0.5550\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Training => Loss : 1.0317\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => f1 : 0.5368\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => precision : 0.5050\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Training => Accuracy : 0.5775\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Training => Loss : 0.9704\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => Accuracy : 0.5233\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => f1 : 0.5072\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => precision : 0.4985\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Training => Accuracy : 0.5437\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Training => Loss : 1.2008\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => Accuracy : 0.5233\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => f1 : 0.5156\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => precision : 0.5111\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Training => Accuracy : 0.5413\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Training => Loss : 1.0575\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => f1 : 0.5007\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => precision : 0.4833\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Training => Accuracy : 0.5863\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Training => Loss : 0.9498\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => Accuracy : 0.6163\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => f1 : 0.6001\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => precision : 0.5894\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => recall : 0.6163\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Training => Accuracy : 0.5275\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Training => Loss : 1.2058\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => Accuracy : 0.4767\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => f1 : 0.4735\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => precision : 0.4749\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Training => Accuracy : 0.5575\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Training => Loss : 1.0393\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => f1 : 0.5237\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => precision : 0.5095\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Training => Accuracy : 0.5587\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Training => Loss : 1.0053\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => f1 : 0.5390\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => precision : 0.5248\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Training => Accuracy : 0.5925\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Training => Loss : 0.9186\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => f1 : 0.5431\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => precision : 0.5429\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => recall : 0.5581\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▆▅▅▅▅▄▆▄▅▄▂▄▃▂▄▃▄▃▁▆▃▁▅▃▆▃▆▅▂▂▅▂▄▃█▅▃█▅▂</td></tr><tr><td>Validation accuracy</td><td>▅▃▆▆▅▆▂▆▄▅▅▄▅▅▆▅▆▅▅▁▅█▃▃▁▃▄▅▆▅▅▅▆▃▃▄▇▁▅▅</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.91861</td></tr><tr><td>Validation accuracy</td><td>0.55814</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-paper-18</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/9lc1frhg' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/9lc1frhg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_011427-9lc1frhg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce7d8fcb6bc4fdd846294fe1c8c4e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011127615743963462, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_012111-osxqvkia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/osxqvkia' target=\"_blank\">lively-monkey-19</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/osxqvkia' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/osxqvkia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.4975\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1884\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.5000\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.4247\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.4009\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5312\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.1423\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.4884\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.4502\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4183\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5675\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0643\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.5407\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.4939\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.4554\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.4813\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1937\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.4274\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.3933\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.4709\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5663\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.1143\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5465\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.4969\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4576\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5775\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0718\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.5283\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.4883\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.5450\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1314\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.5407\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.4904\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.4517\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5550\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.0971\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.5039\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4644\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5775\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0159\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.6105\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.5840\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.5710\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5387\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.1087\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5291\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.4688\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.4386\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.6012\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.0122\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.6105\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.5759\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5855\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.6112\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 0.9822\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.5930\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.5669\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.5590\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5637\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.0943\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5465\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.4984\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.4596\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.5613\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.0133\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.6047\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.5761\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5754\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.6050\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 0.9600\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.6105\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.5860\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.5711\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5288\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.0751\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5291\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.4864\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.4523\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.6062\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 0.9824\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.6395\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.6130\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.6149\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.6395\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.5863\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 0.9513\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.5988\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.5721\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.5575\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Training => Accuracy : 0.5275\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Training => Loss : 1.1127\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => f1 : 0.4871\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => precision : 0.4487\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Training => Accuracy : 0.5663\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Training => Loss : 1.0110\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => f1 : 0.5427\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => precision : 0.5370\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Training => Accuracy : 0.6025\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Training => Loss : 0.9581\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => Accuracy : 0.5465\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => f1 : 0.5239\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => precision : 0.5066\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Training => Accuracy : 0.5713\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Training => Loss : 1.0628\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => f1 : 0.5178\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => precision : 0.4864\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Training => Accuracy : 0.5763\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Training => Loss : 1.0026\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => f1 : 0.5630\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => precision : 0.5533\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Training => Accuracy : 0.6150\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Training => Loss : 0.9508\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => f1 : 0.5674\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => precision : 0.5527\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Training => Accuracy : 0.5125\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Training => Loss : 1.0749\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => f1 : 0.4494\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => precision : 0.4164\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Training => Accuracy : 0.5825\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Training => Loss : 0.9753\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => Accuracy : 0.5756\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => f1 : 0.5527\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => precision : 0.5390\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Training => Accuracy : 0.6025\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Training => Loss : 0.9225\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => f1 : 0.5425\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => precision : 0.5329\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Training => Accuracy : 0.5212\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Training => Loss : 1.1209\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => f1 : 0.4884\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => precision : 0.4951\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Training => Accuracy : 0.5437\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Training => Loss : 1.0261\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => Accuracy : 0.5174\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => f1 : 0.5001\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => precision : 0.4910\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => recall : 0.5174\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Training => Accuracy : 0.5387\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Training => Loss : 0.9841\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => Accuracy : 0.5407\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => f1 : 0.5121\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => precision : 0.5713\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Training => Accuracy : 0.4950\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Training => Loss : 1.1011\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => Accuracy : 0.4826\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => f1 : 0.4476\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => precision : 0.4332\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => recall : 0.4826\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Training => Accuracy : 0.5650\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Training => Loss : 0.9958\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => f1 : 0.5731\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => precision : 0.5662\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => recall : 0.5872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Training => Accuracy : 0.5475\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Training => Loss : 1.0152\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => Accuracy : 0.5349\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => f1 : 0.5028\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => precision : 0.5843\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Training => Accuracy : 0.5675\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Training => Loss : 1.0001\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => Accuracy : 0.5465\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => f1 : 0.5241\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => precision : 0.5062\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Training => Accuracy : 0.5725\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Training => Loss : 0.9749\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => f1 : 0.5441\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => precision : 0.5395\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Training => Accuracy : 0.5563\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Training => Loss : 0.9825\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => f1 : 0.5106\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => precision : 0.5176\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Training => Accuracy : 0.5288\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Training => Loss : 1.0569\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => f1 : 0.5218\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => precision : 0.5063\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Training => Accuracy : 0.5787\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Training => Loss : 0.9996\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => Accuracy : 0.6337\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => f1 : 0.6090\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => precision : 0.6026\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => recall : 0.6337\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Training => Accuracy : 0.5600\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Training => Loss : 0.9575\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => Accuracy : 0.5930\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => f1 : 0.5823\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => precision : 0.5892\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Training => Accuracy : 0.5525\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Training => Loss : 1.0372\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => f1 : 0.4927\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => precision : 0.4895\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Training => Accuracy : 0.5750\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Training => Loss : 0.9660\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => Accuracy : 0.6105\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => f1 : 0.5729\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => precision : 0.5731\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Training => Accuracy : 0.5850\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Training => Loss : 0.9358\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => f1 : 0.5718\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => precision : 0.5690\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Training => Accuracy : 0.5212\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Training => Loss : 1.0379\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => Accuracy : 0.5407\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => f1 : 0.5089\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => precision : 0.5035\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Training => Accuracy : 0.5262\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Training => Loss : 1.0579\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => Accuracy : 0.4767\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => f1 : 0.4391\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => precision : 0.4069\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Training => Accuracy : 0.5787\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Training => Loss : 0.9617\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => Accuracy : 0.6047\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => f1 : 0.5835\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => precision : 0.5894\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => recall : 0.6047\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d24206bc4e4ef6bd249066d9dae63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▇▄█▆▅▆▅▆▃▂▅▃▂▅▂▆▃▂▄▃▁▅▂▆▃▂▅▃▃▃▂▄▃▂▄▂▁▄▂</td></tr><tr><td>Validation accuracy</td><td>▂▂▄▁▄▅▄▄▃▇▆▄▇▇▃█▄▅▄▅▆▆▂▅▄▃▄▁▆▄▄▅▅█▆▃▇▆▄▇</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.96169</td></tr><tr><td>Validation accuracy</td><td>0.60465</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-monkey-19</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/osxqvkia' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/osxqvkia</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_012111-osxqvkia/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [0.01,0.05,0.1,0.3,0.5]\n",
    "epochs = [2000,3000,5000]\n",
    "layer_sizes = [[8],[25],[64]]\n",
    "activations =  [[relu],[tanh],[sigmoid]]\n",
    "activation_names = ['relu','tanh','sigmoid']\n",
    "optimiser = 'batch'\n",
    "\n",
    "table = []\n",
    "\n",
    "#  minibatch\n",
    "for idx,activation in enumerate(activations):\n",
    "    wandb.init(project = \"Multilayer Classification Perceptron\")\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for size in layer_sizes:\n",
    "                classifier = MLP(input_size,output_size,num_layers,size,activation,'minibatch',lr)\n",
    "                size = size[0]\n",
    "        #       Training\n",
    "                classifier.training(x_train,y_train,epoch)\n",
    "        #       Training metrics\n",
    "                out = classifier.forward(x_train)\n",
    "                predicted = np.argmax(out,axis = 1)\n",
    "                actual = np.argmax(y_train,axis = 1)\n",
    "                accuracy_train = accuracy_score(actual,predicted)\n",
    "                loss_train = Xent(y_train,out)\n",
    "                print(\"TRAINING\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Accuracy : {accuracy_train:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "        #       Validation\n",
    "                out = classifier.forward(x_val)\n",
    "                predicted = np.argmax(out,axis = 1)\n",
    "                actual = np.argmax(y_val,axis = 1)\n",
    "                accuracy = accuracy_score(actual,predicted)\n",
    "                f1 = f1_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                precision = precision_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                recall = recall_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                print(\"VALIDATION\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => Accuracy : {accuracy:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": epoch,\n",
    "                    \"layer_size\": size,\n",
    "                    \"Validation accuracy\": accuracy,\n",
    "                    \"Training loss\": loss_train,\n",
    "                })\n",
    "                entry = [lr,epoch,idx,size,accuracy,f1,precision,recall]\n",
    "                table.append(entry)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fecd2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_minibatch = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fbc42c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Activation</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.519951</td>\n",
       "      <td>0.504798</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.529330</td>\n",
       "      <td>0.540306</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.574098</td>\n",
       "      <td>0.564895</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480744</td>\n",
       "      <td>0.509146</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>0.514360</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.572855</td>\n",
       "      <td>0.573110</td>\n",
       "      <td>0.610465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.571758</td>\n",
       "      <td>0.569001</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.508932</td>\n",
       "      <td>0.503473</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.439080</td>\n",
       "      <td>0.406931</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.583458</td>\n",
       "      <td>0.589374</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  Activation  layer size  Accuracy        F1  Precision  \\\n",
       "0    0.01    2000           0           8  0.546512  0.519951   0.504798   \n",
       "1    0.01    2000           0          25  0.540698  0.529330   0.540306   \n",
       "2    0.01    2000           0          64  0.587209  0.574098   0.564895   \n",
       "3    0.01    3000           0           8  0.500000  0.480744   0.509146   \n",
       "4    0.01    3000           0          25  0.540698  0.521637   0.514360   \n",
       "..    ...     ...         ...         ...       ...       ...        ...   \n",
       "130  0.50    3000           2          25  0.610465  0.572855   0.573110   \n",
       "131  0.50    3000           2          64  0.587209  0.571758   0.569001   \n",
       "132  0.50    5000           2           8  0.540698  0.508932   0.503473   \n",
       "133  0.50    5000           2          25  0.476744  0.439080   0.406931   \n",
       "134  0.50    5000           2          64  0.604651  0.583458   0.589374   \n",
       "\n",
       "       Recall  \n",
       "0    0.546512  \n",
       "1    0.540698  \n",
       "2    0.587209  \n",
       "3    0.500000  \n",
       "4    0.540698  \n",
       "..        ...  \n",
       "130  0.610465  \n",
       "131  0.587209  \n",
       "132  0.540698  \n",
       "133  0.476744  \n",
       "134  0.604651  \n",
       "\n",
       "[135 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(table_minibatch,columns = ['LR','Epochs','Activation','layer size','Accuracy','F1','Precision','Recall'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d6372d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Relu Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.597326</td>\n",
       "      <td>0.584009</td>\n",
       "      <td>0.616279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.585167</td>\n",
       "      <td>0.577613</td>\n",
       "      <td>0.610465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.571100</td>\n",
       "      <td>0.598837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.557244</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.569576</td>\n",
       "      <td>0.557255</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.574098</td>\n",
       "      <td>0.564895</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.564507</td>\n",
       "      <td>0.556490</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.561937</td>\n",
       "      <td>0.553235</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.577919</td>\n",
       "      <td>0.577751</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.564289</td>\n",
       "      <td>0.554678</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.555233</td>\n",
       "      <td>0.543120</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.549669</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.555413</td>\n",
       "      <td>0.555097</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.558519</td>\n",
       "      <td>0.556493</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.534294</td>\n",
       "      <td>0.537799</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.535568</td>\n",
       "      <td>0.523181</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.543788</td>\n",
       "      <td>0.530855</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.538976</td>\n",
       "      <td>0.526258</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.534433</td>\n",
       "      <td>0.524858</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.520286</td>\n",
       "      <td>0.513997</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.519457</td>\n",
       "      <td>0.508343</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.522845</td>\n",
       "      <td>0.550867</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.531030</td>\n",
       "      <td>0.528109</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.536662</td>\n",
       "      <td>0.527165</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.519951</td>\n",
       "      <td>0.504798</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.502384</td>\n",
       "      <td>0.471157</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.529330</td>\n",
       "      <td>0.540306</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.516247</td>\n",
       "      <td>0.530263</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.506461</td>\n",
       "      <td>0.529132</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>0.514360</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.504031</td>\n",
       "      <td>0.490939</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.506867</td>\n",
       "      <td>0.524143</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.508548</td>\n",
       "      <td>0.527017</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.497650</td>\n",
       "      <td>0.487264</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.498378</td>\n",
       "      <td>0.485334</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.517442</td>\n",
       "      <td>0.428004</td>\n",
       "      <td>0.447375</td>\n",
       "      <td>0.517442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.463193</td>\n",
       "      <td>0.478134</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>0.513320</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.483790</td>\n",
       "      <td>0.471261</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480744</td>\n",
       "      <td>0.509146</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.442656</td>\n",
       "      <td>0.438427</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.416599</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.430233</td>\n",
       "      <td>0.406090</td>\n",
       "      <td>0.393942</td>\n",
       "      <td>0.430233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LR  Epochs  layer size  Accuracy        F1  Precision    Recall\n",
       "8   0.01    5000          64  0.616279  0.597326   0.584009  0.616279\n",
       "13  0.05    3000          25  0.610465  0.585167   0.577613  0.610465\n",
       "16  0.05    5000          25  0.598837  0.581250   0.571100  0.598837\n",
       "12  0.05    3000           8  0.593023  0.555900   0.557244  0.593023\n",
       "10  0.05    2000          25  0.593023  0.569576   0.557255  0.593023\n",
       "2   0.01    2000          64  0.587209  0.574098   0.564895  0.587209\n",
       "5   0.01    3000          64  0.581395  0.564507   0.556490  0.581395\n",
       "17  0.05    5000          64  0.581395  0.561937   0.553235  0.581395\n",
       "34  0.30    5000          25  0.581395  0.577919   0.577751  0.581395\n",
       "25  0.10    5000          25  0.581395  0.564289   0.554678  0.581395\n",
       "14  0.05    3000          64  0.575581  0.555233   0.543120  0.575581\n",
       "41  0.50    3000          64  0.569767  0.549669   0.549708  0.569767\n",
       "20  0.10    2000          64  0.569767  0.555413   0.555097  0.569767\n",
       "32  0.30    3000          64  0.563953  0.558519   0.556493  0.563953\n",
       "27  0.30    2000           8  0.558140  0.534294   0.537799  0.558140\n",
       "19  0.10    2000          25  0.558140  0.535568   0.523181  0.558140\n",
       "26  0.10    5000          64  0.558140  0.543788   0.530855  0.558140\n",
       "7   0.01    5000          25  0.558140  0.538976   0.526258  0.558140\n",
       "37  0.50    2000          25  0.558140  0.534433   0.524858  0.558140\n",
       "9   0.05    2000           8  0.552326  0.520286   0.513997  0.552326\n",
       "18  0.10    2000           8  0.552326  0.519457   0.508343  0.552326\n",
       "6   0.01    5000           8  0.552326  0.522845   0.550867  0.552326\n",
       "21  0.10    3000           8  0.552326  0.531030   0.528109  0.552326\n",
       "22  0.10    3000          25  0.552326  0.536662   0.527165  0.552326\n",
       "0   0.01    2000           8  0.546512  0.519951   0.504798  0.546512\n",
       "40  0.50    3000          25  0.540698  0.502384   0.471157  0.540698\n",
       "1   0.01    2000          25  0.540698  0.529330   0.540306  0.540698\n",
       "30  0.30    3000           8  0.540698  0.516247   0.530263  0.540698\n",
       "11  0.05    2000          64  0.540698  0.506461   0.529132  0.540698\n",
       "4   0.01    3000          25  0.540698  0.521637   0.514360  0.540698\n",
       "29  0.30    2000          64  0.534884  0.504031   0.490939  0.534884\n",
       "35  0.30    5000          64  0.529070  0.506867   0.524143  0.529070\n",
       "31  0.30    3000          25  0.529070  0.508548   0.527017  0.529070\n",
       "15  0.05    5000           8  0.523256  0.497650   0.487264  0.523256\n",
       "42  0.50    5000           8  0.523256  0.498378   0.485334  0.523256\n",
       "23  0.10    3000          64  0.517442  0.428004   0.447375  0.517442\n",
       "33  0.30    5000           8  0.505814  0.463193   0.478134  0.505814\n",
       "38  0.50    2000          64  0.505814  0.495652   0.513320  0.505814\n",
       "28  0.30    2000          25  0.500000  0.483790   0.471261  0.500000\n",
       "3   0.01    3000           8  0.500000  0.480744   0.509146  0.500000\n",
       "24  0.10    5000           8  0.470930  0.442656   0.438427  0.470930\n",
       "39  0.50    3000           8  0.470930  0.416599   0.414858  0.470930\n",
       "36  0.50    2000           8  0.430233  0.406090   0.393942  0.430233\n",
       "43  0.50    5000          25  0.000000  0.000000   0.000000  0.000000\n",
       "44  0.50    5000          64  0.000000  0.000000   0.000000  0.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Minibatch Relu Metrics\")\n",
    "df_minibatch_relu = table[table['Activation'] == 0].drop('Activation',axis = 1)\n",
    "df_minibatch_relu = df_minibatch_relu.sort_values(by='Accuracy', ascending=False)\n",
    "df_minibatch_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71d5653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Tanh Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.624282</td>\n",
       "      <td>0.619460</td>\n",
       "      <td>0.639535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.600138</td>\n",
       "      <td>0.589410</td>\n",
       "      <td>0.616279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.569543</td>\n",
       "      <td>0.551594</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.566606</td>\n",
       "      <td>0.550510</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.568456</td>\n",
       "      <td>0.556760</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.560363</td>\n",
       "      <td>0.547287</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.552685</td>\n",
       "      <td>0.552906</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.532834</td>\n",
       "      <td>0.587223</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.536822</td>\n",
       "      <td>0.504983</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.552927</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.560952</td>\n",
       "      <td>0.544852</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.517116</td>\n",
       "      <td>0.485323</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.559086</td>\n",
       "      <td>0.554610</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.554977</td>\n",
       "      <td>0.550779</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.555335</td>\n",
       "      <td>0.541707</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.551737</td>\n",
       "      <td>0.533229</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.547896</td>\n",
       "      <td>0.533856</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.540188</td>\n",
       "      <td>0.524027</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.553566</td>\n",
       "      <td>0.545917</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.540223</td>\n",
       "      <td>0.525096</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.524753</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.543601</td>\n",
       "      <td>0.526566</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.548166</td>\n",
       "      <td>0.541714</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.525518</td>\n",
       "      <td>0.506393</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.533315</td>\n",
       "      <td>0.520930</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.543070</td>\n",
       "      <td>0.542873</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.531442</td>\n",
       "      <td>0.521205</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.523747</td>\n",
       "      <td>0.509456</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.516007</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.529426</td>\n",
       "      <td>0.525095</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.527596</td>\n",
       "      <td>0.515229</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.500662</td>\n",
       "      <td>0.483279</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.517068</td>\n",
       "      <td>0.504851</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.507200</td>\n",
       "      <td>0.501730</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.502987</td>\n",
       "      <td>0.489607</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.507192</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.507170</td>\n",
       "      <td>0.498475</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.515577</td>\n",
       "      <td>0.511082</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.517442</td>\n",
       "      <td>0.502067</td>\n",
       "      <td>0.488758</td>\n",
       "      <td>0.517442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.491570</td>\n",
       "      <td>0.477535</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.485371</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.454098</td>\n",
       "      <td>0.441845</td>\n",
       "      <td>0.482558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.473466</td>\n",
       "      <td>0.474855</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.452287</td>\n",
       "      <td>0.446853</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.425542</td>\n",
       "      <td>0.407183</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LR  Epochs  layer size  Accuracy        F1  Precision    Recall\n",
       "68  0.10    3000          64  0.639535  0.624282   0.619460  0.639535\n",
       "85  0.50    3000          25  0.616279  0.600138   0.589410  0.616279\n",
       "50  0.01    3000          64  0.593023  0.569543   0.551594  0.593023\n",
       "52  0.01    5000          25  0.587209  0.566606   0.550510  0.587209\n",
       "76  0.30    3000          25  0.587209  0.568456   0.556760  0.587209\n",
       "60  0.05    5000           8  0.581395  0.560363   0.547287  0.581395\n",
       "47  0.01    2000          64  0.581395  0.552685   0.552906  0.581395\n",
       "48  0.01    3000           8  0.581395  0.532834   0.587223  0.581395\n",
       "81  0.50    2000           8  0.581395  0.536822   0.504983  0.581395\n",
       "63  0.10    2000           8  0.581395  0.552927   0.536100  0.581395\n",
       "62  0.05    5000          64  0.581395  0.560952   0.544852  0.581395\n",
       "45  0.01    2000           8  0.575581  0.517116   0.485323  0.575581\n",
       "77  0.30    3000          64  0.575581  0.559086   0.554610  0.575581\n",
       "65  0.10    2000          64  0.575581  0.554977   0.550779  0.575581\n",
       "61  0.05    5000          25  0.575581  0.555335   0.541707  0.575581\n",
       "67  0.10    3000          25  0.575581  0.551737   0.533229  0.575581\n",
       "55  0.05    2000          25  0.575581  0.547896   0.533856  0.575581\n",
       "64  0.10    2000          25  0.569767  0.540188   0.524027  0.569767\n",
       "59  0.05    3000          64  0.563953  0.553566   0.545917  0.563953\n",
       "53  0.01    5000          64  0.563953  0.540223   0.525096  0.563953\n",
       "88  0.50    5000          25  0.563953  0.539033   0.524753  0.563953\n",
       "56  0.05    2000          64  0.563953  0.543601   0.526566  0.563953\n",
       "79  0.30    5000          25  0.563953  0.548166   0.541714  0.563953\n",
       "49  0.01    3000          25  0.558140  0.525518   0.506393  0.558140\n",
       "75  0.30    3000           8  0.558140  0.533315   0.520930  0.558140\n",
       "89  0.50    5000          64  0.558140  0.543070   0.542873  0.558140\n",
       "58  0.05    3000          25  0.552326  0.531442   0.521205  0.552326\n",
       "87  0.50    5000           8  0.552326  0.523747   0.509456  0.552326\n",
       "78  0.30    5000           8  0.552326  0.524300   0.516007  0.552326\n",
       "54  0.05    2000           8  0.546512  0.529426   0.525095  0.546512\n",
       "80  0.30    5000          64  0.546512  0.527596   0.515229  0.546512\n",
       "84  0.50    3000           8  0.534884  0.500662   0.483279  0.534884\n",
       "57  0.05    3000           8  0.534884  0.517068   0.504851  0.534884\n",
       "74  0.30    2000          64  0.529070  0.507200   0.501730  0.529070\n",
       "69  0.10    5000           8  0.523256  0.502987   0.489607  0.523256\n",
       "73  0.30    2000          25  0.523256  0.507192   0.494247  0.523256\n",
       "82  0.50    2000          25  0.523256  0.507170   0.498475  0.523256\n",
       "83  0.50    2000          64  0.523256  0.515577   0.511082  0.523256\n",
       "71  0.10    5000          64  0.517442  0.502067   0.488758  0.517442\n",
       "70  0.10    5000          25  0.511628  0.491570   0.477535  0.511628\n",
       "46  0.01    2000          25  0.505814  0.485371   0.478873  0.505814\n",
       "51  0.01    5000           8  0.482558  0.454098   0.441845  0.482558\n",
       "86  0.50    3000          64  0.476744  0.473466   0.474855  0.476744\n",
       "72  0.30    2000           8  0.470930  0.452287   0.446853  0.470930\n",
       "66  0.10    3000           8  0.465116  0.425542   0.407183  0.465116"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Minibatch Tanh Metrics\")\n",
    "df_minibatch_tanh = table[table['Activation'] == 1].drop('Activation',axis = 1)\n",
    "df_minibatch_tanh = df_minibatch_tanh.sort_values(by='Accuracy', ascending=False)\n",
    "df_minibatch_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceb7732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch sigmoid Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.613002</td>\n",
       "      <td>0.614935</td>\n",
       "      <td>0.639535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.633721</td>\n",
       "      <td>0.609026</td>\n",
       "      <td>0.602610</td>\n",
       "      <td>0.633721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.572855</td>\n",
       "      <td>0.573110</td>\n",
       "      <td>0.610465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.585981</td>\n",
       "      <td>0.571098</td>\n",
       "      <td>0.610465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.583974</td>\n",
       "      <td>0.570955</td>\n",
       "      <td>0.610465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.575939</td>\n",
       "      <td>0.585541</td>\n",
       "      <td>0.610465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.583458</td>\n",
       "      <td>0.589374</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.576079</td>\n",
       "      <td>0.575427</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.572084</td>\n",
       "      <td>0.557485</td>\n",
       "      <td>0.598837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.566877</td>\n",
       "      <td>0.559020</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.582314</td>\n",
       "      <td>0.589192</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.562995</td>\n",
       "      <td>0.553335</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.573142</td>\n",
       "      <td>0.566198</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.567415</td>\n",
       "      <td>0.552712</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.571758</td>\n",
       "      <td>0.569001</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.552746</td>\n",
       "      <td>0.538966</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.528330</td>\n",
       "      <td>0.488257</td>\n",
       "      <td>0.575581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.542656</td>\n",
       "      <td>0.537002</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.542545</td>\n",
       "      <td>0.532929</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.544147</td>\n",
       "      <td>0.539484</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.521770</td>\n",
       "      <td>0.506254</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.517824</td>\n",
       "      <td>0.486441</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.503855</td>\n",
       "      <td>0.464408</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>0.517562</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.30</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.524086</td>\n",
       "      <td>0.506181</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.498381</td>\n",
       "      <td>0.459639</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.523950</td>\n",
       "      <td>0.506619</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.496909</td>\n",
       "      <td>0.457556</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.512098</td>\n",
       "      <td>0.571330</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.493899</td>\n",
       "      <td>0.455407</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.508932</td>\n",
       "      <td>0.503473</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.490398</td>\n",
       "      <td>0.451668</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.488391</td>\n",
       "      <td>0.495149</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.502780</td>\n",
       "      <td>0.584296</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.448685</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.468775</td>\n",
       "      <td>0.438602</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.486418</td>\n",
       "      <td>0.452302</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.492724</td>\n",
       "      <td>0.489505</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.517442</td>\n",
       "      <td>0.500071</td>\n",
       "      <td>0.491003</td>\n",
       "      <td>0.517442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.424703</td>\n",
       "      <td>0.400899</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.449434</td>\n",
       "      <td>0.416392</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.450212</td>\n",
       "      <td>0.418321</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>0.433188</td>\n",
       "      <td>0.482558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.439080</td>\n",
       "      <td>0.406931</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.427380</td>\n",
       "      <td>0.393328</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size  Accuracy        F1  Precision    Recall\n",
       "106  0.05    5000          25  0.639535  0.613002   0.614935  0.639535\n",
       "127  0.50    2000          25  0.633721  0.609026   0.602610  0.633721\n",
       "130  0.50    3000          25  0.610465  0.572855   0.573110  0.610465\n",
       "104  0.05    3000          64  0.610465  0.585981   0.571098  0.610465\n",
       "98   0.01    5000          64  0.610465  0.583974   0.570955  0.610465\n",
       "100  0.05    2000          25  0.610465  0.575939   0.585541  0.610465\n",
       "134  0.50    5000          64  0.604651  0.583458   0.589374  0.604651\n",
       "103  0.05    3000          25  0.604651  0.576079   0.575427  0.604651\n",
       "107  0.05    5000          64  0.598837  0.572084   0.557485  0.598837\n",
       "101  0.05    2000          64  0.593023  0.566877   0.559020  0.593023\n",
       "128  0.50    2000          64  0.593023  0.582314   0.589192  0.593023\n",
       "112  0.10    3000          25  0.593023  0.562995   0.553335  0.593023\n",
       "121  0.30    3000          25  0.587209  0.573142   0.566198  0.587209\n",
       "113  0.10    3000          64  0.587209  0.567415   0.552712  0.587209\n",
       "131  0.50    3000          64  0.587209  0.571758   0.569001  0.587209\n",
       "115  0.10    5000          25  0.575581  0.552746   0.538966  0.575581\n",
       "95   0.01    3000          64  0.575581  0.528330   0.488257  0.575581\n",
       "109  0.10    2000          25  0.569767  0.542656   0.537002  0.569767\n",
       "116  0.10    5000          64  0.563953  0.542545   0.532929  0.563953\n",
       "124  0.30    5000          25  0.563953  0.544147   0.539484  0.563953\n",
       "126  0.50    2000           8  0.558140  0.521770   0.506254  0.558140\n",
       "111  0.10    3000           8  0.558140  0.517824   0.486441  0.558140\n",
       "97   0.01    5000          25  0.552326  0.503855   0.464408  0.552326\n",
       "125  0.30    5000          64  0.552326  0.510576   0.517562  0.552326\n",
       "123  0.30    5000           8  0.546512  0.524086   0.506181  0.546512\n",
       "102  0.05    3000           8  0.546512  0.498381   0.459639  0.546512\n",
       "110  0.10    2000          64  0.546512  0.523950   0.506619  0.546512\n",
       "94   0.01    3000          25  0.546512  0.496909   0.457556  0.546512\n",
       "119  0.30    2000          64  0.540698  0.512098   0.571330  0.540698\n",
       "92   0.01    2000          64  0.540698  0.493899   0.455407  0.540698\n",
       "132  0.50    5000           8  0.540698  0.508932   0.503473  0.540698\n",
       "96   0.01    5000           8  0.540698  0.490398   0.451668  0.540698\n",
       "117  0.30    2000           8  0.534884  0.488391   0.495149  0.534884\n",
       "122  0.30    3000          64  0.534884  0.502780   0.584296  0.534884\n",
       "108  0.10    2000           8  0.534884  0.487100   0.448685  0.534884\n",
       "99   0.05    2000           8  0.529070  0.468775   0.438602  0.529070\n",
       "105  0.05    5000           8  0.529070  0.486418   0.452302  0.529070\n",
       "129  0.50    3000           8  0.523256  0.492724   0.489505  0.523256\n",
       "118  0.30    2000          25  0.517442  0.500071   0.491003  0.517442\n",
       "90   0.01    2000           8  0.500000  0.424703   0.400899  0.500000\n",
       "114  0.10    5000           8  0.488372  0.449434   0.416392  0.488372\n",
       "91   0.01    2000          25  0.488372  0.450212   0.418321  0.488372\n",
       "120  0.30    3000           8  0.482558  0.447619   0.433188  0.482558\n",
       "133  0.50    5000          25  0.476744  0.439080   0.406931  0.476744\n",
       "93   0.01    3000           8  0.470930  0.427380   0.393328  0.470930"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Minibatch sigmoid Metrics\")\n",
    "df_minibatch_sigmoid = table[table['Activation'] == 2].drop('Activation',axis = 1)\n",
    "df_minibatch_sigmoid = df_minibatch_sigmoid.sort_values(by='Accuracy', ascending=False)\n",
    "df_minibatch_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a7839",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1610a4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_014241-hw6rshv2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/hw6rshv2' target=\"_blank\">clean-flower-20</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/hw6rshv2' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/hw6rshv2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Training => Accuracy : 0.5250\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Training => Loss : 1.1227\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => f1 : 0.4518\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => precision : 0.4204\n",
      "Epoch [2000], lr [0.01], activation [relu], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Training => Accuracy : 0.5563\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Training => Loss : 1.0538\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => Accuracy : 0.5930\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => f1 : 0.5742\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => precision : 0.5863\n",
      "Epoch [2000], lr [0.01], activation [relu], size [25] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Training => Accuracy : 0.5913\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Training => Loss : 0.9897\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => Accuracy : 0.6395\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => f1 : 0.6157\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => precision : 0.6046\n",
      "Epoch [2000], lr [0.01], activation [relu], size [64] Validation => recall : 0.6395\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Training => Accuracy : 0.5363\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Training => Loss : 1.0647\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => f1 : 0.5556\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => precision : 0.5685\n",
      "Epoch [3000], lr [0.01], activation [relu], size [8] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Training => Accuracy : 0.5650\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Training => Loss : 1.0149\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => f1 : 0.5468\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => precision : 0.5314\n",
      "Epoch [3000], lr [0.01], activation [relu], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Training => Accuracy : 0.5938\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Training => Loss : 0.9723\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => f1 : 0.5401\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => precision : 0.5227\n",
      "Epoch [3000], lr [0.01], activation [relu], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Training => Accuracy : 0.5138\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Training => Loss : 1.0638\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => Accuracy : 0.4767\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => f1 : 0.4664\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => precision : 0.4613\n",
      "Epoch [5000], lr [0.01], activation [relu], size [8] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Training => Accuracy : 0.5925\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Training => Loss : 0.9802\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => Accuracy : 0.6047\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => f1 : 0.5783\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => precision : 0.5701\n",
      "Epoch [5000], lr [0.01], activation [relu], size [25] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Training => Accuracy : 0.6075\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Training => Loss : 0.9480\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => f1 : 0.5671\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => precision : 0.5536\n",
      "Epoch [5000], lr [0.01], activation [relu], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Training => Accuracy : 0.5887\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Training => Loss : 0.9857\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => Accuracy : 0.5058\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => f1 : 0.4828\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => precision : 0.4664\n",
      "Epoch [2000], lr [0.05], activation [relu], size [8] Validation => recall : 0.5058\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Training => Accuracy : 0.5863\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Training => Loss : 0.9716\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => f1 : 0.5374\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => precision : 0.5252\n",
      "Epoch [2000], lr [0.05], activation [relu], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Training => Accuracy : 0.5900\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Training => Loss : 0.9465\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => f1 : 0.5534\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => precision : 0.5467\n",
      "Epoch [2000], lr [0.05], activation [relu], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Training => Accuracy : 0.5787\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Training => Loss : 0.9918\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => f1 : 0.5233\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => precision : 0.5082\n",
      "Epoch [3000], lr [0.05], activation [relu], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Training => Accuracy : 0.6112\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Training => Loss : 0.9217\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => f1 : 0.5433\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => precision : 0.5309\n",
      "Epoch [3000], lr [0.05], activation [relu], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Training => Accuracy : 0.5950\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Training => Loss : 0.9327\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => f1 : 0.5612\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => precision : 0.5545\n",
      "Epoch [3000], lr [0.05], activation [relu], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Training => Accuracy : 0.5100\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Training => Loss : 1.0448\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => f1 : 0.4991\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => precision : 0.5031\n",
      "Epoch [5000], lr [0.05], activation [relu], size [8] Validation => recall : 0.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Training => Accuracy : 0.5875\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Training => Loss : 0.9259\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => f1 : 0.5320\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => precision : 0.5160\n",
      "Epoch [5000], lr [0.05], activation [relu], size [25] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Training => Accuracy : 0.6075\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Training => Loss : 0.8690\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => f1 : 0.5519\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => precision : 0.5533\n",
      "Epoch [5000], lr [0.05], activation [relu], size [64] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Training => Accuracy : 0.4963\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Training => Loss : 1.0612\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => Accuracy : 0.5174\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => f1 : 0.5000\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => precision : 0.4943\n",
      "Epoch [2000], lr [0.1], activation [relu], size [8] Validation => recall : 0.5174\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Training => Accuracy : 0.5700\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Training => Loss : 0.9637\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => f1 : 0.5367\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => precision : 0.5254\n",
      "Epoch [2000], lr [0.1], activation [relu], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Training => Accuracy : 0.5913\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Training => Loss : 0.9831\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => Accuracy : 0.5930\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => f1 : 0.5525\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => precision : 0.5792\n",
      "Epoch [2000], lr [0.1], activation [relu], size [64] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Training => Accuracy : 0.4763\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Training => Loss : 1.1002\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => Accuracy : 0.4767\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => f1 : 0.4500\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => precision : 0.4398\n",
      "Epoch [3000], lr [0.1], activation [relu], size [8] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Training => Accuracy : 0.5925\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Training => Loss : 0.9210\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => f1 : 0.5501\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => precision : 0.5363\n",
      "Epoch [3000], lr [0.1], activation [relu], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Training => Accuracy : 0.5925\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Training => Loss : 0.9677\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => Accuracy : 0.6047\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => f1 : 0.5775\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => precision : 0.5795\n",
      "Epoch [3000], lr [0.1], activation [relu], size [64] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Training => Accuracy : 0.5775\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Training => Loss : 0.9863\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => f1 : 0.5355\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => precision : 0.5245\n",
      "Epoch [5000], lr [0.1], activation [relu], size [8] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Training => Accuracy : 0.6100\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Training => Loss : 0.8945\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => Accuracy : 0.6221\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => f1 : 0.6098\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => precision : 0.6244\n",
      "Epoch [5000], lr [0.1], activation [relu], size [25] Validation => recall : 0.6221\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Training => Accuracy : 0.6000\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Training => Loss : 0.9121\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => f1 : 0.5586\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => precision : 0.5621\n",
      "Epoch [5000], lr [0.1], activation [relu], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Training => Accuracy : 0.5350\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Training => Loss : 1.0634\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => f1 : 0.4993\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => precision : 0.4969\n",
      "Epoch [2000], lr [0.3], activation [relu], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Training => Accuracy : 0.5863\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Training => Loss : 0.9773\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => f1 : 0.5457\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => precision : 0.5357\n",
      "Epoch [2000], lr [0.3], activation [relu], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Training => Accuracy : 0.5813\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Training => Loss : 0.9556\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => Accuracy : 0.5407\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => f1 : 0.5109\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => precision : 0.5009\n",
      "Epoch [2000], lr [0.3], activation [relu], size [64] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Training => Accuracy : 0.5487\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Training => Loss : 1.0121\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => f1 : 0.5743\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => precision : 0.5603\n",
      "Epoch [3000], lr [0.3], activation [relu], size [8] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Training => Accuracy : 0.6025\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Training => Loss : 0.9399\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => Accuracy : 0.5523\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => f1 : 0.5364\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => precision : 0.5309\n",
      "Epoch [3000], lr [0.3], activation [relu], size [25] Validation => recall : 0.5523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Training => Accuracy : 0.5900\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Training => Loss : 0.9418\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => Accuracy : 0.5465\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => f1 : 0.5169\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => precision : 0.5009\n",
      "Epoch [3000], lr [0.3], activation [relu], size [64] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Training => Accuracy : 0.5050\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Training => Loss : 1.0636\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => Accuracy : 0.5116\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => f1 : 0.4736\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => precision : 0.4797\n",
      "Epoch [5000], lr [0.3], activation [relu], size [8] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Training => Accuracy : 0.5500\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Training => Loss : 1.0186\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => f1 : 0.5546\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => precision : 0.5579\n",
      "Epoch [5000], lr [0.3], activation [relu], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Training => Accuracy : 0.5700\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Training => Loss : 0.9152\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => Accuracy : 0.5349\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => f1 : 0.5215\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => precision : 0.5360\n",
      "Epoch [5000], lr [0.3], activation [relu], size [64] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Training => Accuracy : 0.5437\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Training => Loss : 1.0421\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => Accuracy : 0.4942\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => f1 : 0.4653\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => precision : 0.4588\n",
      "Epoch [2000], lr [0.5], activation [relu], size [8] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Training => Accuracy : 0.4400\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Training => Loss : 1.1880\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => Accuracy : 0.4884\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => f1 : 0.4207\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => precision : 0.4764\n",
      "Epoch [2000], lr [0.5], activation [relu], size [25] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Training => Accuracy : 0.5500\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Training => Loss : 1.0409\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => Accuracy : 0.5698\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => f1 : 0.5444\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => precision : 0.5528\n",
      "Epoch [2000], lr [0.5], activation [relu], size [64] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Training => Accuracy : 0.4500\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Training => Loss : 1.1281\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => Accuracy : 0.4651\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => f1 : 0.4389\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => precision : 0.4393\n",
      "Epoch [3000], lr [0.5], activation [relu], size [8] Validation => recall : 0.4651\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Training => Accuracy : 0.5975\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Training => Loss : 0.9697\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => f1 : 0.5245\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => precision : 0.5329\n",
      "Epoch [3000], lr [0.5], activation [relu], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Training => Accuracy : 0.5475\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Training => Loss : 0.9961\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => Accuracy : 0.4651\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => f1 : 0.4531\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => precision : 0.4658\n",
      "Epoch [3000], lr [0.5], activation [relu], size [64] Validation => recall : 0.4651\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Training => Accuracy : 0.4650\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Training => Loss : 1.1464\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => Accuracy : 0.4942\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => f1 : 0.4550\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => precision : 0.4216\n",
      "Epoch [5000], lr [0.5], activation [relu], size [8] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Training => Accuracy : 0.5188\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Training => Loss : 1.0343\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => Accuracy : 0.4709\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => f1 : 0.4526\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => precision : 0.4737\n",
      "Epoch [5000], lr [0.5], activation [relu], size [25] Validation => recall : 0.4709\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Training => Accuracy : 0.5837\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Training => Loss : 1.0103\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => f1 : 0.5065\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => precision : 0.4933\n",
      "Epoch [5000], lr [0.5], activation [relu], size [64] Validation => recall : 0.5233\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▆▅▃▅▄▃▅▃▃▃▂▃▂▂▅▂▅▃▃▆▂▃▃▁▅▃▂▄▂▂▅▄▅█▄▇▃▃▇▄</td></tr><tr><td>Validation accuracy</td><td>▂▆█▆▅▅▁▇▃▅▅▅▅▆▃▅▃▅▆▁▅▇▅▇▄▆▄▆▅▄▃▆▂▂▅▁▅▁▂▃</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>1.01029</td></tr><tr><td>Validation accuracy</td><td>0.52326</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-flower-20</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/hw6rshv2' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/hw6rshv2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_014241-hw6rshv2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a081db9fcfa44f759c7511de58e54d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168036111242449, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_022607-qxe0hz8o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/qxe0hz8o' target=\"_blank\">rosy-pyramid-21</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/qxe0hz8o' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/qxe0hz8o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5262\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.1181\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.4942\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.4472\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.4123\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5675\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Training => Loss : 1.0340\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.6105\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.5801\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.5657\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.5938\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Training => Loss : 1.0056\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5523\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5313\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5233\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5475\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.0720\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.5988\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.5663\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.5571\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5650\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Training => Loss : 1.0387\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.5400\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.5241\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.5600\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.9955\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5684\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5567\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5375\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.0996\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.5407\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.5011\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.4670\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.5407\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5700\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Training => Loss : 1.0116\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5988\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.5730\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.5591\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.5938\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.9637\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5337\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5180\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5475\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0493\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5930\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.5436\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.5038\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5825\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Training => Loss : 0.9808\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5556\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.5385\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.6138\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.9156\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5420\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.5275\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5225\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0867\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.4798\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.4638\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5575\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Training => Loss : 0.9800\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5447\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.5377\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.6200\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.8879\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5466\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.5319\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5300\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0764\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5407\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.5193\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.5059\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5763\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Loss : 0.9537\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.6163\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5944\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.5757\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.6163\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.6125\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.8813\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5465\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5263\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.5187\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Training => Accuracy : 0.5262\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Training => Loss : 1.0339\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => f1 : 0.5011\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => precision : 0.4924\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Training => Accuracy : 0.5925\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Training => Loss : 0.9431\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => f1 : 0.5519\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => precision : 0.5332\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Training => Accuracy : 0.6412\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Training => Loss : 0.8878\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => f1 : 0.5443\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => precision : 0.5288\n",
      "Epoch [2000], lr [0.1], activation [tanh], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Training => Accuracy : 0.5012\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Training => Loss : 1.0837\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => f1 : 0.4644\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => precision : 0.4507\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Training => Accuracy : 0.5950\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Training => Loss : 0.9350\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => Accuracy : 0.6047\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => f1 : 0.5852\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => precision : 0.5724\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [25] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Training => Accuracy : 0.6100\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Training => Loss : 0.8604\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => Accuracy : 0.5814\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => f1 : 0.5657\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => precision : 0.5596\n",
      "Epoch [3000], lr [0.1], activation [tanh], size [64] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Training => Accuracy : 0.5275\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Training => Loss : 1.0432\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => f1 : 0.5068\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => precision : 0.4985\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Training => Accuracy : 0.5600\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Training => Loss : 0.9587\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => f1 : 0.5328\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => precision : 0.5153\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Training => Accuracy : 0.6388\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Training => Loss : 0.8385\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => f1 : 0.5528\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => precision : 0.5497\n",
      "Epoch [5000], lr [0.1], activation [tanh], size [64] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Training => Accuracy : 0.5425\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Training => Loss : 1.0388\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => f1 : 0.5291\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => precision : 0.5181\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Training => Accuracy : 0.5863\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Training => Loss : 0.9565\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => f1 : 0.5469\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => precision : 0.5361\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Training => Accuracy : 0.5463\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Training => Loss : 1.0412\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => f1 : 0.4987\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => precision : 0.4878\n",
      "Epoch [2000], lr [0.3], activation [tanh], size [64] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Training => Accuracy : 0.5238\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Training => Loss : 1.0959\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => f1 : 0.5105\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => precision : 0.4746\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Training => Accuracy : 0.5875\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Training => Loss : 0.9206\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => f1 : 0.5374\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => precision : 0.5230\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [25] Validation => recall : 0.5581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Training => Accuracy : 0.5225\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Training => Loss : 1.0676\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => Accuracy : 0.4767\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => f1 : 0.4652\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => precision : 0.4564\n",
      "Epoch [3000], lr [0.3], activation [tanh], size [64] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Training => Accuracy : 0.5525\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Training => Loss : 1.0258\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => f1 : 0.5288\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => precision : 0.5108\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Training => Accuracy : 0.5625\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Training => Loss : 0.9592\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => Accuracy : 0.5872\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => f1 : 0.5663\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => precision : 0.5587\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [25] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Training => Accuracy : 0.5600\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Training => Loss : 1.0046\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => Accuracy : 0.5291\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => f1 : 0.5244\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => precision : 0.5344\n",
      "Epoch [5000], lr [0.3], activation [tanh], size [64] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Training => Accuracy : 0.5188\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Training => Loss : 1.0578\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => f1 : 0.4670\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => precision : 0.4670\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Training => Accuracy : 0.5900\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Training => Loss : 0.9242\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => f1 : 0.5671\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => precision : 0.5636\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Training => Accuracy : 0.4900\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Training => Loss : 1.5535\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => Accuracy : 0.4942\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => f1 : 0.4902\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => precision : 0.5213\n",
      "Epoch [2000], lr [0.5], activation [tanh], size [64] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Training => Accuracy : 0.5513\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Training => Loss : 1.0514\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => f1 : 0.5131\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => precision : 0.5261\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Training => Accuracy : 0.5625\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Training => Loss : 0.9607\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => Accuracy : 0.5465\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => f1 : 0.5274\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => precision : 0.5123\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [25] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Training => Accuracy : 0.5413\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Training => Loss : 1.5139\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => Accuracy : 0.5058\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => f1 : 0.4938\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => precision : 0.4929\n",
      "Epoch [3000], lr [0.5], activation [tanh], size [64] Validation => recall : 0.5058\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Training => Accuracy : 0.5425\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Training => Loss : 1.0411\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => f1 : 0.4971\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => precision : 0.4911\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Training => Accuracy : 0.5537\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Training => Loss : 0.9695\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => f1 : 0.5478\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => precision : 0.5393\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Training => Accuracy : 0.5288\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Training => Loss : 1.2532\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => f1 : 0.5515\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => precision : 0.5396\n",
      "Epoch [5000], lr [0.5], activation [tanh], size [64] Validation => recall : 0.5640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▄▃▂▃▃▂▃▃▃▂▂▃▂▁▃▂▃▂▁▃▂▁▃▂▃▂▃▃▂▃▃▂▃▂█▃▂█▃▅</td></tr><tr><td>Validation accuracy</td><td>▂█▅▇▅▇▄▇▇▆▅▃▅▅▄█▃▆▅▂▇▆▅▅▅▆▃▅▅▁▅▇▂▆▂▄▅▂▃▅</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>1.25321</td></tr><tr><td>Validation accuracy</td><td>0.56395</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-pyramid-21</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/qxe0hz8o' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/qxe0hz8o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_022607-qxe0hz8o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6176952f0f244250b4f655907f2d9524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011223116666466618, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_025357-kfgs2zzd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/kfgs2zzd' target=\"_blank\">polished-vortex-22</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/kfgs2zzd' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/kfgs2zzd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.5038\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1745\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.5000\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.4477\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.4119\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5563\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.1253\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.5283\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4882\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5537\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0870\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.4739\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.4505\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.5100\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1695\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.4418\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.4086\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5663\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.1149\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.5064\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4680\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5700\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0441\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.5288\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.5875\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.5175\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1259\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.5291\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.4824\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.4444\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5663\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.0685\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.5365\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.5373\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5962\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0067\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.6047\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.5704\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.5649\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5437\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.1080\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.4652\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.4283\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.5837\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.0195\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.5502\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5483\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.6050\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 0.9766\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.5988\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.5748\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.5627\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5513\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.0721\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.5375\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.5006\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.5825\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.0063\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.5578\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5525\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.6025\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 0.9623\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.6163\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.5941\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.5818\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.6163\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5425\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.0823\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.4828\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.4506\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.6000\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 0.9819\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.5427\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5353\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.6188\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 0.9431\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.5988\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.5755\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.5610\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Training => Accuracy : 0.5550\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Training => Loss : 1.0359\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => Accuracy : 0.5640\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => f1 : 0.5415\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => precision : 0.5540\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [8] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Training => Accuracy : 0.5850\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Training => Loss : 1.0110\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => Accuracy : 0.5988\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => f1 : 0.5722\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => precision : 0.5552\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [25] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Training => Accuracy : 0.6088\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Training => Loss : 0.9581\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => f1 : 0.5656\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => precision : 0.5552\n",
      "Epoch [2000], lr [0.1], activation [sigmoid], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Training => Accuracy : 0.5413\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Training => Loss : 1.0558\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => f1 : 0.5042\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => precision : 0.4909\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Training => Accuracy : 0.5663\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Training => Loss : 1.0190\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => f1 : 0.5371\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => precision : 0.5480\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Training => Accuracy : 0.6088\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Training => Loss : 0.9495\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => f1 : 0.5644\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => precision : 0.5469\n",
      "Epoch [3000], lr [0.1], activation [sigmoid], size [64] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Training => Accuracy : 0.5337\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Training => Loss : 1.0902\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => Accuracy : 0.4942\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => f1 : 0.4606\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => precision : 0.4322\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [8] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Training => Accuracy : 0.5975\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Training => Loss : 0.9640\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => Accuracy : 0.6105\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => f1 : 0.5885\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => precision : 0.5777\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [25] Validation => recall : 0.6105\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Training => Accuracy : 0.6075\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Training => Loss : 0.9217\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => f1 : 0.5575\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => precision : 0.5423\n",
      "Epoch [5000], lr [0.1], activation [sigmoid], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Training => Accuracy : 0.4938\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Training => Loss : 1.1236\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => Accuracy : 0.5523\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => f1 : 0.5136\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => precision : 0.4925\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [8] Validation => recall : 0.5523\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Training => Accuracy : 0.6038\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Training => Loss : 0.9497\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => Accuracy : 0.5988\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => f1 : 0.5763\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => precision : 0.5602\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [25] Validation => recall : 0.5988\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Training => Accuracy : 0.5687\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Training => Loss : 0.9523\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => Accuracy : 0.6047\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => f1 : 0.5810\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => precision : 0.6119\n",
      "Epoch [2000], lr [0.3], activation [sigmoid], size [64] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Training => Accuracy : 0.5337\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Training => Loss : 1.0480\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => Accuracy : 0.5698\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => f1 : 0.5317\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => precision : 0.5120\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [8] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Training => Accuracy : 0.5900\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Training => Loss : 0.9487\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => f1 : 0.5724\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => precision : 0.5626\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [25] Validation => recall : 0.5930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Training => Accuracy : 0.6038\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Training => Loss : 0.9210\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => Accuracy : 0.6047\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => f1 : 0.5895\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => precision : 0.5950\n",
      "Epoch [3000], lr [0.3], activation [sigmoid], size [64] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Training => Accuracy : 0.5513\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Training => Loss : 1.0130\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => f1 : 0.5453\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => precision : 0.5425\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [8] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Training => Accuracy : 0.5950\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Training => Loss : 0.9537\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => Accuracy : 0.6047\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => f1 : 0.5844\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => precision : 0.5700\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [25] Validation => recall : 0.6047\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Training => Accuracy : 0.6150\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Training => Loss : 0.8881\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => Accuracy : 0.5756\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => f1 : 0.5565\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => precision : 0.5422\n",
      "Epoch [5000], lr [0.3], activation [sigmoid], size [64] Validation => recall : 0.5756\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Training => Accuracy : 0.5537\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Training => Loss : 1.0491\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => f1 : 0.5130\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => precision : 0.5040\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Training => Accuracy : 0.5962\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Training => Loss : 0.9526\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => f1 : 0.5626\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => precision : 0.5494\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [25] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Training => Accuracy : 0.6100\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Training => Loss : 0.9309\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => Accuracy : 0.6279\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => f1 : 0.6098\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => precision : 0.6085\n",
      "Epoch [2000], lr [0.5], activation [sigmoid], size [64] Validation => recall : 0.6279\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Training => Accuracy : 0.5850\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Training => Loss : 0.9779\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => Accuracy : 0.5930\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => f1 : 0.5696\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => precision : 0.5526\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [8] Validation => recall : 0.5930\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Training => Accuracy : 0.5825\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Training => Loss : 0.9910\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => f1 : 0.5649\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => precision : 0.5493\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [25] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Training => Accuracy : 0.5875\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Training => Loss : 0.9428\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => f1 : 0.5297\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => precision : 0.5226\n",
      "Epoch [3000], lr [0.5], activation [sigmoid], size [64] Validation => recall : 0.5640\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Training => Accuracy : 0.5737\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Training => Loss : 1.0028\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => f1 : 0.5042\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => precision : 0.4826\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [8] Validation => recall : 0.5349\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Training => Accuracy : 0.5687\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Training => Loss : 0.9962\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => Accuracy : 0.6163\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => f1 : 0.5894\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => precision : 0.5885\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [25] Validation => recall : 0.6163\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Training => Accuracy : 0.5925\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Training => Loss : 0.9212\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => f1 : 0.5288\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => precision : 0.5215\n",
      "Epoch [5000], lr [0.5], activation [sigmoid], size [64] Validation => recall : 0.5581\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▇▆█▆▄▇▅▆▄▃▅▃▂▅▃▄▃▂▅▄▂▆▂▇▂▂▅▂▁▄▂▅▂▁▃▃▂▃▁</td></tr><tr><td>Validation accuracy</td><td>▂▆▂▁▅▅▃▆▂▆▇▆▆▇▃▅▅▇▆▃▅▆▁▇▄▇▇▅▆▇▅▇▃▆█▆▆▅▃▅</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.92123</td></tr><tr><td>Validation accuracy</td><td>0.55814</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-vortex-22</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/kfgs2zzd' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/kfgs2zzd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_025357-kfgs2zzd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [0.01,0.05,0.1,0.3,0.5]\n",
    "epochs = [2000,3000,5000]\n",
    "layer_sizes = [[8],[25],[64]]\n",
    "activations =  [[relu],[tanh],[sigmoid]]\n",
    "activation_names = ['relu','tanh','sigmoid']\n",
    "optimiser = 'batch'\n",
    "\n",
    "table = []\n",
    "\n",
    "#  batch\n",
    "for idx,activation in enumerate(activations):\n",
    "    wandb.init(project = \"Multilayer Classification Perceptron\")\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for size in layer_sizes:\n",
    "                classifier = MLP(input_size,output_size,num_layers,size,activation,'batch',lr)\n",
    "                size = size[0]\n",
    "        #       Training\n",
    "                classifier.training(x_train,y_train,epoch)\n",
    "        #       Training metrics\n",
    "                out = classifier.forward(x_train)\n",
    "                predicted = np.argmax(out,axis = 1)\n",
    "                actual = np.argmax(y_train,axis = 1)\n",
    "                accuracy_train = accuracy_score(actual,predicted)\n",
    "                loss_train = Xent(y_train,out)\n",
    "                print(\"TRAINING\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Accuracy : {accuracy_train:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "        #       Validation\n",
    "                out = classifier.forward(x_val)\n",
    "                predicted = np.argmax(out,axis = 1)\n",
    "                actual = np.argmax(y_val,axis = 1)\n",
    "                accuracy = accuracy_score(actual,predicted)\n",
    "                f1 = f1_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                precision = precision_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                recall = recall_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                print(\"VALIDATION\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => Accuracy : {accuracy:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": epoch,\n",
    "                    \"layer_size\": size,\n",
    "                    \"Validation accuracy\": accuracy,\n",
    "                    \"Training loss\": loss_train,\n",
    "                })\n",
    "                entry = [lr,epoch,idx,size,accuracy,f1,precision,recall]\n",
    "                table.append(entry)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2183992",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_batch = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842d29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(table_batch,columns = ['LR','Epochs','Activation','layer size','Accuracy','F1','Precision','Recall'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdce052",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4964887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ksp4n72h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▁▁▁▁▁▁▁▁▁█▁ ▁▁ </td></tr><tr><td>Validation accuracy</td><td>▆████▇▇█▇▇█▁█▇▁</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>nan</td></tr><tr><td>Validation accuracy</td><td>0.0</td></tr><tr><td>epochs</td><td>3000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.002</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-smoke-26</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ksp4n72h' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ksp4n72h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_133257-ksp4n72h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ksp4n72h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2908576d86cc4513b510cc1470c53fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168392600181203, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_141554-ip695sqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ip695sqo' target=\"_blank\">sunny-cloud-27</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ip695sqo' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ip695sqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.4688\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.1017\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.4244\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.3950\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.3902\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.4244\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5563\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Training => Loss : 1.0014\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.4933\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.4799\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.6538\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.8093\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5465\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5353\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5252\n",
      "Epoch [2000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5400\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.0530\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.5044\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.5008\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5825\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Training => Loss : 0.9627\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.4942\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.4767\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.4622\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.6062\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.8494\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5233\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5049\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.4947\n",
      "Epoch [3000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Training => Accuracy : 0.5600\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Training => Loss : 1.0383\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => Accuracy : 0.5814\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => f1 : 0.5520\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => precision : 0.5431\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [8] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Training => Accuracy : 0.5750\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Training => Loss : 0.9330\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => f1 : 0.5462\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => precision : 0.5344\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [25] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Training => Accuracy : 0.6038\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Training => Loss : 0.8822\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => Accuracy : 0.5116\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => f1 : 0.5046\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => precision : 0.5016\n",
      "Epoch [5000], lr [0.01], activation [tanh], size [64] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [8] Training => Accuracy : 0.5062\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [8] Training => Loss : 1.0959\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [8] Validation => f1 : 0.4706\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [8] Validation => precision : 0.4679\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [25] Training => Accuracy : 0.5737\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [25] Training => Loss : 0.9596\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [25] Validation => Accuracy : 0.5174\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [25] Validation => f1 : 0.5021\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [25] Validation => precision : 0.4884\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [25] Validation => recall : 0.5174\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [64] Training => Accuracy : 0.6188\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [64] Training => Loss : 0.7988\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [64] Validation => Accuracy : 0.5581\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [64] Validation => f1 : 0.5652\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [64] Validation => precision : 0.5867\n",
      "Epoch [2000], lr [0.02], activation [tanh], size [64] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [8] Training => Accuracy : 0.4713\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [8] Training => Loss : 1.1113\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [8] Validation => Accuracy : 0.5058\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [8] Validation => f1 : 0.4891\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [8] Validation => precision : 0.4847\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [8] Validation => recall : 0.5058\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [25] Training => Accuracy : 0.5675\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [25] Training => Loss : 0.9389\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [25] Validation => Accuracy : 0.5291\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [25] Validation => f1 : 0.5089\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [25] Validation => precision : 0.5078\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [25] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [64] Training => Accuracy : 0.6338\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [64] Training => Loss : 0.7962\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [64] Validation => Accuracy : 0.5291\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [64] Validation => f1 : 0.5336\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [64] Validation => precision : 0.5432\n",
      "Epoch [3000], lr [0.02], activation [tanh], size [64] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [8] Training => Accuracy : 0.4788\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [8] Training => Loss : 1.1047\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [8] Validation => Accuracy : 0.5407\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [8] Validation => f1 : 0.5229\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [8] Validation => precision : 0.5107\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [8] Validation => recall : 0.5407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [25] Training => Accuracy : 0.5975\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [25] Training => Loss : 0.9242\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [25] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [25] Validation => f1 : 0.5514\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [25] Validation => precision : 0.5522\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [25] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [64] Training => Accuracy : 0.6038\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [64] Training => Loss : 0.8733\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [64] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [64] Validation => f1 : 0.4960\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [64] Validation => precision : 0.4964\n",
      "Epoch [5000], lr [0.02], activation [tanh], size [64] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5513\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0263\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5698\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.5378\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.5180\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5698\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.4512\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Training => Loss : 1.2492\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.4128\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.4054\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.4097\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.4128\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.6062\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Training => Loss : 0.9932\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.5233\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.5029\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.4992\n",
      "Epoch [2000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.4688\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.1513\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.4244\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.3934\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.3713\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.4244\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5162\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Training => Loss : 1.2305\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.4826\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.4618\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.4511\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.4826\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.4975\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Training => Loss : 1.4351\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.4651\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.4454\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.4560\n",
      "Epoch [3000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.4651\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Training => Accuracy : 0.5375\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Training => Loss : 1.0267\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => Accuracy : 0.5349\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => f1 : 0.4966\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => precision : 0.4830\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [8] Validation => recall : 0.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.0075\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Loss : nan\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.0000\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.0000\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.0000\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.0000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Training => Accuracy : 0.5613\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Training => Loss : 1.0506\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => Accuracy : 0.4767\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => f1 : 0.4631\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => precision : 0.4534\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [64] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [8] Training => Accuracy : 0.4363\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [8] Training => Loss : 1.1914\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [8] Validation => Accuracy : 0.4593\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [8] Validation => f1 : 0.4379\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [8] Validation => precision : 0.4430\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [8] Validation => recall : 0.4593\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [25] Training => Accuracy : 0.5200\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [25] Training => Loss : 1.0881\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [25] Validation => Accuracy : 0.4360\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [25] Validation => f1 : 0.4382\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [25] Validation => precision : 0.4629\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [25] Validation => recall : 0.4360\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [64] Training => Accuracy : 0.5200\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [64] Training => Loss : 1.2881\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [64] Validation => Accuracy : 0.4651\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [64] Validation => f1 : 0.4602\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [64] Validation => precision : 0.4564\n",
      "Epoch [2000], lr [0.06], activation [tanh], size [64] Validation => recall : 0.4651\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [8] Training => Accuracy : 0.5587\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [8] Training => Loss : 1.0292\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [8] Validation => Accuracy : 0.5291\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [8] Validation => f1 : 0.5070\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [8] Validation => precision : 0.4948\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [8] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [25] Training => Accuracy : 0.5750\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [25] Training => Loss : 1.0853\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [25] Validation => Accuracy : 0.5872\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [25] Validation => f1 : 0.5780\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [25] Validation => precision : 0.5889\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [25] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [64] Training => Accuracy : 0.5238\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [64] Training => Loss : 1.2874\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [64] Validation => Accuracy : 0.4826\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [64] Validation => f1 : 0.4791\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [64] Validation => precision : 0.4817\n",
      "Epoch [3000], lr [0.06], activation [tanh], size [64] Validation => recall : 0.4826\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [8] Training => Accuracy : 0.4662\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [8] Training => Loss : 1.1316\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [8] Validation => Accuracy : 0.5581\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [8] Validation => f1 : 0.5453\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [8] Validation => precision : 0.5353\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [8] Validation => recall : 0.5581\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [25] Training => Accuracy : 0.5425\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [25] Training => Loss : 1.0938\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [25] Validation => Accuracy : 0.5698\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [25] Validation => f1 : 0.5591\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [25] Validation => precision : 0.5566\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [25] Validation => recall : 0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [64] Training => Accuracy : 0.0075\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [64] Training => Loss : nan\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [64] Validation => Accuracy : 0.0000\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [64] Validation => f1 : 0.0000\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [64] Validation => precision : 0.0000\n",
      "Epoch [5000], lr [0.06], activation [tanh], size [64] Validation => recall : 0.0000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [8] Training => Accuracy : 0.5375\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [8] Training => Loss : 1.2117\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [8] Validation => Accuracy : 0.5291\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [8] Validation => f1 : 0.4840\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [8] Validation => precision : 0.4896\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [8] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [25] Training => Accuracy : 0.5050\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [25] Training => Loss : 1.2703\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [25] Validation => Accuracy : 0.4942\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [25] Validation => f1 : 0.4591\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [25] Validation => precision : 0.4395\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [25] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [64] Training => Accuracy : 0.4713\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [64] Training => Loss : 2.1407\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [64] Validation => Accuracy : 0.4360\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [64] Validation => f1 : 0.4301\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [64] Validation => precision : 0.4455\n",
      "Epoch [2000], lr [0.09], activation [tanh], size [64] Validation => recall : 0.4360\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [8] Training => Accuracy : 0.5500\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [8] Training => Loss : 1.1841\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [8] Validation => Accuracy : 0.5116\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [8] Validation => f1 : 0.4643\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [8] Validation => precision : 0.4296\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [8] Validation => recall : 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [25] Training => Accuracy : 0.0075\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [25] Training => Loss : nan\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [25] Validation => Accuracy : 0.0000\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [25] Validation => f1 : 0.0000\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [25] Validation => precision : 0.0000\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [25] Validation => recall : 0.0000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [64] Training => Accuracy : 0.5350\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [64] Training => Loss : 1.6818\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [64] Validation => Accuracy : 0.5465\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [64] Validation => f1 : 0.5366\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [64] Validation => precision : 0.5415\n",
      "Epoch [3000], lr [0.09], activation [tanh], size [64] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [8] Training => Accuracy : 0.5175\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [8] Training => Loss : 1.0863\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [8] Validation => Accuracy : 0.4884\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [8] Validation => f1 : 0.4563\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [8] Validation => precision : 0.4366\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [8] Validation => recall : 0.4884\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [25] Training => Accuracy : 0.5337\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [25] Training => Loss : 1.1318\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [25] Validation => Accuracy : 0.5233\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [25] Validation => f1 : 0.5135\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [25] Validation => precision : 0.5058\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [25] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [64] Training => Accuracy : 0.5325\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [64] Training => Loss : 1.7641\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [64] Validation => Accuracy : 0.5058\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [64] Validation => f1 : 0.4838\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [64] Validation => precision : 0.4922\n",
      "Epoch [5000], lr [0.09], activation [tanh], size [64] Validation => recall : 0.5058\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da654b969338407ba5b86dcd6b3e058b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▃▂▁▂▂▁▂▂▃▂▁▃▂▁▃▂▂▃▂▃▃▄▂ ▃▃▄▂▃▄▃▃▃▃█▃ ▆▃▆</td></tr><tr><td>Validation accuracy</td><td>▆▇█▇▇▇██▇▇█▇▇▇▇██▆▇▆▇▇▇▁▆▆▇▇█▇██▇▇▆▇▁█▇▇</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>1.7641</td></tr><tr><td>Validation accuracy</td><td>0.50581</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.09</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-cloud-27</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ip695sqo' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/ip695sqo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_141554-ip695sqo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231016_185010-uhovh7b4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/uhovh7b4' target=\"_blank\">warm-sound-28</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/uhovh7b4' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/uhovh7b4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.4800\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.1086\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.4419\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.3868\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.3834\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.4419\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.5288\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.0370\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.4780\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4686\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.6550\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 0.8008\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.5814\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.5652\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.5502\n",
      "Epoch [2000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.5814\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.5375\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 1.0856\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.5465\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.5055\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.4941\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.5465\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.4412\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.1518\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.5291\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.4993\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4980\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.5291\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.5100\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.0306\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.4360\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.3570\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.3716\n",
      "Epoch [3000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.4360\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Training => Accuracy : 0.5663\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Training => Loss : 0.9660\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => Accuracy : 0.5872\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => f1 : 0.5699\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => precision : 0.5562\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [8] Validation => recall : 0.5872\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Training => Accuracy : 0.4913\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Training => Loss : 1.1515\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => Accuracy : 0.4767\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => f1 : 0.4373\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => precision : 0.4043\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [25] Validation => recall : 0.4767\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Training => Accuracy : 0.4725\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Training => Loss : 1.1042\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => Accuracy : 0.4535\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => f1 : 0.3822\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => precision : 0.5303\n",
      "Epoch [5000], lr [0.01], activation [sigmoid], size [64] Validation => recall : 0.4535\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [8] Training => Accuracy : 0.4462\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [8] Training => Loss : 1.1484\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [8] Validation => Accuracy : 0.4651\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [8] Validation => f1 : 0.3292\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [8] Validation => precision : 0.4114\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [8] Validation => recall : 0.4651\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [25] Training => Accuracy : 0.5325\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [25] Training => Loss : 1.0775\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [25] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [25] Validation => f1 : 0.4792\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [25] Validation => precision : 0.5349\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [25] Validation => recall : 0.5116\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [64] Training => Accuracy : 0.4637\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [64] Training => Loss : 1.1925\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [64] Validation => Accuracy : 0.4477\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [64] Validation => f1 : 0.3971\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [64] Validation => precision : 0.4647\n",
      "Epoch [2000], lr [0.02], activation [sigmoid], size [64] Validation => recall : 0.4477\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [8] Training => Accuracy : 0.5437\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [8] Training => Loss : 1.0752\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [8] Validation => Accuracy : 0.5233\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [8] Validation => f1 : 0.4843\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [8] Validation => precision : 0.4886\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [25] Training => Accuracy : 0.4725\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [25] Training => Loss : 1.1384\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [25] Validation => Accuracy : 0.4651\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [25] Validation => f1 : 0.4362\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [25] Validation => precision : 0.5149\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [25] Validation => recall : 0.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [64] Training => Accuracy : 0.4150\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [64] Training => Loss : 1.4478\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [64] Validation => Accuracy : 0.4070\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [64] Validation => f1 : 0.2764\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [64] Validation => precision : 0.6678\n",
      "Epoch [3000], lr [0.02], activation [sigmoid], size [64] Validation => recall : 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [8] Training => Accuracy : 0.4550\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [8] Training => Loss : 1.1609\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [8] Validation => f1 : 0.4399\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [8] Validation => precision : 0.4204\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [8] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [25] Training => Accuracy : 0.5125\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [25] Training => Loss : 1.1349\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [25] Validation => Accuracy : 0.4942\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [25] Validation => f1 : 0.4508\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [25] Validation => precision : 0.4363\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [25] Validation => recall : 0.4942\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [64] Training => Accuracy : 0.4138\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [64] Training => Loss : 1.5035\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [64] Validation => Accuracy : 0.3721\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [64] Validation => f1 : 0.2251\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [64] Validation => precision : 0.2766\n",
      "Epoch [5000], lr [0.02], activation [sigmoid], size [64] Validation => recall : 0.3721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.5075\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.1615\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.5058\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.4597\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.4237\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.5337\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.1249\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.5116\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.4764\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5615\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.4012\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 1.5745\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.3953\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.3013\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.2492\n",
      "Epoch [2000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.4250\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.1941\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.4767\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.3142\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.6010\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.5750\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.0070\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.5640\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.5360\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.5617\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.2562\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 1.3246\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.2209\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.2083\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.4851\n",
      "Epoch [3000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Training => Accuracy : 0.4763\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Training => Loss : 1.1815\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => Accuracy : 0.4302\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => f1 : 0.3988\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => precision : 0.3846\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [8] Validation => recall : 0.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Training => Accuracy : 0.4512\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Training => Loss : 1.1862\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => Accuracy : 0.4302\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => f1 : 0.3959\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => precision : 0.3672\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [25] Validation => recall : 0.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Training => Accuracy : 0.5450\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Training => Loss : 1.1438\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => Accuracy : 0.5000\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => f1 : 0.4547\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => precision : 0.4678\n",
      "Epoch [5000], lr [0.05], activation [sigmoid], size [64] Validation => recall : 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [8] Training => Accuracy : 0.4263\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [8] Training => Loss : 1.1956\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [8] Validation => f1 : 0.3015\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [8] Validation => precision : 0.2218\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [8] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [25] Training => Accuracy : 0.4825\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [25] Training => Loss : 1.2869\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [25] Validation => Accuracy : 0.4884\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [25] Validation => f1 : 0.4454\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [25] Validation => precision : 0.4105\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [25] Validation => recall : 0.4884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [64] Training => Accuracy : 0.4950\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [64] Training => Loss : 1.4338\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [64] Validation => Accuracy : 0.4942\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [64] Validation => f1 : 0.4552\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [64] Validation => precision : 0.4458\n",
      "Epoch [2000], lr [0.06], activation [sigmoid], size [64] Validation => recall : 0.4942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [8] Training => Accuracy : 0.4300\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [8] Training => Loss : 1.2219\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [8] Validation => Accuracy : 0.4709\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [8] Validation => f1 : 0.3027\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [8] Validation => precision : 0.2231\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [8] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [25] Training => Accuracy : 0.5050\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [25] Training => Loss : 1.2254\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [25] Validation => Accuracy : 0.5174\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [25] Validation => f1 : 0.4725\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [25] Validation => precision : 0.4359\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [25] Validation => recall : 0.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [64] Training => Accuracy : 0.5025\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [64] Training => Loss : 1.3752\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [64] Validation => Accuracy : 0.5116\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [64] Validation => f1 : 0.4737\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [64] Validation => precision : 0.4642\n",
      "Epoch [3000], lr [0.06], activation [sigmoid], size [64] Validation => recall : 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [8] Training => Accuracy : 0.5250\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [8] Training => Loss : 1.1123\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [8] Validation => Accuracy : 0.5058\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [8] Validation => f1 : 0.4365\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [8] Validation => precision : 0.4058\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [8] Validation => recall : 0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [25] Training => Accuracy : 0.4325\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [25] Training => Loss : 1.2607\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [25] Validation => Accuracy : 0.4709\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [25] Validation => f1 : 0.3015\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [25] Validation => precision : 0.2218\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [25] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [64] Training => Accuracy : 0.3962\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [64] Training => Loss : 1.6863\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [64] Validation => Accuracy : 0.3779\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [64] Validation => f1 : 0.2073\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [64] Validation => precision : 0.1428\n",
      "Epoch [5000], lr [0.06], activation [sigmoid], size [64] Validation => recall : 0.3779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [8] Training => Accuracy : 0.5012\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [8] Training => Loss : 1.1696\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [8] Validation => Accuracy : 0.5000\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [8] Validation => f1 : 0.4438\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [8] Validation => precision : 0.4079\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [8] Validation => recall : 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [25] Training => Accuracy : 0.4313\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [25] Training => Loss : 1.4017\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [25] Validation => Accuracy : 0.4709\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [25] Validation => f1 : 0.3112\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [25] Validation => precision : 0.3205\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [25] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [64] Training => Accuracy : 0.4250\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [64] Training => Loss : 1.9879\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [64] Validation => Accuracy : 0.4709\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [64] Validation => f1 : 0.3015\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [64] Validation => precision : 0.2218\n",
      "Epoch [2000], lr [0.09], activation [sigmoid], size [64] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [8] Training => Accuracy : 0.3975\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [8] Training => Loss : 1.1715\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [8] Validation => Accuracy : 0.3837\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [8] Validation => f1 : 0.2197\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [8] Validation => precision : 0.6146\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [8] Validation => recall : 0.3837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [25] Training => Accuracy : 0.3950\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [25] Training => Loss : 1.4429\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [25] Validation => Accuracy : 0.3779\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [25] Validation => f1 : 0.2073\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [25] Validation => precision : 0.1428\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [25] Validation => recall : 0.3779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [64] Training => Accuracy : 0.3212\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [64] Training => Loss : 2.0022\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [64] Validation => Accuracy : 0.2849\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [64] Validation => f1 : 0.1999\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [64] Validation => precision : 0.6148\n",
      "Epoch [3000], lr [0.09], activation [sigmoid], size [64] Validation => recall : 0.2849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [8] Training => Accuracy : 0.5600\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [8] Training => Loss : 1.0756\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [8] Validation => Accuracy : 0.5988\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [8] Validation => f1 : 0.5549\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [8] Validation => precision : 0.5245\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [8] Validation => recall : 0.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [25] Training => Accuracy : 0.3475\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [25] Training => Loss : 1.2790\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [25] Validation => Accuracy : 0.3314\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [25] Validation => f1 : 0.2200\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [25] Validation => precision : 0.1666\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [25] Validation => recall : 0.3314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n",
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [64] Training => Accuracy : 0.4263\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [64] Training => Loss : 2.3061\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [64] Validation => Accuracy : 0.4709\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [64] Validation => f1 : 0.3015\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [64] Validation => precision : 0.2218\n",
      "Epoch [5000], lr [0.09], activation [sigmoid], size [64] Validation => recall : 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_29453/2451769450.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▂▂▁▂▃▂▂▃▃▂▃▂▃▄▃▃▃▃▅▃▂▃▃▃▃▃▄▃▃▄▂▃▃▄▇▃▄▇▂█</td></tr><tr><td>Validation accuracy</td><td>▅▆█▇▇▅█▆▆▆▅▇▆▄▆▆▆▆▄▆▇▁▅▅▆▆▆▆▆▆▆▆▆▆▆▄▄▂█▆</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██▁▁▁▃▃▃██</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁▃▁▃█▁▃█▁█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>2.30612</td></tr><tr><td>Validation accuracy</td><td>0.47093</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.09</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-sound-28</strong> at: <a href='https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/uhovh7b4' target=\"_blank\">https://wandb.ai/smai-khushi/Multilayer%20Classification%20Perceptron/runs/uhovh7b4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231016_185010-uhovh7b4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [0.01,0.02,0.05,0.06,0.09]\n",
    "epochs = [2000,3000,5000]\n",
    "layer_sizes = [[8],[25],[64]]\n",
    "activations =  [[tanh],[sigmoid], [relu]]\n",
    "activation_names = ['tanh','sigmoid', 'relu']\n",
    "\n",
    "table = []\n",
    "\n",
    "#  sgd\n",
    "for idx,activation in enumerate(activations):\n",
    "    wandb.init(project = \"Multilayer Classification Perceptron\")\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for size in layer_sizes:\n",
    "                classifier = MLP(input_size,output_size,num_layers,size,activation,'sgd',lr)\n",
    "                size = size[0]\n",
    "        #       Training\n",
    "                classifier.training(x_train,y_train,epoch)\n",
    "        #       Training metrics\n",
    "                out = classifier.forward(x_train)\n",
    "                predicted = np.argmax(out,axis = 1)\n",
    "                actual = np.argmax(y_train,axis = 1)\n",
    "                accuracy_train = accuracy_score(actual,predicted)\n",
    "                loss_train = Xent(y_train,out)\n",
    "                print(\"TRAINING\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Accuracy : {accuracy_train:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "        #       Validation\n",
    "                out = classifier.forward(x_val)\n",
    "                predicted = np.argmax(out,axis = 1)\n",
    "                actual = np.argmax(y_val,axis = 1)\n",
    "                accuracy = accuracy_score(actual,predicted)\n",
    "                f1 = f1_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                precision = precision_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                recall = recall_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "                print(\"VALIDATION\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => Accuracy : {accuracy:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": epoch,\n",
    "                    \"layer_size\": size,\n",
    "                    \"Validation accuracy\": accuracy,\n",
    "                    \"Training loss\": loss_train,\n",
    "                })\n",
    "                entry = [lr,epoch,idx,size,accuracy,f1,precision,recall]\n",
    "                table.append(entry)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18dc10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sgd = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3009f92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Activation</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>0.395047</td>\n",
       "      <td>0.390202</td>\n",
       "      <td>0.424419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.535281</td>\n",
       "      <td>0.525150</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.504373</td>\n",
       "      <td>0.500833</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.476655</td>\n",
       "      <td>0.462155</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.377907</td>\n",
       "      <td>0.207291</td>\n",
       "      <td>0.142814</td>\n",
       "      <td>0.377907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0.284884</td>\n",
       "      <td>0.199945</td>\n",
       "      <td>0.614844</td>\n",
       "      <td>0.284884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.554902</td>\n",
       "      <td>0.524491</td>\n",
       "      <td>0.598837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.331395</td>\n",
       "      <td>0.220017</td>\n",
       "      <td>0.166595</td>\n",
       "      <td>0.331395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.301544</td>\n",
       "      <td>0.221775</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LR  Epochs  Activation  layer size  Accuracy        F1  Precision  \\\n",
       "0   0.01    2000           0           8  0.424419  0.395047   0.390202   \n",
       "1   0.01    2000           0          25  0.511628  0.493304   0.479876   \n",
       "2   0.01    2000           0          64  0.546512  0.535281   0.525150   \n",
       "3   0.01    3000           0           8  0.523256  0.504373   0.500833   \n",
       "4   0.01    3000           0          25  0.494186  0.476655   0.462155   \n",
       "..   ...     ...         ...         ...       ...       ...        ...   \n",
       "85  0.09    3000           1          25  0.377907  0.207291   0.142814   \n",
       "86  0.09    3000           1          64  0.284884  0.199945   0.614844   \n",
       "87  0.09    5000           1           8  0.598837  0.554902   0.524491   \n",
       "88  0.09    5000           1          25  0.331395  0.220017   0.166595   \n",
       "89  0.09    5000           1          64  0.470930  0.301544   0.221775   \n",
       "\n",
       "      Recall  \n",
       "0   0.424419  \n",
       "1   0.511628  \n",
       "2   0.546512  \n",
       "3   0.523256  \n",
       "4   0.494186  \n",
       "..       ...  \n",
       "85  0.377907  \n",
       "86  0.284884  \n",
       "87  0.598837  \n",
       "88  0.331395  \n",
       "89  0.470930  \n",
       "\n",
       "[90 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(table_sgd,columns = ['LR','Epochs','Activation','layer size','Accuracy','F1','Precision','Recall'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45e5fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sgd tanh Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.577983</td>\n",
       "      <td>0.588876</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.552031</td>\n",
       "      <td>0.543131</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.546175</td>\n",
       "      <td>0.534365</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.559118</td>\n",
       "      <td>0.556557</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.537845</td>\n",
       "      <td>0.518025</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.545336</td>\n",
       "      <td>0.535306</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.565214</td>\n",
       "      <td>0.586682</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.551352</td>\n",
       "      <td>0.552223</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.536625</td>\n",
       "      <td>0.541511</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.535281</td>\n",
       "      <td>0.525150</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.510707</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.496554</td>\n",
       "      <td>0.482986</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.507049</td>\n",
       "      <td>0.494822</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.508855</td>\n",
       "      <td>0.507773</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.533602</td>\n",
       "      <td>0.543189</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.484024</td>\n",
       "      <td>0.489644</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.502897</td>\n",
       "      <td>0.499246</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.504885</td>\n",
       "      <td>0.494663</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.496013</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.504373</td>\n",
       "      <td>0.500833</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.513466</td>\n",
       "      <td>0.505818</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.517442</td>\n",
       "      <td>0.502142</td>\n",
       "      <td>0.488355</td>\n",
       "      <td>0.517442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.429587</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.504562</td>\n",
       "      <td>0.501629</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.489137</td>\n",
       "      <td>0.484706</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.483843</td>\n",
       "      <td>0.492152</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.459061</td>\n",
       "      <td>0.439484</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.476655</td>\n",
       "      <td>0.462155</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.456285</td>\n",
       "      <td>0.436584</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.470581</td>\n",
       "      <td>0.467883</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.479131</td>\n",
       "      <td>0.481711</td>\n",
       "      <td>0.482558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.461825</td>\n",
       "      <td>0.451121</td>\n",
       "      <td>0.482558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.463108</td>\n",
       "      <td>0.453367</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.460226</td>\n",
       "      <td>0.456382</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.445421</td>\n",
       "      <td>0.456047</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.459302</td>\n",
       "      <td>0.437906</td>\n",
       "      <td>0.443039</td>\n",
       "      <td>0.459302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.438162</td>\n",
       "      <td>0.462933</td>\n",
       "      <td>0.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.430137</td>\n",
       "      <td>0.445506</td>\n",
       "      <td>0.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>0.393411</td>\n",
       "      <td>0.371262</td>\n",
       "      <td>0.424419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>0.395047</td>\n",
       "      <td>0.390202</td>\n",
       "      <td>0.424419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.412791</td>\n",
       "      <td>0.405419</td>\n",
       "      <td>0.409688</td>\n",
       "      <td>0.412791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LR  Epochs  layer size  Accuracy        F1  Precision    Recall\n",
       "31  0.06    3000          25  0.587209  0.577983   0.588876  0.587209\n",
       "6   0.01    5000           8  0.581395  0.552031   0.543131  0.581395\n",
       "7   0.01    5000          25  0.569767  0.546175   0.534365  0.569767\n",
       "34  0.06    5000          25  0.569767  0.559118   0.556557  0.569767\n",
       "18  0.05    2000           8  0.569767  0.537845   0.518025  0.569767\n",
       "33  0.06    5000           8  0.558140  0.545336   0.535306  0.558140\n",
       "11  0.02    2000          64  0.558140  0.565214   0.586682  0.558140\n",
       "16  0.02    5000          25  0.558140  0.551352   0.552223  0.558140\n",
       "41  0.09    3000          64  0.546512  0.536625   0.541511  0.546512\n",
       "2   0.01    2000          64  0.546512  0.535281   0.525150  0.546512\n",
       "15  0.02    5000           8  0.540698  0.522900   0.510707  0.540698\n",
       "24  0.05    5000           8  0.534884  0.496554   0.482986  0.534884\n",
       "30  0.06    3000           8  0.529070  0.507049   0.494822  0.529070\n",
       "13  0.02    3000          25  0.529070  0.508855   0.507773  0.529070\n",
       "14  0.02    3000          64  0.529070  0.533602   0.543189  0.529070\n",
       "36  0.09    2000           8  0.529070  0.484024   0.489644  0.529070\n",
       "20  0.05    2000          64  0.523256  0.502897   0.499246  0.523256\n",
       "5   0.01    3000          64  0.523256  0.504885   0.494663  0.523256\n",
       "17  0.02    5000          64  0.523256  0.496013   0.496423  0.523256\n",
       "3   0.01    3000           8  0.523256  0.504373   0.500833  0.523256\n",
       "43  0.09    5000          25  0.523256  0.513466   0.505818  0.523256\n",
       "10  0.02    2000          25  0.517442  0.502142   0.488355  0.517442\n",
       "39  0.09    3000           8  0.511628  0.464338   0.429587  0.511628\n",
       "8   0.01    5000          64  0.511628  0.504562   0.501629  0.511628\n",
       "1   0.01    2000          25  0.511628  0.493304   0.479876  0.511628\n",
       "12  0.02    3000           8  0.505814  0.489137   0.484706  0.505814\n",
       "44  0.09    5000          64  0.505814  0.483843   0.492152  0.505814\n",
       "37  0.09    2000          25  0.494186  0.459061   0.439484  0.494186\n",
       "4   0.01    3000          25  0.494186  0.476655   0.462155  0.494186\n",
       "42  0.09    5000           8  0.488372  0.456285   0.436584  0.488372\n",
       "9   0.02    2000           8  0.488372  0.470581   0.467883  0.488372\n",
       "32  0.06    3000          64  0.482558  0.479131   0.481711  0.482558\n",
       "22  0.05    3000          25  0.482558  0.461825   0.451121  0.482558\n",
       "26  0.05    5000          64  0.476744  0.463108   0.453367  0.476744\n",
       "29  0.06    2000          64  0.465116  0.460226   0.456382  0.465116\n",
       "23  0.05    3000          64  0.465116  0.445421   0.456047  0.465116\n",
       "27  0.06    2000           8  0.459302  0.437906   0.443039  0.459302\n",
       "28  0.06    2000          25  0.436047  0.438162   0.462933  0.436047\n",
       "38  0.09    2000          64  0.436047  0.430137   0.445506  0.436047\n",
       "21  0.05    3000           8  0.424419  0.393411   0.371262  0.424419\n",
       "0   0.01    2000           8  0.424419  0.395047   0.390202  0.424419\n",
       "19  0.05    2000          25  0.412791  0.405419   0.409688  0.412791\n",
       "35  0.06    5000          64  0.000000  0.000000   0.000000  0.000000\n",
       "25  0.05    5000          25  0.000000  0.000000   0.000000  0.000000\n",
       "40  0.09    3000          25  0.000000  0.000000   0.000000  0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sgd tanh Metrics\")\n",
    "df_sgd_tanh = table[table['Activation'] == 0].drop('Activation',axis = 1)\n",
    "df_sgd_tanh = df_sgd_tanh.sort_values(by='Accuracy', ascending=False)\n",
    "df_sgd_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32e1f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sgd Sigmoid Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.554902</td>\n",
       "      <td>0.524491</td>\n",
       "      <td>0.598837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.556162</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.565228</td>\n",
       "      <td>0.550177</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.563953</td>\n",
       "      <td>0.536023</td>\n",
       "      <td>0.561679</td>\n",
       "      <td>0.563953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.505467</td>\n",
       "      <td>0.494068</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.499265</td>\n",
       "      <td>0.498022</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.488626</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.517442</td>\n",
       "      <td>0.472522</td>\n",
       "      <td>0.435924</td>\n",
       "      <td>0.517442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.476353</td>\n",
       "      <td>0.561485</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.473717</td>\n",
       "      <td>0.464199</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.479233</td>\n",
       "      <td>0.534863</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.477957</td>\n",
       "      <td>0.468560</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.436518</td>\n",
       "      <td>0.405767</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.459748</td>\n",
       "      <td>0.423705</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.443841</td>\n",
       "      <td>0.407942</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.454684</td>\n",
       "      <td>0.467812</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.455187</td>\n",
       "      <td>0.445844</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.450789</td>\n",
       "      <td>0.436253</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.445433</td>\n",
       "      <td>0.410494</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.314193</td>\n",
       "      <td>0.600979</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.437315</td>\n",
       "      <td>0.404327</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.301544</td>\n",
       "      <td>0.221775</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.302741</td>\n",
       "      <td>0.223072</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.311176</td>\n",
       "      <td>0.320451</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.301544</td>\n",
       "      <td>0.221775</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.301544</td>\n",
       "      <td>0.221775</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.301544</td>\n",
       "      <td>0.221775</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.439933</td>\n",
       "      <td>0.420408</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.329162</td>\n",
       "      <td>0.411417</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.436174</td>\n",
       "      <td>0.514915</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.382211</td>\n",
       "      <td>0.530284</td>\n",
       "      <td>0.453488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.447674</td>\n",
       "      <td>0.397110</td>\n",
       "      <td>0.464727</td>\n",
       "      <td>0.447674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.386832</td>\n",
       "      <td>0.383399</td>\n",
       "      <td>0.441860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.356978</td>\n",
       "      <td>0.371598</td>\n",
       "      <td>0.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.430233</td>\n",
       "      <td>0.398831</td>\n",
       "      <td>0.384593</td>\n",
       "      <td>0.430233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.430233</td>\n",
       "      <td>0.395946</td>\n",
       "      <td>0.367159</td>\n",
       "      <td>0.430233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>0.276395</td>\n",
       "      <td>0.667825</td>\n",
       "      <td>0.406977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.301342</td>\n",
       "      <td>0.249195</td>\n",
       "      <td>0.395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.219655</td>\n",
       "      <td>0.614579</td>\n",
       "      <td>0.383721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.377907</td>\n",
       "      <td>0.207291</td>\n",
       "      <td>0.142814</td>\n",
       "      <td>0.377907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.377907</td>\n",
       "      <td>0.207291</td>\n",
       "      <td>0.142814</td>\n",
       "      <td>0.377907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.225147</td>\n",
       "      <td>0.276553</td>\n",
       "      <td>0.372093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.331395</td>\n",
       "      <td>0.220017</td>\n",
       "      <td>0.166595</td>\n",
       "      <td>0.331395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.284884</td>\n",
       "      <td>0.199945</td>\n",
       "      <td>0.614844</td>\n",
       "      <td>0.284884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.208272</td>\n",
       "      <td>0.485143</td>\n",
       "      <td>0.220930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LR  Epochs  layer size  Accuracy        F1  Precision    Recall\n",
       "87  0.09    5000           8  0.598837  0.554902   0.524491  0.598837\n",
       "51  0.01    5000           8  0.587209  0.569900   0.556162  0.587209\n",
       "47  0.01    2000          64  0.581395  0.565228   0.550177  0.581395\n",
       "67  0.05    3000          25  0.563953  0.536023   0.561679  0.563953\n",
       "48  0.01    3000           8  0.546512  0.505467   0.494068  0.546512\n",
       "49  0.01    3000          25  0.529070  0.499265   0.498022  0.529070\n",
       "57  0.02    3000           8  0.523256  0.484300   0.488626  0.523256\n",
       "76  0.06    3000          25  0.517442  0.472522   0.435924  0.517442\n",
       "64  0.05    2000          25  0.511628  0.476353   0.561485  0.511628\n",
       "77  0.06    3000          64  0.511628  0.473717   0.464199  0.511628\n",
       "55  0.02    2000          25  0.511628  0.479233   0.534863  0.511628\n",
       "46  0.01    2000          25  0.511628  0.477957   0.468560  0.511628\n",
       "78  0.06    5000           8  0.505814  0.436518   0.405767  0.505814\n",
       "63  0.05    2000           8  0.505814  0.459748   0.423705  0.505814\n",
       "81  0.09    2000           8  0.500000  0.443841   0.407942  0.500000\n",
       "71  0.05    5000          64  0.500000  0.454684   0.467812  0.500000\n",
       "74  0.06    2000          64  0.494186  0.455187   0.445844  0.494186\n",
       "61  0.02    5000          25  0.494186  0.450789   0.436253  0.494186\n",
       "73  0.06    2000          25  0.488372  0.445433   0.410494  0.488372\n",
       "66  0.05    3000           8  0.476744  0.314193   0.600979  0.476744\n",
       "52  0.01    5000          25  0.476744  0.437315   0.404327  0.476744\n",
       "79  0.06    5000          25  0.470930  0.301544   0.221775  0.470930\n",
       "75  0.06    3000           8  0.470930  0.302741   0.223072  0.470930\n",
       "82  0.09    2000          25  0.470930  0.311176   0.320451  0.470930\n",
       "83  0.09    2000          64  0.470930  0.301544   0.221775  0.470930\n",
       "72  0.06    2000           8  0.470930  0.301544   0.221775  0.470930\n",
       "89  0.09    5000          64  0.470930  0.301544   0.221775  0.470930\n",
       "60  0.02    5000           8  0.470930  0.439933   0.420408  0.470930\n",
       "54  0.02    2000           8  0.465116  0.329162   0.411417  0.465116\n",
       "58  0.02    3000          25  0.465116  0.436174   0.514915  0.465116\n",
       "53  0.01    5000          64  0.453488  0.382211   0.530284  0.453488\n",
       "56  0.02    2000          64  0.447674  0.397110   0.464727  0.447674\n",
       "45  0.01    2000           8  0.441860  0.386832   0.383399  0.441860\n",
       "50  0.01    3000          64  0.436047  0.356978   0.371598  0.436047\n",
       "69  0.05    5000           8  0.430233  0.398831   0.384593  0.430233\n",
       "70  0.05    5000          25  0.430233  0.395946   0.367159  0.430233\n",
       "59  0.02    3000          64  0.406977  0.276395   0.667825  0.406977\n",
       "65  0.05    2000          64  0.395349  0.301342   0.249195  0.395349\n",
       "84  0.09    3000           8  0.383721  0.219655   0.614579  0.383721\n",
       "80  0.06    5000          64  0.377907  0.207291   0.142814  0.377907\n",
       "85  0.09    3000          25  0.377907  0.207291   0.142814  0.377907\n",
       "62  0.02    5000          64  0.372093  0.225147   0.276553  0.372093\n",
       "88  0.09    5000          25  0.331395  0.220017   0.166595  0.331395\n",
       "86  0.09    3000          64  0.284884  0.199945   0.614844  0.284884\n",
       "68  0.05    3000          64  0.220930  0.208272   0.485143  0.220930"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sgd Sigmoid Metrics\")\n",
    "df_sgd_sigmoid = table[table['Activation'] == 1].drop('Activation',axis = 1)\n",
    "df_sgd_sigmoid = df_sgd_sigmoid.sort_values(by='Accuracy', ascending=False)\n",
    "df_sgd_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c2c0897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sgd relu Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.577983</td>\n",
       "      <td>0.588876</td>\n",
       "      <td>0.587209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.552031</td>\n",
       "      <td>0.543131</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.546175</td>\n",
       "      <td>0.534365</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.559118</td>\n",
       "      <td>0.556557</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.537845</td>\n",
       "      <td>0.518025</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.545336</td>\n",
       "      <td>0.535306</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.565214</td>\n",
       "      <td>0.586682</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.551352</td>\n",
       "      <td>0.552223</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.536625</td>\n",
       "      <td>0.541511</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.535281</td>\n",
       "      <td>0.525150</td>\n",
       "      <td>0.546512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.510707</td>\n",
       "      <td>0.540698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.496554</td>\n",
       "      <td>0.482986</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.507049</td>\n",
       "      <td>0.494822</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.508855</td>\n",
       "      <td>0.507773</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.533602</td>\n",
       "      <td>0.543189</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529070</td>\n",
       "      <td>0.484024</td>\n",
       "      <td>0.489644</td>\n",
       "      <td>0.529070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.502897</td>\n",
       "      <td>0.499246</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.504885</td>\n",
       "      <td>0.494663</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.496013</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.504373</td>\n",
       "      <td>0.500833</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.513466</td>\n",
       "      <td>0.505818</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.517442</td>\n",
       "      <td>0.502142</td>\n",
       "      <td>0.488355</td>\n",
       "      <td>0.517442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.429587</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.504562</td>\n",
       "      <td>0.501629</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.02</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.489137</td>\n",
       "      <td>0.484706</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.505814</td>\n",
       "      <td>0.483843</td>\n",
       "      <td>0.492152</td>\n",
       "      <td>0.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.459061</td>\n",
       "      <td>0.439484</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.494186</td>\n",
       "      <td>0.476655</td>\n",
       "      <td>0.462155</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.456285</td>\n",
       "      <td>0.436584</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.02</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.470581</td>\n",
       "      <td>0.467883</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.06</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.479131</td>\n",
       "      <td>0.481711</td>\n",
       "      <td>0.482558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.461825</td>\n",
       "      <td>0.451121</td>\n",
       "      <td>0.482558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.463108</td>\n",
       "      <td>0.453367</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.460226</td>\n",
       "      <td>0.456382</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.445421</td>\n",
       "      <td>0.456047</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.459302</td>\n",
       "      <td>0.437906</td>\n",
       "      <td>0.443039</td>\n",
       "      <td>0.459302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.06</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.438162</td>\n",
       "      <td>0.462933</td>\n",
       "      <td>0.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.09</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.430137</td>\n",
       "      <td>0.445506</td>\n",
       "      <td>0.436047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>0.393411</td>\n",
       "      <td>0.371262</td>\n",
       "      <td>0.424419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>0.395047</td>\n",
       "      <td>0.390202</td>\n",
       "      <td>0.424419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.412791</td>\n",
       "      <td>0.405419</td>\n",
       "      <td>0.409688</td>\n",
       "      <td>0.412791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.06</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.09</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LR  Epochs  layer size  Accuracy        F1  Precision    Recall\n",
       "31  0.06    3000          25  0.587209  0.577983   0.588876  0.587209\n",
       "6   0.01    5000           8  0.581395  0.552031   0.543131  0.581395\n",
       "7   0.01    5000          25  0.569767  0.546175   0.534365  0.569767\n",
       "34  0.06    5000          25  0.569767  0.559118   0.556557  0.569767\n",
       "18  0.05    2000           8  0.569767  0.537845   0.518025  0.569767\n",
       "33  0.06    5000           8  0.558140  0.545336   0.535306  0.558140\n",
       "11  0.02    2000          64  0.558140  0.565214   0.586682  0.558140\n",
       "16  0.02    5000          25  0.558140  0.551352   0.552223  0.558140\n",
       "41  0.09    3000          64  0.546512  0.536625   0.541511  0.546512\n",
       "2   0.01    2000          64  0.546512  0.535281   0.525150  0.546512\n",
       "15  0.02    5000           8  0.540698  0.522900   0.510707  0.540698\n",
       "24  0.05    5000           8  0.534884  0.496554   0.482986  0.534884\n",
       "30  0.06    3000           8  0.529070  0.507049   0.494822  0.529070\n",
       "13  0.02    3000          25  0.529070  0.508855   0.507773  0.529070\n",
       "14  0.02    3000          64  0.529070  0.533602   0.543189  0.529070\n",
       "36  0.09    2000           8  0.529070  0.484024   0.489644  0.529070\n",
       "20  0.05    2000          64  0.523256  0.502897   0.499246  0.523256\n",
       "5   0.01    3000          64  0.523256  0.504885   0.494663  0.523256\n",
       "17  0.02    5000          64  0.523256  0.496013   0.496423  0.523256\n",
       "3   0.01    3000           8  0.523256  0.504373   0.500833  0.523256\n",
       "43  0.09    5000          25  0.523256  0.513466   0.505818  0.523256\n",
       "10  0.02    2000          25  0.517442  0.502142   0.488355  0.517442\n",
       "39  0.09    3000           8  0.511628  0.464338   0.429587  0.511628\n",
       "8   0.01    5000          64  0.511628  0.504562   0.501629  0.511628\n",
       "1   0.01    2000          25  0.511628  0.493304   0.479876  0.511628\n",
       "12  0.02    3000           8  0.505814  0.489137   0.484706  0.505814\n",
       "44  0.09    5000          64  0.505814  0.483843   0.492152  0.505814\n",
       "37  0.09    2000          25  0.494186  0.459061   0.439484  0.494186\n",
       "4   0.01    3000          25  0.494186  0.476655   0.462155  0.494186\n",
       "42  0.09    5000           8  0.488372  0.456285   0.436584  0.488372\n",
       "9   0.02    2000           8  0.488372  0.470581   0.467883  0.488372\n",
       "32  0.06    3000          64  0.482558  0.479131   0.481711  0.482558\n",
       "22  0.05    3000          25  0.482558  0.461825   0.451121  0.482558\n",
       "26  0.05    5000          64  0.476744  0.463108   0.453367  0.476744\n",
       "29  0.06    2000          64  0.465116  0.460226   0.456382  0.465116\n",
       "23  0.05    3000          64  0.465116  0.445421   0.456047  0.465116\n",
       "27  0.06    2000           8  0.459302  0.437906   0.443039  0.459302\n",
       "28  0.06    2000          25  0.436047  0.438162   0.462933  0.436047\n",
       "38  0.09    2000          64  0.436047  0.430137   0.445506  0.436047\n",
       "21  0.05    3000           8  0.424419  0.393411   0.371262  0.424419\n",
       "0   0.01    2000           8  0.424419  0.395047   0.390202  0.424419\n",
       "19  0.05    2000          25  0.412791  0.405419   0.409688  0.412791\n",
       "35  0.06    5000          64  0.000000  0.000000   0.000000  0.000000\n",
       "25  0.05    5000          25  0.000000  0.000000   0.000000  0.000000\n",
       "40  0.09    3000          25  0.000000  0.000000   0.000000  0.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sgd relu Metrics\")\n",
    "df_sgd_relu = table[table['Activation'] == 0].drop('Activation',axis = 1)\n",
    "df_sgd_relu = df_sgd_relu.sort_values(by='Accuracy', ascending=False)\n",
    "df_sgd_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557b037",
   "metadata": {},
   "source": [
    "# 2.3 \n",
    "## best architecture: \n",
    "### Minibatch tanh with layer size 25 trained with lr = 0.05/0.5 and epochs 5000/2000 respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "add6766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Accuracy : 0.5212\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Training => Loss : 1.0610\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => Accuracy : 0.5465\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => f1 : 0.5126\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => precision : 0.4953\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Validation => recall : 0.5465\n"
     ]
    }
   ],
   "source": [
    "epoch = 5000\n",
    "lr = 0.05\n",
    "activation = [tanh]\n",
    "idx = 0\n",
    "size = 25\n",
    "activation_names = ['tanh']\n",
    "\n",
    "classifier = MLP(input_size,output_size,num_layers,[size],activation,'minibatch',lr)\n",
    "#       Training\n",
    "classifier.training(x_train,y_train,epoch)\n",
    "#       Training metrics\n",
    "out = classifier.forward(x_train)\n",
    "predicted = np.argmax(out,axis = 1)\n",
    "actual = np.argmax(y_train,axis = 1)\n",
    "accuracy_train = accuracy_score(actual,predicted)\n",
    "loss_train = Xent(y_train,out)\n",
    "print(\"TRAINING\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Accuracy : {accuracy_train:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "#       Validation\n",
    "out = classifier.forward(x_val)\n",
    "predicted = np.argmax(out,axis = 1)\n",
    "actual = np.argmax(y_val,axis = 1)\n",
    "accuracy = accuracy_score(actual,predicted)\n",
    "f1 = f1_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "precision = precision_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "recall = recall_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "print(\"VALIDATION\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da58eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Testing => Accuracy : 0.5205\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Testing => f1 : 0.4888\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Testing => precision : 0.4969\n",
      "Epoch [5000], lr [0.05], activation [tanh], size [25] Testing => recall : 0.5205\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "out = classifier.forward(x_test)\n",
    "predicted = np.argmax(out,axis = 1)\n",
    "actual = np.argmax(y_test,axis = 1)\n",
    "accuracy = accuracy_score(actual,predicted)\n",
    "f1 = f1_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "precision = precision_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "recall = recall_score(actual,predicted,zero_division = 0,average='weighted')\n",
    "print(\"Testing\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Testing => Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Testing => f1 : {f1:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Testing => precision : {precision:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Testing => recall : {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8812d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.51      0.69      0.59        62\n",
      "           3       0.54      0.54      0.54        81\n",
      "           4       0.40      0.08      0.14        24\n",
      "           5       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.52       171\n",
      "   macro avg       0.29      0.26      0.25       171\n",
      "weighted avg       0.50      0.52      0.49       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(actual,predicted, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaebb7",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- converges to about 60% loss on both the train and validation set\n",
    "- shows less sign of overfitting\n",
    "### MLP\n",
    "- converges well on the train set on higher accuracies if the model is kept sufficiently complex however shows overfitting in case of validation set\n",
    "- Kept the architecture of the model simpler to improve generalisation and mlp performed better than logistic regression on some hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb7caf",
   "metadata": {},
   "source": [
    "# 2.4 multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea29258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>married</th>\n",
       "      <th>children</th>\n",
       "      <th>city</th>\n",
       "      <th>occupation</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>most bought item</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>61271.953359</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>366</td>\n",
       "      <td>2</td>\n",
       "      <td>87.697118</td>\n",
       "      <td>16</td>\n",
       "      <td>electronics clothing sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>53229.101074</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>115.135586</td>\n",
       "      <td>13</td>\n",
       "      <td>furniture beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>30066.046684</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>403</td>\n",
       "      <td>3</td>\n",
       "      <td>101.694559</td>\n",
       "      <td>3</td>\n",
       "      <td>clothing electronics food sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>48950.246384</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>959</td>\n",
       "      <td>6</td>\n",
       "      <td>97.964887</td>\n",
       "      <td>14</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>44792.627094</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>521</td>\n",
       "      <td>1</td>\n",
       "      <td>86.847281</td>\n",
       "      <td>4</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>34246.773063</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>763</td>\n",
       "      <td>4</td>\n",
       "      <td>103.357441</td>\n",
       "      <td>2</td>\n",
       "      <td>food furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>45494.225591</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>85.159023</td>\n",
       "      <td>3</td>\n",
       "      <td>home clothing food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>68740.442006</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>8</td>\n",
       "      <td>101.434650</td>\n",
       "      <td>1</td>\n",
       "      <td>sports clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>44348.446680</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>7</td>\n",
       "      <td>97.649988</td>\n",
       "      <td>19</td>\n",
       "      <td>beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>39160.083677</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>661</td>\n",
       "      <td>5</td>\n",
       "      <td>105.591485</td>\n",
       "      <td>20</td>\n",
       "      <td>books clothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender        income  education  married  children  city  \\\n",
       "0     45       1  61271.953359          2    False         3   366   \n",
       "1     24       0  53229.101074          1    False         1    98   \n",
       "2     45       0  30066.046684          0     True         3   403   \n",
       "3     19       1  48950.246384          3    False         0   959   \n",
       "4     29       0  44792.627094          2    False         0   521   \n",
       "..   ...     ...           ...        ...      ...       ...   ...   \n",
       "995   61       1  34246.773063          2     True         3   763   \n",
       "996   56       0  45494.225591          0    False         0   111   \n",
       "997   64       0  68740.442006          3     True         0   341   \n",
       "998   18       0  44348.446680          0     True         0   195   \n",
       "999   61       1  39160.083677          2    False         0   661   \n",
       "\n",
       "     occupation  purchase_amount  most bought item  \\\n",
       "0             2        87.697118                16   \n",
       "1             1       115.135586                13   \n",
       "2             3       101.694559                 3   \n",
       "3             6        97.964887                14   \n",
       "4             1        86.847281                 4   \n",
       "..          ...              ...               ...   \n",
       "995           4       103.357441                 2   \n",
       "996           3        85.159023                 3   \n",
       "997           8       101.434650                 1   \n",
       "998           7        97.649988                19   \n",
       "999           5       105.591485                20   \n",
       "\n",
       "                               labels  \n",
       "0         electronics clothing sports  \n",
       "1                    furniture beauty  \n",
       "2    clothing electronics food sports  \n",
       "3                                food  \n",
       "4                                home  \n",
       "..                                ...  \n",
       "995                    food furniture  \n",
       "996                home clothing food  \n",
       "997                   sports clothing  \n",
       "998                            beauty  \n",
       "999                    books clothing  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/advertisement.csv')\n",
    "labels = {}\n",
    "for label in df['labels']:\n",
    "    label_list = label.split(' ')\n",
    "    for lab in label_list:\n",
    "        if lab in labels.keys():\n",
    "            labels[lab] += 1\n",
    "        else:\n",
    "            labels[lab] = 1\n",
    "unique_labels = sorted(list(labels.keys()))\n",
    "label_encoder = LabelEncoder()\n",
    "columns=[\"gender\",\"education\",\"city\",\"occupation\",\"most bought item\"]\n",
    "for i in columns:\n",
    "    df[i] = label_encoder.fit_transform(df[i])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d238691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present(arr,str):\n",
    "    return 1 if str in arr else 0\n",
    "\n",
    "for i in unique_labels:\n",
    "    df[i] = df['labels'].apply(lambda arr: present(arr,i))\n",
    "df.drop('labels',axis = 1,inplace = True)\n",
    "y = df.iloc[:,-7:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251cd97",
   "metadata": {},
   "source": [
    "#  Dataset Analysis and Preprocessing\n",
    "- We have splitted the data into 70% train, 15% validation and 15% test\n",
    "- for normalising the data we have used standardscalar of the sklearn which transforms each attribute by so that the mean and variance for that feature in the data space are 0 and 1 respectively. This helps in convergence and bringing all features down to same scale so that they dont influence the gradient descent more than they should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf7650c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(labels.keys(),labels.values())\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequecy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc9e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_rest, y_train, y_rest = train_test_split(df.iloc[:,:10], y, test_size=0.3, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_rest, y_rest, test_size=0.5, random_state=42)\n",
    "scalar = StandardScaler()\n",
    "scalar.fit(x_train.values)\n",
    "x_train = scalar.transform(x_train.values)\n",
    "x_val = scalar.transform(x_val.values)\n",
    "x_test = scalar.transform(x_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594f16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xent(y_true,y_pred):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return np.sum(loss) / len(y_true)\n",
    "\n",
    "def tanh(y):\n",
    "    return (np.exp(y)-np.exp(-y)) / (np.exp(y)+np.exp(-y))\n",
    "\n",
    "def relu(y):\n",
    "    return np.where(y >= 0,y,0.0)\n",
    "\n",
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,input_size,output_size,num_layers,layer_sizes,activations,optimiser = 'batch',lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.optimiser = optimiser\n",
    "#         Initialising weights and biases\n",
    "        self.w_and_b = []\n",
    "        prev_size = input_size\n",
    "        for layer in range(num_layers):\n",
    "            self.w_and_b.append(np.random.rand(layer_sizes[layer],prev_size+1))\n",
    "            prev_size = layer_sizes[layer]\n",
    "        self.w_and_b.append(np.random.rand(output_size,prev_size+1))\n",
    "                            \n",
    "        self.layer_inputs = []    \n",
    "        self.layer_outputs = []\n",
    "        self.out = np.zeros(output_size)\n",
    "    \n",
    "    def tanh_grad(self,z):\n",
    "        return 1 - tanh(z)**2\n",
    "    \n",
    "    def relu_grad(self,z):\n",
    "        return np.where(z > 0,1.0,0.0)\n",
    "                            \n",
    "    def sigmoid_grad(self,z):\n",
    "        return sigmoid(z)*(1 - sigmoid(z))\n",
    "    \n",
    "    def XentGrad(self,y_true,y_pred):\n",
    "        eps = 1e-10\n",
    "        y = np.clip(y_pred, eps, 1 - eps)\n",
    "        return - (y_true / (y_pred+eps) - (1 - y_true) / (1 - y_pred + eps)) / len(y_true)              \n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = np.append(x,np.ones((x.shape[0],1)),axis=1)\n",
    "        for layer in range(self.num_layers):    \n",
    "            self.layer_inputs.append(inp) # (914,12)\n",
    "            z = inp @ self.w_and_b[layer].T\n",
    "            y = (self.activations[layer])(z)\n",
    "            self.layer_outputs.append(z)\n",
    "            inp = np.append(y,np.ones((y.shape[0],1)),axis=1)\n",
    "\n",
    "        self.layer_inputs.append(inp) # (914,12)\n",
    "        z = inp @ self.w_and_b[self.num_layers].T\n",
    "        self.layer_outputs.append(z)\n",
    "        y_prob = sigmoid(z) \n",
    "        self.out = y_prob\n",
    "        return y_prob\n",
    "    \n",
    "    def backward(self,y_true):\n",
    "        w_and_b_gradients = []\n",
    "        grad_y_prob = self.XentGrad(y_true,self.out) # (batch,output)\n",
    "        grad_y_linear = grad_y_prob*self.sigmoid_grad(self.layer_outputs[-1])  # (batch,output)\n",
    "        grad_w_and_b = grad_y_linear.T @ self.layer_inputs[-1]\n",
    "        w_and_b_gradients.append(grad_w_and_b)\n",
    "        grad_y = grad_y_linear@self.w_and_b[-1][:,:-1]\n",
    "        for layer in range(self.num_layers-1,-1,-1):\n",
    "            grad_z = []\n",
    "            if(self.activations[layer] == tanh):\n",
    "                grad_z = grad_y*self.tanh_grad(self.layer_outputs[layer])\n",
    "            elif(self.activations[layer] == relu):\n",
    "                grad_z = grad_y*self.relu_grad(self.layer_outputs[layer])\n",
    "            elif(self.activations[layer] == sigmoid):\n",
    "                grad_z = grad_y*self.sigmoid_grad(self.layer_outputs[layer])\n",
    "            \n",
    "            grad_w_and_b = grad_z.T @ self.layer_inputs[layer] \n",
    "            w_and_b_gradients.append(grad_w_and_b)\n",
    "            grad_y = grad_z@self.w_and_b[layer][:,:-1]\n",
    "            \n",
    "        for layer in range(self.num_layers,-1,-1):\n",
    "            self.w_and_b[layer] -= self.lr*w_and_b_gradients[self.num_layers-layer]\n",
    "    \n",
    "    def training(self,x,y,epochs):\n",
    "        num_samples = len(x)\n",
    "        batch_size = 0\n",
    "        if(self.optimiser == 'batch'):\n",
    "            batch_size = x.shape[0]\n",
    "        elif(self.optimiser == 'sgd'):\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = 50\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(len(x))\n",
    "            x_shuffled = x[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            total_batches = math.ceil(num_samples / batch_size)\n",
    "            for i in range(0,total_batches,batch_size):\n",
    "                out = self.forward(x_shuffled[i:batch_size+i])\n",
    "                self.backward(y_shuffled[i:batch_size+i])\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c0b797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhushi1703\u001b[0m (\u001b[33msmai-khushi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0950fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cac4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true,y_pred):\n",
    "    hamming_distance = np.sum(np.abs(y_true - y_pred)) / len(y_true)\n",
    "    hamming_score = 1 - np.mean(hamming_distance) / y_true.shape[1]\n",
    "    return hamming_score\n",
    "\n",
    "def f1_score_hamming(y_true,y_pred):\n",
    "    x=np.array([])\n",
    "    x2=np.array([])\n",
    "    for i in range(len(y_true)):\n",
    "        x = np.hstack((x,y_true[i]))\n",
    "        x2 = np.hstack((x2,y_pred[i]))\n",
    "    return f1_score(x,x2,average='macro',zero_division=0)\n",
    "def precision_hamming(y_true,y_pred):\n",
    "    x=np.array([])\n",
    "    x2=np.array([])\n",
    "    for i in range(len(y_true)):\n",
    "        x = np.hstack((x,y_true[i]))\n",
    "        x2 = np.hstack((x2,y_pred[i]))\n",
    "    return precision_score(x,x2,average='macro',zero_division=0)\n",
    "def recall_hamming(y_true,y_pred):\n",
    "    x=np.array([])\n",
    "    x2=np.array([])\n",
    "    for i in range(len(y_true)):\n",
    "        x = np.hstack((x,y_true[i]))\n",
    "        x2 = np.hstack((x2,y_pred[i]))\n",
    "    return recall_score(x,x2,average='macro',zero_division=0)\n",
    "\n",
    "# input_size = x_train.shape[1]\n",
    "# output_size = y_train.shape[1]\n",
    "# num_layers = 1\n",
    "# epoch = 2000\n",
    "# classifier = MLP(input_size,output_size,num_layers,[8],[relu],'batch',0.01)\n",
    "# #       Training\n",
    "# classifier.training(x_train,y_train,epoch)\n",
    "# #       Training metrics\n",
    "# out = classifier.forward(x_train)\n",
    "# predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "# hamming_train = hamming_score(y_train,predicted)\n",
    "# loss_train = Xent(y_train,out)\n",
    "# print(\"TRAINING\")\n",
    "# print(f\"Epoch [{epoch}] Training => Accuracy : {hamming_train:.4f}\")\n",
    "# print(f\"Epoch [{epoch}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "# #       Validation\n",
    "# out = classifier.forward(x_val)\n",
    "# predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "# hamming_test = hamming_score(y_val,predicted)\n",
    "# loss_test = Xent(y_val,out)\n",
    "# print(\"VALIDATION\")\n",
    "# print(f\"Epoch [{epoch}] Validation => Accuracy : {hamming_test:.4f}\")\n",
    "# print(f\"Epoch [{epoch}] Validation => Loss : {loss_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61da3f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:c61fsnjs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60ed8bbb6d4452eba041b2836f2caec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▁</td></tr><tr><td>Validation accuracy</td><td>▁</td></tr><tr><td>epochs</td><td>▁</td></tr><tr><td>layer_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>5.28713</td></tr><tr><td>Validation accuracy</td><td>0.48667</td></tr><tr><td>epochs</td><td>2000</td></tr><tr><td>layer_size</td><td>8</td></tr><tr><td>learning_rate</td><td>0.001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-sky-6</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/c61fsnjs' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/c61fsnjs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_194406-c61fsnjs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:c61fsnjs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794c23a9c05b4242992a97255eb4d81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011149166211382382, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_194436-k98t6tkq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/k98t6tkq' target=\"_blank\">swift-frog-7</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/k98t6tkq' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/k98t6tkq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.4916\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Training => Loss : 5.3543\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.4781\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4710\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.4970\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.4966\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.5306\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Training => Loss : 5.1266\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.5086\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.4881\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.4988\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.4986\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.5394\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Training => Loss : 6.3349\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.5495\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5199\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5236\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5261\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.5576\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.7647\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.5390\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4919\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.4928\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.4924\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.5663\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.7987\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.5629\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.5020\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5021\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5020\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.5863\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.9593\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.5810\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5047\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5065\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5059\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.6351\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.5922\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.6581\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4590\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.5366\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5101\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.6190\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.6261\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.6295\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.5022\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5270\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5169\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.6161\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.7112\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.6086\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5141\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5228\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5183\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.5884\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.7196\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6076\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4995\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.5107\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5080\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6133\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.6503\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6248\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4990\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5208\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5134\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.5967\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.8953\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.5895\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5054\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5090\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5078\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6294\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.6113\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6400\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4925\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.5304\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5158\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6271\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.6115\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6238\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4780\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5028\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5015\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6182\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.6726\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.6143\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5129\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5246\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5189\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6467\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.5550\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6486\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4084\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.4162\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.4874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6506\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.5062\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6562\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4712\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5414\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5138\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6424\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.5124\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.6429\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5125\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5466\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5276\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6496\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.5267\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6619\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4285\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.5126\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.5018\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6476\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.5449\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6457\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4822\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5303\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5134\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6531\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.4905\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6381\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5053\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5370\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5218\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.5026\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4103\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.4783\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.4987\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6531\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.4759\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6467\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4603\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5129\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5045\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6429\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.4971\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6514\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5460\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5732\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5510\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6508\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.4742\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4145\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.5446\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.5023\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6584\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.4187\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4705\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5646\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5180\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6586\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.4169\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5239\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5850\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5424\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6520\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4852\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6838\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4554\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.7210\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5233\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6549\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.4628\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6571\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4565\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5318\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5086\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6406\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.5009\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6438\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.5170\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5503\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5305\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4889\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6752\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4246\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.6710\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5088\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6563\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.4369\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4882\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5860\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6490\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.4382\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6438\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.5022\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5414\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5224\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6567\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4392\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6790\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4662\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.6389\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5242\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6569\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.4213\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4687\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5779\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5193\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6622\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.3741\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6533\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.4996\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5533\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7c0e69edde409087bbc8c6ecd0c910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▄▄█▂▃▃▂▂▂▂▂▃▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation accuracy</td><td>▁▂▃▃▄▅▇▆▅▅▆▅▇▆▆▇▇▇▇▇▆▇▇▇█▇▇█▇▇██▇█▇▇</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.3741</td></tr><tr><td>Validation accuracy</td><td>0.65333</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-frog-7</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/k98t6tkq' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/k98t6tkq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_194436-k98t6tkq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_195011-e56sgxit</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e56sgxit' target=\"_blank\">soft-silence-8</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e56sgxit' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e56sgxit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.5231\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 5.0166\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.5362\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.5076\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.5126\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5140\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6147\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.6444\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6267\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4869\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.5128\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.5074\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6243\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.6841\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6457\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4652\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5161\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5060\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6447\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.6169\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4341\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.5312\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5047\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6490\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.5695\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4041\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.5024\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6390\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.6275\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6400\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4185\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.4376\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.4855\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.5253\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6484\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.5354\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6771\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4305\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.6955\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.5117\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6431\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.6259\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4238\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5411\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5038\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6486\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.5504\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4010\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.3354\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.4986\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6514\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.5436\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4007\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.3352\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.4979\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6410\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.5935\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6629\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4168\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.4876\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.4988\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.5606\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.5262\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4069\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.5359\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.5008\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6471\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.5916\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4217\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5471\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.5189\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6500\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.5316\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4054\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.4464\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.4979\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6484\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.5245\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4395\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5800\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5104\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.5260\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6500\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.5578\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4103\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.6364\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5029\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.4984\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6610\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4135\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.4654\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.4966\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.5124\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6494\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.5032\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4045\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.5859\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5007\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.4945\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4151\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.4935\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.4995\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.5122\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6512\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.4831\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4110\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.8367\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5043\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6545\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.4398\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4292\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.5571\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.5060\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.5393\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6514\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.5023\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4027\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.4065\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.4972\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6518\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.4926\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6629\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4118\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.4667\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.4973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6510\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.5097\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4010\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.3354\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.4986\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6496\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4973\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4041\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.5024\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6529\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.4578\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6771\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4379\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.6638\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5139\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.4968\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6520\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4533\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6752\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4220\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.6875\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5080\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6584\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.4134\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6848\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4736\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.6801\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdbf32d85324d73a0758f4fd9d3b178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▄▄▃▃▃▂▂▃▃▃▃▃▂▃▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁</td></tr><tr><td>Validation accuracy</td><td>▁▅▆▇▇▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇██</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.41342</td></tr><tr><td>Validation accuracy</td><td>0.68476</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-silence-8</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e56sgxit' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e56sgxit</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_195011-e56sgxit/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d611c5c03294909a0f49f8b63379d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168109722590695, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_195342-7eny20uy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/7eny20uy' target=\"_blank\">different-lion-9</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/7eny20uy' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/7eny20uy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.5759\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Training => Loss : 4.7106\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.5103\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5160\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5134\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.6073\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Training => Loss : 4.8253\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.6305\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.5152\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5368\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5250\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.5037\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Training => Loss : 10.8464\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.5095\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4986\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => precision : 0.5177\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => recall : 0.5200\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.5739\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Training => Loss : 4.7032\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.5829\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.5300\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5299\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5302\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.5802\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Training => Loss : 4.8730\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.5886\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.5057\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5090\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5079\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6127\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Training => Loss : 5.5439\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.6286\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4867\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => precision : 0.5143\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => recall : 0.5080\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6212\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Training => Loss : 4.6331\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6333\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.4475\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => precision : 0.4797\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => recall : 0.4924\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.6239\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Training => Loss : 4.6796\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.6390\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.5005\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5348\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5195\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6296\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Training => Loss : 5.1916\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.6381\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4694\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => precision : 0.5093\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => recall : 0.5040\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6047\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Training => Loss : 4.6430\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6038\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4958\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5059\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5044\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6214\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Training => Loss : 4.7529\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6257\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4716\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => precision : 0.4986\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => recall : 0.4993\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6373\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Training => Loss : 5.2529\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6505\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.4529\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5123\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5036\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6424\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Training => Loss : 4.5488\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4303\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5368\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5046\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6090\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Training => Loss : 4.7546\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4833\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => precision : 0.4930\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => recall : 0.4949\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6104\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Training => Loss : 5.3473\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6190\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.4927\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5118\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5076\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6490\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Training => Loss : 4.5061\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4326\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5401\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6478\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Training => Loss : 4.5080\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6610\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4899\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => precision : 0.5632\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => recall : 0.5240\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6222\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Training => Loss : 4.8520\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6390\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.4813\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5207\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5099\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6478\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Training => Loss : 4.5392\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4096\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => precision : 0.4601\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => recall : 0.4973\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6457\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Training => Loss : 4.5301\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6752\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4745\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => precision : 0.6094\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5250\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6280\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Training => Loss : 5.0608\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6295\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4543\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => precision : 0.4840\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => recall : 0.4932\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6490\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Training => Loss : 4.4890\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4230\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => precision : 0.5870\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => recall : 0.5059\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Training => Loss : 4.4851\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4201\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5098\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5010\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6443\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Training => Loss : 4.7344\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6581\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4359\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5101\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5019\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6522\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Training => Loss : 4.4972\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4115\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => precision : 0.5177\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => recall : 0.5008\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6545\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Training => Loss : 4.4417\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4519\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5957\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5155\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Training => Loss : 4.5116\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4621\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5568\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5143\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6478\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Training => Loss : 4.5139\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4092\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => precision : 0.5234\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => recall : 0.5008\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6484\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Training => Loss : 4.4919\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6543\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4251\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => precision : 0.4777\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => recall : 0.4961\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6384\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Training => Loss : 4.6721\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6448\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4647\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5141\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5053\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Training => Loss : 4.4967\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4017\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => precision : 0.3357\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Training => Loss : 4.4768\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4504\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5769\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6500\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Loss : 4.4862\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6514\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4454\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5053\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5014\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Training => Loss : 4.4753\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4069\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => precision : 0.5359\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => recall : 0.5008\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6573\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Training => Loss : 4.4176\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4516\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5669\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5127\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6604\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Training => Loss : 4.4064\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4575\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5654\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36df87f62d484768bd778ff8f23bfdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▁▁█▁▂▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation accuracy</td><td>▅▆▁▄▄▆▆▆▆▅▆▇█▅▆█▇▆██▆██▇████▇▇██▇███</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.40641</td></tr><tr><td>Validation accuracy</td><td>0.66667</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-lion-9</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/7eny20uy' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/7eny20uy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_195342-7eny20uy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [0.001,0.002,0.005,0.007]\n",
    "epochs = [2000,3000,5000]\n",
    "layer_sizes = [[8],[25],[64]]\n",
    "activations =  [[tanh],[sigmoid],[relu]]\n",
    "activation_names = ['tanh','sigmoid','relu']\n",
    "optimiser = 'batch'\n",
    "\n",
    "table = []\n",
    "#  batch\n",
    "for idx,activation in enumerate(activations):\n",
    "    wandb.init(project = \"Multilabel Classification Perceptron\")\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for size in layer_sizes:\n",
    "                classifier = MLP(input_size,output_size,num_layers,size,activation,'batch',lr)\n",
    "                size = size[0]\n",
    "                #       Training\n",
    "                classifier.training(x_train,y_train,epoch)\n",
    "        #       Training metrics\n",
    "                out = classifier.forward(x_train)\n",
    "                predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "                hamming_train = hamming_score(y_train,predicted)\n",
    "                loss_train = Xent(y_train,out)\n",
    "                print(\"TRAINING\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Hamming Accuracy : {hamming_train:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "        #       Validation\n",
    "                out = classifier.forward(x_val)\n",
    "                predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "                hamming_val = hamming_score(y_val,predicted)\n",
    "                loss_val = Xent(y_val,out)\n",
    "                f1 = f1_score_hamming(y_val,predicted)\n",
    "                precision = precision_hamming(y_val,predicted)\n",
    "                recall = recall_hamming(y_val,predicted)\n",
    "                print(\"VALIDATION\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => hamming Accuracy : {hamming_val:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": epoch,\n",
    "                    \"layer_size\": size,\n",
    "                    \"Validation accuracy\": hamming_val,\n",
    "                    \"Training loss\": loss_train,\n",
    "                })\n",
    "                entry = [lr,epoch,idx,size,hamming_val,f1,precision,recall]\n",
    "                table.append(entry)\n",
    "    wandb.finish()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb79198",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_batch = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e919297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Activation</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.478095</td>\n",
       "      <td>0.470953</td>\n",
       "      <td>0.496997</td>\n",
       "      <td>0.496639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.498771</td>\n",
       "      <td>0.498612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.549524</td>\n",
       "      <td>0.519857</td>\n",
       "      <td>0.523637</td>\n",
       "      <td>0.526149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.539048</td>\n",
       "      <td>0.491851</td>\n",
       "      <td>0.492823</td>\n",
       "      <td>0.492445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.562857</td>\n",
       "      <td>0.502021</td>\n",
       "      <td>0.502057</td>\n",
       "      <td>0.502035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.450395</td>\n",
       "      <td>0.576885</td>\n",
       "      <td>0.513383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.445375</td>\n",
       "      <td>0.505271</td>\n",
       "      <td>0.501388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.535885</td>\n",
       "      <td>0.500771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.451602</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>0.512704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.457481</td>\n",
       "      <td>0.565432</td>\n",
       "      <td>0.514215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR  Epochs  Activation  layer size   Hamming        F1  Precision  \\\n",
       "0    0.001    2000           0           8  0.478095  0.470953   0.496997   \n",
       "1    0.001    2000           0          25  0.508571  0.488095   0.498771   \n",
       "2    0.001    2000           0          64  0.549524  0.519857   0.523637   \n",
       "3    0.001    3000           0           8  0.539048  0.491851   0.492823   \n",
       "4    0.001    3000           0          25  0.562857  0.502021   0.502057   \n",
       "..     ...     ...         ...         ...       ...       ...        ...   \n",
       "103  0.007    3000           2          25  0.669524  0.450395   0.576885   \n",
       "104  0.007    3000           2          64  0.651429  0.445375   0.505271   \n",
       "105  0.007    5000           2           8  0.670476  0.406857   0.535885   \n",
       "106  0.007    5000           2          25  0.667619  0.451602   0.566906   \n",
       "107  0.007    5000           2          64  0.666667  0.457481   0.565432   \n",
       "\n",
       "       Recall  \n",
       "0    0.496639  \n",
       "1    0.498612  \n",
       "2    0.526149  \n",
       "3    0.492445  \n",
       "4    0.502035  \n",
       "..        ...  \n",
       "103  0.513383  \n",
       "104  0.501388  \n",
       "105  0.500771  \n",
       "106  0.512704  \n",
       "107  0.514215  \n",
       "\n",
       "[108 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(table_batch,columns = ['LR','Epochs','Activation','layer size','Hamming','F1','Precision','Recall'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83d59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch tanh Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.683810</td>\n",
       "      <td>0.455398</td>\n",
       "      <td>0.720976</td>\n",
       "      <td>0.523281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.679048</td>\n",
       "      <td>0.466234</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.524175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.675238</td>\n",
       "      <td>0.424589</td>\n",
       "      <td>0.671002</td>\n",
       "      <td>0.508757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.488183</td>\n",
       "      <td>0.586018</td>\n",
       "      <td>0.527444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.414465</td>\n",
       "      <td>0.544557</td>\n",
       "      <td>0.502282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.468713</td>\n",
       "      <td>0.577948</td>\n",
       "      <td>0.519334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.523942</td>\n",
       "      <td>0.585042</td>\n",
       "      <td>0.542368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.410338</td>\n",
       "      <td>0.478282</td>\n",
       "      <td>0.498705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.470490</td>\n",
       "      <td>0.564597</td>\n",
       "      <td>0.517977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.428525</td>\n",
       "      <td>0.512593</td>\n",
       "      <td>0.501788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.658095</td>\n",
       "      <td>0.458973</td>\n",
       "      <td>0.536615</td>\n",
       "      <td>0.510052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.456472</td>\n",
       "      <td>0.531771</td>\n",
       "      <td>0.508603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.656190</td>\n",
       "      <td>0.471235</td>\n",
       "      <td>0.541408</td>\n",
       "      <td>0.513814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.499592</td>\n",
       "      <td>0.553345</td>\n",
       "      <td>0.525008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.545994</td>\n",
       "      <td>0.573207</td>\n",
       "      <td>0.550971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.648571</td>\n",
       "      <td>0.408418</td>\n",
       "      <td>0.416174</td>\n",
       "      <td>0.487419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.460316</td>\n",
       "      <td>0.512935</td>\n",
       "      <td>0.504502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.482238</td>\n",
       "      <td>0.530342</td>\n",
       "      <td>0.513414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.643810</td>\n",
       "      <td>0.502218</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>0.522356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.643810</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.550301</td>\n",
       "      <td>0.530496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.512475</td>\n",
       "      <td>0.546609</td>\n",
       "      <td>0.527567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.492549</td>\n",
       "      <td>0.530384</td>\n",
       "      <td>0.515819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.505309</td>\n",
       "      <td>0.537047</td>\n",
       "      <td>0.521801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.629524</td>\n",
       "      <td>0.502217</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.516898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.499046</td>\n",
       "      <td>0.520829</td>\n",
       "      <td>0.513352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.477957</td>\n",
       "      <td>0.502778</td>\n",
       "      <td>0.501542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.512920</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.518871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.608571</td>\n",
       "      <td>0.514092</td>\n",
       "      <td>0.522784</td>\n",
       "      <td>0.518316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.607619</td>\n",
       "      <td>0.499512</td>\n",
       "      <td>0.510746</td>\n",
       "      <td>0.507986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.589524</td>\n",
       "      <td>0.505416</td>\n",
       "      <td>0.509002</td>\n",
       "      <td>0.507832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.504694</td>\n",
       "      <td>0.506458</td>\n",
       "      <td>0.505890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.562857</td>\n",
       "      <td>0.502021</td>\n",
       "      <td>0.502057</td>\n",
       "      <td>0.502035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.549524</td>\n",
       "      <td>0.519857</td>\n",
       "      <td>0.523637</td>\n",
       "      <td>0.526149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.539048</td>\n",
       "      <td>0.491851</td>\n",
       "      <td>0.492823</td>\n",
       "      <td>0.492445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.498771</td>\n",
       "      <td>0.498612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.478095</td>\n",
       "      <td>0.470953</td>\n",
       "      <td>0.496997</td>\n",
       "      <td>0.496639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "27  0.007    2000           8  0.683810  0.455398   0.720976  0.523281\n",
       "33  0.007    5000           8  0.679048  0.466234   0.638889  0.524175\n",
       "30  0.007    3000           8  0.675238  0.424589   0.671002  0.508757\n",
       "31  0.007    3000          25  0.669524  0.488183   0.586018  0.527444\n",
       "24  0.005    5000           8  0.669524  0.414465   0.544557  0.502282\n",
       "34  0.007    5000          25  0.668571  0.468713   0.577948  0.519334\n",
       "26  0.005    5000          64  0.665714  0.523942   0.585042  0.542368\n",
       "21  0.005    3000           8  0.665714  0.410338   0.478282  0.498705\n",
       "25  0.005    5000          25  0.664762  0.470490   0.564597  0.517977\n",
       "18  0.005    2000           8  0.661905  0.428525   0.512593  0.501788\n",
       "6   0.001    5000           8  0.658095  0.458973   0.536615  0.510052\n",
       "28  0.007    2000          25  0.657143  0.456472   0.531771  0.508603\n",
       "16  0.002    5000          25  0.656190  0.471235   0.541408  0.513814\n",
       "35  0.007    5000          64  0.653333  0.499592   0.553345  0.525008\n",
       "23  0.005    3000          64  0.651429  0.545994   0.573207  0.550971\n",
       "15  0.002    5000           8  0.648571  0.408418   0.416174  0.487419\n",
       "22  0.005    3000          25  0.646667  0.460316   0.512935  0.504502\n",
       "19  0.005    2000          25  0.645714  0.482238   0.530342  0.513414\n",
       "32  0.007    3000          64  0.643810  0.502218   0.541436  0.522356\n",
       "29  0.007    2000          64  0.643810  0.517007   0.550301  0.530496\n",
       "17  0.002    5000          64  0.642857  0.512475   0.546609  0.527567\n",
       "12  0.002    3000           8  0.640000  0.492549   0.530384  0.515819\n",
       "20  0.005    2000          64  0.638095  0.505309   0.537047  0.521801\n",
       "7   0.001    5000          25  0.629524  0.502217   0.526964  0.516898\n",
       "10  0.002    2000          25  0.624762  0.499046   0.520829  0.513352\n",
       "13  0.002    3000          25  0.623810  0.477957   0.502778  0.501542\n",
       "14  0.002    3000          64  0.614286  0.512920   0.524648  0.518871\n",
       "8   0.001    5000          64  0.608571  0.514092   0.522784  0.518316\n",
       "9   0.002    2000           8  0.607619  0.499512   0.510746  0.507986\n",
       "11  0.002    2000          64  0.589524  0.505416   0.509002  0.507832\n",
       "5   0.001    3000          64  0.580952  0.504694   0.506458  0.505890\n",
       "4   0.001    3000          25  0.562857  0.502021   0.502057  0.502035\n",
       "2   0.001    2000          64  0.549524  0.519857   0.523637  0.526149\n",
       "3   0.001    3000           8  0.539048  0.491851   0.492823  0.492445\n",
       "1   0.001    2000          25  0.508571  0.488095   0.498771  0.498612\n",
       "0   0.001    2000           8  0.478095  0.470953   0.496997  0.496639"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Batch tanh Metrics\")\n",
    "df_batch_tanh = table[table['Activation'] == 0].drop('Activation',axis = 1)\n",
    "df_batch_tanh = df_batch_tanh.sort_values(by='Hamming', ascending=False)\n",
    "df_batch_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86da1dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch sigmoid Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.684762</td>\n",
       "      <td>0.473628</td>\n",
       "      <td>0.680074</td>\n",
       "      <td>0.529911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.677143</td>\n",
       "      <td>0.437922</td>\n",
       "      <td>0.663835</td>\n",
       "      <td>0.513876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.677143</td>\n",
       "      <td>0.430497</td>\n",
       "      <td>0.695463</td>\n",
       "      <td>0.511656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.675238</td>\n",
       "      <td>0.422011</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.508017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.411018</td>\n",
       "      <td>0.836676</td>\n",
       "      <td>0.504348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.410286</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.502929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.404479</td>\n",
       "      <td>0.585878</td>\n",
       "      <td>0.500740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.535885</td>\n",
       "      <td>0.500771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.404129</td>\n",
       "      <td>0.502388</td>\n",
       "      <td>0.500031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.439546</td>\n",
       "      <td>0.580012</td>\n",
       "      <td>0.510392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.404129</td>\n",
       "      <td>0.502388</td>\n",
       "      <td>0.500031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.401027</td>\n",
       "      <td>0.335401</td>\n",
       "      <td>0.498582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.401027</td>\n",
       "      <td>0.335401</td>\n",
       "      <td>0.498582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.421748</td>\n",
       "      <td>0.547093</td>\n",
       "      <td>0.503793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.429152</td>\n",
       "      <td>0.557073</td>\n",
       "      <td>0.506013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.400685</td>\n",
       "      <td>0.335244</td>\n",
       "      <td>0.497872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.423831</td>\n",
       "      <td>0.541121</td>\n",
       "      <td>0.503824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.402726</td>\n",
       "      <td>0.406520</td>\n",
       "      <td>0.497194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.405421</td>\n",
       "      <td>0.446366</td>\n",
       "      <td>0.497934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.415101</td>\n",
       "      <td>0.493491</td>\n",
       "      <td>0.499476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.434069</td>\n",
       "      <td>0.531229</td>\n",
       "      <td>0.504687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.411778</td>\n",
       "      <td>0.466690</td>\n",
       "      <td>0.497317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.416828</td>\n",
       "      <td>0.487617</td>\n",
       "      <td>0.498797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.660952</td>\n",
       "      <td>0.413533</td>\n",
       "      <td>0.465391</td>\n",
       "      <td>0.496639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.465225</td>\n",
       "      <td>0.516120</td>\n",
       "      <td>0.506013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.418543</td>\n",
       "      <td>0.437589</td>\n",
       "      <td>0.485476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.486908</td>\n",
       "      <td>0.512785</td>\n",
       "      <td>0.507370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.536190</td>\n",
       "      <td>0.507603</td>\n",
       "      <td>0.512597</td>\n",
       "      <td>0.513999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "71  0.007    5000          64  0.684762  0.473628   0.680074  0.529911\n",
       "68  0.007    3000          64  0.677143  0.437922   0.663835  0.513876\n",
       "43  0.001    5000          25  0.677143  0.430497   0.695463  0.511656\n",
       "70  0.007    5000          25  0.675238  0.422011   0.687500  0.508017\n",
       "61  0.005    5000          25  0.674286  0.411018   0.836676  0.504348\n",
       "55  0.005    2000          25  0.672381  0.410286   0.636364  0.502929\n",
       "51  0.002    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "69  0.007    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "63  0.007    2000           8  0.671429  0.401709   0.335714  0.500000\n",
       "60  0.005    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "58  0.005    3000          25  0.671429  0.404479   0.585878  0.500740\n",
       "57  0.005    3000           8  0.671429  0.401709   0.335714  0.500000\n",
       "54  0.005    2000           8  0.671429  0.401709   0.335714  0.500000\n",
       "48  0.002    3000           8  0.671429  0.401709   0.335714  0.500000\n",
       "42  0.001    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "49  0.002    3000          25  0.670476  0.406857   0.535885  0.500771\n",
       "40  0.001    3000          25  0.670476  0.404129   0.502388  0.500031\n",
       "53  0.002    5000          64  0.670476  0.439546   0.580012  0.510392\n",
       "67  0.007    3000          25  0.670476  0.404129   0.502388  0.500031\n",
       "66  0.007    3000           8  0.669524  0.401027   0.335401  0.498582\n",
       "45  0.002    2000           8  0.669524  0.401027   0.335401  0.498582\n",
       "50  0.002    3000          64  0.668571  0.421748   0.547093  0.503793\n",
       "62  0.005    5000          64  0.668571  0.429152   0.557073  0.506013\n",
       "46  0.002    2000          25  0.668571  0.400685   0.335244  0.497872\n",
       "44  0.001    5000          64  0.667619  0.423831   0.541121  0.503824\n",
       "64  0.007    2000          25  0.666667  0.402726   0.406520  0.497194\n",
       "52  0.002    5000          25  0.666667  0.405421   0.446366  0.497934\n",
       "59  0.005    3000          64  0.664762  0.415101   0.493491  0.499476\n",
       "39  0.001    3000           8  0.663810  0.434069   0.531229  0.504687\n",
       "65  0.007    2000          64  0.662857  0.411778   0.466690  0.497317\n",
       "47  0.002    2000          64  0.662857  0.416828   0.487617  0.498797\n",
       "56  0.005    2000          64  0.660952  0.413533   0.465391  0.496639\n",
       "38  0.001    2000          64  0.645714  0.465225   0.516120  0.506013\n",
       "41  0.001    3000          64  0.640000  0.418543   0.437589  0.485476\n",
       "37  0.001    2000          25  0.626667  0.486908   0.512785  0.507370\n",
       "36  0.001    2000           8  0.536190  0.507603   0.512597  0.513999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Batch sigmoid Metrics\")\n",
    "df_batch_sigmoid = table[table['Activation'] == 1].drop('Activation',axis = 1)\n",
    "df_batch_sigmoid = df_batch_sigmoid.sort_values(by='Hamming', ascending=False)\n",
    "df_batch_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0900ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch relu Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.675238</td>\n",
       "      <td>0.474460</td>\n",
       "      <td>0.609406</td>\n",
       "      <td>0.525039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.451850</td>\n",
       "      <td>0.595678</td>\n",
       "      <td>0.515510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.422966</td>\n",
       "      <td>0.587041</td>\n",
       "      <td>0.505920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.535885</td>\n",
       "      <td>0.500771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.450395</td>\n",
       "      <td>0.576885</td>\n",
       "      <td>0.513383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.409185</td>\n",
       "      <td>0.523393</td>\n",
       "      <td>0.500802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.411465</td>\n",
       "      <td>0.517718</td>\n",
       "      <td>0.500833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.451602</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>0.512704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.457481</td>\n",
       "      <td>0.565432</td>\n",
       "      <td>0.514215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.432626</td>\n",
       "      <td>0.540060</td>\n",
       "      <td>0.505365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.430261</td>\n",
       "      <td>0.536765</td>\n",
       "      <td>0.504625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.420123</td>\n",
       "      <td>0.509843</td>\n",
       "      <td>0.500956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.462086</td>\n",
       "      <td>0.556811</td>\n",
       "      <td>0.514308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.409585</td>\n",
       "      <td>0.460106</td>\n",
       "      <td>0.497286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.660952</td>\n",
       "      <td>0.489927</td>\n",
       "      <td>0.563202</td>\n",
       "      <td>0.524021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.658095</td>\n",
       "      <td>0.435889</td>\n",
       "      <td>0.510068</td>\n",
       "      <td>0.501912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.654286</td>\n",
       "      <td>0.425053</td>\n",
       "      <td>0.477679</td>\n",
       "      <td>0.496115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.445375</td>\n",
       "      <td>0.505271</td>\n",
       "      <td>0.501388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.452891</td>\n",
       "      <td>0.512254</td>\n",
       "      <td>0.503639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.464680</td>\n",
       "      <td>0.514085</td>\n",
       "      <td>0.505304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.639048</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.520743</td>\n",
       "      <td>0.509929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.639048</td>\n",
       "      <td>0.500496</td>\n",
       "      <td>0.534836</td>\n",
       "      <td>0.519550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.469440</td>\n",
       "      <td>0.509279</td>\n",
       "      <td>0.504039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.447458</td>\n",
       "      <td>0.479691</td>\n",
       "      <td>0.492353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.630476</td>\n",
       "      <td>0.515205</td>\n",
       "      <td>0.536774</td>\n",
       "      <td>0.525008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.629524</td>\n",
       "      <td>0.454290</td>\n",
       "      <td>0.484043</td>\n",
       "      <td>0.493216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.486711</td>\n",
       "      <td>0.514263</td>\n",
       "      <td>0.508048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.625714</td>\n",
       "      <td>0.471646</td>\n",
       "      <td>0.498570</td>\n",
       "      <td>0.499260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.492707</td>\n",
       "      <td>0.511778</td>\n",
       "      <td>0.507616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.603810</td>\n",
       "      <td>0.495789</td>\n",
       "      <td>0.505893</td>\n",
       "      <td>0.504409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.483279</td>\n",
       "      <td>0.492977</td>\n",
       "      <td>0.494943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.510298</td>\n",
       "      <td>0.516003</td>\n",
       "      <td>0.513444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.505667</td>\n",
       "      <td>0.508974</td>\n",
       "      <td>0.507863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.582857</td>\n",
       "      <td>0.530041</td>\n",
       "      <td>0.529904</td>\n",
       "      <td>0.530250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.498598</td>\n",
       "      <td>0.517727</td>\n",
       "      <td>0.520043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "91   0.005    2000          25  0.675238  0.474460   0.609406  0.525039\n",
       "97   0.005    5000          25  0.672381  0.451850   0.595678  0.515510\n",
       "102  0.007    3000           8  0.671429  0.401709   0.335714  0.500000\n",
       "93   0.005    3000           8  0.671429  0.422966   0.587041  0.505920\n",
       "105  0.007    5000           8  0.670476  0.406857   0.535885  0.500771\n",
       "103  0.007    3000          25  0.669524  0.450395   0.576885  0.513383\n",
       "99   0.007    2000           8  0.669524  0.409185   0.523393  0.500802\n",
       "96   0.005    5000           8  0.668571  0.411465   0.517718  0.500833\n",
       "106  0.007    5000          25  0.667619  0.451602   0.566906  0.512704\n",
       "107  0.007    5000          64  0.666667  0.457481   0.565432  0.514215\n",
       "87   0.002    5000           8  0.665714  0.432626   0.540060  0.505365\n",
       "84   0.002    3000           8  0.665714  0.430261   0.536765  0.504625\n",
       "94   0.005    3000          25  0.664762  0.420123   0.509843  0.500956\n",
       "98   0.005    5000          64  0.663810  0.462086   0.556811  0.514308\n",
       "90   0.005    2000           8  0.663810  0.409585   0.460106  0.497286\n",
       "88   0.002    5000          25  0.660952  0.489927   0.563202  0.524021\n",
       "95   0.005    3000          64  0.658095  0.435889   0.510068  0.501912\n",
       "100  0.007    2000          25  0.654286  0.425053   0.477679  0.496115\n",
       "104  0.007    3000          64  0.651429  0.445375   0.505271  0.501388\n",
       "83   0.002    2000          64  0.650476  0.452891   0.512254  0.503639\n",
       "101  0.007    2000          64  0.644762  0.464680   0.514085  0.505304\n",
       "89   0.002    5000          64  0.639048  0.481336   0.520743  0.509929\n",
       "79   0.001    5000          25  0.639048  0.500496   0.534836  0.519550\n",
       "80   0.001    5000          64  0.638095  0.469440   0.509279  0.504039\n",
       "78   0.001    5000           8  0.633333  0.447458   0.479691  0.492353\n",
       "73   0.001    2000          25  0.630476  0.515205   0.536774  0.525008\n",
       "92   0.005    2000          64  0.629524  0.454290   0.484043  0.493216\n",
       "77   0.001    3000          64  0.628571  0.486711   0.514263  0.508048\n",
       "82   0.002    2000          25  0.625714  0.471646   0.498570  0.499260\n",
       "86   0.002    3000          64  0.619048  0.492707   0.511778  0.507616\n",
       "81   0.002    2000           8  0.603810  0.495789   0.505893  0.504409\n",
       "85   0.002    3000          25  0.599048  0.483279   0.492977  0.494943\n",
       "72   0.001    2000           8  0.599048  0.510298   0.516003  0.513444\n",
       "76   0.001    3000          25  0.588571  0.505667   0.508974  0.507863\n",
       "75   0.001    3000           8  0.582857  0.530041   0.529904  0.530250\n",
       "74   0.001    2000          64  0.509524  0.498598   0.517727  0.520043"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Batch relu Metrics\")\n",
    "df_batch_relu = table[table['Activation'] == 2].drop('Activation',axis = 1)\n",
    "df_batch_relu = df_batch_relu.sort_values(by='Hamming', ascending=False)\n",
    "df_batch_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "947fa0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:olp5gfo6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979f87f05b4b48ea9090a2011f7deb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-wind-10</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/olp5gfo6' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/olp5gfo6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_200434-olp5gfo6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:olp5gfo6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_200540-wtbvdzk7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wtbvdzk7' target=\"_blank\">sweet-grass-11</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wtbvdzk7' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wtbvdzk7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.5090\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Training => Loss : 5.4305\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.5171\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4972\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.5077\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5087\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.5251\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Training => Loss : 5.3918\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.5057\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.4906\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5054\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5061\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.5469\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Training => Loss : 6.1921\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.5276\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.4926\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.4961\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.4958\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.5904\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.7343\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4926\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.5011\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5009\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.5855\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.7557\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.5848\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.5074\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5097\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5087\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.5998\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.9020\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.5895\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5118\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5145\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5130\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.6318\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.6117\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.6533\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.5056\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.5565\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5280\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.6086\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.6841\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.6086\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.4833\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.4975\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.4983\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.6212\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.6582\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.6143\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5029\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5170\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5122\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.5873\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.6911\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6076\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.5184\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.5257\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5213\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.5976\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.6923\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.5829\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4869\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.4909\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.4925\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.5882\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.8827\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.5895\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5153\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5175\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5160\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6380\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.5768\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4771\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.5543\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5181\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6382\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.5977\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6467\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.5253\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5580\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5363\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6092\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.7318\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.5962\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5148\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5188\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5165\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6488\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.5024\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4055\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.4176\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.4937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6359\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.5169\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6371\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4848\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5213\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5107\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6418\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.5379\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.6343\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.4946\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5257\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5145\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6537\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.5198\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6590\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4295\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.5025\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.5004\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6422\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.5129\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4816\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5782\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5238\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6384\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.5572\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6381\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5118\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5413\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5255\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6496\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.4860\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6505\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4365\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.4904\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.4977\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6514\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.4791\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6524\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4539\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5175\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5051\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6410\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.4968\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6314\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5086\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5330\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5213\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6524\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.4783\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6733\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4238\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.6231\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.5073\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6547\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.4286\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4929\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.6030\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5317\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6561\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.4439\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6362\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.4842\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5197\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5100\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6510\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4832\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4267\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.6377\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5088\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6480\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.4755\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6571\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4524\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5279\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5071\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6420\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.4980\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6381\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.5066\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5379\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5225\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4921\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6733\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4133\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.6700\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5044\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6529\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.4309\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4782\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5915\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6508\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.4403\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6419\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.5231\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5518\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5335\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6533\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4468\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4238\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.5411\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5038\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6533\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.4002\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4810\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5750\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5231\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6645\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.3687\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6562\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.5030\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5599\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▅▅█▂▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation accuracy</td><td>▁▁▂▅▄▄▇▅▆▅▄▄▇▇▅▇▆▆▇█▆▇▇▆██▆█▇▆██▇██▇</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.36873</td></tr><tr><td>Validation accuracy</td><td>0.65619</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-grass-11</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wtbvdzk7' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wtbvdzk7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_200540-wtbvdzk7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5f342dca234dd39d8afca5f2312332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011125686110204293, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_200615-wktflw2h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wktflw2h' target=\"_blank\">vital-hill-12</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wktflw2h' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wktflw2h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.5316\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.8749\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.5162\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4803\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.4843\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.4828\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.5612\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.7909\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.5476\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4739\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.4737\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.4751\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6176\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.6869\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6295\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4983\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5241\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5147\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6455\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.6135\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4118\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.5361\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5015\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6506\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.5441\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6610\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4059\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.4227\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.4944\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6445\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.6401\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6590\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4102\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.4432\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.4945\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.5513\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6488\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.5504\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4034\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.4354\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.4986\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6461\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.6031\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4255\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5287\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5031\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.5662\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4010\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.3354\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.4986\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6480\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.5674\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4031\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.4186\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.4979\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6473\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.5521\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4222\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5090\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5010\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.5444\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4041\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.5024\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.5538\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4017\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.3357\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6439\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.5895\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4322\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5337\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.5330\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6494\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.5315\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4061\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.4784\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.4994\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.5301\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4208\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.6063\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5059\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.5285\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6480\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.5272\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4209\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.5267\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5024\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.5137\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6733\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4288\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.6155\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.5088\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.5159\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6494\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.5023\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4072\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.5860\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5015\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.4647\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4357\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.5980\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.5103\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6506\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.5089\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6506\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.4941\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4130\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.6222\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5037\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6539\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.4332\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4196\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.5553\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.5038\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.5040\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6500\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.5169\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4110\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.8367\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5043\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.4985\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4287\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.5485\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6502\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.5065\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6506\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4785\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4103\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.6364\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5029\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6561\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.4523\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6771\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4497\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.6397\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5176\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6504\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.5144\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4017\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.3357\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5000\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6539\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4508\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4276\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.5753\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5067\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6573\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.4082\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4409\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.5665\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5097\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▇▅▄▃▄▃▃▄▃▃▃▃▃▄▃▃▃▃▃▃▃▂▂▃▂▁▂▃▂▂▂▂▃▂▁</td></tr><tr><td>Validation accuracy</td><td>▁▂▆█▇▇█████▇██▇█████████████████████</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.40824</td></tr><tr><td>Validation accuracy</td><td>0.66857</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-hill-12</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wktflw2h' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/wktflw2h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_200615-wktflw2h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082c828adb5946b6bdaae4f8c20433ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011152763433185303, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_200639-e9bes8p0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e9bes8p0' target=\"_blank\">volcanic-dust-13</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e9bes8p0' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e9bes8p0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6029\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Training => Loss : 4.7453\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6257\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.5059\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5267\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5178\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.5767\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Training => Loss : 5.0857\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.5714\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.5076\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5079\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5077\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.5224\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Training => Loss : 8.5891\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.4790\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4695\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => precision : 0.4917\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => recall : 0.4907\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6182\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Training => Loss : 4.6387\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6257\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.4572\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => precision : 0.4839\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => recall : 0.4926\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.5855\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Training => Loss : 5.0534\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.4980\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5058\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5046\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6029\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Training => Loss : 7.8041\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4784\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => precision : 0.4885\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => recall : 0.4920\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6178\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Training => Loss : 4.6302\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6219\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.4866\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5088\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5053\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.6080\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Training => Loss : 4.7441\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.4746\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => precision : 0.4850\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => recall : 0.4898\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6182\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Training => Loss : 5.1789\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.6219\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4866\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => precision : 0.5088\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => recall : 0.5053\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6390\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Training => Loss : 4.6197\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6619\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4355\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5241\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5040\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.5949\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Training => Loss : 4.7222\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.5990\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4784\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => precision : 0.4885\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => recall : 0.4920\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6124\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Training => Loss : 5.9383\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6238\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.5106\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5287\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5200\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6490\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Training => Loss : 4.5795\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4176\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5024\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5002\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6110\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Training => Loss : 4.7569\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6352\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4540\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => precision : 0.4897\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => recall : 0.4960\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6002\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Training => Loss : 5.7786\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6086\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.5013\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5127\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5094\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6478\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Training => Loss : 4.5319\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6410\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4400\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => precision : 0.4789\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => recall : 0.4936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6357\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Training => Loss : 4.6075\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6333\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4650\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => precision : 0.4994\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => recall : 0.4998\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6463\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Training => Loss : 4.6916\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6629\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.4513\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5456\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5099\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6492\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Training => Loss : 4.5146\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4180\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => precision : 0.5109\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => recall : 0.5009\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6514\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Training => Loss : 4.5209\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4631\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5644\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5157\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6388\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Training => Loss : 4.7344\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.5039\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5673\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5300\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Training => Loss : 4.4954\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4065\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => precision : 0.5024\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => recall : 0.5001\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6486\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Training => Loss : 4.5243\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6619\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4309\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5168\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5025\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6380\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Training => Loss : 4.5769\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6533\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4785\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5405\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5154\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6500\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Training => Loss : 4.4865\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4069\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => precision : 0.5359\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => recall : 0.5008\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6522\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Training => Loss : 4.4428\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6619\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4444\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5352\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5070\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6545\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Training => Loss : 4.4934\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4498\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5340\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5078\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6529\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Training => Loss : 4.4981\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4279\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => precision : 0.5330\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => recall : 0.5039\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6453\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Training => Loss : 4.5278\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4456\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5295\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5063\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6410\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Training => Loss : 4.5801\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6448\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4459\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => precision : 0.4931\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => recall : 0.4979\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6549\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Training => Loss : 4.4762\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4103\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => precision : 0.4783\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => recall : 0.4987\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6547\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Training => Loss : 4.4695\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6629\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4336\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5258\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6473\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Loss : 4.5451\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6514\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4592\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5208\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5066\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Training => Loss : 4.4597\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4217\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => precision : 0.5471\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => recall : 0.5038\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6559\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Training => Loss : 4.4315\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4364\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5343\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5054\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6555\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Training => Loss : 4.4399\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6524\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4396\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => precision : 0.4995\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => recall : 0.4999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42b701675cf4cd0acc9d5d8a05d1421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.025 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.038217…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▂▂█▁▂▇▁▂▂▁▁▄▁▂▃▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation accuracy</td><td>▆▄▁▆▅▅▆▅▆█▅▆█▇▆▇▇██████▇█████▇██▇██▇</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.43995</td></tr><tr><td>Validation accuracy</td><td>0.65238</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-dust-13</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e9bes8p0' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/e9bes8p0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_200639-e9bes8p0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [0.001,0.002,0.005,0.007]\n",
    "epochs = [2000,3000,5000]\n",
    "layer_sizes = [[8],[25],[64]]\n",
    "activations =  [[tanh],[sigmoid],[relu]]\n",
    "activation_names = ['tanh','sigmoid','relu']\n",
    "\n",
    "table = []\n",
    "#  minibatch\n",
    "for idx,activation in enumerate(activations):\n",
    "    wandb.init(project = \"Multilabel Classification Perceptron\")\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for size in layer_sizes:\n",
    "                classifier = MLP(input_size,output_size,num_layers,size,activation,'minibatch',lr)\n",
    "                size = size[0]\n",
    "                #       Training\n",
    "                classifier.training(x_train,y_train,epoch)\n",
    "        #       Training metrics\n",
    "                out = classifier.forward(x_train)\n",
    "                predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "                hamming_train = hamming_score(y_train,predicted)\n",
    "                loss_train = Xent(y_train,out)\n",
    "                print(\"TRAINING\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Hamming Accuracy : {hamming_train:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "        #       Validation\n",
    "                out = classifier.forward(x_val)\n",
    "                predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "                hamming_val = hamming_score(y_val,predicted)\n",
    "                loss_val = Xent(y_val,out)\n",
    "                f1 = f1_score_hamming(y_val,predicted)\n",
    "                precision = precision_hamming(y_val,predicted)\n",
    "                recall = recall_hamming(y_val,predicted)\n",
    "                print(\"VALIDATION\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => hamming Accuracy : {hamming_val:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": epoch,\n",
    "                    \"layer_size\": size,\n",
    "                    \"Validation accuracy\": hamming_val,\n",
    "                    \"Training loss\": loss_train,\n",
    "                })\n",
    "                entry = [lr,epoch,idx,size,hamming_val,f1,precision,recall]\n",
    "                table.append(entry)\n",
    "    wandb.finish()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea005a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_minibatch = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5e0f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Activation</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.517143</td>\n",
       "      <td>0.497223</td>\n",
       "      <td>0.507697</td>\n",
       "      <td>0.508696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.505714</td>\n",
       "      <td>0.490577</td>\n",
       "      <td>0.505388</td>\n",
       "      <td>0.506105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.527619</td>\n",
       "      <td>0.492558</td>\n",
       "      <td>0.496143</td>\n",
       "      <td>0.495776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.492561</td>\n",
       "      <td>0.501135</td>\n",
       "      <td>0.500863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.584762</td>\n",
       "      <td>0.507447</td>\n",
       "      <td>0.509651</td>\n",
       "      <td>0.508726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.433622</td>\n",
       "      <td>0.525813</td>\n",
       "      <td>0.503978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.459208</td>\n",
       "      <td>0.520825</td>\n",
       "      <td>0.506568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.421748</td>\n",
       "      <td>0.547093</td>\n",
       "      <td>0.503793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.436369</td>\n",
       "      <td>0.534325</td>\n",
       "      <td>0.505427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.439604</td>\n",
       "      <td>0.499479</td>\n",
       "      <td>0.499877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR  Epochs  Activation  layer size   Hamming        F1  Precision  \\\n",
       "0    0.001    2000           0           8  0.517143  0.497223   0.507697   \n",
       "1    0.001    2000           0          25  0.505714  0.490577   0.505388   \n",
       "2    0.001    2000           0          64  0.527619  0.492558   0.496143   \n",
       "3    0.001    3000           0           8  0.599048  0.492561   0.501135   \n",
       "4    0.001    3000           0          25  0.584762  0.507447   0.509651   \n",
       "..     ...     ...         ...         ...       ...       ...        ...   \n",
       "103  0.007    3000           2          25  0.662857  0.433622   0.525813   \n",
       "104  0.007    3000           2          64  0.651429  0.459208   0.520825   \n",
       "105  0.007    5000           2           8  0.668571  0.421748   0.547093   \n",
       "106  0.007    5000           2          25  0.663810  0.436369   0.534325   \n",
       "107  0.007    5000           2          64  0.652381  0.439604   0.499479   \n",
       "\n",
       "       Recall  \n",
       "0    0.508696  \n",
       "1    0.506105  \n",
       "2    0.495776  \n",
       "3    0.500863  \n",
       "4    0.508726  \n",
       "..        ...  \n",
       "103  0.503978  \n",
       "104  0.506568  \n",
       "105  0.503793  \n",
       "106  0.505427  \n",
       "107  0.499877  \n",
       "\n",
       "[108 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(table_minibatch,columns = ['LR','Epochs','Activation','layer size','Hamming','F1','Precision','Recall'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6572b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch tanh Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.426724</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.508788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.492880</td>\n",
       "      <td>0.603009</td>\n",
       "      <td>0.531730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.413340</td>\n",
       "      <td>0.670019</td>\n",
       "      <td>0.504379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.423778</td>\n",
       "      <td>0.623069</td>\n",
       "      <td>0.507339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.478161</td>\n",
       "      <td>0.591463</td>\n",
       "      <td>0.524422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.423831</td>\n",
       "      <td>0.541121</td>\n",
       "      <td>0.503824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.481580</td>\n",
       "      <td>0.578218</td>\n",
       "      <td>0.523805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.481017</td>\n",
       "      <td>0.574979</td>\n",
       "      <td>0.523096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.477094</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.518131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.405501</td>\n",
       "      <td>0.417636</td>\n",
       "      <td>0.493710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.659048</td>\n",
       "      <td>0.429542</td>\n",
       "      <td>0.502473</td>\n",
       "      <td>0.500401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.452441</td>\n",
       "      <td>0.527862</td>\n",
       "      <td>0.507123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.656190</td>\n",
       "      <td>0.502958</td>\n",
       "      <td>0.559886</td>\n",
       "      <td>0.527875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.505578</td>\n",
       "      <td>0.556504</td>\n",
       "      <td>0.527968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.453913</td>\n",
       "      <td>0.517468</td>\n",
       "      <td>0.505057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.436533</td>\n",
       "      <td>0.490354</td>\n",
       "      <td>0.497718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.525251</td>\n",
       "      <td>0.557963</td>\n",
       "      <td>0.536324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.641905</td>\n",
       "      <td>0.523145</td>\n",
       "      <td>0.551831</td>\n",
       "      <td>0.533518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.506638</td>\n",
       "      <td>0.537920</td>\n",
       "      <td>0.522541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.511833</td>\n",
       "      <td>0.541265</td>\n",
       "      <td>0.525501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>0.484776</td>\n",
       "      <td>0.521264</td>\n",
       "      <td>0.510731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.636190</td>\n",
       "      <td>0.484182</td>\n",
       "      <td>0.519733</td>\n",
       "      <td>0.510022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.634286</td>\n",
       "      <td>0.494608</td>\n",
       "      <td>0.525739</td>\n",
       "      <td>0.514524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.631429</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.533046</td>\n",
       "      <td>0.521277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.512211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.608571</td>\n",
       "      <td>0.483304</td>\n",
       "      <td>0.497501</td>\n",
       "      <td>0.498335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.607619</td>\n",
       "      <td>0.518368</td>\n",
       "      <td>0.525702</td>\n",
       "      <td>0.521307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.492561</td>\n",
       "      <td>0.501135</td>\n",
       "      <td>0.500863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.596190</td>\n",
       "      <td>0.514821</td>\n",
       "      <td>0.518827</td>\n",
       "      <td>0.516497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.589524</td>\n",
       "      <td>0.511780</td>\n",
       "      <td>0.514485</td>\n",
       "      <td>0.513013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.589524</td>\n",
       "      <td>0.515253</td>\n",
       "      <td>0.517479</td>\n",
       "      <td>0.515973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.584762</td>\n",
       "      <td>0.507447</td>\n",
       "      <td>0.509651</td>\n",
       "      <td>0.508726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.582857</td>\n",
       "      <td>0.486939</td>\n",
       "      <td>0.490912</td>\n",
       "      <td>0.492507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.527619</td>\n",
       "      <td>0.492558</td>\n",
       "      <td>0.496143</td>\n",
       "      <td>0.495776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.517143</td>\n",
       "      <td>0.497223</td>\n",
       "      <td>0.507697</td>\n",
       "      <td>0.508696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.505714</td>\n",
       "      <td>0.490577</td>\n",
       "      <td>0.505388</td>\n",
       "      <td>0.506105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "27  0.007    2000           8  0.674286  0.426724   0.637681  0.508788\n",
       "25  0.005    5000          25  0.674286  0.492880   0.603009  0.531730\n",
       "30  0.007    3000           8  0.673333  0.413340   0.670019  0.504379\n",
       "24  0.005    5000           8  0.673333  0.423778   0.623069  0.507339\n",
       "31  0.007    3000          25  0.671429  0.478161   0.591463  0.524422\n",
       "33  0.007    5000           8  0.667619  0.423831   0.541121  0.503824\n",
       "19  0.005    2000          25  0.667619  0.481580   0.578218  0.523805\n",
       "34  0.007    5000          25  0.666667  0.481017   0.574979  0.523096\n",
       "12  0.002    3000           8  0.660000  0.477094   0.554348  0.518131\n",
       "15  0.002    5000           8  0.660000  0.405501   0.417636  0.493710\n",
       "18  0.005    2000           8  0.659048  0.429542   0.502473  0.500401\n",
       "28  0.007    2000          25  0.657143  0.452441   0.527862  0.507123\n",
       "35  0.007    5000          64  0.656190  0.502958   0.559886  0.527875\n",
       "6   0.001    5000           8  0.653333  0.505578   0.556504  0.527968\n",
       "22  0.005    3000          25  0.652381  0.453913   0.517468  0.505057\n",
       "21  0.005    3000           8  0.650476  0.436533   0.490354  0.497718\n",
       "13  0.002    3000          25  0.646667  0.525251   0.557963  0.536324\n",
       "32  0.007    3000          64  0.641905  0.523145   0.551831  0.533518\n",
       "29  0.007    2000          64  0.638095  0.506638   0.537920  0.522541\n",
       "20  0.005    2000          64  0.638095  0.511833   0.541265  0.525501\n",
       "16  0.002    5000          25  0.637143  0.484776   0.521264  0.510731\n",
       "26  0.005    5000          64  0.636190  0.484182   0.519733  0.510022\n",
       "17  0.002    5000          64  0.634286  0.494608   0.525739  0.514524\n",
       "23  0.005    3000          64  0.631429  0.508571   0.533046  0.521277\n",
       "8   0.001    5000          64  0.614286  0.502917   0.516958  0.512211\n",
       "7   0.001    5000          25  0.608571  0.483304   0.497501  0.498335\n",
       "9   0.002    2000           8  0.607619  0.518368   0.525702  0.521307\n",
       "3   0.001    3000           8  0.599048  0.492561   0.501135  0.500863\n",
       "14  0.002    3000          64  0.596190  0.514821   0.518827  0.516497\n",
       "5   0.001    3000          64  0.589524  0.511780   0.514485  0.513013\n",
       "11  0.002    2000          64  0.589524  0.515253   0.517479  0.515973\n",
       "4   0.001    3000          25  0.584762  0.507447   0.509651  0.508726\n",
       "10  0.002    2000          25  0.582857  0.486939   0.490912  0.492507\n",
       "2   0.001    2000          64  0.527619  0.492558   0.496143  0.495776\n",
       "0   0.001    2000           8  0.517143  0.497223   0.507697  0.508696\n",
       "1   0.001    2000          25  0.505714  0.490577   0.505388  0.506105"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"MiniBatch tanh Metrics\")\n",
    "df_minibatch_tanh = table[table['Activation'] == 0].drop('Activation',axis = 1)\n",
    "df_minibatch_tanh = df_minibatch_tanh.sort_values(by='Hamming', ascending=False)\n",
    "df_minibatch_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33ede802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch sigmoid Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.677143</td>\n",
       "      <td>0.449739</td>\n",
       "      <td>0.639706</td>\n",
       "      <td>0.517576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.411018</td>\n",
       "      <td>0.836676</td>\n",
       "      <td>0.504348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.428815</td>\n",
       "      <td>0.615472</td>\n",
       "      <td>0.508819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.435713</td>\n",
       "      <td>0.598049</td>\n",
       "      <td>0.510330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.420816</td>\n",
       "      <td>0.606261</td>\n",
       "      <td>0.505890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.410286</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.502929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.412965</td>\n",
       "      <td>0.622244</td>\n",
       "      <td>0.503669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.586042</td>\n",
       "      <td>0.501480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.404129</td>\n",
       "      <td>0.502388</td>\n",
       "      <td>0.500031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.427553</td>\n",
       "      <td>0.575316</td>\n",
       "      <td>0.506691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.419621</td>\n",
       "      <td>0.555307</td>\n",
       "      <td>0.503762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.401027</td>\n",
       "      <td>0.335401</td>\n",
       "      <td>0.498582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.411841</td>\n",
       "      <td>0.536058</td>\n",
       "      <td>0.501542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.440942</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.509713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.406139</td>\n",
       "      <td>0.478428</td>\n",
       "      <td>0.499352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.403429</td>\n",
       "      <td>0.435407</td>\n",
       "      <td>0.498612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.403078</td>\n",
       "      <td>0.418582</td>\n",
       "      <td>0.497903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.428724</td>\n",
       "      <td>0.548453</td>\n",
       "      <td>0.505304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.420935</td>\n",
       "      <td>0.526725</td>\n",
       "      <td>0.502374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.425450</td>\n",
       "      <td>0.528733</td>\n",
       "      <td>0.503145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.432185</td>\n",
       "      <td>0.533745</td>\n",
       "      <td>0.504656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.422177</td>\n",
       "      <td>0.509014</td>\n",
       "      <td>0.500987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.660952</td>\n",
       "      <td>0.405871</td>\n",
       "      <td>0.422698</td>\n",
       "      <td>0.494419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.659048</td>\n",
       "      <td>0.410238</td>\n",
       "      <td>0.443165</td>\n",
       "      <td>0.494480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.629524</td>\n",
       "      <td>0.498299</td>\n",
       "      <td>0.524093</td>\n",
       "      <td>0.514678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.473921</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>0.475146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.480281</td>\n",
       "      <td>0.484319</td>\n",
       "      <td>0.482825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "68  0.007    3000          64  0.677143  0.449739   0.639706  0.517576\n",
       "64  0.007    2000          25  0.674286  0.411018   0.836676  0.504348\n",
       "56  0.005    2000          64  0.673333  0.428815   0.615472  0.508819\n",
       "59  0.005    3000          64  0.672381  0.435713   0.598049  0.510330\n",
       "53  0.002    5000          64  0.672381  0.420816   0.606261  0.505890\n",
       "67  0.007    3000          25  0.672381  0.410286   0.636364  0.502929\n",
       "61  0.005    5000          25  0.672381  0.412965   0.622244  0.503669\n",
       "54  0.005    2000           8  0.671429  0.401709   0.335714  0.500000\n",
       "49  0.002    3000          25  0.671429  0.401709   0.335714  0.500000\n",
       "60  0.005    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "51  0.002    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "63  0.007    2000           8  0.671429  0.401709   0.335714  0.500000\n",
       "58  0.005    3000          25  0.671429  0.407216   0.586042  0.501480\n",
       "66  0.007    3000           8  0.671429  0.401709   0.335714  0.500000\n",
       "42  0.001    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "69  0.007    5000           8  0.671429  0.401709   0.335714  0.500000\n",
       "57  0.005    3000           8  0.671429  0.401709   0.335714  0.500000\n",
       "48  0.002    3000           8  0.670476  0.404129   0.502388  0.500031\n",
       "70  0.007    5000          25  0.670476  0.427553   0.575316  0.506691\n",
       "62  0.005    5000          64  0.669524  0.419621   0.555307  0.503762\n",
       "45  0.002    2000           8  0.669524  0.401027   0.335401  0.498582\n",
       "39  0.001    3000           8  0.669524  0.411841   0.536058  0.501542\n",
       "71  0.007    5000          64  0.668571  0.440942   0.566502  0.509713\n",
       "52  0.002    5000          25  0.668571  0.406139   0.478428  0.499352\n",
       "43  0.001    5000          25  0.668571  0.403429   0.435407  0.498612\n",
       "46  0.002    2000          25  0.667619  0.403078   0.418582  0.497903\n",
       "65  0.007    2000          64  0.667619  0.428724   0.548453  0.505304\n",
       "55  0.005    2000          25  0.666667  0.420935   0.526725  0.502374\n",
       "44  0.001    5000          64  0.665714  0.425450   0.528733  0.503145\n",
       "50  0.002    3000          64  0.664762  0.432185   0.533745  0.504656\n",
       "47  0.002    2000          64  0.663810  0.422177   0.509014  0.500987\n",
       "40  0.001    3000          25  0.660952  0.405871   0.422698  0.494419\n",
       "41  0.001    3000          64  0.659048  0.410238   0.443165  0.494480\n",
       "38  0.001    2000          64  0.629524  0.498299   0.524093  0.514678\n",
       "37  0.001    2000          25  0.547619  0.473921   0.473747  0.475146\n",
       "36  0.001    2000           8  0.516190  0.480281   0.484319  0.482825"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"MiniBatch sigmoid Metrics\")\n",
    "df_minibatch_sigmoid = table[table['Activation'] == 1].drop('Activation',axis = 1)\n",
    "df_minibatch_sigmoid = df_minibatch_sigmoid.sort_values(by='Hamming', ascending=False)\n",
    "df_minibatch_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300160b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch relu Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.535885</td>\n",
       "      <td>0.500771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.406498</td>\n",
       "      <td>0.502395</td>\n",
       "      <td>0.500062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.421748</td>\n",
       "      <td>0.547093</td>\n",
       "      <td>0.503793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.418025</td>\n",
       "      <td>0.510922</td>\n",
       "      <td>0.500925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.463127</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>0.515726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.410338</td>\n",
       "      <td>0.478282</td>\n",
       "      <td>0.498705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.427869</td>\n",
       "      <td>0.533023</td>\n",
       "      <td>0.503885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.417626</td>\n",
       "      <td>0.502430</td>\n",
       "      <td>0.500216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.436369</td>\n",
       "      <td>0.534325</td>\n",
       "      <td>0.505427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.451281</td>\n",
       "      <td>0.545561</td>\n",
       "      <td>0.509898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.433622</td>\n",
       "      <td>0.525813</td>\n",
       "      <td>0.503978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.444362</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.506969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.430863</td>\n",
       "      <td>0.516847</td>\n",
       "      <td>0.502529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.435462</td>\n",
       "      <td>0.524134</td>\n",
       "      <td>0.504009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.449801</td>\n",
       "      <td>0.533954</td>\n",
       "      <td>0.507771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.445563</td>\n",
       "      <td>0.529482</td>\n",
       "      <td>0.506290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.503890</td>\n",
       "      <td>0.567286</td>\n",
       "      <td>0.529972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>0.540485</td>\n",
       "      <td>0.515387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.439604</td>\n",
       "      <td>0.499479</td>\n",
       "      <td>0.499877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.459208</td>\n",
       "      <td>0.520825</td>\n",
       "      <td>0.506568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.445929</td>\n",
       "      <td>0.493110</td>\n",
       "      <td>0.497903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.640952</td>\n",
       "      <td>0.439987</td>\n",
       "      <td>0.478926</td>\n",
       "      <td>0.493586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.635238</td>\n",
       "      <td>0.453953</td>\n",
       "      <td>0.489737</td>\n",
       "      <td>0.495991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.464980</td>\n",
       "      <td>0.499446</td>\n",
       "      <td>0.499753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.625714</td>\n",
       "      <td>0.505933</td>\n",
       "      <td>0.526657</td>\n",
       "      <td>0.517761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.625714</td>\n",
       "      <td>0.457236</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.492599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.510605</td>\n",
       "      <td>0.528676</td>\n",
       "      <td>0.520043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.621905</td>\n",
       "      <td>0.486625</td>\n",
       "      <td>0.508791</td>\n",
       "      <td>0.505304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.621905</td>\n",
       "      <td>0.486625</td>\n",
       "      <td>0.508791</td>\n",
       "      <td>0.505304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.608571</td>\n",
       "      <td>0.501289</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.509436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.498046</td>\n",
       "      <td>0.505813</td>\n",
       "      <td>0.504564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.474613</td>\n",
       "      <td>0.485006</td>\n",
       "      <td>0.489763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.478392</td>\n",
       "      <td>0.488529</td>\n",
       "      <td>0.491983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.478392</td>\n",
       "      <td>0.488529</td>\n",
       "      <td>0.491983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.507616</td>\n",
       "      <td>0.507899</td>\n",
       "      <td>0.507678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.479048</td>\n",
       "      <td>0.469481</td>\n",
       "      <td>0.491737</td>\n",
       "      <td>0.490688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "96   0.005    5000           8  0.670476  0.406857   0.535885  0.500771\n",
       "93   0.005    3000           8  0.669524  0.406498   0.502395  0.500062\n",
       "105  0.007    5000           8  0.668571  0.421748   0.547093  0.503793\n",
       "90   0.005    2000           8  0.665714  0.418025   0.510922  0.500925\n",
       "91   0.005    2000          25  0.665714  0.463127   0.564394  0.515726\n",
       "102  0.007    3000           8  0.665714  0.410338   0.478282  0.498705\n",
       "99   0.007    2000           8  0.665714  0.427869   0.533023  0.503885\n",
       "84   0.002    3000           8  0.664762  0.417626   0.502430  0.500216\n",
       "106  0.007    5000          25  0.663810  0.436369   0.534325  0.505427\n",
       "89   0.002    5000          64  0.662857  0.451281   0.545561  0.509898\n",
       "103  0.007    3000          25  0.662857  0.433622   0.525813  0.503978\n",
       "97   0.005    5000          25  0.661905  0.444362   0.535242  0.506969\n",
       "94   0.005    3000          25  0.661905  0.430863   0.516847  0.502529\n",
       "81   0.002    2000           8  0.661905  0.435462   0.524134  0.504009\n",
       "98   0.005    5000          64  0.660000  0.449801   0.533954  0.507771\n",
       "100  0.007    2000          25  0.660000  0.445563   0.529482  0.506290\n",
       "92   0.005    2000          64  0.660000  0.503890   0.567286  0.529972\n",
       "95   0.005    3000          64  0.653333  0.478464   0.540485  0.515387\n",
       "107  0.007    5000          64  0.652381  0.439604   0.499479  0.499877\n",
       "104  0.007    3000          64  0.651429  0.459208   0.520825  0.506568\n",
       "101  0.007    2000          64  0.644762  0.445929   0.493110  0.497903\n",
       "87   0.002    5000           8  0.640952  0.439987   0.478926  0.493586\n",
       "85   0.002    3000          25  0.635238  0.453953   0.489737  0.495991\n",
       "88   0.002    5000          25  0.633333  0.464980   0.499446  0.499753\n",
       "72   0.001    2000           8  0.625714  0.505933   0.526657  0.517761\n",
       "75   0.001    3000           8  0.625714  0.457236   0.483871  0.492599\n",
       "83   0.002    2000          64  0.623810  0.510605   0.528676  0.520043\n",
       "80   0.001    5000          64  0.621905  0.486625   0.508791  0.505304\n",
       "78   0.001    5000           8  0.621905  0.486625   0.508791  0.505304\n",
       "86   0.002    3000          64  0.608571  0.501289   0.512653  0.509436\n",
       "76   0.001    3000          25  0.599048  0.498046   0.505813  0.504564\n",
       "79   0.001    5000          25  0.599048  0.474613   0.485006  0.489763\n",
       "77   0.001    3000          64  0.599048  0.478392   0.488529  0.491983\n",
       "82   0.002    2000          25  0.599048  0.478392   0.488529  0.491983\n",
       "73   0.001    2000          25  0.571429  0.507616   0.507899  0.507678\n",
       "74   0.001    2000          64  0.479048  0.469481   0.491737  0.490688"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"MiniBatch relu Metrics\")\n",
    "df_minibatch_relu = table[table['Activation'] == 2].drop('Activation',axis = 1)\n",
    "df_minibatch_relu = df_minibatch_relu.sort_values(by='Hamming', ascending=False)\n",
    "df_minibatch_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02854897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8ee5a580c448c989d29b41f0330db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167591199692753, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_201239-vf6ol36z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/vf6ol36z' target=\"_blank\">absurd-forest-14</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/vf6ol36z' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/vf6ol36z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.6510\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.4422\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4455\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.5690\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5112\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.6702\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.2930\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.6333\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.4854\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5177\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5094\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.6841\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.1102\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.6105\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.4871\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5018\n",
      "Epoch [2000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5012\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.6561\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.4241\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4516\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.5669\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5127\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.6678\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.3244\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.6581\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.5057\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5645\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5300\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.6922\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.0939\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.6438\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5507\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5686\n",
      "Epoch [3000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5527\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Training => Hamming Accuracy : 0.6529\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Training => Loss : 4.4382\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => f1 : 0.4460\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => precision : 0.6133\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [8] Validation => recall : 0.5147\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Training => Hamming Accuracy : 0.6598\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Training => Loss : 4.3412\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => f1 : 0.4824\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => precision : 0.5573\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [25] Validation => recall : 0.5204\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Training => Hamming Accuracy : 0.6894\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Training => Loss : 4.1404\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => hamming Accuracy : 0.6305\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => f1 : 0.5164\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => precision : 0.5376\n",
      "Epoch [5000], lr [0.001], activation [tanh], size [64] Validation => recall : 0.5257\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6524\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.4251\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4658\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.6060\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5214\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6627\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.3332\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6486\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4651\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5208\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5074\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6790\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.1658\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.6476\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5377\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5653\n",
      "Epoch [2000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5444\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6533\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.4069\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6733\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4887\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.5995\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5295\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6682\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.3534\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6610\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4721\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5540\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5166\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6955\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.0710\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.6200\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5169\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5308\n",
      "Epoch [3000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5231\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Training => Hamming Accuracy : 0.6508\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Training => Loss : 4.4391\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => hamming Accuracy : 0.6790\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => f1 : 0.4619\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => precision : 0.6435\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [8] Validation => recall : 0.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Training => Hamming Accuracy : 0.6633\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Training => Loss : 4.3327\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => hamming Accuracy : 0.6619\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => f1 : 0.4988\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => precision : 0.5691\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [25] Validation => recall : 0.5284\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Training => Hamming Accuracy : 0.6929\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Training => Loss : 4.0955\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => hamming Accuracy : 0.6238\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => f1 : 0.5071\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => precision : 0.5261\n",
      "Epoch [5000], lr [0.002], activation [tanh], size [64] Validation => recall : 0.5178\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6514\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.4540\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4437\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.5744\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.5112\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6563\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.3413\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6571\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4893\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5547\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5219\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6943\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.1004\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6086\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5035\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5144\n",
      "Epoch [2000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5109\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6514\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.4595\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4309\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.5996\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.5088\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6612\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.3532\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6438\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.4659\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5138\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5053\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6816\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.1259\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.5981\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5086\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5143\n",
      "Epoch [3000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5120\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Training => Hamming Accuracy : 0.6547\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Training => Loss : 4.4611\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => hamming Accuracy : 0.6552\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => f1 : 0.4136\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => precision : 0.4471\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [8] Validation => recall : 0.4931\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Training => Hamming Accuracy : 0.6682\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Training => Loss : 4.3117\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => f1 : 0.5250\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => precision : 0.5970\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [25] Validation => recall : 0.5451\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Training => Hamming Accuracy : 0.6916\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Training => Loss : 4.1210\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => hamming Accuracy : 0.6248\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => f1 : 0.5246\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => precision : 0.5391\n",
      "Epoch [5000], lr [0.005], activation [tanh], size [64] Validation => recall : 0.5296\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6524\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4630\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4382\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.5582\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5083\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6692\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.3390\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6552\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.5039\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5587\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5279\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6804\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.1851\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6152\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.4889\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5062\n",
      "Epoch [2000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5040\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6527\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4522\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6733\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4456\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.6045\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5140\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6678\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.3626\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6457\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.5020\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.5437\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.5230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6702\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.2713\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.6190\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.5315\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.5407\n",
      "Epoch [3000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.5335\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Training => Hamming Accuracy : 0.6516\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Training => Loss : 4.4513\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => f1 : 0.4247\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => precision : 0.5619\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [8] Validation => recall : 0.5052\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Training => Hamming Accuracy : 0.6635\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Training => Loss : 4.3496\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => hamming Accuracy : 0.6238\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => f1 : 0.4561\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => precision : 0.4811\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [25] Validation => recall : 0.4912\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Training => Hamming Accuracy : 0.6922\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Training => Loss : 4.1067\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => hamming Accuracy : 0.5886\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => f1 : 0.4833\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => precision : 0.4892\n",
      "Epoch [5000], lr [0.007], activation [tanh], size [64] Validation => recall : 0.4916\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa0758122b0414eb95cd54cc3aedd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▅▂▇▆▁█▆▂▇▆▃▇▆▁█▆▁█▆▂█▆▂█▅▂█▆▃█▆▅█▆▂</td></tr><tr><td>Validation accuracy</td><td>▇▄▃▇▆▅█▇▄█▆▆█▇▃█▇▄▇▆▃▇▅▂▆▇▄▇▆▃█▅▃▇▄▁</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.1067</td></tr><tr><td>Validation accuracy</td><td>0.58857</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-forest-14</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/vf6ol36z' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/vf6ol36z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_201239-vf6ol36z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_210611-rrkibgvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/rrkibgvp' target=\"_blank\">ethereal-wind-15</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/rrkibgvp' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/rrkibgvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6537\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.4475\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4267\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.5543\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5053\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6627\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.3446\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6724\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4987\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.5966\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.5333\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6720\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.2737\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4775\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5649\n",
      "Epoch [2000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5202\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6545\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.4343\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4594\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.6076\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5191\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6641\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.3397\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6629\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4842\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.5653\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.5225\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6773\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.2305\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.4940\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5740\n",
      "Epoch [3000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5276\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6569\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Training => Loss : 4.4244\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => f1 : 0.4391\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => precision : 0.5723\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [8] Validation => recall : 0.5097\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6659\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Training => Loss : 4.3404\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => f1 : 0.4838\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => precision : 0.5920\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [25] Validation => recall : 0.5266\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6786\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Training => Loss : 4.2148\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => f1 : 0.5219\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => precision : 0.5827\n",
      "Epoch [5000], lr [0.001], activation [sigmoid], size [64] Validation => recall : 0.5409\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6571\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.4164\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6790\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4903\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.6238\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5331\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6608\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.3740\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4797\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.5777\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.5231\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6657\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.2848\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6552\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.4688\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5375\n",
      "Epoch [2000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5124\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6510\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.4306\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4324\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.5776\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5082\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6686\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.3237\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6648\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4940\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.5740\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.5276\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6780\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.2099\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6400\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.5011\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5363\n",
      "Epoch [3000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6533\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Training => Loss : 4.4428\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => f1 : 0.4344\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => precision : 0.5697\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [8] Validation => recall : 0.5082\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6637\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Training => Loss : 4.3530\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6600\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => f1 : 0.4842\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => precision : 0.5582\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [25] Validation => recall : 0.5211\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6802\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Training => Loss : 4.1950\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6419\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => f1 : 0.5051\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => precision : 0.5410\n",
      "Epoch [5000], lr [0.002], activation [sigmoid], size [64] Validation => recall : 0.5232\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6549\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.4682\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4276\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.5753\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5067\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6600\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.3583\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6657\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4710\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.5681\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5187\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6753\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.2139\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6410\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4946\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.5330\n",
      "Epoch [2000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.5173\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6559\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.4590\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6629\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4266\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.5135\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5018\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6596\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.4003\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6571\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4441\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.5185\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.5042\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6663\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.3108\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6552\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4255\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.4813\n",
      "Epoch [3000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.4968\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6569\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Training => Loss : 4.4821\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6714\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => f1 : 0.4255\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => precision : 0.5872\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [8] Validation => recall : 0.5067\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6604\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Training => Loss : 4.4009\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6552\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => f1 : 0.4185\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => precision : 0.4631\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [25] Validation => recall : 0.4946\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6639\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Training => Loss : 4.3794\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6533\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => f1 : 0.4314\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => precision : 0.4882\n",
      "Epoch [5000], lr [0.005], activation [sigmoid], size [64] Validation => recall : 0.4976\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6537\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.4720\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6695\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4247\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.5619\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5052\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6533\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4342\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6638\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4386\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.5371\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5062\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6692\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.3509\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6400\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4584\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.5007\n",
      "Epoch [2000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6498\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.4944\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4159\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.5124\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5009\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6571\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4243\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6552\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4796\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.5451\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.5168\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6586\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.3169\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6105\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.4753\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.4913\n",
      "Epoch [3000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.4946\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Training => Hamming Accuracy : 0.6527\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Training => Loss : 4.4722\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => f1 : 0.4259\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => precision : 0.5366\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [8] Validation => recall : 0.5039\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Training => Hamming Accuracy : 0.6559\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Training => Loss : 4.4771\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => hamming Accuracy : 0.6486\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => f1 : 0.4132\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => precision : 0.4332\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [25] Validation => recall : 0.4889\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Training => Hamming Accuracy : 0.6849\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Training => Loss : 4.1157\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => hamming Accuracy : 0.6286\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => f1 : 0.5284\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => precision : 0.5441\n",
      "Epoch [5000], lr [0.007], activation [sigmoid], size [64] Validation => recall : 0.5332\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▇▅▄▇▅▃▇▅▃▇▆▄▇▅▃▇▅▂█▅▃▇▆▅█▆▆█▇▅█▇▅██▁</td></tr><tr><td>Validation accuracy</td><td>▇▇▆█▆▇▇▇▇█▇▆▇▇▄▇▆▄▇▇▄▆▆▆▇▆▅▇▆▄▇▆▁▇▅▃</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.11565</td></tr><tr><td>Validation accuracy</td><td>0.62857</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-wind-15</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/rrkibgvp' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/rrkibgvp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_210611-rrkibgvp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab2171d2e1a404c9fb4cb04b8f3a203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168622222935989, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231021_220248-jmuw1ea5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/jmuw1ea5' target=\"_blank\">fearless-universe-16</a></strong> to <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/jmuw1ea5' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/jmuw1ea5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6543\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Training => Loss : 4.4623\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6619\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.4262\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5078\n",
      "Epoch [2000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5010\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.6590\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Training => Loss : 4.4384\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.6543\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.4773\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5416\n",
      "Epoch [2000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5154\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6563\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Training => Loss : 16.6641\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.6267\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.4544\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => precision : 0.4815\n",
      "Epoch [2000], lr [0.001], activation [relu], size [64] Validation => recall : 0.4918\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6539\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Training => Loss : 4.4587\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.4455\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5690\n",
      "Epoch [3000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.6529\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Training => Loss : 4.5406\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.6505\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.5291\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5643\n",
      "Epoch [3000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5399\n",
      "TRAINING\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6424\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Training => Loss : 23.4941\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.6133\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.5291\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => precision : 0.5363\n",
      "Epoch [3000], lr [0.001], activation [relu], size [64] Validation => recall : 0.5307\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Training => Hamming Accuracy : 0.6351\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Training => Loss : 16.9798\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => hamming Accuracy : 0.6590\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => f1 : 0.4765\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => precision : 0.5517\n",
      "Epoch [5000], lr [0.001], activation [relu], size [8] Validation => recall : 0.5174\n",
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Training => Hamming Accuracy : 0.6124\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Training => Loss : 26.2170\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => hamming Accuracy : 0.5886\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => f1 : 0.5162\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => precision : 0.5181\n",
      "Epoch [5000], lr [0.001], activation [relu], size [25] Validation => recall : 0.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Training => Hamming Accuracy : 0.6702\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Training => Loss : 20.0318\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => hamming Accuracy : 0.6133\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => f1 : 0.5123\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => precision : 0.5236\n",
      "Epoch [5000], lr [0.001], activation [relu], size [64] Validation => recall : 0.5182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6376\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Training => Loss : 32.1854\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6362\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4901\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5243\n",
      "Epoch [2000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6194\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Training => Loss : 40.9980\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6162\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.5099\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => precision : 0.5234\n",
      "Epoch [2000], lr [0.002], activation [relu], size [25] Validation => recall : 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6384\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Training => Loss : 39.1707\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6086\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.5079\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5179\n",
      "Epoch [2000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6469\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Training => Loss : 36.4304\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6524\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4578\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => precision : 0.5215\n",
      "Epoch [3000], lr [0.002], activation [relu], size [8] Validation => recall : 0.5065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6406\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Training => Loss : 33.9410\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6486\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.4439\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => precision : 0.4973\n",
      "Epoch [3000], lr [0.002], activation [relu], size [25] Validation => recall : 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6551\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Training => Loss : 38.8381\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.6010\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.5136\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5194\n",
      "Epoch [3000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Training => Hamming Accuracy : 0.6529\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Training => Loss : 36.0329\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => f1 : 0.4027\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => precision : 0.4065\n",
      "Epoch [5000], lr [0.002], activation [relu], size [8] Validation => recall : 0.4972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Training => Hamming Accuracy : 0.6386\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Training => Loss : 41.3430\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => hamming Accuracy : 0.6076\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => f1 : 0.5018\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => precision : 0.5126\n",
      "Epoch [5000], lr [0.002], activation [relu], size [25] Validation => recall : 0.5095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Training => Hamming Accuracy : 0.6471\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Training => Loss : 43.3226\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => hamming Accuracy : 0.5952\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => f1 : 0.5228\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => precision : 0.5253\n",
      "Epoch [5000], lr [0.002], activation [relu], size [64] Validation => recall : 0.5232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6190\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Training => Loss : 42.8639\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6248\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4600\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => precision : 0.4861\n",
      "Epoch [2000], lr [0.005], activation [relu], size [8] Validation => recall : 0.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6231\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Training => Loss : 39.0943\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6371\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4979\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5310\n",
      "Epoch [2000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6082\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Training => Loss : 47.5011\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.5819\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.5088\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5103\n",
      "Epoch [2000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6522\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Training => Loss : 31.5699\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6733\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4160\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => precision : 0.6494\n",
      "Epoch [3000], lr [0.005], activation [relu], size [8] Validation => recall : 0.5051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6371\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Training => Loss : 38.4802\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6505\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4365\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => precision : 0.4904\n",
      "Epoch [3000], lr [0.005], activation [relu], size [25] Validation => recall : 0.4977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6278\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Training => Loss : 45.7002\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.5705\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4997\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => precision : 0.5003\n",
      "Epoch [3000], lr [0.005], activation [relu], size [64] Validation => recall : 0.5003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Training => Hamming Accuracy : 0.6337\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Training => Loss : 39.7064\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => hamming Accuracy : 0.6476\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => f1 : 0.4287\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => precision : 0.4705\n",
      "Epoch [5000], lr [0.005], activation [relu], size [8] Validation => recall : 0.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Training => Hamming Accuracy : 0.6598\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Training => Loss : 32.3670\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => hamming Accuracy : 0.6457\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => f1 : 0.4523\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => precision : 0.5026\n",
      "Epoch [5000], lr [0.005], activation [relu], size [25] Validation => recall : 0.5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Training => Hamming Accuracy : 0.6600\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Training => Loss : 33.4575\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => hamming Accuracy : 0.6667\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => f1 : 0.4081\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => precision : 0.4718\n",
      "Epoch [5000], lr [0.005], activation [relu], size [64] Validation => recall : 0.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6355\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Training => Loss : 40.7466\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6505\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4344\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => precision : 0.4868\n",
      "Epoch [2000], lr [0.007], activation [relu], size [8] Validation => recall : 0.4970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6361\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Training => Loss : 39.7797\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6476\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4474\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5001\n",
      "Epoch [2000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6596\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Training => Loss : 34.5129\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6676\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4137\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5145\n",
      "Epoch [2000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.6520\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Training => Loss : 36.2928\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.6686\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4007\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => precision : 0.3352\n",
      "Epoch [3000], lr [0.007], activation [relu], size [8] Validation => recall : 0.4979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6255\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Training => Loss : 39.6483\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6505\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4947\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5460\n",
      "Epoch [3000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6561\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Loss : 38.9815\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6181\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4971\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5147\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Training => Hamming Accuracy : 0.5949\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Training => Loss : 48.1597\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => hamming Accuracy : 0.5800\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => f1 : 0.4839\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => precision : 0.4875\n",
      "Epoch [5000], lr [0.007], activation [relu], size [8] Validation => recall : 0.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Training => Hamming Accuracy : 0.6220\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Training => Loss : 42.6682\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => hamming Accuracy : 0.6210\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => f1 : 0.4952\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => precision : 0.5151\n",
      "Epoch [5000], lr [0.007], activation [relu], size [25] Validation => recall : 0.5098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/v9mn77wx1qlfmtfw3d4vz3k80000gn/T/ipykernel_75312/71239260.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6388\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Training => Loss : 38.7341\n",
      "VALIDATION\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6248\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4785\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5041\n",
      "Epoch [5000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5023\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▁▁▃▁▁▄▃▄▃▅▇▇▆▆▇▆▇▇▇▇█▅▆█▇▅▆▇▇▆▆▇▇█▇▆</td></tr><tr><td>Validation accuracy</td><td>▇▇▅█▆▄▇▂▄▅▄▄▇▆▃█▄▃▅▆▂█▆▁▆▆█▆▆██▆▄▂▄▅</td></tr><tr><td>epochs</td><td>▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███▁▁▁▃▃▃███</td></tr><tr><td>layer_size</td><td>▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█▁▃█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▆▆▆▆▆▆▆▆▆█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>38.73414</td></tr><tr><td>Validation accuracy</td><td>0.62476</td></tr><tr><td>epochs</td><td>5000</td></tr><tr><td>layer_size</td><td>64</td></tr><tr><td>learning_rate</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-universe-16</strong> at: <a href='https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/jmuw1ea5' target=\"_blank\">https://wandb.ai/smai-khushi/Multilabel%20Classification%20Perceptron/runs/jmuw1ea5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_220248-jmuw1ea5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [0.001,0.002,0.005,0.007]\n",
    "epochs = [2000,3000,5000]\n",
    "layer_sizes = [[8],[25],[64]]\n",
    "activations =  [[tanh],[sigmoid],[relu]]\n",
    "activation_names = ['tanh','sigmoid','relu']\n",
    "\n",
    "table = []\n",
    "#  sgd\n",
    "for idx,activation in enumerate(activations):\n",
    "    wandb.init(project = \"Multilabel Classification Perceptron\")\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for size in layer_sizes:\n",
    "                classifier = MLP(input_size,output_size,num_layers,size,activation,'sgd',lr)\n",
    "                size = size[0]\n",
    "                #       Training\n",
    "                classifier.training(x_train,y_train,epoch)\n",
    "        #       Training metrics\n",
    "                out = classifier.forward(x_train)\n",
    "                predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "                hamming_train = hamming_score(y_train,predicted)\n",
    "                loss_train = Xent(y_train,out)\n",
    "                print(\"TRAINING\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Hamming Accuracy : {hamming_train:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "        #       Validation\n",
    "                out = classifier.forward(x_val)\n",
    "                predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "                hamming_val = hamming_score(y_val,predicted)\n",
    "                loss_val = Xent(y_val,out)\n",
    "                f1 = f1_score_hamming(y_val,predicted)\n",
    "                precision = precision_hamming(y_val,predicted)\n",
    "                recall = recall_hamming(y_val,predicted)\n",
    "                print(\"VALIDATION\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => hamming Accuracy : {hamming_val:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "                print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": epoch,\n",
    "                    \"layer_size\": size,\n",
    "                    \"Validation accuracy\": hamming_val,\n",
    "                    \"Training loss\": loss_train,\n",
    "                })\n",
    "                entry = [lr,epoch,idx,size,hamming_val,f1,precision,recall]\n",
    "                table.append(entry)\n",
    "    wandb.finish()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b11a15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sgd = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e659a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Activation</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.445476</td>\n",
       "      <td>0.569048</td>\n",
       "      <td>0.511193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.485380</td>\n",
       "      <td>0.517683</td>\n",
       "      <td>0.509374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.610476</td>\n",
       "      <td>0.487086</td>\n",
       "      <td>0.501836</td>\n",
       "      <td>0.501233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.451602</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>0.512704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.658095</td>\n",
       "      <td>0.505711</td>\n",
       "      <td>0.564523</td>\n",
       "      <td>0.530034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.494697</td>\n",
       "      <td>0.545974</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.618095</td>\n",
       "      <td>0.497119</td>\n",
       "      <td>0.514688</td>\n",
       "      <td>0.509867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.483947</td>\n",
       "      <td>0.487469</td>\n",
       "      <td>0.489639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.620952</td>\n",
       "      <td>0.495244</td>\n",
       "      <td>0.515115</td>\n",
       "      <td>0.509775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.478550</td>\n",
       "      <td>0.504078</td>\n",
       "      <td>0.502251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR  Epochs  Activation  layer size   Hamming        F1  Precision  \\\n",
       "0    0.001    2000           0           8  0.668571  0.445476   0.569048   \n",
       "1    0.001    2000           0          25  0.633333  0.485380   0.517683   \n",
       "2    0.001    2000           0          64  0.610476  0.487086   0.501836   \n",
       "3    0.001    3000           0           8  0.667619  0.451602   0.566906   \n",
       "4    0.001    3000           0          25  0.658095  0.505711   0.564523   \n",
       "..     ...     ...         ...         ...       ...       ...        ...   \n",
       "103  0.007    3000           2          25  0.650476  0.494697   0.545974   \n",
       "104  0.007    3000           2          64  0.618095  0.497119   0.514688   \n",
       "105  0.007    5000           2           8  0.580000  0.483947   0.487469   \n",
       "106  0.007    5000           2          25  0.620952  0.495244   0.515115   \n",
       "107  0.007    5000           2          64  0.624762  0.478550   0.504078   \n",
       "\n",
       "       Recall  \n",
       "0    0.511193  \n",
       "1    0.509374  \n",
       "2    0.501233  \n",
       "3    0.512704  \n",
       "4    0.530034  \n",
       "..        ...  \n",
       "103  0.521400  \n",
       "104  0.509867  \n",
       "105  0.489639  \n",
       "106  0.509775  \n",
       "107  0.502251  \n",
       "\n",
       "[108 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(table_sgd,columns = ['LR','Epochs','Activation','layer size','Hamming','F1','Precision','Recall'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ee44862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd sigmoid Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.679048</td>\n",
       "      <td>0.490261</td>\n",
       "      <td>0.623799</td>\n",
       "      <td>0.533056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.459395</td>\n",
       "      <td>0.607561</td>\n",
       "      <td>0.519149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.498734</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.533272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.425484</td>\n",
       "      <td>0.587209</td>\n",
       "      <td>0.506660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.483835</td>\n",
       "      <td>0.592025</td>\n",
       "      <td>0.526642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.427553</td>\n",
       "      <td>0.575316</td>\n",
       "      <td>0.506691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670476</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.577561</td>\n",
       "      <td>0.508171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.561893</td>\n",
       "      <td>0.505242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.439091</td>\n",
       "      <td>0.572292</td>\n",
       "      <td>0.509682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.569716</td>\n",
       "      <td>0.508202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.426712</td>\n",
       "      <td>0.554295</td>\n",
       "      <td>0.505273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.479725</td>\n",
       "      <td>0.577675</td>\n",
       "      <td>0.523065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.425871</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.503854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.415884</td>\n",
       "      <td>0.512385</td>\n",
       "      <td>0.500894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.665714</td>\n",
       "      <td>0.471030</td>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.518686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.573987</td>\n",
       "      <td>0.527598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.521878</td>\n",
       "      <td>0.582670</td>\n",
       "      <td>0.540919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.573987</td>\n",
       "      <td>0.527598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.477495</td>\n",
       "      <td>0.564894</td>\n",
       "      <td>0.520228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.438643</td>\n",
       "      <td>0.537129</td>\n",
       "      <td>0.506167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.426587</td>\n",
       "      <td>0.513533</td>\n",
       "      <td>0.501758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.484163</td>\n",
       "      <td>0.565259</td>\n",
       "      <td>0.522479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.484197</td>\n",
       "      <td>0.558206</td>\n",
       "      <td>0.521092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.444118</td>\n",
       "      <td>0.518502</td>\n",
       "      <td>0.504163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.468849</td>\n",
       "      <td>0.537471</td>\n",
       "      <td>0.512365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.545054</td>\n",
       "      <td>0.516805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.425487</td>\n",
       "      <td>0.481327</td>\n",
       "      <td>0.496824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.418452</td>\n",
       "      <td>0.463054</td>\n",
       "      <td>0.494604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.431389</td>\n",
       "      <td>0.488226</td>\n",
       "      <td>0.497626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.648571</td>\n",
       "      <td>0.413199</td>\n",
       "      <td>0.433168</td>\n",
       "      <td>0.488899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.641905</td>\n",
       "      <td>0.505137</td>\n",
       "      <td>0.541041</td>\n",
       "      <td>0.523158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.640952</td>\n",
       "      <td>0.494622</td>\n",
       "      <td>0.532967</td>\n",
       "      <td>0.517268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.458405</td>\n",
       "      <td>0.500730</td>\n",
       "      <td>0.500278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.501121</td>\n",
       "      <td>0.536299</td>\n",
       "      <td>0.520259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.528358</td>\n",
       "      <td>0.544089</td>\n",
       "      <td>0.533210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.610476</td>\n",
       "      <td>0.475264</td>\n",
       "      <td>0.491259</td>\n",
       "      <td>0.494573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "45  0.002    2000           8  0.679048  0.490261   0.623799  0.533056\n",
       "39  0.001    3000           8  0.674286  0.459395   0.607561  0.519149\n",
       "37  0.001    2000          25  0.672381  0.498734   0.596591  0.533272\n",
       "60  0.005    5000           8  0.671429  0.425484   0.587209  0.506660\n",
       "43  0.001    5000          25  0.671429  0.483835   0.592025  0.526642\n",
       "54  0.005    2000           8  0.670476  0.427553   0.575316  0.506691\n",
       "48  0.002    3000           8  0.670476  0.432432   0.577561  0.508171\n",
       "63  0.007    2000           8  0.669524  0.424658   0.561893  0.505242\n",
       "42  0.001    5000           8  0.669524  0.439091   0.572292  0.509682\n",
       "51  0.002    5000           8  0.669524  0.434389   0.569716  0.508202\n",
       "36  0.001    2000           8  0.668571  0.426712   0.554295  0.505273\n",
       "46  0.002    2000          25  0.667619  0.479725   0.577675  0.523065\n",
       "69  0.007    5000           8  0.666667  0.425871   0.536585  0.503854\n",
       "66  0.007    3000           8  0.666667  0.415884   0.512385  0.500894\n",
       "55  0.005    2000          25  0.665714  0.471030   0.568063  0.518686\n",
       "49  0.002    3000          25  0.664762  0.493976   0.573987  0.527598\n",
       "44  0.001    5000          64  0.664762  0.521878   0.582670  0.540919\n",
       "41  0.001    3000          64  0.664762  0.493976   0.573987  0.527598\n",
       "38  0.001    2000          64  0.663810  0.477495   0.564894  0.520228\n",
       "64  0.007    2000          25  0.663810  0.438643   0.537129  0.506167\n",
       "57  0.005    3000           8  0.662857  0.426587   0.513533  0.501758\n",
       "40  0.001    3000          25  0.662857  0.484163   0.565259  0.522479\n",
       "52  0.002    5000          25  0.660000  0.484197   0.558206  0.521092\n",
       "58  0.005    3000          25  0.657143  0.444118   0.518502  0.504163\n",
       "47  0.002    2000          64  0.655238  0.468849   0.537471  0.512365\n",
       "67  0.007    3000          25  0.655238  0.479600   0.545054  0.516805\n",
       "59  0.005    3000          64  0.655238  0.425487   0.481327  0.496824\n",
       "61  0.005    5000          25  0.655238  0.418452   0.463054  0.494604\n",
       "62  0.005    5000          64  0.653333  0.431389   0.488226  0.497626\n",
       "70  0.007    5000          25  0.648571  0.413199   0.433168  0.488899\n",
       "53  0.002    5000          64  0.641905  0.505137   0.541041  0.523158\n",
       "56  0.005    2000          64  0.640952  0.494622   0.532967  0.517268\n",
       "65  0.007    2000          64  0.640000  0.458405   0.500730  0.500278\n",
       "50  0.002    3000          64  0.640000  0.501121   0.536299  0.520259\n",
       "71  0.007    5000          64  0.628571  0.528358   0.544089  0.533210\n",
       "68  0.007    3000          64  0.610476  0.475264   0.491259  0.494573"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"sgd sigmoid Metrics\")\n",
    "df_sgd_sigmoid = table[table['Activation'] == 1].drop('Activation',axis = 1)\n",
    "df_sgd_sigmoid = df_sgd_sigmoid.sort_values(by='Hamming', ascending=False)\n",
    "df_sgd_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64e58ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd tanh Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.679048</td>\n",
       "      <td>0.461916</td>\n",
       "      <td>0.643541</td>\n",
       "      <td>0.522695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.465755</td>\n",
       "      <td>0.605965</td>\n",
       "      <td>0.521369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.446025</td>\n",
       "      <td>0.613251</td>\n",
       "      <td>0.514709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.445557</td>\n",
       "      <td>0.604525</td>\n",
       "      <td>0.513999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.488670</td>\n",
       "      <td>0.599482</td>\n",
       "      <td>0.529541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.430862</td>\n",
       "      <td>0.599611</td>\n",
       "      <td>0.508850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.524987</td>\n",
       "      <td>0.596983</td>\n",
       "      <td>0.545143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.443689</td>\n",
       "      <td>0.574375</td>\n",
       "      <td>0.511163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.669524</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.561893</td>\n",
       "      <td>0.505242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.445476</td>\n",
       "      <td>0.569048</td>\n",
       "      <td>0.511193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.451602</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>0.512704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.438183</td>\n",
       "      <td>0.558187</td>\n",
       "      <td>0.508264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>0.569106</td>\n",
       "      <td>0.528430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.660952</td>\n",
       "      <td>0.472120</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.516620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.482448</td>\n",
       "      <td>0.557292</td>\n",
       "      <td>0.520352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.658095</td>\n",
       "      <td>0.505711</td>\n",
       "      <td>0.564523</td>\n",
       "      <td>0.530034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.489255</td>\n",
       "      <td>0.554669</td>\n",
       "      <td>0.521924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.503851</td>\n",
       "      <td>0.558703</td>\n",
       "      <td>0.527906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.413629</td>\n",
       "      <td>0.447054</td>\n",
       "      <td>0.493124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.648571</td>\n",
       "      <td>0.465051</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.507401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.537696</td>\n",
       "      <td>0.565341</td>\n",
       "      <td>0.544434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.502042</td>\n",
       "      <td>0.543712</td>\n",
       "      <td>0.523034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.643810</td>\n",
       "      <td>0.465906</td>\n",
       "      <td>0.513781</td>\n",
       "      <td>0.505335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.643810</td>\n",
       "      <td>0.550695</td>\n",
       "      <td>0.568608</td>\n",
       "      <td>0.552698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.485380</td>\n",
       "      <td>0.517683</td>\n",
       "      <td>0.509374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.630476</td>\n",
       "      <td>0.516382</td>\n",
       "      <td>0.537562</td>\n",
       "      <td>0.525748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.524579</td>\n",
       "      <td>0.539082</td>\n",
       "      <td>0.529633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.456145</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.491181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.507059</td>\n",
       "      <td>0.526104</td>\n",
       "      <td>0.517823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.516935</td>\n",
       "      <td>0.530805</td>\n",
       "      <td>0.523127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.531451</td>\n",
       "      <td>0.540651</td>\n",
       "      <td>0.533518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.615238</td>\n",
       "      <td>0.488926</td>\n",
       "      <td>0.506192</td>\n",
       "      <td>0.504039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.610476</td>\n",
       "      <td>0.487086</td>\n",
       "      <td>0.501836</td>\n",
       "      <td>0.501233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.608571</td>\n",
       "      <td>0.503515</td>\n",
       "      <td>0.514444</td>\n",
       "      <td>0.510916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.598095</td>\n",
       "      <td>0.508647</td>\n",
       "      <td>0.514315</td>\n",
       "      <td>0.511995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.483268</td>\n",
       "      <td>0.489177</td>\n",
       "      <td>0.491582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "15  0.002    5000           8  0.679048  0.461916   0.643541  0.522695\n",
       "9   0.002    2000           8  0.674286  0.465755   0.605965  0.521369\n",
       "6   0.001    5000           8  0.674286  0.446025   0.613251  0.514709\n",
       "30  0.007    3000           8  0.673333  0.445557   0.604525  0.513999\n",
       "12  0.002    3000           8  0.673333  0.488670   0.599482  0.529541\n",
       "21  0.005    3000           8  0.672381  0.430862   0.599611  0.508850\n",
       "25  0.005    5000          25  0.671429  0.524987   0.596983  0.545143\n",
       "18  0.005    2000           8  0.669524  0.443689   0.574375  0.511163\n",
       "33  0.007    5000           8  0.669524  0.424658   0.561893  0.505242\n",
       "0   0.001    2000           8  0.668571  0.445476   0.569048  0.511193\n",
       "3   0.001    3000           8  0.667619  0.451602   0.566906  0.512704\n",
       "27  0.007    2000           8  0.667619  0.438183   0.558187  0.508264\n",
       "16  0.002    5000          25  0.661905  0.498789   0.569106  0.528430\n",
       "13  0.002    3000          25  0.660952  0.472120   0.553957  0.516620\n",
       "7   0.001    5000          25  0.660000  0.482448   0.557292  0.520352\n",
       "4   0.001    3000          25  0.658095  0.505711   0.564523  0.530034\n",
       "19  0.005    2000          25  0.657143  0.489255   0.554669  0.521924\n",
       "28  0.007    2000          25  0.655238  0.503851   0.558703  0.527906\n",
       "24  0.005    5000           8  0.655238  0.413629   0.447054  0.493124\n",
       "10  0.002    2000          25  0.648571  0.465051   0.520833  0.507401\n",
       "11  0.002    2000          64  0.647619  0.537696   0.565341  0.544434\n",
       "31  0.007    3000          25  0.645714  0.502042   0.543712  0.523034\n",
       "22  0.005    3000          25  0.643810  0.465906   0.513781  0.505335\n",
       "5   0.001    3000          64  0.643810  0.550695   0.568608  0.552698\n",
       "1   0.001    2000          25  0.633333  0.485380   0.517683  0.509374\n",
       "8   0.001    5000          64  0.630476  0.516382   0.537562  0.525748\n",
       "26  0.005    5000          64  0.624762  0.524579   0.539082  0.529633\n",
       "34  0.007    5000          25  0.623810  0.456145   0.481054  0.491181\n",
       "17  0.002    5000          64  0.623810  0.507059   0.526104  0.517823\n",
       "14  0.002    3000          64  0.620000  0.516935   0.530805  0.523127\n",
       "32  0.007    3000          64  0.619048  0.531451   0.540651  0.533518\n",
       "29  0.007    2000          64  0.615238  0.488926   0.506192  0.504039\n",
       "2   0.001    2000          64  0.610476  0.487086   0.501836  0.501233\n",
       "20  0.005    2000          64  0.608571  0.503515   0.514444  0.510916\n",
       "23  0.005    3000          64  0.598095  0.508647   0.514315  0.511995\n",
       "35  0.007    5000          64  0.588571  0.483268   0.489177  0.491582"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"sgd tanh Metrics\")\n",
    "df_sgd_tanh = table[table['Activation'] == 0].drop('Activation',axis = 1)\n",
    "df_sgd_tanh = df_sgd_tanh.sort_values(by='Hamming', ascending=False)\n",
    "df_sgd_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f2c91c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd relu Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>layer size</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.415996</td>\n",
       "      <td>0.649352</td>\n",
       "      <td>0.505119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.445476</td>\n",
       "      <td>0.569048</td>\n",
       "      <td>0.511193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.400685</td>\n",
       "      <td>0.335244</td>\n",
       "      <td>0.497872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.413698</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>0.500863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.402726</td>\n",
       "      <td>0.406520</td>\n",
       "      <td>0.497194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.408083</td>\n",
       "      <td>0.471782</td>\n",
       "      <td>0.498674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.426160</td>\n",
       "      <td>0.507828</td>\n",
       "      <td>0.501048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.659048</td>\n",
       "      <td>0.476535</td>\n",
       "      <td>0.551661</td>\n",
       "      <td>0.517422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.654286</td>\n",
       "      <td>0.477289</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.515356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.457813</td>\n",
       "      <td>0.521480</td>\n",
       "      <td>0.506537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.486796</td>\n",
       "      <td>0.496978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.436533</td>\n",
       "      <td>0.490354</td>\n",
       "      <td>0.497718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.494697</td>\n",
       "      <td>0.545974</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.650476</td>\n",
       "      <td>0.529149</td>\n",
       "      <td>0.564265</td>\n",
       "      <td>0.539901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.648571</td>\n",
       "      <td>0.443903</td>\n",
       "      <td>0.497304</td>\n",
       "      <td>0.499260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.428676</td>\n",
       "      <td>0.470534</td>\n",
       "      <td>0.493370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.447428</td>\n",
       "      <td>0.500105</td>\n",
       "      <td>0.500031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.005</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.452269</td>\n",
       "      <td>0.502580</td>\n",
       "      <td>0.500833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>0.497861</td>\n",
       "      <td>0.530990</td>\n",
       "      <td>0.517391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.636190</td>\n",
       "      <td>0.490120</td>\n",
       "      <td>0.524344</td>\n",
       "      <td>0.512982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.454401</td>\n",
       "      <td>0.481516</td>\n",
       "      <td>0.491829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.459992</td>\n",
       "      <td>0.486054</td>\n",
       "      <td>0.493370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.478550</td>\n",
       "      <td>0.504078</td>\n",
       "      <td>0.502251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.620952</td>\n",
       "      <td>0.495244</td>\n",
       "      <td>0.515115</td>\n",
       "      <td>0.509775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.618095</td>\n",
       "      <td>0.497119</td>\n",
       "      <td>0.514688</td>\n",
       "      <td>0.509867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.616190</td>\n",
       "      <td>0.509891</td>\n",
       "      <td>0.523398</td>\n",
       "      <td>0.517330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.512251</td>\n",
       "      <td>0.523646</td>\n",
       "      <td>0.518162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.529140</td>\n",
       "      <td>0.536311</td>\n",
       "      <td>0.530743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.608571</td>\n",
       "      <td>0.507855</td>\n",
       "      <td>0.517895</td>\n",
       "      <td>0.513876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.607619</td>\n",
       "      <td>0.501753</td>\n",
       "      <td>0.512568</td>\n",
       "      <td>0.509467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.600952</td>\n",
       "      <td>0.513585</td>\n",
       "      <td>0.519352</td>\n",
       "      <td>0.516343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.002</td>\n",
       "      <td>5000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.522839</td>\n",
       "      <td>0.525271</td>\n",
       "      <td>0.523188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.516245</td>\n",
       "      <td>0.518136</td>\n",
       "      <td>0.516744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581905</td>\n",
       "      <td>0.508831</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>0.509559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.007</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.483947</td>\n",
       "      <td>0.487469</td>\n",
       "      <td>0.489639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.570476</td>\n",
       "      <td>0.499670</td>\n",
       "      <td>0.500327</td>\n",
       "      <td>0.500308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR  Epochs  layer size   Hamming        F1  Precision    Recall\n",
       "93   0.005    3000           8  0.673333  0.415996   0.649352  0.505119\n",
       "75   0.001    3000           8  0.668571  0.445476   0.569048  0.511193\n",
       "102  0.007    3000           8  0.668571  0.400685   0.335244  0.497872\n",
       "101  0.007    2000          64  0.667619  0.413698   0.514479  0.500863\n",
       "87   0.002    5000           8  0.666667  0.402726   0.406520  0.497194\n",
       "98   0.005    5000          64  0.666667  0.408083   0.471782  0.498674\n",
       "72   0.001    2000           8  0.661905  0.426160   0.507828  0.501048\n",
       "78   0.001    5000           8  0.659048  0.476535   0.551661  0.517422\n",
       "73   0.001    2000          25  0.654286  0.477289   0.541563  0.515356\n",
       "84   0.002    3000           8  0.652381  0.457813   0.521480  0.506537\n",
       "99   0.007    2000           8  0.650476  0.434389   0.486796  0.496978\n",
       "94   0.005    3000          25  0.650476  0.436533   0.490354  0.497718\n",
       "103  0.007    3000          25  0.650476  0.494697   0.545974  0.521400\n",
       "76   0.001    3000          25  0.650476  0.529149   0.564265  0.539901\n",
       "85   0.002    3000          25  0.648571  0.443903   0.497304  0.499260\n",
       "96   0.005    5000           8  0.647619  0.428676   0.470534  0.493370\n",
       "100  0.007    2000          25  0.647619  0.447428   0.500105  0.500031\n",
       "97   0.005    5000          25  0.645714  0.452269   0.502580  0.500833\n",
       "91   0.005    2000          25  0.637143  0.497861   0.530990  0.517391\n",
       "81   0.002    2000           8  0.636190  0.490120   0.524344  0.512982\n",
       "74   0.001    2000          64  0.626667  0.454401   0.481516  0.491829\n",
       "90   0.005    2000           8  0.624762  0.459992   0.486054  0.493370\n",
       "107  0.007    5000          64  0.624762  0.478550   0.504078  0.502251\n",
       "106  0.007    5000          25  0.620952  0.495244   0.515115  0.509775\n",
       "104  0.007    3000          64  0.618095  0.497119   0.514688  0.509867\n",
       "82   0.002    2000          25  0.616190  0.509891   0.523398  0.517330\n",
       "80   0.001    5000          64  0.613333  0.512251   0.523646  0.518162\n",
       "77   0.001    3000          64  0.613333  0.529140   0.536311  0.530743\n",
       "83   0.002    2000          64  0.608571  0.507855   0.517895  0.513876\n",
       "88   0.002    5000          25  0.607619  0.501753   0.512568  0.509467\n",
       "86   0.002    3000          64  0.600952  0.513585   0.519352  0.516343\n",
       "89   0.002    5000          64  0.595238  0.522839   0.525271  0.523188\n",
       "79   0.001    5000          25  0.588571  0.516245   0.518136  0.516744\n",
       "92   0.005    2000          64  0.581905  0.508831   0.510333  0.509559\n",
       "105  0.007    5000           8  0.580000  0.483947   0.487469  0.489639\n",
       "95   0.005    3000          64  0.570476  0.499670   0.500327  0.500308"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"sgd relu Metrics\")\n",
    "df_sgd_relu = table[table['Activation'] == 2].drop('Activation',axis = 1)\n",
    "df_sgd_relu = df_sgd_relu.sort_values(by='Hamming', ascending=False)\n",
    "df_sgd_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1edf9a9",
   "metadata": {},
   "source": [
    "# best architecture: \n",
    "### Minibatch sigmoid with layer size 64 trained with lr = 0.007 and epochs 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7926e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Hamming Accuracy : 0.6518\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Training => Loss : 4.4513\n",
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6743\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4437\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => precision : 0.6147\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5140\n"
     ]
    }
   ],
   "source": [
    "activation = [sigmoid]\n",
    "size = 64\n",
    "lr = 0.007\n",
    "epoch = 3000\n",
    "classifier = MLP(input_size,output_size,num_layers,[size],activation,'minibatch',lr)\n",
    "#       Training\n",
    "classifier.training(x_train,y_train,epoch)\n",
    "#       Training metrics\n",
    "out = classifier.forward(x_train)\n",
    "predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "hamming_train = hamming_score(y_train,predicted)\n",
    "loss_train = Xent(y_train,out)\n",
    "print(\"TRAINING\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Hamming Accuracy : {hamming_train:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Training => Loss : {loss_train:.4f}\")\n",
    "\n",
    "\n",
    "#       Validation\n",
    "out = classifier.forward(x_val)\n",
    "predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "hamming_val = hamming_score(y_val,predicted)\n",
    "loss_val = Xent(y_val,out)\n",
    "f1 = f1_score_hamming(y_val,predicted)\n",
    "precision = precision_hamming(y_val,predicted)\n",
    "recall = recall_hamming(y_val,predicted)\n",
    "print(\"VALIDATION\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => hamming Accuracy : {hamming_val:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e42a53e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => hamming Accuracy : 0.6705\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => f1 : 0.4276\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => precision : 0.5648\n",
      "Epoch [3000], lr [0.007], activation [relu], size [64] Validation => recall : 0.5060\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "out = classifier.forward(x_test)\n",
    "predicted = np.where(out >= 0.5,1.0,0.0)\n",
    "hamming_val = hamming_score(y_test,predicted)\n",
    "loss_val = Xent(y_test,out)\n",
    "f1 = f1_score_hamming(y_test,predicted)\n",
    "precision = precision_hamming(y_test,predicted)\n",
    "recall = recall_hamming(y_test,predicted)\n",
    "print(\"VALIDATION\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => hamming Accuracy : {hamming_val:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => f1 : {f1:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => precision : {precision:.4f}\")\n",
    "print(f\"Epoch [{epoch}], lr [{lr}], activation [{activation_names[idx]}], size [{size}] Validation => recall : {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
