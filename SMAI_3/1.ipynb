{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e21e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a8a323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1138            6.3             0.510         0.13             2.3      0.076   \n",
       "1139            6.8             0.620         0.08             1.9      0.068   \n",
       "1140            6.2             0.600         0.08             2.0      0.090   \n",
       "1141            5.9             0.550         0.10             2.2      0.062   \n",
       "1142            5.9             0.645         0.12             2.0      0.075   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "\n",
       "      alcohol  quality    Id  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     1  \n",
       "2         9.8        5     2  \n",
       "3         9.8        6     3  \n",
       "4         9.4        5     4  \n",
       "...       ...      ...   ...  \n",
       "1138     11.0        6  1592  \n",
       "1139      9.5        6  1593  \n",
       "1140     10.5        5  1594  \n",
       "1141     11.2        6  1595  \n",
       "1142     10.2        5  1597  \n",
       "\n",
       "[1143 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/WineQT.csv')\n",
    "y = pd.get_dummies(df['quality'])\n",
    "categories,counts = np.unique(df['quality'],return_counts = True)\n",
    "y = np.where(y == True, 1.0,0.0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632b686",
   "metadata": {},
   "source": [
    "# 1.1 Dataset Analysis and Preprocessing\n",
    "- We have splitted the data into 70% train, 15% validation and 15% test\n",
    "- for normalising the data we have used standardscalar of the sklearn which transforms each attribute by so that the mean and variance for that feature in the data space are 0 and 1 respectively. This helps in convergence and bringing all features down to same scale so that they dont influence the gradient descent more than they should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3bff814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.311111</td>\n",
       "      <td>0.531339</td>\n",
       "      <td>0.268364</td>\n",
       "      <td>2.532152</td>\n",
       "      <td>0.086933</td>\n",
       "      <td>15.615486</td>\n",
       "      <td>45.914698</td>\n",
       "      <td>0.996730</td>\n",
       "      <td>3.311015</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>10.442111</td>\n",
       "      <td>5.657043</td>\n",
       "      <td>804.969379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.747595</td>\n",
       "      <td>0.179633</td>\n",
       "      <td>0.196686</td>\n",
       "      <td>1.355917</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>10.250486</td>\n",
       "      <td>32.782130</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.156664</td>\n",
       "      <td>0.170399</td>\n",
       "      <td>1.082196</td>\n",
       "      <td>0.805824</td>\n",
       "      <td>463.997116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1597.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "mean       8.311111          0.531339     0.268364        2.532152   0.086933   \n",
       "std        1.747595          0.179633     0.196686        1.355917   0.047267   \n",
       "min        4.600000          0.120000     0.000000        0.900000   0.012000   \n",
       "max       15.900000          1.580000     1.000000       15.500000   0.611000   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "mean            15.615486             45.914698  0.996730  3.311015   \n",
       "std             10.250486             32.782130  0.001925  0.156664   \n",
       "min              1.000000              6.000000  0.990070  2.740000   \n",
       "max             68.000000            289.000000  1.003690  4.010000   \n",
       "\n",
       "      sulphates    alcohol   quality           Id  \n",
       "mean   0.657708  10.442111  5.657043   804.969379  \n",
       "std    0.170399   1.082196  0.805824   463.997116  \n",
       "min    0.330000   8.400000  3.000000     0.000000  \n",
       "max    2.000000  14.900000  8.000000  1597.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats= (df.describe()).loc[[\"mean\",\"std\",\"min\",\"max\"]]\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4791ab08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(categories,counts)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequecy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1bc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_rest, y_train, y_rest = train_test_split(df.iloc[:,:-2], y, test_size=0.3, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_rest, y_rest, test_size=0.5, random_state=42)\n",
    "scalar = StandardScaler()\n",
    "scalar.fit(x_train.values)\n",
    "x_train = scalar.transform(x_train.values)\n",
    "x_val = scalar.transform(x_val.values)\n",
    "x_test = scalar.transform(x_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3736ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing nan values after normalisation with 0\n",
    "x_train[np.isnan(x_train)] = 0\n",
    "x_val[np.isnan(x_val)] = 0\n",
    "x_test[np.isnan(x_test)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60572652",
   "metadata": {},
   "source": [
    "# 1.2 Model Building from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda58e3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xent(y_true,y_pred):\n",
    "    eps = 1e-15\n",
    "    return -np.sum(y_true*np.log(y_pred+eps)) / len(y_true)\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,input_size,output_size,lr):\n",
    "        self.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights_and_bias = np.random.randn(output_size,input_size+1)\n",
    "        self.out = np.zeros(output_size)\n",
    "        \n",
    "    def Softmax(self,y):\n",
    "        total = np.exp(y)\n",
    "        return total / np.sum(total,axis =1,keepdims = True)\n",
    "    \n",
    "    def XentGrad(self,y_true,y_pred):\n",
    "        eps = 1e-15\n",
    "        return -(1/len(y_true))*(y_true*(1/(y_pred+eps)))\n",
    "    \n",
    "    def softmax_jacobian(self,p):\n",
    "        n = p.shape[1]\n",
    "        jacobian = np.zeros((len(p),n, n))\n",
    "        \n",
    "        for sample in range(len(p)):\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i == j:\n",
    "                        jacobian[sample,i, j] = p[sample,i] * (1 - p[sample,i])\n",
    "                    else:\n",
    "                        jacobian[sample,i, j] = -p[sample,i] * p[sample,j]\n",
    "\n",
    "        return jacobian\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = np.append(x,np.ones((x.shape[0],1)),axis=1)\n",
    "        self.input = x # (914.12)\n",
    "        y = x @ self.weights_and_bias.T\n",
    "        y_prob = self.Softmax(y) # (914,7)\n",
    "        self.out = y_prob\n",
    "        return y_prob\n",
    "    \n",
    "    def backward(self,y_true):\n",
    "        grad_y_prob = self.XentGrad(y_true,self.out) # (914,7)\n",
    "        grad_y = np.vectorize(lambda jac,grad: np.dot(jac,grad),signature='(n,n),(n)->(n)')(self.softmax_jacobian(self.out),grad_y_prob)  # (914,7)\n",
    "        grad_w_and_b = grad_y.T @  self.input # (7,12)\n",
    "        \n",
    "        self.weights_and_bias -= self.lr*grad_w_and_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a79ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhushi1703\u001b[0m (\u001b[33msmai-khushi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c9691",
   "metadata": {},
   "source": [
    "## Model training, validation and W&B logging of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c01958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9l28p6qn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-voice-9</strong> at: <a href='https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/9l28p6qn' target=\"_blank\">https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/9l28p6qn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231013_201150-9l28p6qn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9l28p6qn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7aaf94a26e43a0adca742ddef18299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168847222062241, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhythmaggarwal/Desktop/ML/SMAI_ASSIGNMENT/A3/wandb/run-20231013_201457-o5u6iik2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/o5u6iik2' target=\"_blank\">fresh-blaze-10</a></strong> to <a href='https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression' target=\"_blank\">https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/o5u6iik2' target=\"_blank\">https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/o5u6iik2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [500], lr [0.01] Training => Accuracy : 0.3425\n",
      "Epoch [500], lr [0.01] Training => Loss : 1.7755\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.01] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.08      0.17      0.11         6\n",
      "           2       0.58      0.38      0.46        81\n",
      "           3       0.44      0.43      0.43        65\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.35       172\n",
      "   macro avg       0.18      0.16      0.17       172\n",
      "weighted avg       0.44      0.35      0.39       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.01] Training => Accuracy : 0.5062\n",
      "Epoch [1000], lr [0.01] Training => Loss : 1.4479\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.01] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.51      0.59        81\n",
      "           3       0.45      0.60      0.51        65\n",
      "           4       0.36      0.29      0.32        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.49       172\n",
      "   macro avg       0.25      0.23      0.24       172\n",
      "weighted avg       0.53      0.49      0.50       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.01] Training => Accuracy : 0.5375\n",
      "Epoch [1500], lr [0.01] Training => Loss : 1.2165\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.01] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.72      0.59      0.65        81\n",
      "           3       0.53      0.62      0.57        65\n",
      "           4       0.35      0.35      0.35        17\n",
      "           5       0.25      0.33      0.29         3\n",
      "\n",
      "    accuracy                           0.55       172\n",
      "   macro avg       0.31      0.32      0.31       172\n",
      "weighted avg       0.58      0.55      0.56       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.01] Training => Accuracy : 0.5613\n",
      "Epoch [2000], lr [0.01] Training => Loss : 1.0873\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.01] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.65      0.67        81\n",
      "           3       0.51      0.62      0.56        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.58       172\n",
      "   macro avg       0.28      0.27      0.27       172\n",
      "weighted avg       0.56      0.58      0.57       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.01] Training => Accuracy : 0.5837\n",
      "Epoch [2500], lr [0.01] Training => Loss : 0.9981\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.01] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.69      0.69        81\n",
      "           3       0.50      0.55      0.53        65\n",
      "           4       0.36      0.24      0.29        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.56       172\n",
      "   macro avg       0.26      0.25      0.25       172\n",
      "weighted avg       0.55      0.56      0.55       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.060000000000000005] Training => Accuracy : 0.6038\n",
      "Epoch [500], lr [0.060000000000000005] Training => Loss : 1.0186\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.060000000000000005] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.74      0.71        81\n",
      "           3       0.57      0.62      0.59        65\n",
      "           4       0.57      0.47      0.52        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63       172\n",
      "   macro avg       0.37      0.37      0.36       172\n",
      "weighted avg       0.60      0.63      0.61       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.060000000000000005] Training => Accuracy : 0.6212\n",
      "Epoch [1000], lr [0.060000000000000005] Training => Loss : 0.9352\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.060000000000000005] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.55      0.63      0.59        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.59      0.62      0.60       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.060000000000000005] Training => Accuracy : 0.6112\n",
      "Epoch [1500], lr [0.060000000000000005] Training => Loss : 0.9246\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.060000000000000005] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.72      0.71        81\n",
      "           3       0.52      0.62      0.56        65\n",
      "           4       0.36      0.24      0.29        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.59       172\n",
      "   macro avg       0.32      0.31      0.31       172\n",
      "weighted avg       0.56      0.59      0.57       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.060000000000000005] Training => Accuracy : 0.6138\n",
      "Epoch [2000], lr [0.060000000000000005] Training => Loss : 0.9339\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.060000000000000005] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.55      0.63      0.59        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.59      0.62      0.60       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.060000000000000005] Training => Accuracy : 0.6262\n",
      "Epoch [2500], lr [0.060000000000000005] Training => Loss : 0.9189\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.060000000000000005] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.56      0.63      0.59        65\n",
      "           4       0.57      0.47      0.52        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62       172\n",
      "   macro avg       0.36      0.36      0.36       172\n",
      "weighted avg       0.59      0.62      0.61       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.11] Training => Accuracy : 0.6175\n",
      "Epoch [500], lr [0.11] Training => Loss : 0.9680\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.11] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.11] Training => Accuracy : 0.6175\n",
      "Epoch [1000], lr [0.11] Training => Loss : 0.9371\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.11] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.57      0.65      0.60        65\n",
      "           4       0.54      0.41      0.47        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63       172\n",
      "   macro avg       0.36      0.36      0.36       172\n",
      "weighted avg       0.60      0.63      0.61       172\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [1500], lr [0.11] Training => Accuracy : 0.6200\n",
      "Epoch [1500], lr [0.11] Training => Loss : 0.9170\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.11] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.56      0.62      0.59        65\n",
      "           4       0.50      0.41      0.45        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62       172\n",
      "   macro avg       0.35      0.35      0.35       172\n",
      "weighted avg       0.59      0.62      0.60       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.11] Training => Accuracy : 0.6200\n",
      "Epoch [2000], lr [0.11] Training => Loss : 0.9143\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.11] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.11] Training => Accuracy : 0.6200\n",
      "Epoch [2500], lr [0.11] Training => Loss : 0.9118\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.11] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.16] Training => Accuracy : 0.6238\n",
      "Epoch [500], lr [0.16] Training => Loss : 0.9346\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.16] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.58      0.63      0.60        65\n",
      "           4       0.53      0.47      0.50        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63       172\n",
      "   macro avg       0.36      0.37      0.36       172\n",
      "weighted avg       0.60      0.63      0.61       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.16] Training => Accuracy : 0.6175\n",
      "Epoch [1000], lr [0.16] Training => Loss : 0.9200\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.16] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.72      0.71        81\n",
      "           3       0.54      0.63      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.16] Training => Accuracy : 0.6212\n",
      "Epoch [1500], lr [0.16] Training => Loss : 0.9162\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.16] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.16] Training => Accuracy : 0.6212\n",
      "Epoch [2000], lr [0.16] Training => Loss : 0.9125\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.16] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.53      0.62      0.57        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.16] Training => Accuracy : 0.6175\n",
      "Epoch [2500], lr [0.16] Training => Loss : 0.9094\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.16] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.21000000000000002] Training => Accuracy : 0.6238\n",
      "Epoch [500], lr [0.21000000000000002] Training => Loss : 0.9346\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.21000000000000002] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.56      0.62      0.58        65\n",
      "           4       0.36      0.29      0.32        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.32      0.33      0.32       172\n",
      "weighted avg       0.57      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.21000000000000002] Training => Accuracy : 0.6188\n",
      "Epoch [1000], lr [0.21000000000000002] Training => Loss : 0.9128\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.21000000000000002] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.21000000000000002] Training => Accuracy : 0.6212\n",
      "Epoch [1500], lr [0.21000000000000002] Training => Loss : 0.9112\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.21000000000000002] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.53      0.60      0.57        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.57      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.21000000000000002] Training => Accuracy : 0.6175\n",
      "Epoch [2000], lr [0.21000000000000002] Training => Loss : 0.9092\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.21000000000000002] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.53      0.62      0.57        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.21000000000000002] Training => Accuracy : 0.6200\n",
      "Epoch [2500], lr [0.21000000000000002] Training => Loss : 0.9059\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.21000000000000002] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.53      0.62      0.57        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [500], lr [0.26] Training => Accuracy : 0.6150\n",
      "Epoch [500], lr [0.26] Training => Loss : 0.9243\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.26] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.57      0.65      0.60        65\n",
      "           4       0.54      0.41      0.47        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63       172\n",
      "   macro avg       0.30      0.30      0.30       172\n",
      "weighted avg       0.60      0.63      0.61       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.26] Training => Accuracy : 0.6238\n",
      "Epoch [1000], lr [0.26] Training => Loss : 0.9098\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.26] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.26] Training => Accuracy : 0.6188\n",
      "Epoch [1500], lr [0.26] Training => Loss : 0.9087\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.26] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.53      0.60      0.56        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.33      0.33       172\n",
      "weighted avg       0.57      0.60      0.58       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.26] Training => Accuracy : 0.6200\n",
      "Epoch [2000], lr [0.26] Training => Loss : 0.9058\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.26] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.26] Training => Accuracy : 0.6175\n",
      "Epoch [2500], lr [0.26] Training => Loss : 0.9039\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.26] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.53      0.60      0.57        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.57      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.31] Training => Accuracy : 0.6200\n",
      "Epoch [500], lr [0.31] Training => Loss : 0.9172\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.31] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.31] Training => Accuracy : 0.6175\n",
      "Epoch [1000], lr [0.31] Training => Loss : 0.9100\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.31] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.31] Training => Accuracy : 0.6188\n",
      "Epoch [1500], lr [0.31] Training => Loss : 0.9079\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.31] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.31] Training => Accuracy : 0.6175\n",
      "Epoch [2000], lr [0.31] Training => Loss : 0.9047\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.31] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.31] Training => Accuracy : 0.6175\n",
      "Epoch [2500], lr [0.31] Training => Loss : 0.9053\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.31] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.36000000000000004] Training => Accuracy : 0.6175\n",
      "Epoch [500], lr [0.36000000000000004] Training => Loss : 0.9155\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.36000000000000004] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.72      0.71        81\n",
      "           3       0.54      0.63      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.35      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.36000000000000004] Training => Accuracy : 0.6212\n",
      "Epoch [1000], lr [0.36000000000000004] Training => Loss : 0.9076\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.36000000000000004] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.53      0.60      0.57        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.57      0.60      0.59       172\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [1500], lr [0.36000000000000004] Training => Accuracy : 0.6175\n",
      "Epoch [1500], lr [0.36000000000000004] Training => Loss : 0.9084\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.36000000000000004] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.36000000000000004] Training => Accuracy : 0.6212\n",
      "Epoch [2000], lr [0.36000000000000004] Training => Loss : 0.9041\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.36000000000000004] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.36000000000000004] Training => Accuracy : 0.6188\n",
      "Epoch [2500], lr [0.36000000000000004] Training => Loss : 0.9035\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.36000000000000004] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.41000000000000003] Training => Accuracy : 0.6175\n",
      "Epoch [500], lr [0.41000000000000003] Training => Loss : 0.9133\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.41000000000000003] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.70      0.70        81\n",
      "           3       0.53      0.62      0.57        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.33      0.33       172\n",
      "weighted avg       0.57      0.60      0.58       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.41000000000000003] Training => Accuracy : 0.6188\n",
      "Epoch [1000], lr [0.41000000000000003] Training => Loss : 0.9093\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.41000000000000003] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.53      0.60      0.57        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.28      0.28      0.28       172\n",
      "weighted avg       0.57      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.41000000000000003] Training => Accuracy : 0.6175\n",
      "Epoch [1500], lr [0.41000000000000003] Training => Loss : 0.9060\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.41000000000000003] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.53      0.60      0.57        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.28      0.28      0.28       172\n",
      "weighted avg       0.57      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.41000000000000003] Training => Accuracy : 0.6175\n",
      "Epoch [2000], lr [0.41000000000000003] Training => Loss : 0.9039\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.41000000000000003] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.41000000000000003] Training => Accuracy : 0.6200\n",
      "Epoch [2500], lr [0.41000000000000003] Training => Loss : 0.9031\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.41000000000000003] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.46] Training => Accuracy : 0.6212\n",
      "Epoch [500], lr [0.46] Training => Loss : 0.9112\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.46] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.53      0.60      0.56        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.33      0.33       172\n",
      "weighted avg       0.57      0.60      0.58       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.46] Training => Accuracy : 0.6175\n",
      "Epoch [1000], lr [0.46] Training => Loss : 0.9061\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.46] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.46] Training => Accuracy : 0.6175\n",
      "Epoch [1500], lr [0.46] Training => Loss : 0.9045\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.46] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.72      0.70        81\n",
      "           3       0.53      0.62      0.57        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.60      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.46] Training => Accuracy : 0.6200\n",
      "Epoch [2000], lr [0.46] Training => Loss : 0.9032\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.46] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.55      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch [2500], lr [0.46] Training => Accuracy : 0.6175\n",
      "Epoch [2500], lr [0.46] Training => Loss : 0.9025\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.46] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [500], lr [0.51] Training => Accuracy : 0.6212\n",
      "Epoch [500], lr [0.51] Training => Loss : 0.9121\n",
      "VALIDATION\n",
      "Epoch [500], lr [0.51] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.70      0.73      0.72        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.46      0.35      0.40        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.34      0.34      0.34       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1000], lr [0.51] Training => Accuracy : 0.6200\n",
      "Epoch [1000], lr [0.51] Training => Loss : 0.9080\n",
      "VALIDATION\n",
      "Epoch [1000], lr [0.51] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [1500], lr [0.51] Training => Accuracy : 0.6175\n",
      "Epoch [1500], lr [0.51] Training => Loss : 0.9046\n",
      "VALIDATION\n",
      "Epoch [1500], lr [0.51] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2000], lr [0.51] Training => Accuracy : 0.6175\n",
      "Epoch [2000], lr [0.51] Training => Loss : 0.9032\n",
      "VALIDATION\n",
      "Epoch [2000], lr [0.51] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n",
      "TRAINING\n",
      "Epoch [2500], lr [0.51] Training => Accuracy : 0.6175\n",
      "Epoch [2500], lr [0.51] Training => Loss : 0.9019\n",
      "VALIDATION\n",
      "Epoch [2500], lr [0.51] Validation => Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.69      0.73      0.71        81\n",
      "           3       0.54      0.62      0.58        65\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       172\n",
      "   macro avg       0.29      0.28      0.28       172\n",
      "weighted avg       0.58      0.61      0.59       172\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e923dc754647198d364a23fd193926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.043 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.022354…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▆█████████▇█▇▇▇▇█▇▇▇███▇██▇▇▇█▇▇█████</td></tr><tr><td>epochs</td><td>▁▃▅█▁▃▆█▃▅▆▁▃▆█▁▅▆█▃▅█▁▃▆█▃▅▆▁▃▅█▁▅▆█▃▅█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.61047</td></tr><tr><td>epochs</td><td>2500</td></tr><tr><td>learning_rate</td><td>0.51</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-blaze-10</strong> at: <a href='https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/o5u6iik2' target=\"_blank\">https://wandb.ai/smai-khushi/Multinomial%20Logistic%20Regression/runs/o5u6iik2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231013_201457-o5u6iik2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project = \"Multinomial Logistic Regression\")\n",
    "lrs = np.arange(0.01,0.55,0.05)\n",
    "epochs = np.arange(500,3000,500)\n",
    "for lr in lrs:\n",
    "    for epoch in epochs:\n",
    "        classifier = LogisticRegression(x_train.shape[1],y_train.shape[1],lr)\n",
    "#       Training\n",
    "        for epoch_ in range(epoch):\n",
    "            out = classifier.forward(x_train)\n",
    "            loss = Xent(y_train,out)\n",
    "            classifier.backward(y_train)\n",
    "#       Training metrics\n",
    "        out = classifier.forward(x_train)\n",
    "        predicted = np.argmax(out,axis = 1)\n",
    "        actual = np.argmax(y_train,axis = 1)\n",
    "        accuracy = accuracy_score(actual,predicted)\n",
    "        loss = Xent(y_train,out)\n",
    "        print(\"TRAINING\")\n",
    "        print(f\"Epoch [{epoch}], lr [{lr}] Training => Accuracy : {accuracy:.4f}\")\n",
    "        print(f\"Epoch [{epoch}], lr [{lr}] Training => Loss : {loss:.4f}\")\n",
    "\n",
    "\n",
    "#       Validation\n",
    "        out = classifier.forward(x_val)\n",
    "        predicted = np.argmax(out,axis = 1)\n",
    "        actual = np.argmax(y_val,axis = 1)\n",
    "        metrics = classification_report(actual,predicted,zero_division=0)\n",
    "        accuracy = accuracy_score(actual,predicted)\n",
    "        print(\"VALIDATION\")\n",
    "        print(f\"Epoch [{epoch}], lr [{lr}] Validation => Metrics\")\n",
    "        print(metrics)\n",
    "        wandb.log({\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": epoch,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6275434",
   "metadata": {},
   "source": [
    "# 1.3 Hyperparameter Tuning and Evaluation\n",
    "- From the W&B logging of the model training and validation, we can see that the model performed well for 1000-1500 epochs after which for a given lr the model showed signs of overfitting and equivalengtly keeping the learning rate around 0.1\n",
    "- thus for optimal results well train the model keeping lr to be 0.11 and epochs to be 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c9ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0] training loss: 4.579589\n",
      "epoch [1] training loss: 4.476105\n",
      "epoch [2] training loss: 4.374793\n",
      "epoch [3] training loss: 4.275652\n",
      "epoch [4] training loss: 4.178682\n",
      "epoch [5] training loss: 4.083886\n",
      "epoch [6] training loss: 3.991264\n",
      "epoch [7] training loss: 3.900824\n",
      "epoch [8] training loss: 3.812572\n",
      "epoch [9] training loss: 3.726519\n",
      "epoch [10] training loss: 3.642677\n",
      "epoch [11] training loss: 3.561061\n",
      "epoch [12] training loss: 3.481686\n",
      "epoch [13] training loss: 3.404563\n",
      "epoch [14] training loss: 3.329702\n",
      "epoch [15] training loss: 3.257107\n",
      "epoch [16] training loss: 3.186777\n",
      "epoch [17] training loss: 3.118701\n",
      "epoch [18] training loss: 3.052864\n",
      "epoch [19] training loss: 2.989241\n",
      "epoch [20] training loss: 2.927800\n",
      "epoch [21] training loss: 2.868507\n",
      "epoch [22] training loss: 2.811319\n",
      "epoch [23] training loss: 2.756193\n",
      "epoch [24] training loss: 2.703081\n",
      "epoch [25] training loss: 2.651933\n",
      "epoch [26] training loss: 2.602698\n",
      "epoch [27] training loss: 2.555321\n",
      "epoch [28] training loss: 2.509749\n",
      "epoch [29] training loss: 2.465926\n",
      "epoch [30] training loss: 2.423795\n",
      "epoch [31] training loss: 2.383300\n",
      "epoch [32] training loss: 2.344384\n",
      "epoch [33] training loss: 2.306989\n",
      "epoch [34] training loss: 2.271059\n",
      "epoch [35] training loss: 2.236537\n",
      "epoch [36] training loss: 2.203367\n",
      "epoch [37] training loss: 2.171493\n",
      "epoch [38] training loss: 2.140861\n",
      "epoch [39] training loss: 2.111419\n",
      "epoch [40] training loss: 2.083113\n",
      "epoch [41] training loss: 2.055893\n",
      "epoch [42] training loss: 2.029711\n",
      "epoch [43] training loss: 2.004518\n",
      "epoch [44] training loss: 1.980270\n",
      "epoch [45] training loss: 1.956921\n",
      "epoch [46] training loss: 1.934430\n",
      "epoch [47] training loss: 1.912755\n",
      "epoch [48] training loss: 1.891859\n",
      "epoch [49] training loss: 1.871704\n",
      "epoch [50] training loss: 1.852254\n",
      "epoch [51] training loss: 1.833475\n",
      "epoch [52] training loss: 1.815336\n",
      "epoch [53] training loss: 1.797806\n",
      "epoch [54] training loss: 1.780856\n",
      "epoch [55] training loss: 1.764459\n",
      "epoch [56] training loss: 1.748587\n",
      "epoch [57] training loss: 1.733217\n",
      "epoch [58] training loss: 1.718324\n",
      "epoch [59] training loss: 1.703887\n",
      "epoch [60] training loss: 1.689885\n",
      "epoch [61] training loss: 1.676297\n",
      "epoch [62] training loss: 1.663105\n",
      "epoch [63] training loss: 1.650291\n",
      "epoch [64] training loss: 1.637838\n",
      "epoch [65] training loss: 1.625730\n",
      "epoch [66] training loss: 1.613952\n",
      "epoch [67] training loss: 1.602489\n",
      "epoch [68] training loss: 1.591328\n",
      "epoch [69] training loss: 1.580457\n",
      "epoch [70] training loss: 1.569863\n",
      "epoch [71] training loss: 1.559535\n",
      "epoch [72] training loss: 1.549462\n",
      "epoch [73] training loss: 1.539633\n",
      "epoch [74] training loss: 1.530040\n",
      "epoch [75] training loss: 1.520672\n",
      "epoch [76] training loss: 1.511522\n",
      "epoch [77] training loss: 1.502581\n",
      "epoch [78] training loss: 1.493841\n",
      "epoch [79] training loss: 1.485295\n",
      "epoch [80] training loss: 1.476936\n",
      "epoch [81] training loss: 1.468757\n",
      "epoch [82] training loss: 1.460753\n",
      "epoch [83] training loss: 1.452916\n",
      "epoch [84] training loss: 1.445242\n",
      "epoch [85] training loss: 1.437725\n",
      "epoch [86] training loss: 1.430359\n",
      "epoch [87] training loss: 1.423141\n",
      "epoch [88] training loss: 1.416065\n",
      "epoch [89] training loss: 1.409128\n",
      "epoch [90] training loss: 1.402324\n",
      "epoch [91] training loss: 1.395651\n",
      "epoch [92] training loss: 1.389104\n",
      "epoch [93] training loss: 1.382679\n",
      "epoch [94] training loss: 1.376374\n",
      "epoch [95] training loss: 1.370185\n",
      "epoch [96] training loss: 1.364108\n",
      "epoch [97] training loss: 1.358142\n",
      "epoch [98] training loss: 1.352283\n",
      "epoch [99] training loss: 1.346528\n",
      "epoch [100] training loss: 1.340875\n",
      "epoch [101] training loss: 1.335321\n",
      "epoch [102] training loss: 1.329864\n",
      "epoch [103] training loss: 1.324502\n",
      "epoch [104] training loss: 1.319233\n",
      "epoch [105] training loss: 1.314054\n",
      "epoch [106] training loss: 1.308963\n",
      "epoch [107] training loss: 1.303959\n",
      "epoch [108] training loss: 1.299039\n",
      "epoch [109] training loss: 1.294203\n",
      "epoch [110] training loss: 1.289447\n",
      "epoch [111] training loss: 1.284771\n",
      "epoch [112] training loss: 1.280174\n",
      "epoch [113] training loss: 1.275652\n",
      "epoch [114] training loss: 1.271205\n",
      "epoch [115] training loss: 1.266832\n",
      "epoch [116] training loss: 1.262531\n",
      "epoch [117] training loss: 1.258301\n",
      "epoch [118] training loss: 1.254140\n",
      "epoch [119] training loss: 1.250047\n",
      "epoch [120] training loss: 1.246021\n",
      "epoch [121] training loss: 1.242061\n",
      "epoch [122] training loss: 1.238166\n",
      "epoch [123] training loss: 1.234334\n",
      "epoch [124] training loss: 1.230564\n",
      "epoch [125] training loss: 1.226856\n",
      "epoch [126] training loss: 1.223208\n",
      "epoch [127] training loss: 1.219619\n",
      "epoch [128] training loss: 1.216088\n",
      "epoch [129] training loss: 1.212615\n",
      "epoch [130] training loss: 1.209198\n",
      "epoch [131] training loss: 1.205836\n",
      "epoch [132] training loss: 1.202529\n",
      "epoch [133] training loss: 1.199275\n",
      "epoch [134] training loss: 1.196074\n",
      "epoch [135] training loss: 1.192925\n",
      "epoch [136] training loss: 1.189827\n",
      "epoch [137] training loss: 1.186778\n",
      "epoch [138] training loss: 1.183780\n",
      "epoch [139] training loss: 1.180829\n",
      "epoch [140] training loss: 1.177927\n",
      "epoch [141] training loss: 1.175071\n",
      "epoch [142] training loss: 1.172262\n",
      "epoch [143] training loss: 1.169498\n",
      "epoch [144] training loss: 1.166778\n",
      "epoch [145] training loss: 1.164103\n",
      "epoch [146] training loss: 1.161471\n",
      "epoch [147] training loss: 1.158881\n",
      "epoch [148] training loss: 1.156334\n",
      "epoch [149] training loss: 1.153827\n",
      "epoch [150] training loss: 1.151361\n",
      "epoch [151] training loss: 1.148934\n",
      "epoch [152] training loss: 1.146547\n",
      "epoch [153] training loss: 1.144199\n",
      "epoch [154] training loss: 1.141888\n",
      "epoch [155] training loss: 1.139614\n",
      "epoch [156] training loss: 1.137377\n",
      "epoch [157] training loss: 1.135176\n",
      "epoch [158] training loss: 1.133010\n",
      "epoch [159] training loss: 1.130879\n",
      "epoch [160] training loss: 1.128782\n",
      "epoch [161] training loss: 1.126719\n",
      "epoch [162] training loss: 1.124689\n",
      "epoch [163] training loss: 1.122691\n",
      "epoch [164] training loss: 1.120725\n",
      "epoch [165] training loss: 1.118791\n",
      "epoch [166] training loss: 1.116887\n",
      "epoch [167] training loss: 1.115013\n",
      "epoch [168] training loss: 1.113170\n",
      "epoch [169] training loss: 1.111355\n",
      "epoch [170] training loss: 1.109569\n",
      "epoch [171] training loss: 1.107811\n",
      "epoch [172] training loss: 1.106081\n",
      "epoch [173] training loss: 1.104379\n",
      "epoch [174] training loss: 1.102702\n",
      "epoch [175] training loss: 1.101053\n",
      "epoch [176] training loss: 1.099428\n",
      "epoch [177] training loss: 1.097829\n",
      "epoch [178] training loss: 1.096255\n",
      "epoch [179] training loss: 1.094706\n",
      "epoch [180] training loss: 1.093180\n",
      "epoch [181] training loss: 1.091678\n",
      "epoch [182] training loss: 1.090199\n",
      "epoch [183] training loss: 1.088742\n",
      "epoch [184] training loss: 1.087308\n",
      "epoch [185] training loss: 1.085895\n",
      "epoch [186] training loss: 1.084504\n",
      "epoch [187] training loss: 1.083134\n",
      "epoch [188] training loss: 1.081785\n",
      "epoch [189] training loss: 1.080456\n",
      "epoch [190] training loss: 1.079147\n",
      "epoch [191] training loss: 1.077857\n",
      "epoch [192] training loss: 1.076586\n",
      "epoch [193] training loss: 1.075334\n",
      "epoch [194] training loss: 1.074101\n",
      "epoch [195] training loss: 1.072886\n",
      "epoch [196] training loss: 1.071688\n",
      "epoch [197] training loss: 1.070508\n",
      "epoch [198] training loss: 1.069345\n",
      "epoch [199] training loss: 1.068199\n",
      "epoch [200] training loss: 1.067069\n",
      "epoch [201] training loss: 1.065956\n",
      "epoch [202] training loss: 1.064858\n",
      "epoch [203] training loss: 1.063776\n",
      "epoch [204] training loss: 1.062709\n",
      "epoch [205] training loss: 1.061657\n",
      "epoch [206] training loss: 1.060619\n",
      "epoch [207] training loss: 1.059596\n",
      "epoch [208] training loss: 1.058588\n",
      "epoch [209] training loss: 1.057593\n",
      "epoch [210] training loss: 1.056611\n",
      "epoch [211] training loss: 1.055643\n",
      "epoch [212] training loss: 1.054689\n",
      "epoch [213] training loss: 1.053747\n",
      "epoch [214] training loss: 1.052817\n",
      "epoch [215] training loss: 1.051900\n",
      "epoch [216] training loss: 1.050996\n",
      "epoch [217] training loss: 1.050103\n",
      "epoch [218] training loss: 1.049222\n",
      "epoch [219] training loss: 1.048352\n",
      "epoch [220] training loss: 1.047494\n",
      "epoch [221] training loss: 1.046646\n",
      "epoch [222] training loss: 1.045810\n",
      "epoch [223] training loss: 1.044984\n",
      "epoch [224] training loss: 1.044169\n",
      "epoch [225] training loss: 1.043364\n",
      "epoch [226] training loss: 1.042569\n",
      "epoch [227] training loss: 1.041784\n",
      "epoch [228] training loss: 1.041008\n",
      "epoch [229] training loss: 1.040243\n",
      "epoch [230] training loss: 1.039486\n",
      "epoch [231] training loss: 1.038739\n",
      "epoch [232] training loss: 1.038001\n",
      "epoch [233] training loss: 1.037271\n",
      "epoch [234] training loss: 1.036551\n",
      "epoch [235] training loss: 1.035839\n",
      "epoch [236] training loss: 1.035135\n",
      "epoch [237] training loss: 1.034440\n",
      "epoch [238] training loss: 1.033753\n",
      "epoch [239] training loss: 1.033074\n",
      "epoch [240] training loss: 1.032402\n",
      "epoch [241] training loss: 1.031738\n",
      "epoch [242] training loss: 1.031082\n",
      "epoch [243] training loss: 1.030434\n",
      "epoch [244] training loss: 1.029792\n",
      "epoch [245] training loss: 1.029158\n",
      "epoch [246] training loss: 1.028531\n",
      "epoch [247] training loss: 1.027910\n",
      "epoch [248] training loss: 1.027297\n",
      "epoch [249] training loss: 1.026690\n",
      "epoch [250] training loss: 1.026090\n",
      "epoch [251] training loss: 1.025496\n",
      "epoch [252] training loss: 1.024909\n",
      "epoch [253] training loss: 1.024328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [254] training loss: 1.023753\n",
      "epoch [255] training loss: 1.023184\n",
      "epoch [256] training loss: 1.022622\n",
      "epoch [257] training loss: 1.022065\n",
      "epoch [258] training loss: 1.021513\n",
      "epoch [259] training loss: 1.020968\n",
      "epoch [260] training loss: 1.020428\n",
      "epoch [261] training loss: 1.019893\n",
      "epoch [262] training loss: 1.019364\n",
      "epoch [263] training loss: 1.018840\n",
      "epoch [264] training loss: 1.018322\n",
      "epoch [265] training loss: 1.017809\n",
      "epoch [266] training loss: 1.017300\n",
      "epoch [267] training loss: 1.016797\n",
      "epoch [268] training loss: 1.016299\n",
      "epoch [269] training loss: 1.015805\n",
      "epoch [270] training loss: 1.015316\n",
      "epoch [271] training loss: 1.014832\n",
      "epoch [272] training loss: 1.014353\n",
      "epoch [273] training loss: 1.013878\n",
      "epoch [274] training loss: 1.013407\n",
      "epoch [275] training loss: 1.012941\n",
      "epoch [276] training loss: 1.012480\n",
      "epoch [277] training loss: 1.012022\n",
      "epoch [278] training loss: 1.011569\n",
      "epoch [279] training loss: 1.011120\n",
      "epoch [280] training loss: 1.010675\n",
      "epoch [281] training loss: 1.010234\n",
      "epoch [282] training loss: 1.009798\n",
      "epoch [283] training loss: 1.009365\n",
      "epoch [284] training loss: 1.008936\n",
      "epoch [285] training loss: 1.008510\n",
      "epoch [286] training loss: 1.008089\n",
      "epoch [287] training loss: 1.007671\n",
      "epoch [288] training loss: 1.007257\n",
      "epoch [289] training loss: 1.006847\n",
      "epoch [290] training loss: 1.006440\n",
      "epoch [291] training loss: 1.006036\n",
      "epoch [292] training loss: 1.005637\n",
      "epoch [293] training loss: 1.005240\n",
      "epoch [294] training loss: 1.004847\n",
      "epoch [295] training loss: 1.004457\n",
      "epoch [296] training loss: 1.004070\n",
      "epoch [297] training loss: 1.003687\n",
      "epoch [298] training loss: 1.003307\n",
      "epoch [299] training loss: 1.002930\n",
      "epoch [300] training loss: 1.002556\n",
      "epoch [301] training loss: 1.002186\n",
      "epoch [302] training loss: 1.001818\n",
      "epoch [303] training loss: 1.001453\n",
      "epoch [304] training loss: 1.001091\n",
      "epoch [305] training loss: 1.000732\n",
      "epoch [306] training loss: 1.000376\n",
      "epoch [307] training loss: 1.000023\n",
      "epoch [308] training loss: 0.999673\n",
      "epoch [309] training loss: 0.999325\n",
      "epoch [310] training loss: 0.998980\n",
      "epoch [311] training loss: 0.998638\n",
      "epoch [312] training loss: 0.998299\n",
      "epoch [313] training loss: 0.997962\n",
      "epoch [314] training loss: 0.997628\n",
      "epoch [315] training loss: 0.997296\n",
      "epoch [316] training loss: 0.996967\n",
      "epoch [317] training loss: 0.996640\n",
      "epoch [318] training loss: 0.996316\n",
      "epoch [319] training loss: 0.995995\n",
      "epoch [320] training loss: 0.995675\n",
      "epoch [321] training loss: 0.995358\n",
      "epoch [322] training loss: 0.995044\n",
      "epoch [323] training loss: 0.994732\n",
      "epoch [324] training loss: 0.994422\n",
      "epoch [325] training loss: 0.994114\n",
      "epoch [326] training loss: 0.993809\n",
      "epoch [327] training loss: 0.993506\n",
      "epoch [328] training loss: 0.993205\n",
      "epoch [329] training loss: 0.992907\n",
      "epoch [330] training loss: 0.992610\n",
      "epoch [331] training loss: 0.992316\n",
      "epoch [332] training loss: 0.992023\n",
      "epoch [333] training loss: 0.991733\n",
      "epoch [334] training loss: 0.991445\n",
      "epoch [335] training loss: 0.991159\n",
      "epoch [336] training loss: 0.990875\n",
      "epoch [337] training loss: 0.990593\n",
      "epoch [338] training loss: 0.990313\n",
      "epoch [339] training loss: 0.990034\n",
      "epoch [340] training loss: 0.989758\n",
      "epoch [341] training loss: 0.989484\n",
      "epoch [342] training loss: 0.989211\n",
      "epoch [343] training loss: 0.988941\n",
      "epoch [344] training loss: 0.988672\n",
      "epoch [345] training loss: 0.988405\n",
      "epoch [346] training loss: 0.988140\n",
      "epoch [347] training loss: 0.987877\n",
      "epoch [348] training loss: 0.987615\n",
      "epoch [349] training loss: 0.987355\n",
      "epoch [350] training loss: 0.987097\n",
      "epoch [351] training loss: 0.986841\n",
      "epoch [352] training loss: 0.986586\n",
      "epoch [353] training loss: 0.986333\n",
      "epoch [354] training loss: 0.986082\n",
      "epoch [355] training loss: 0.985832\n",
      "epoch [356] training loss: 0.985584\n",
      "epoch [357] training loss: 0.985337\n",
      "epoch [358] training loss: 0.985093\n",
      "epoch [359] training loss: 0.984849\n",
      "epoch [360] training loss: 0.984608\n",
      "epoch [361] training loss: 0.984367\n",
      "epoch [362] training loss: 0.984129\n",
      "epoch [363] training loss: 0.983892\n",
      "epoch [364] training loss: 0.983656\n",
      "epoch [365] training loss: 0.983422\n",
      "epoch [366] training loss: 0.983189\n",
      "epoch [367] training loss: 0.982958\n",
      "epoch [368] training loss: 0.982728\n",
      "epoch [369] training loss: 0.982500\n",
      "epoch [370] training loss: 0.982273\n",
      "epoch [371] training loss: 0.982047\n",
      "epoch [372] training loss: 0.981823\n",
      "epoch [373] training loss: 0.981600\n",
      "epoch [374] training loss: 0.981379\n",
      "epoch [375] training loss: 0.981159\n",
      "epoch [376] training loss: 0.980940\n",
      "epoch [377] training loss: 0.980722\n",
      "epoch [378] training loss: 0.980506\n",
      "epoch [379] training loss: 0.980291\n",
      "epoch [380] training loss: 0.980078\n",
      "epoch [381] training loss: 0.979865\n",
      "epoch [382] training loss: 0.979654\n",
      "epoch [383] training loss: 0.979445\n",
      "epoch [384] training loss: 0.979236\n",
      "epoch [385] training loss: 0.979029\n",
      "epoch [386] training loss: 0.978823\n",
      "epoch [387] training loss: 0.978618\n",
      "epoch [388] training loss: 0.978414\n",
      "epoch [389] training loss: 0.978211\n",
      "epoch [390] training loss: 0.978010\n",
      "epoch [391] training loss: 0.977810\n",
      "epoch [392] training loss: 0.977611\n",
      "epoch [393] training loss: 0.977413\n",
      "epoch [394] training loss: 0.977216\n",
      "epoch [395] training loss: 0.977020\n",
      "epoch [396] training loss: 0.976825\n",
      "epoch [397] training loss: 0.976632\n",
      "epoch [398] training loss: 0.976439\n",
      "epoch [399] training loss: 0.976248\n",
      "epoch [400] training loss: 0.976058\n",
      "epoch [401] training loss: 0.975869\n",
      "epoch [402] training loss: 0.975680\n",
      "epoch [403] training loss: 0.975493\n",
      "epoch [404] training loss: 0.975307\n",
      "epoch [405] training loss: 0.975122\n",
      "epoch [406] training loss: 0.974938\n",
      "epoch [407] training loss: 0.974755\n",
      "epoch [408] training loss: 0.974573\n",
      "epoch [409] training loss: 0.974392\n",
      "epoch [410] training loss: 0.974211\n",
      "epoch [411] training loss: 0.974032\n",
      "epoch [412] training loss: 0.973854\n",
      "epoch [413] training loss: 0.973677\n",
      "epoch [414] training loss: 0.973500\n",
      "epoch [415] training loss: 0.973325\n",
      "epoch [416] training loss: 0.973151\n",
      "epoch [417] training loss: 0.972977\n",
      "epoch [418] training loss: 0.972804\n",
      "epoch [419] training loss: 0.972633\n",
      "epoch [420] training loss: 0.972462\n",
      "epoch [421] training loss: 0.972292\n",
      "epoch [422] training loss: 0.972123\n",
      "epoch [423] training loss: 0.971955\n",
      "epoch [424] training loss: 0.971787\n",
      "epoch [425] training loss: 0.971621\n",
      "epoch [426] training loss: 0.971455\n",
      "epoch [427] training loss: 0.971290\n",
      "epoch [428] training loss: 0.971127\n",
      "epoch [429] training loss: 0.970963\n",
      "epoch [430] training loss: 0.970801\n",
      "epoch [431] training loss: 0.970640\n",
      "epoch [432] training loss: 0.970479\n",
      "epoch [433] training loss: 0.970319\n",
      "epoch [434] training loss: 0.970160\n",
      "epoch [435] training loss: 0.970002\n",
      "epoch [436] training loss: 0.969844\n",
      "epoch [437] training loss: 0.969688\n",
      "epoch [438] training loss: 0.969532\n",
      "epoch [439] training loss: 0.969377\n",
      "epoch [440] training loss: 0.969222\n",
      "epoch [441] training loss: 0.969069\n",
      "epoch [442] training loss: 0.968916\n",
      "epoch [443] training loss: 0.968764\n",
      "epoch [444] training loss: 0.968612\n",
      "epoch [445] training loss: 0.968462\n",
      "epoch [446] training loss: 0.968312\n",
      "epoch [447] training loss: 0.968162\n",
      "epoch [448] training loss: 0.968014\n",
      "epoch [449] training loss: 0.967866\n",
      "epoch [450] training loss: 0.967719\n",
      "epoch [451] training loss: 0.967573\n",
      "epoch [452] training loss: 0.967427\n",
      "epoch [453] training loss: 0.967282\n",
      "epoch [454] training loss: 0.967138\n",
      "epoch [455] training loss: 0.966994\n",
      "epoch [456] training loss: 0.966851\n",
      "epoch [457] training loss: 0.966709\n",
      "epoch [458] training loss: 0.966567\n",
      "epoch [459] training loss: 0.966426\n",
      "epoch [460] training loss: 0.966286\n",
      "epoch [461] training loss: 0.966146\n",
      "epoch [462] training loss: 0.966007\n",
      "epoch [463] training loss: 0.965868\n",
      "epoch [464] training loss: 0.965731\n",
      "epoch [465] training loss: 0.965593\n",
      "epoch [466] training loss: 0.965457\n",
      "epoch [467] training loss: 0.965321\n",
      "epoch [468] training loss: 0.965186\n",
      "epoch [469] training loss: 0.965051\n",
      "epoch [470] training loss: 0.964917\n",
      "epoch [471] training loss: 0.964783\n",
      "epoch [472] training loss: 0.964650\n",
      "epoch [473] training loss: 0.964518\n",
      "epoch [474] training loss: 0.964386\n",
      "epoch [475] training loss: 0.964255\n",
      "epoch [476] training loss: 0.964125\n",
      "epoch [477] training loss: 0.963995\n",
      "epoch [478] training loss: 0.963865\n",
      "epoch [479] training loss: 0.963736\n",
      "epoch [480] training loss: 0.963608\n",
      "epoch [481] training loss: 0.963480\n",
      "epoch [482] training loss: 0.963353\n",
      "epoch [483] training loss: 0.963227\n",
      "epoch [484] training loss: 0.963101\n",
      "epoch [485] training loss: 0.962975\n",
      "epoch [486] training loss: 0.962850\n",
      "epoch [487] training loss: 0.962726\n",
      "epoch [488] training loss: 0.962602\n",
      "epoch [489] training loss: 0.962478\n",
      "epoch [490] training loss: 0.962355\n",
      "epoch [491] training loss: 0.962233\n",
      "epoch [492] training loss: 0.962111\n",
      "epoch [493] training loss: 0.961990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [494] training loss: 0.961869\n",
      "epoch [495] training loss: 0.961749\n",
      "epoch [496] training loss: 0.961629\n",
      "epoch [497] training loss: 0.961510\n",
      "epoch [498] training loss: 0.961391\n",
      "epoch [499] training loss: 0.961272\n",
      "epoch [500] training loss: 0.961155\n",
      "epoch [501] training loss: 0.961037\n",
      "epoch [502] training loss: 0.960920\n",
      "epoch [503] training loss: 0.960804\n",
      "epoch [504] training loss: 0.960688\n",
      "epoch [505] training loss: 0.960573\n",
      "epoch [506] training loss: 0.960458\n",
      "epoch [507] training loss: 0.960343\n",
      "epoch [508] training loss: 0.960229\n",
      "epoch [509] training loss: 0.960116\n",
      "epoch [510] training loss: 0.960003\n",
      "epoch [511] training loss: 0.959890\n",
      "epoch [512] training loss: 0.959778\n",
      "epoch [513] training loss: 0.959666\n",
      "epoch [514] training loss: 0.959555\n",
      "epoch [515] training loss: 0.959444\n",
      "epoch [516] training loss: 0.959333\n",
      "epoch [517] training loss: 0.959223\n",
      "epoch [518] training loss: 0.959114\n",
      "epoch [519] training loss: 0.959005\n",
      "epoch [520] training loss: 0.958896\n",
      "epoch [521] training loss: 0.958788\n",
      "epoch [522] training loss: 0.958680\n",
      "epoch [523] training loss: 0.958573\n",
      "epoch [524] training loss: 0.958466\n",
      "epoch [525] training loss: 0.958359\n",
      "epoch [526] training loss: 0.958253\n",
      "epoch [527] training loss: 0.958147\n",
      "epoch [528] training loss: 0.958042\n",
      "epoch [529] training loss: 0.957937\n",
      "epoch [530] training loss: 0.957832\n",
      "epoch [531] training loss: 0.957728\n",
      "epoch [532] training loss: 0.957624\n",
      "epoch [533] training loss: 0.957521\n",
      "epoch [534] training loss: 0.957418\n",
      "epoch [535] training loss: 0.957315\n",
      "epoch [536] training loss: 0.957213\n",
      "epoch [537] training loss: 0.957111\n",
      "epoch [538] training loss: 0.957010\n",
      "epoch [539] training loss: 0.956909\n",
      "epoch [540] training loss: 0.956808\n",
      "epoch [541] training loss: 0.956708\n",
      "epoch [542] training loss: 0.956608\n",
      "epoch [543] training loss: 0.956509\n",
      "epoch [544] training loss: 0.956409\n",
      "epoch [545] training loss: 0.956311\n",
      "epoch [546] training loss: 0.956212\n",
      "epoch [547] training loss: 0.956114\n",
      "epoch [548] training loss: 0.956016\n",
      "epoch [549] training loss: 0.955919\n",
      "epoch [550] training loss: 0.955822\n",
      "epoch [551] training loss: 0.955725\n",
      "epoch [552] training loss: 0.955629\n",
      "epoch [553] training loss: 0.955533\n",
      "epoch [554] training loss: 0.955438\n",
      "epoch [555] training loss: 0.955342\n",
      "epoch [556] training loss: 0.955248\n",
      "epoch [557] training loss: 0.955153\n",
      "epoch [558] training loss: 0.955059\n",
      "epoch [559] training loss: 0.954965\n",
      "epoch [560] training loss: 0.954871\n",
      "epoch [561] training loss: 0.954778\n",
      "epoch [562] training loss: 0.954685\n",
      "epoch [563] training loss: 0.954593\n",
      "epoch [564] training loss: 0.954501\n",
      "epoch [565] training loss: 0.954409\n",
      "epoch [566] training loss: 0.954317\n",
      "epoch [567] training loss: 0.954226\n",
      "epoch [568] training loss: 0.954135\n",
      "epoch [569] training loss: 0.954044\n",
      "epoch [570] training loss: 0.953954\n",
      "epoch [571] training loss: 0.953864\n",
      "epoch [572] training loss: 0.953774\n",
      "epoch [573] training loss: 0.953685\n",
      "epoch [574] training loss: 0.953596\n",
      "epoch [575] training loss: 0.953507\n",
      "epoch [576] training loss: 0.953419\n",
      "epoch [577] training loss: 0.953331\n",
      "epoch [578] training loss: 0.953243\n",
      "epoch [579] training loss: 0.953155\n",
      "epoch [580] training loss: 0.953068\n",
      "epoch [581] training loss: 0.952981\n",
      "epoch [582] training loss: 0.952895\n",
      "epoch [583] training loss: 0.952808\n",
      "epoch [584] training loss: 0.952722\n",
      "epoch [585] training loss: 0.952637\n",
      "epoch [586] training loss: 0.952551\n",
      "epoch [587] training loss: 0.952466\n",
      "epoch [588] training loss: 0.952381\n",
      "epoch [589] training loss: 0.952297\n",
      "epoch [590] training loss: 0.952212\n",
      "epoch [591] training loss: 0.952128\n",
      "epoch [592] training loss: 0.952044\n",
      "epoch [593] training loss: 0.951961\n",
      "epoch [594] training loss: 0.951878\n",
      "epoch [595] training loss: 0.951795\n",
      "epoch [596] training loss: 0.951712\n",
      "epoch [597] training loss: 0.951630\n",
      "epoch [598] training loss: 0.951548\n",
      "epoch [599] training loss: 0.951466\n",
      "epoch [600] training loss: 0.951385\n",
      "epoch [601] training loss: 0.951303\n",
      "epoch [602] training loss: 0.951222\n",
      "epoch [603] training loss: 0.951141\n",
      "epoch [604] training loss: 0.951061\n",
      "epoch [605] training loss: 0.950981\n",
      "epoch [606] training loss: 0.950901\n",
      "epoch [607] training loss: 0.950821\n",
      "epoch [608] training loss: 0.950742\n",
      "epoch [609] training loss: 0.950663\n",
      "epoch [610] training loss: 0.950584\n",
      "epoch [611] training loss: 0.950505\n",
      "epoch [612] training loss: 0.950426\n",
      "epoch [613] training loss: 0.950348\n",
      "epoch [614] training loss: 0.950270\n",
      "epoch [615] training loss: 0.950193\n",
      "epoch [616] training loss: 0.950115\n",
      "epoch [617] training loss: 0.950038\n",
      "epoch [618] training loss: 0.949961\n",
      "epoch [619] training loss: 0.949885\n",
      "epoch [620] training loss: 0.949808\n",
      "epoch [621] training loss: 0.949732\n",
      "epoch [622] training loss: 0.949656\n",
      "epoch [623] training loss: 0.949580\n",
      "epoch [624] training loss: 0.949505\n",
      "epoch [625] training loss: 0.949429\n",
      "epoch [626] training loss: 0.949354\n",
      "epoch [627] training loss: 0.949280\n",
      "epoch [628] training loss: 0.949205\n",
      "epoch [629] training loss: 0.949131\n",
      "epoch [630] training loss: 0.949057\n",
      "epoch [631] training loss: 0.948983\n",
      "epoch [632] training loss: 0.948909\n",
      "epoch [633] training loss: 0.948836\n",
      "epoch [634] training loss: 0.948763\n",
      "epoch [635] training loss: 0.948690\n",
      "epoch [636] training loss: 0.948617\n",
      "epoch [637] training loss: 0.948544\n",
      "epoch [638] training loss: 0.948472\n",
      "epoch [639] training loss: 0.948400\n",
      "epoch [640] training loss: 0.948328\n",
      "epoch [641] training loss: 0.948257\n",
      "epoch [642] training loss: 0.948185\n",
      "epoch [643] training loss: 0.948114\n",
      "epoch [644] training loss: 0.948043\n",
      "epoch [645] training loss: 0.947972\n",
      "epoch [646] training loss: 0.947902\n",
      "epoch [647] training loss: 0.947832\n",
      "epoch [648] training loss: 0.947761\n",
      "epoch [649] training loss: 0.947691\n",
      "epoch [650] training loss: 0.947622\n",
      "epoch [651] training loss: 0.947552\n",
      "epoch [652] training loss: 0.947483\n",
      "epoch [653] training loss: 0.947414\n",
      "epoch [654] training loss: 0.947345\n",
      "epoch [655] training loss: 0.947276\n",
      "epoch [656] training loss: 0.947208\n",
      "epoch [657] training loss: 0.947140\n",
      "epoch [658] training loss: 0.947072\n",
      "epoch [659] training loss: 0.947004\n",
      "epoch [660] training loss: 0.946936\n",
      "epoch [661] training loss: 0.946869\n",
      "epoch [662] training loss: 0.946801\n",
      "epoch [663] training loss: 0.946734\n",
      "epoch [664] training loss: 0.946668\n",
      "epoch [665] training loss: 0.946601\n",
      "epoch [666] training loss: 0.946534\n",
      "epoch [667] training loss: 0.946468\n",
      "epoch [668] training loss: 0.946402\n",
      "epoch [669] training loss: 0.946336\n",
      "epoch [670] training loss: 0.946270\n",
      "epoch [671] training loss: 0.946205\n",
      "epoch [672] training loss: 0.946140\n",
      "epoch [673] training loss: 0.946074\n",
      "epoch [674] training loss: 0.946010\n",
      "epoch [675] training loss: 0.945945\n",
      "epoch [676] training loss: 0.945880\n",
      "epoch [677] training loss: 0.945816\n",
      "epoch [678] training loss: 0.945752\n",
      "epoch [679] training loss: 0.945688\n",
      "epoch [680] training loss: 0.945624\n",
      "epoch [681] training loss: 0.945560\n",
      "epoch [682] training loss: 0.945497\n",
      "epoch [683] training loss: 0.945433\n",
      "epoch [684] training loss: 0.945370\n",
      "epoch [685] training loss: 0.945307\n",
      "epoch [686] training loss: 0.945245\n",
      "epoch [687] training loss: 0.945182\n",
      "epoch [688] training loss: 0.945120\n",
      "epoch [689] training loss: 0.945057\n",
      "epoch [690] training loss: 0.944995\n",
      "epoch [691] training loss: 0.944933\n",
      "epoch [692] training loss: 0.944872\n",
      "epoch [693] training loss: 0.944810\n",
      "epoch [694] training loss: 0.944749\n",
      "epoch [695] training loss: 0.944688\n",
      "epoch [696] training loss: 0.944627\n",
      "epoch [697] training loss: 0.944566\n",
      "epoch [698] training loss: 0.944505\n",
      "epoch [699] training loss: 0.944445\n",
      "epoch [700] training loss: 0.944384\n",
      "epoch [701] training loss: 0.944324\n",
      "epoch [702] training loss: 0.944264\n",
      "epoch [703] training loss: 0.944204\n",
      "epoch [704] training loss: 0.944145\n",
      "epoch [705] training loss: 0.944085\n",
      "epoch [706] training loss: 0.944026\n",
      "epoch [707] training loss: 0.943966\n",
      "epoch [708] training loss: 0.943907\n",
      "epoch [709] training loss: 0.943849\n",
      "epoch [710] training loss: 0.943790\n",
      "epoch [711] training loss: 0.943731\n",
      "epoch [712] training loss: 0.943673\n",
      "epoch [713] training loss: 0.943615\n",
      "epoch [714] training loss: 0.943557\n",
      "epoch [715] training loss: 0.943499\n",
      "epoch [716] training loss: 0.943441\n",
      "epoch [717] training loss: 0.943383\n",
      "epoch [718] training loss: 0.943326\n",
      "epoch [719] training loss: 0.943269\n",
      "epoch [720] training loss: 0.943212\n",
      "epoch [721] training loss: 0.943155\n",
      "epoch [722] training loss: 0.943098\n",
      "epoch [723] training loss: 0.943041\n",
      "epoch [724] training loss: 0.942985\n",
      "epoch [725] training loss: 0.942928\n",
      "epoch [726] training loss: 0.942872\n",
      "epoch [727] training loss: 0.942816\n",
      "epoch [728] training loss: 0.942760\n",
      "epoch [729] training loss: 0.942704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [730] training loss: 0.942649\n",
      "epoch [731] training loss: 0.942593\n",
      "epoch [732] training loss: 0.942538\n",
      "epoch [733] training loss: 0.942483\n",
      "epoch [734] training loss: 0.942428\n",
      "epoch [735] training loss: 0.942373\n",
      "epoch [736] training loss: 0.942318\n",
      "epoch [737] training loss: 0.942263\n",
      "epoch [738] training loss: 0.942209\n",
      "epoch [739] training loss: 0.942154\n",
      "epoch [740] training loss: 0.942100\n",
      "epoch [741] training loss: 0.942046\n",
      "epoch [742] training loss: 0.941992\n",
      "epoch [743] training loss: 0.941939\n",
      "epoch [744] training loss: 0.941885\n",
      "epoch [745] training loss: 0.941832\n",
      "epoch [746] training loss: 0.941778\n",
      "epoch [747] training loss: 0.941725\n",
      "epoch [748] training loss: 0.941672\n",
      "epoch [749] training loss: 0.941619\n",
      "epoch [750] training loss: 0.941566\n",
      "epoch [751] training loss: 0.941514\n",
      "epoch [752] training loss: 0.941461\n",
      "epoch [753] training loss: 0.941409\n",
      "epoch [754] training loss: 0.941356\n",
      "epoch [755] training loss: 0.941304\n",
      "epoch [756] training loss: 0.941252\n",
      "epoch [757] training loss: 0.941201\n",
      "epoch [758] training loss: 0.941149\n",
      "epoch [759] training loss: 0.941097\n",
      "epoch [760] training loss: 0.941046\n",
      "epoch [761] training loss: 0.940995\n",
      "epoch [762] training loss: 0.940943\n",
      "epoch [763] training loss: 0.940892\n",
      "epoch [764] training loss: 0.940841\n",
      "epoch [765] training loss: 0.940791\n",
      "epoch [766] training loss: 0.940740\n",
      "epoch [767] training loss: 0.940689\n",
      "epoch [768] training loss: 0.940639\n",
      "epoch [769] training loss: 0.940589\n",
      "epoch [770] training loss: 0.940539\n",
      "epoch [771] training loss: 0.940489\n",
      "epoch [772] training loss: 0.940439\n",
      "epoch [773] training loss: 0.940389\n",
      "epoch [774] training loss: 0.940339\n",
      "epoch [775] training loss: 0.940290\n",
      "epoch [776] training loss: 0.940240\n",
      "epoch [777] training loss: 0.940191\n",
      "epoch [778] training loss: 0.940142\n",
      "epoch [779] training loss: 0.940093\n",
      "epoch [780] training loss: 0.940044\n",
      "epoch [781] training loss: 0.939995\n",
      "epoch [782] training loss: 0.939946\n",
      "epoch [783] training loss: 0.939898\n",
      "epoch [784] training loss: 0.939850\n",
      "epoch [785] training loss: 0.939801\n",
      "epoch [786] training loss: 0.939753\n",
      "epoch [787] training loss: 0.939705\n",
      "epoch [788] training loss: 0.939657\n",
      "epoch [789] training loss: 0.939609\n",
      "epoch [790] training loss: 0.939561\n",
      "epoch [791] training loss: 0.939514\n",
      "epoch [792] training loss: 0.939466\n",
      "epoch [793] training loss: 0.939419\n",
      "epoch [794] training loss: 0.939372\n",
      "epoch [795] training loss: 0.939325\n",
      "epoch [796] training loss: 0.939278\n",
      "epoch [797] training loss: 0.939231\n",
      "epoch [798] training loss: 0.939184\n",
      "epoch [799] training loss: 0.939137\n",
      "epoch [800] training loss: 0.939091\n",
      "epoch [801] training loss: 0.939044\n",
      "epoch [802] training loss: 0.938998\n",
      "epoch [803] training loss: 0.938952\n",
      "epoch [804] training loss: 0.938906\n",
      "epoch [805] training loss: 0.938860\n",
      "epoch [806] training loss: 0.938814\n",
      "epoch [807] training loss: 0.938768\n",
      "epoch [808] training loss: 0.938722\n",
      "epoch [809] training loss: 0.938677\n",
      "epoch [810] training loss: 0.938631\n",
      "epoch [811] training loss: 0.938586\n",
      "epoch [812] training loss: 0.938541\n",
      "epoch [813] training loss: 0.938496\n",
      "epoch [814] training loss: 0.938451\n",
      "epoch [815] training loss: 0.938406\n",
      "epoch [816] training loss: 0.938361\n",
      "epoch [817] training loss: 0.938316\n",
      "epoch [818] training loss: 0.938272\n",
      "epoch [819] training loss: 0.938227\n",
      "epoch [820] training loss: 0.938183\n",
      "epoch [821] training loss: 0.938139\n",
      "epoch [822] training loss: 0.938095\n",
      "epoch [823] training loss: 0.938051\n",
      "epoch [824] training loss: 0.938007\n",
      "epoch [825] training loss: 0.937963\n",
      "epoch [826] training loss: 0.937919\n",
      "epoch [827] training loss: 0.937875\n",
      "epoch [828] training loss: 0.937832\n",
      "epoch [829] training loss: 0.937788\n",
      "epoch [830] training loss: 0.937745\n",
      "epoch [831] training loss: 0.937702\n",
      "epoch [832] training loss: 0.937659\n",
      "epoch [833] training loss: 0.937616\n",
      "epoch [834] training loss: 0.937573\n",
      "epoch [835] training loss: 0.937530\n",
      "epoch [836] training loss: 0.937487\n",
      "epoch [837] training loss: 0.937445\n",
      "epoch [838] training loss: 0.937402\n",
      "epoch [839] training loss: 0.937360\n",
      "epoch [840] training loss: 0.937318\n",
      "epoch [841] training loss: 0.937275\n",
      "epoch [842] training loss: 0.937233\n",
      "epoch [843] training loss: 0.937191\n",
      "epoch [844] training loss: 0.937149\n",
      "epoch [845] training loss: 0.937107\n",
      "epoch [846] training loss: 0.937066\n",
      "epoch [847] training loss: 0.937024\n",
      "epoch [848] training loss: 0.936983\n",
      "epoch [849] training loss: 0.936941\n",
      "epoch [850] training loss: 0.936900\n",
      "epoch [851] training loss: 0.936859\n",
      "epoch [852] training loss: 0.936817\n",
      "epoch [853] training loss: 0.936776\n",
      "epoch [854] training loss: 0.936735\n",
      "epoch [855] training loss: 0.936695\n",
      "epoch [856] training loss: 0.936654\n",
      "epoch [857] training loss: 0.936613\n",
      "epoch [858] training loss: 0.936573\n",
      "epoch [859] training loss: 0.936532\n",
      "epoch [860] training loss: 0.936492\n",
      "epoch [861] training loss: 0.936451\n",
      "epoch [862] training loss: 0.936411\n",
      "epoch [863] training loss: 0.936371\n",
      "epoch [864] training loss: 0.936331\n",
      "epoch [865] training loss: 0.936291\n",
      "epoch [866] training loss: 0.936251\n",
      "epoch [867] training loss: 0.936211\n",
      "epoch [868] training loss: 0.936172\n",
      "epoch [869] training loss: 0.936132\n",
      "epoch [870] training loss: 0.936093\n",
      "epoch [871] training loss: 0.936053\n",
      "epoch [872] training loss: 0.936014\n",
      "epoch [873] training loss: 0.935975\n",
      "epoch [874] training loss: 0.935936\n",
      "epoch [875] training loss: 0.935897\n",
      "epoch [876] training loss: 0.935858\n",
      "epoch [877] training loss: 0.935819\n",
      "epoch [878] training loss: 0.935780\n",
      "epoch [879] training loss: 0.935741\n",
      "epoch [880] training loss: 0.935703\n",
      "epoch [881] training loss: 0.935664\n",
      "epoch [882] training loss: 0.935626\n",
      "epoch [883] training loss: 0.935587\n",
      "epoch [884] training loss: 0.935549\n",
      "epoch [885] training loss: 0.935511\n",
      "epoch [886] training loss: 0.935473\n",
      "epoch [887] training loss: 0.935435\n",
      "epoch [888] training loss: 0.935397\n",
      "epoch [889] training loss: 0.935359\n",
      "epoch [890] training loss: 0.935321\n",
      "epoch [891] training loss: 0.935284\n",
      "epoch [892] training loss: 0.935246\n",
      "epoch [893] training loss: 0.935209\n",
      "epoch [894] training loss: 0.935171\n",
      "epoch [895] training loss: 0.935134\n",
      "epoch [896] training loss: 0.935097\n",
      "epoch [897] training loss: 0.935059\n",
      "epoch [898] training loss: 0.935022\n",
      "epoch [899] training loss: 0.934985\n",
      "epoch [900] training loss: 0.934948\n",
      "epoch [901] training loss: 0.934911\n",
      "epoch [902] training loss: 0.934875\n",
      "epoch [903] training loss: 0.934838\n",
      "epoch [904] training loss: 0.934801\n",
      "epoch [905] training loss: 0.934765\n",
      "epoch [906] training loss: 0.934728\n",
      "epoch [907] training loss: 0.934692\n",
      "epoch [908] training loss: 0.934656\n",
      "epoch [909] training loss: 0.934620\n",
      "epoch [910] training loss: 0.934583\n",
      "epoch [911] training loss: 0.934547\n",
      "epoch [912] training loss: 0.934511\n",
      "epoch [913] training loss: 0.934476\n",
      "epoch [914] training loss: 0.934440\n",
      "epoch [915] training loss: 0.934404\n",
      "epoch [916] training loss: 0.934368\n",
      "epoch [917] training loss: 0.934333\n",
      "epoch [918] training loss: 0.934297\n",
      "epoch [919] training loss: 0.934262\n",
      "epoch [920] training loss: 0.934227\n",
      "epoch [921] training loss: 0.934191\n",
      "epoch [922] training loss: 0.934156\n",
      "epoch [923] training loss: 0.934121\n",
      "epoch [924] training loss: 0.934086\n",
      "epoch [925] training loss: 0.934051\n",
      "epoch [926] training loss: 0.934016\n",
      "epoch [927] training loss: 0.933981\n",
      "epoch [928] training loss: 0.933946\n",
      "epoch [929] training loss: 0.933912\n",
      "epoch [930] training loss: 0.933877\n",
      "epoch [931] training loss: 0.933843\n",
      "epoch [932] training loss: 0.933808\n",
      "epoch [933] training loss: 0.933774\n",
      "epoch [934] training loss: 0.933740\n",
      "epoch [935] training loss: 0.933705\n",
      "epoch [936] training loss: 0.933671\n",
      "epoch [937] training loss: 0.933637\n",
      "epoch [938] training loss: 0.933603\n",
      "epoch [939] training loss: 0.933569\n",
      "epoch [940] training loss: 0.933535\n",
      "epoch [941] training loss: 0.933501\n",
      "epoch [942] training loss: 0.933468\n",
      "epoch [943] training loss: 0.933434\n",
      "epoch [944] training loss: 0.933400\n",
      "epoch [945] training loss: 0.933367\n",
      "epoch [946] training loss: 0.933334\n",
      "epoch [947] training loss: 0.933300\n",
      "epoch [948] training loss: 0.933267\n",
      "epoch [949] training loss: 0.933234\n",
      "epoch [950] training loss: 0.933200\n",
      "epoch [951] training loss: 0.933167\n",
      "epoch [952] training loss: 0.933134\n",
      "epoch [953] training loss: 0.933101\n",
      "epoch [954] training loss: 0.933068\n",
      "epoch [955] training loss: 0.933036\n",
      "epoch [956] training loss: 0.933003\n",
      "epoch [957] training loss: 0.932970\n",
      "epoch [958] training loss: 0.932938\n",
      "epoch [959] training loss: 0.932905\n",
      "epoch [960] training loss: 0.932873\n",
      "epoch [961] training loss: 0.932840\n",
      "epoch [962] training loss: 0.932808\n",
      "epoch [963] training loss: 0.932776\n",
      "epoch [964] training loss: 0.932743\n",
      "epoch [965] training loss: 0.932711\n",
      "epoch [966] training loss: 0.932679\n",
      "epoch [967] training loss: 0.932647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [968] training loss: 0.932615\n",
      "epoch [969] training loss: 0.932583\n",
      "epoch [970] training loss: 0.932552\n",
      "epoch [971] training loss: 0.932520\n",
      "epoch [972] training loss: 0.932488\n",
      "epoch [973] training loss: 0.932456\n",
      "epoch [974] training loss: 0.932425\n",
      "epoch [975] training loss: 0.932393\n",
      "epoch [976] training loss: 0.932362\n",
      "epoch [977] training loss: 0.932331\n",
      "epoch [978] training loss: 0.932299\n",
      "epoch [979] training loss: 0.932268\n",
      "epoch [980] training loss: 0.932237\n",
      "epoch [981] training loss: 0.932206\n",
      "epoch [982] training loss: 0.932175\n",
      "epoch [983] training loss: 0.932144\n",
      "epoch [984] training loss: 0.932113\n",
      "epoch [985] training loss: 0.932082\n",
      "epoch [986] training loss: 0.932051\n",
      "epoch [987] training loss: 0.932021\n",
      "epoch [988] training loss: 0.931990\n",
      "epoch [989] training loss: 0.931959\n",
      "epoch [990] training loss: 0.931929\n",
      "epoch [991] training loss: 0.931898\n",
      "epoch [992] training loss: 0.931868\n",
      "epoch [993] training loss: 0.931837\n",
      "epoch [994] training loss: 0.931807\n",
      "epoch [995] training loss: 0.931777\n",
      "epoch [996] training loss: 0.931747\n",
      "epoch [997] training loss: 0.931717\n",
      "epoch [998] training loss: 0.931687\n",
      "epoch [999] training loss: 0.931657\n",
      "epoch [1000] training loss: 0.931627\n",
      "epoch [1001] training loss: 0.931597\n",
      "epoch [1002] training loss: 0.931567\n",
      "epoch [1003] training loss: 0.931537\n",
      "epoch [1004] training loss: 0.931508\n",
      "epoch [1005] training loss: 0.931478\n",
      "epoch [1006] training loss: 0.931448\n",
      "epoch [1007] training loss: 0.931419\n",
      "epoch [1008] training loss: 0.931389\n",
      "epoch [1009] training loss: 0.931360\n",
      "epoch [1010] training loss: 0.931331\n",
      "epoch [1011] training loss: 0.931301\n",
      "epoch [1012] training loss: 0.931272\n",
      "epoch [1013] training loss: 0.931243\n",
      "epoch [1014] training loss: 0.931214\n",
      "epoch [1015] training loss: 0.931185\n",
      "epoch [1016] training loss: 0.931156\n",
      "epoch [1017] training loss: 0.931127\n",
      "epoch [1018] training loss: 0.931098\n",
      "epoch [1019] training loss: 0.931069\n",
      "epoch [1020] training loss: 0.931040\n",
      "epoch [1021] training loss: 0.931012\n",
      "epoch [1022] training loss: 0.930983\n",
      "epoch [1023] training loss: 0.930954\n",
      "epoch [1024] training loss: 0.930926\n",
      "epoch [1025] training loss: 0.930897\n",
      "epoch [1026] training loss: 0.930869\n",
      "epoch [1027] training loss: 0.930840\n",
      "epoch [1028] training loss: 0.930812\n",
      "epoch [1029] training loss: 0.930784\n",
      "epoch [1030] training loss: 0.930756\n",
      "epoch [1031] training loss: 0.930727\n",
      "epoch [1032] training loss: 0.930699\n",
      "epoch [1033] training loss: 0.930671\n",
      "epoch [1034] training loss: 0.930643\n",
      "epoch [1035] training loss: 0.930615\n",
      "epoch [1036] training loss: 0.930587\n",
      "epoch [1037] training loss: 0.930560\n",
      "epoch [1038] training loss: 0.930532\n",
      "epoch [1039] training loss: 0.930504\n",
      "epoch [1040] training loss: 0.930476\n",
      "epoch [1041] training loss: 0.930449\n",
      "epoch [1042] training loss: 0.930421\n",
      "epoch [1043] training loss: 0.930394\n",
      "epoch [1044] training loss: 0.930366\n",
      "epoch [1045] training loss: 0.930339\n",
      "epoch [1046] training loss: 0.930311\n",
      "epoch [1047] training loss: 0.930284\n",
      "epoch [1048] training loss: 0.930257\n",
      "epoch [1049] training loss: 0.930230\n",
      "epoch [1050] training loss: 0.930203\n",
      "epoch [1051] training loss: 0.930175\n",
      "epoch [1052] training loss: 0.930148\n",
      "epoch [1053] training loss: 0.930121\n",
      "epoch [1054] training loss: 0.930094\n",
      "epoch [1055] training loss: 0.930068\n",
      "epoch [1056] training loss: 0.930041\n",
      "epoch [1057] training loss: 0.930014\n",
      "epoch [1058] training loss: 0.929987\n",
      "epoch [1059] training loss: 0.929960\n",
      "epoch [1060] training loss: 0.929934\n",
      "epoch [1061] training loss: 0.929907\n",
      "epoch [1062] training loss: 0.929881\n",
      "epoch [1063] training loss: 0.929854\n",
      "epoch [1064] training loss: 0.929828\n",
      "epoch [1065] training loss: 0.929801\n",
      "epoch [1066] training loss: 0.929775\n",
      "epoch [1067] training loss: 0.929749\n",
      "epoch [1068] training loss: 0.929722\n",
      "epoch [1069] training loss: 0.929696\n",
      "epoch [1070] training loss: 0.929670\n",
      "epoch [1071] training loss: 0.929644\n",
      "epoch [1072] training loss: 0.929618\n",
      "epoch [1073] training loss: 0.929592\n",
      "epoch [1074] training loss: 0.929566\n",
      "epoch [1075] training loss: 0.929540\n",
      "epoch [1076] training loss: 0.929514\n",
      "epoch [1077] training loss: 0.929488\n",
      "epoch [1078] training loss: 0.929462\n",
      "epoch [1079] training loss: 0.929437\n",
      "epoch [1080] training loss: 0.929411\n",
      "epoch [1081] training loss: 0.929385\n",
      "epoch [1082] training loss: 0.929360\n",
      "epoch [1083] training loss: 0.929334\n",
      "epoch [1084] training loss: 0.929309\n",
      "epoch [1085] training loss: 0.929283\n",
      "epoch [1086] training loss: 0.929258\n",
      "epoch [1087] training loss: 0.929233\n",
      "epoch [1088] training loss: 0.929207\n",
      "epoch [1089] training loss: 0.929182\n",
      "epoch [1090] training loss: 0.929157\n",
      "epoch [1091] training loss: 0.929132\n",
      "epoch [1092] training loss: 0.929107\n",
      "epoch [1093] training loss: 0.929082\n",
      "epoch [1094] training loss: 0.929057\n",
      "epoch [1095] training loss: 0.929032\n",
      "epoch [1096] training loss: 0.929007\n",
      "epoch [1097] training loss: 0.928982\n",
      "epoch [1098] training loss: 0.928957\n",
      "epoch [1099] training loss: 0.928932\n",
      "epoch [1100] training loss: 0.928907\n",
      "epoch [1101] training loss: 0.928883\n",
      "epoch [1102] training loss: 0.928858\n",
      "epoch [1103] training loss: 0.928833\n",
      "epoch [1104] training loss: 0.928809\n",
      "epoch [1105] training loss: 0.928784\n",
      "epoch [1106] training loss: 0.928760\n",
      "epoch [1107] training loss: 0.928735\n",
      "epoch [1108] training loss: 0.928711\n",
      "epoch [1109] training loss: 0.928686\n",
      "epoch [1110] training loss: 0.928662\n",
      "epoch [1111] training loss: 0.928638\n",
      "epoch [1112] training loss: 0.928614\n",
      "epoch [1113] training loss: 0.928589\n",
      "epoch [1114] training loss: 0.928565\n",
      "epoch [1115] training loss: 0.928541\n",
      "epoch [1116] training loss: 0.928517\n",
      "epoch [1117] training loss: 0.928493\n",
      "epoch [1118] training loss: 0.928469\n",
      "epoch [1119] training loss: 0.928445\n",
      "epoch [1120] training loss: 0.928421\n",
      "epoch [1121] training loss: 0.928397\n",
      "epoch [1122] training loss: 0.928374\n",
      "epoch [1123] training loss: 0.928350\n",
      "epoch [1124] training loss: 0.928326\n",
      "epoch [1125] training loss: 0.928302\n",
      "epoch [1126] training loss: 0.928279\n",
      "epoch [1127] training loss: 0.928255\n",
      "epoch [1128] training loss: 0.928232\n",
      "epoch [1129] training loss: 0.928208\n",
      "epoch [1130] training loss: 0.928185\n",
      "epoch [1131] training loss: 0.928161\n",
      "epoch [1132] training loss: 0.928138\n",
      "epoch [1133] training loss: 0.928114\n",
      "epoch [1134] training loss: 0.928091\n",
      "epoch [1135] training loss: 0.928068\n",
      "epoch [1136] training loss: 0.928045\n",
      "epoch [1137] training loss: 0.928021\n",
      "epoch [1138] training loss: 0.927998\n",
      "epoch [1139] training loss: 0.927975\n",
      "epoch [1140] training loss: 0.927952\n",
      "epoch [1141] training loss: 0.927929\n",
      "epoch [1142] training loss: 0.927906\n",
      "epoch [1143] training loss: 0.927883\n",
      "epoch [1144] training loss: 0.927860\n",
      "epoch [1145] training loss: 0.927837\n",
      "epoch [1146] training loss: 0.927814\n",
      "epoch [1147] training loss: 0.927792\n",
      "epoch [1148] training loss: 0.927769\n",
      "epoch [1149] training loss: 0.927746\n",
      "epoch [1150] training loss: 0.927724\n",
      "epoch [1151] training loss: 0.927701\n",
      "epoch [1152] training loss: 0.927678\n",
      "epoch [1153] training loss: 0.927656\n",
      "epoch [1154] training loss: 0.927633\n",
      "epoch [1155] training loss: 0.927611\n",
      "epoch [1156] training loss: 0.927588\n",
      "epoch [1157] training loss: 0.927566\n",
      "epoch [1158] training loss: 0.927543\n",
      "epoch [1159] training loss: 0.927521\n",
      "epoch [1160] training loss: 0.927499\n",
      "epoch [1161] training loss: 0.927477\n",
      "epoch [1162] training loss: 0.927454\n",
      "epoch [1163] training loss: 0.927432\n",
      "epoch [1164] training loss: 0.927410\n",
      "epoch [1165] training loss: 0.927388\n",
      "epoch [1166] training loss: 0.927366\n",
      "epoch [1167] training loss: 0.927344\n",
      "epoch [1168] training loss: 0.927322\n",
      "epoch [1169] training loss: 0.927300\n",
      "epoch [1170] training loss: 0.927278\n",
      "epoch [1171] training loss: 0.927256\n",
      "epoch [1172] training loss: 0.927234\n",
      "epoch [1173] training loss: 0.927212\n",
      "epoch [1174] training loss: 0.927191\n",
      "epoch [1175] training loss: 0.927169\n",
      "epoch [1176] training loss: 0.927147\n",
      "epoch [1177] training loss: 0.927125\n",
      "epoch [1178] training loss: 0.927104\n",
      "epoch [1179] training loss: 0.927082\n",
      "epoch [1180] training loss: 0.927061\n",
      "epoch [1181] training loss: 0.927039\n",
      "epoch [1182] training loss: 0.927018\n",
      "epoch [1183] training loss: 0.926996\n",
      "epoch [1184] training loss: 0.926975\n",
      "epoch [1185] training loss: 0.926953\n",
      "epoch [1186] training loss: 0.926932\n",
      "epoch [1187] training loss: 0.926911\n",
      "epoch [1188] training loss: 0.926889\n",
      "epoch [1189] training loss: 0.926868\n",
      "epoch [1190] training loss: 0.926847\n",
      "epoch [1191] training loss: 0.926826\n",
      "epoch [1192] training loss: 0.926805\n",
      "epoch [1193] training loss: 0.926783\n",
      "epoch [1194] training loss: 0.926762\n",
      "epoch [1195] training loss: 0.926741\n",
      "epoch [1196] training loss: 0.926720\n",
      "epoch [1197] training loss: 0.926699\n",
      "epoch [1198] training loss: 0.926678\n",
      "epoch [1199] training loss: 0.926657\n",
      "epoch [1200] training loss: 0.926637\n",
      "epoch [1201] training loss: 0.926616\n",
      "epoch [1202] training loss: 0.926595\n",
      "epoch [1203] training loss: 0.926574\n",
      "epoch [1204] training loss: 0.926553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1205] training loss: 0.926533\n",
      "epoch [1206] training loss: 0.926512\n",
      "epoch [1207] training loss: 0.926491\n",
      "epoch [1208] training loss: 0.926471\n",
      "epoch [1209] training loss: 0.926450\n",
      "epoch [1210] training loss: 0.926430\n",
      "epoch [1211] training loss: 0.926409\n",
      "epoch [1212] training loss: 0.926389\n",
      "epoch [1213] training loss: 0.926368\n",
      "epoch [1214] training loss: 0.926348\n",
      "epoch [1215] training loss: 0.926328\n",
      "epoch [1216] training loss: 0.926307\n",
      "epoch [1217] training loss: 0.926287\n",
      "epoch [1218] training loss: 0.926267\n",
      "epoch [1219] training loss: 0.926246\n",
      "epoch [1220] training loss: 0.926226\n",
      "epoch [1221] training loss: 0.926206\n",
      "epoch [1222] training loss: 0.926186\n",
      "epoch [1223] training loss: 0.926166\n",
      "epoch [1224] training loss: 0.926146\n",
      "epoch [1225] training loss: 0.926126\n",
      "epoch [1226] training loss: 0.926106\n",
      "epoch [1227] training loss: 0.926086\n",
      "epoch [1228] training loss: 0.926066\n",
      "epoch [1229] training loss: 0.926046\n",
      "epoch [1230] training loss: 0.926026\n",
      "epoch [1231] training loss: 0.926006\n",
      "epoch [1232] training loss: 0.925986\n",
      "epoch [1233] training loss: 0.925966\n",
      "epoch [1234] training loss: 0.925946\n",
      "epoch [1235] training loss: 0.925927\n",
      "epoch [1236] training loss: 0.925907\n",
      "epoch [1237] training loss: 0.925887\n",
      "epoch [1238] training loss: 0.925868\n",
      "epoch [1239] training loss: 0.925848\n",
      "epoch [1240] training loss: 0.925828\n",
      "epoch [1241] training loss: 0.925809\n",
      "epoch [1242] training loss: 0.925789\n",
      "epoch [1243] training loss: 0.925770\n",
      "epoch [1244] training loss: 0.925750\n",
      "epoch [1245] training loss: 0.925731\n",
      "epoch [1246] training loss: 0.925712\n",
      "epoch [1247] training loss: 0.925692\n",
      "epoch [1248] training loss: 0.925673\n",
      "epoch [1249] training loss: 0.925654\n",
      "epoch [1250] training loss: 0.925634\n",
      "epoch [1251] training loss: 0.925615\n",
      "epoch [1252] training loss: 0.925596\n",
      "epoch [1253] training loss: 0.925577\n",
      "epoch [1254] training loss: 0.925557\n",
      "epoch [1255] training loss: 0.925538\n",
      "epoch [1256] training loss: 0.925519\n",
      "epoch [1257] training loss: 0.925500\n",
      "epoch [1258] training loss: 0.925481\n",
      "epoch [1259] training loss: 0.925462\n",
      "epoch [1260] training loss: 0.925443\n",
      "epoch [1261] training loss: 0.925424\n",
      "epoch [1262] training loss: 0.925405\n",
      "epoch [1263] training loss: 0.925386\n",
      "epoch [1264] training loss: 0.925367\n",
      "epoch [1265] training loss: 0.925348\n",
      "epoch [1266] training loss: 0.925330\n",
      "epoch [1267] training loss: 0.925311\n",
      "epoch [1268] training loss: 0.925292\n",
      "epoch [1269] training loss: 0.925273\n",
      "epoch [1270] training loss: 0.925255\n",
      "epoch [1271] training loss: 0.925236\n",
      "epoch [1272] training loss: 0.925217\n",
      "epoch [1273] training loss: 0.925199\n",
      "epoch [1274] training loss: 0.925180\n",
      "epoch [1275] training loss: 0.925162\n",
      "epoch [1276] training loss: 0.925143\n",
      "epoch [1277] training loss: 0.925124\n",
      "epoch [1278] training loss: 0.925106\n",
      "epoch [1279] training loss: 0.925088\n",
      "epoch [1280] training loss: 0.925069\n",
      "epoch [1281] training loss: 0.925051\n",
      "epoch [1282] training loss: 0.925032\n",
      "epoch [1283] training loss: 0.925014\n",
      "epoch [1284] training loss: 0.924996\n",
      "epoch [1285] training loss: 0.924977\n",
      "epoch [1286] training loss: 0.924959\n",
      "epoch [1287] training loss: 0.924941\n",
      "epoch [1288] training loss: 0.924923\n",
      "epoch [1289] training loss: 0.924904\n",
      "epoch [1290] training loss: 0.924886\n",
      "epoch [1291] training loss: 0.924868\n",
      "epoch [1292] training loss: 0.924850\n",
      "epoch [1293] training loss: 0.924832\n",
      "epoch [1294] training loss: 0.924814\n",
      "epoch [1295] training loss: 0.924796\n",
      "epoch [1296] training loss: 0.924778\n",
      "epoch [1297] training loss: 0.924760\n",
      "epoch [1298] training loss: 0.924742\n",
      "epoch [1299] training loss: 0.924724\n",
      "epoch [1300] training loss: 0.924706\n",
      "epoch [1301] training loss: 0.924688\n",
      "epoch [1302] training loss: 0.924671\n",
      "epoch [1303] training loss: 0.924653\n",
      "epoch [1304] training loss: 0.924635\n",
      "epoch [1305] training loss: 0.924617\n",
      "epoch [1306] training loss: 0.924599\n",
      "epoch [1307] training loss: 0.924582\n",
      "epoch [1308] training loss: 0.924564\n",
      "epoch [1309] training loss: 0.924546\n",
      "epoch [1310] training loss: 0.924529\n",
      "epoch [1311] training loss: 0.924511\n",
      "epoch [1312] training loss: 0.924494\n",
      "epoch [1313] training loss: 0.924476\n",
      "epoch [1314] training loss: 0.924459\n",
      "epoch [1315] training loss: 0.924441\n",
      "epoch [1316] training loss: 0.924424\n",
      "epoch [1317] training loss: 0.924406\n",
      "epoch [1318] training loss: 0.924389\n",
      "epoch [1319] training loss: 0.924371\n",
      "epoch [1320] training loss: 0.924354\n",
      "epoch [1321] training loss: 0.924337\n",
      "epoch [1322] training loss: 0.924319\n",
      "epoch [1323] training loss: 0.924302\n",
      "epoch [1324] training loss: 0.924285\n",
      "epoch [1325] training loss: 0.924267\n",
      "epoch [1326] training loss: 0.924250\n",
      "epoch [1327] training loss: 0.924233\n",
      "epoch [1328] training loss: 0.924216\n",
      "epoch [1329] training loss: 0.924199\n",
      "epoch [1330] training loss: 0.924181\n",
      "epoch [1331] training loss: 0.924164\n",
      "epoch [1332] training loss: 0.924147\n",
      "epoch [1333] training loss: 0.924130\n",
      "epoch [1334] training loss: 0.924113\n",
      "epoch [1335] training loss: 0.924096\n",
      "epoch [1336] training loss: 0.924079\n",
      "epoch [1337] training loss: 0.924062\n",
      "epoch [1338] training loss: 0.924045\n",
      "epoch [1339] training loss: 0.924028\n",
      "epoch [1340] training loss: 0.924012\n",
      "epoch [1341] training loss: 0.923995\n",
      "epoch [1342] training loss: 0.923978\n",
      "epoch [1343] training loss: 0.923961\n",
      "epoch [1344] training loss: 0.923944\n",
      "epoch [1345] training loss: 0.923927\n",
      "epoch [1346] training loss: 0.923911\n",
      "epoch [1347] training loss: 0.923894\n",
      "epoch [1348] training loss: 0.923877\n",
      "epoch [1349] training loss: 0.923861\n",
      "epoch [1350] training loss: 0.923844\n",
      "epoch [1351] training loss: 0.923827\n",
      "epoch [1352] training loss: 0.923811\n",
      "epoch [1353] training loss: 0.923794\n",
      "epoch [1354] training loss: 0.923778\n",
      "epoch [1355] training loss: 0.923761\n",
      "epoch [1356] training loss: 0.923745\n",
      "epoch [1357] training loss: 0.923728\n",
      "epoch [1358] training loss: 0.923712\n",
      "epoch [1359] training loss: 0.923695\n",
      "epoch [1360] training loss: 0.923679\n",
      "epoch [1361] training loss: 0.923662\n",
      "epoch [1362] training loss: 0.923646\n",
      "epoch [1363] training loss: 0.923630\n",
      "epoch [1364] training loss: 0.923613\n",
      "epoch [1365] training loss: 0.923597\n",
      "epoch [1366] training loss: 0.923581\n",
      "epoch [1367] training loss: 0.923565\n",
      "epoch [1368] training loss: 0.923548\n",
      "epoch [1369] training loss: 0.923532\n",
      "epoch [1370] training loss: 0.923516\n",
      "epoch [1371] training loss: 0.923500\n",
      "epoch [1372] training loss: 0.923484\n",
      "epoch [1373] training loss: 0.923468\n",
      "epoch [1374] training loss: 0.923451\n",
      "epoch [1375] training loss: 0.923435\n",
      "epoch [1376] training loss: 0.923419\n",
      "epoch [1377] training loss: 0.923403\n",
      "epoch [1378] training loss: 0.923387\n",
      "epoch [1379] training loss: 0.923371\n",
      "epoch [1380] training loss: 0.923355\n",
      "epoch [1381] training loss: 0.923339\n",
      "epoch [1382] training loss: 0.923323\n",
      "epoch [1383] training loss: 0.923308\n",
      "epoch [1384] training loss: 0.923292\n",
      "epoch [1385] training loss: 0.923276\n",
      "epoch [1386] training loss: 0.923260\n",
      "epoch [1387] training loss: 0.923244\n",
      "epoch [1388] training loss: 0.923228\n",
      "epoch [1389] training loss: 0.923213\n",
      "epoch [1390] training loss: 0.923197\n",
      "epoch [1391] training loss: 0.923181\n",
      "epoch [1392] training loss: 0.923165\n",
      "epoch [1393] training loss: 0.923150\n",
      "epoch [1394] training loss: 0.923134\n",
      "epoch [1395] training loss: 0.923118\n",
      "epoch [1396] training loss: 0.923103\n",
      "epoch [1397] training loss: 0.923087\n",
      "epoch [1398] training loss: 0.923072\n",
      "epoch [1399] training loss: 0.923056\n",
      "epoch [1400] training loss: 0.923041\n",
      "epoch [1401] training loss: 0.923025\n",
      "epoch [1402] training loss: 0.923010\n",
      "epoch [1403] training loss: 0.922994\n",
      "epoch [1404] training loss: 0.922979\n",
      "epoch [1405] training loss: 0.922963\n",
      "epoch [1406] training loss: 0.922948\n",
      "epoch [1407] training loss: 0.922932\n",
      "epoch [1408] training loss: 0.922917\n",
      "epoch [1409] training loss: 0.922902\n",
      "epoch [1410] training loss: 0.922886\n",
      "epoch [1411] training loss: 0.922871\n",
      "epoch [1412] training loss: 0.922856\n",
      "epoch [1413] training loss: 0.922840\n",
      "epoch [1414] training loss: 0.922825\n",
      "epoch [1415] training loss: 0.922810\n",
      "epoch [1416] training loss: 0.922795\n",
      "epoch [1417] training loss: 0.922780\n",
      "epoch [1418] training loss: 0.922764\n",
      "epoch [1419] training loss: 0.922749\n",
      "epoch [1420] training loss: 0.922734\n",
      "epoch [1421] training loss: 0.922719\n",
      "epoch [1422] training loss: 0.922704\n",
      "epoch [1423] training loss: 0.922689\n",
      "epoch [1424] training loss: 0.922674\n",
      "epoch [1425] training loss: 0.922659\n",
      "epoch [1426] training loss: 0.922644\n",
      "epoch [1427] training loss: 0.922629\n",
      "epoch [1428] training loss: 0.922614\n",
      "epoch [1429] training loss: 0.922599\n",
      "epoch [1430] training loss: 0.922584\n",
      "epoch [1431] training loss: 0.922569\n",
      "epoch [1432] training loss: 0.922554\n",
      "epoch [1433] training loss: 0.922539\n",
      "epoch [1434] training loss: 0.922525\n",
      "epoch [1435] training loss: 0.922510\n",
      "epoch [1436] training loss: 0.922495\n",
      "epoch [1437] training loss: 0.922480\n",
      "epoch [1438] training loss: 0.922465\n",
      "epoch [1439] training loss: 0.922451\n",
      "epoch [1440] training loss: 0.922436\n",
      "epoch [1441] training loss: 0.922421\n",
      "epoch [1442] training loss: 0.922407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1443] training loss: 0.922392\n",
      "epoch [1444] training loss: 0.922377\n",
      "epoch [1445] training loss: 0.922363\n",
      "epoch [1446] training loss: 0.922348\n",
      "epoch [1447] training loss: 0.922333\n",
      "epoch [1448] training loss: 0.922319\n",
      "epoch [1449] training loss: 0.922304\n",
      "epoch [1450] training loss: 0.922290\n",
      "epoch [1451] training loss: 0.922275\n",
      "epoch [1452] training loss: 0.922261\n",
      "epoch [1453] training loss: 0.922246\n",
      "epoch [1454] training loss: 0.922232\n",
      "epoch [1455] training loss: 0.922217\n",
      "epoch [1456] training loss: 0.922203\n",
      "epoch [1457] training loss: 0.922189\n",
      "epoch [1458] training loss: 0.922174\n",
      "epoch [1459] training loss: 0.922160\n",
      "epoch [1460] training loss: 0.922145\n",
      "epoch [1461] training loss: 0.922131\n",
      "epoch [1462] training loss: 0.922117\n",
      "epoch [1463] training loss: 0.922103\n",
      "epoch [1464] training loss: 0.922088\n",
      "epoch [1465] training loss: 0.922074\n",
      "epoch [1466] training loss: 0.922060\n",
      "epoch [1467] training loss: 0.922046\n",
      "epoch [1468] training loss: 0.922031\n",
      "epoch [1469] training loss: 0.922017\n",
      "epoch [1470] training loss: 0.922003\n",
      "epoch [1471] training loss: 0.921989\n",
      "epoch [1472] training loss: 0.921975\n",
      "epoch [1473] training loss: 0.921961\n",
      "epoch [1474] training loss: 0.921947\n",
      "epoch [1475] training loss: 0.921933\n",
      "epoch [1476] training loss: 0.921918\n",
      "epoch [1477] training loss: 0.921904\n",
      "epoch [1478] training loss: 0.921890\n",
      "epoch [1479] training loss: 0.921876\n",
      "epoch [1480] training loss: 0.921862\n",
      "epoch [1481] training loss: 0.921848\n",
      "epoch [1482] training loss: 0.921835\n",
      "epoch [1483] training loss: 0.921821\n",
      "epoch [1484] training loss: 0.921807\n",
      "epoch [1485] training loss: 0.921793\n",
      "epoch [1486] training loss: 0.921779\n",
      "epoch [1487] training loss: 0.921765\n",
      "epoch [1488] training loss: 0.921751\n",
      "epoch [1489] training loss: 0.921737\n",
      "epoch [1490] training loss: 0.921724\n",
      "epoch [1491] training loss: 0.921710\n",
      "epoch [1492] training loss: 0.921696\n",
      "epoch [1493] training loss: 0.921682\n",
      "epoch [1494] training loss: 0.921669\n",
      "epoch [1495] training loss: 0.921655\n",
      "epoch [1496] training loss: 0.921641\n",
      "epoch [1497] training loss: 0.921628\n",
      "epoch [1498] training loss: 0.921614\n",
      "epoch [1499] training loss: 0.921600\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(x_train.shape[1],y_train.shape[1],lr = 0.11)\n",
    "for epoch in range(1500):\n",
    "    out = classifier.forward(x_train)\n",
    "    loss = Xent(y_train,out)\n",
    "    print(f\"epoch [{epoch}] training loss: {loss:.6f}\")\n",
    "    classifier.backward(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34c29a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5654e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.908362\n",
      "Test Accuracy : 0.6257\n",
      "Test Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.62      0.77      0.69        62\n",
      "           3       0.64      0.62      0.63        81\n",
      "           4       0.60      0.38      0.46        24\n",
      "           5       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63       171\n",
      "   macro avg       0.37      0.35      0.36       171\n",
      "weighted avg       0.61      0.63      0.61       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = classifier.forward(x_test)\n",
    "loss = Xent(y_test,out)\n",
    "predicted = np.argmax(out,axis = 1)\n",
    "actual = np.argmax(y_test,axis = 1)\n",
    "metrics = classification_report(actual,predicted,zero_division=0)\n",
    "accuracy = accuracy_score(actual,predicted)\n",
    "print(f\"Test Loss: {loss:.6f}\")\n",
    "print(f\"Test Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Test Metrics\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
