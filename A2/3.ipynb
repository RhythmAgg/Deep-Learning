{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035ad390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d37cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "#         self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.start = 26\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = torch.tensor([self.start]+[ord(c) - ord('a') for c in self.data.iloc[idx, 0]])\n",
    "#         input_sentence = self.embedding(input_sentence)\n",
    "        \n",
    "        target_sentence = torch.tensor([self.start]+[ord(c) - ord('a') for c in self.data.iloc[idx, 1]])\n",
    "        target_sentence_encoded = torch.zeros(target_sentence.shape[0],27)\n",
    "        for idx in range(target_sentence.shape[0]):\n",
    "            target_sentence_encoded[idx][target_sentence[idx]] = 1.0\n",
    "#         target_sentence_ = self.embedding(target_sentence)\n",
    "        return (input_sentence,target_sentence,target_sentence_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58dd6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DataLoader(CustomDataset(\"Data/train_data.csv\"))\n",
    "training_source = []\n",
    "training_target = []\n",
    "training_target_encoded = []\n",
    "for input,target,target_encoded in training_data:\n",
    "    training_source.append(input)\n",
    "    training_target.append(target)\n",
    "    training_target_encoded.append(target_encoded)\n",
    "#     print(x.shape)\n",
    "training_source = torch.cat(training_source,dim=0)[:,1:]\n",
    "training_target = torch.cat(training_target,dim=0)\n",
    "training_target_encoded = torch.cat(training_target_encoded,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b82990",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataLoader(CustomDataset(\"Data/eval_data.csv\"))\n",
    "test_source = []\n",
    "test_target = []\n",
    "test_target_encoded = []\n",
    "for input,target,test_encoded in test_data:\n",
    "    test_source.append(input)\n",
    "    test_target.append(target)\n",
    "    test_target_encoded.append(test_encoded)\n",
    "#     print(x.shape)\n",
    "test_source = torch.cat(test_source,dim=0)[:,1:]\n",
    "test_target = torch.cat(test_target,dim=0)\n",
    "test_target_encoded = torch.cat(test_target_encoded,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9c5e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 8]) torch.Size([2000, 9]) torch.Size([2000, 9, 27])\n",
      "torch.Size([7000, 8]) torch.Size([7000, 9]) torch.Size([7000, 9, 27])\n"
     ]
    }
   ],
   "source": [
    "print(test_source.shape,test_target.shape,test_target_encoded.shape)\n",
    "print(training_source.shape,training_target.shape,training_target_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80467bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,bidirectional = 0):\n",
    "        super(Encoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(26,input_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.RNN(input_size, hidden_size,num_layers,batch_first = True,bidirectional=bool(self.bidirectional))\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        input_seq = self.embedding(input_)\n",
    "        hidden = torch.zeros((self.bidirectional+1)*self.num_layers,input_seq.shape[0],self.hidden_size)\n",
    "        _, encoder_hidden = self.encoder(input_seq,hidden)\n",
    "        if(self.bidirectional == 1):\n",
    "            encoder_hidden = torch.cat((encoder_hidden[0::2],encoder_hidden[1::2]),dim=2)\n",
    "        return encoder_hidden\n",
    "    \n",
    "    def predict(self,input_str):\n",
    "        input_sentence = torch.tensor([ord(c) - ord('a') for c in input_str])\n",
    "        input_seq = self.embedding(input_sentence)\n",
    "        hidden = torch.zeros((self.bidirectional+1)*self.num_layers,self.hidden_size)\n",
    "        _, encoder_hidden = self.encoder(input_seq,hidden)\n",
    "        if(self.bidirectional == 1):\n",
    "            encoder_hidden = torch.cat((encoder_hidden[::2],encoder_hidden[1::2]),dim=1)\n",
    "        return encoder_hidden\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size,num_layers,bidirectional = 0):\n",
    "        super(Decoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(27,input_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.sequence_len = 9\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.dec_cells = nn.ModuleList([nn.RNNCell((bidirectional+1)*hidden_size+input_size, (bidirectional+1)*hidden_size)])\n",
    "            \n",
    "        self.decoder = nn.GRU((bidirectional+1)*hidden_size+input_size, (bidirectional+1)*hidden_size,batch_first = True)\n",
    "        self.linear = nn.Linear((bidirectional+1)*hidden_size,26)\n",
    "        self.output_layer = nn.LogSoftmax(dim = 2)\n",
    "        self.output_layer_timestep = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, context, target_,teacher_ratio):\n",
    "        target_seq = self.embedding(target_)\n",
    "        initial_hidden = torch.zeros_like(context)\n",
    "        outputs = []\n",
    "        \n",
    "        hidden_states = []\n",
    "        for timestep in range(self.sequence_len):\n",
    "            h_t = []\n",
    "            if(timestep == 0):\n",
    "                h_t = self.dec_cells[0](torch.cat((target_seq[:,timestep],context[0]),dim=1), initial_hidden[0])\n",
    "            else:\n",
    "                input = []\n",
    "                if(torch.rand(1).item() < teacher_ratio):\n",
    "                    input = target_seq[:,timestep]\n",
    "                else:\n",
    "                    input = self.embedding(torch.argmax(outputs[-1],dim=1))\n",
    "#                     print(torch.argmax(outputs[-1],dim=1)[0],target_[0,layer_idx])\n",
    "                    \n",
    "                h_t = self.dec_cells[0](torch.cat((input,context[0]),dim=1), hidden_states[-1])\n",
    "            hidden_states.append(h_t)\n",
    "            out = self.output_layer_timestep(self.linear(h_t))\n",
    "            outputs.append(out)\n",
    "    \n",
    " \n",
    "        hidden_states = torch.cat(hidden_states,dim = 1).reshape(-1,9,context.shape[2])\n",
    "        output_prob = torch.cat(outputs,dim = 1).reshape(-1,9,26)\n",
    "        \n",
    "        \n",
    "#         out,_ = self.decoder(target_seq,context)\n",
    "#         decoder_output = self.linear(hidden_states)\n",
    "#         output_prob = self.output_layer(decoder_output)\n",
    "        return output_prob\n",
    "\n",
    "    def predict(self,context):\n",
    "        target_seq = self.embedding(torch.tensor(1))\n",
    "        initial_hidden = torch.zeros_like(context)\n",
    "        outputs = []\n",
    "        \n",
    "        hidden_states = []\n",
    "        for layer_idx in range(self.sequence_len):\n",
    "            h_t = []\n",
    "            if(layer_idx == 0):\n",
    "                h_t = self.dec_cells[0](torch.cat((target_seq,context[0]),dim=0), initial_hidden[0])\n",
    "            else:\n",
    "                input = self.embedding(torch.argmax(outputs[-1],dim=0))\n",
    "                    \n",
    "                h_t = self.dec_cells[0](torch.cat((input,context[0]),dim=0), hidden_states[-1])\n",
    "            hidden_states.append(h_t)\n",
    "            out = nn.LogSoftmax(dim=0)(self.linear(h_t))\n",
    "            outputs.append(out)\n",
    "        \n",
    "        output = ''.join([chr(torch.argmax(out,dim=0).item()+ord('a')) for out in outputs[:-1]])\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85c073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Train Loss: 26.2139\n",
      "Epoch [1/2000], Eval Loss: 26.1419\n",
      "Epoch [2/2000], Train Loss: 26.1134\n",
      "Epoch [2/2000], Eval Loss: 26.0259\n",
      "Epoch [3/2000], Train Loss: 26.0080\n",
      "Epoch [3/2000], Eval Loss: 25.9625\n",
      "Epoch [4/2000], Train Loss: 25.9366\n",
      "Epoch [4/2000], Eval Loss: 25.9185\n",
      "Epoch [5/2000], Train Loss: 25.8624\n",
      "Epoch [5/2000], Eval Loss: 25.8373\n",
      "Epoch [6/2000], Train Loss: 25.7672\n",
      "Epoch [6/2000], Eval Loss: 25.7665\n",
      "Epoch [7/2000], Train Loss: 25.6858\n",
      "Epoch [7/2000], Eval Loss: 25.6920\n",
      "Epoch [8/2000], Train Loss: 25.5817\n",
      "Epoch [8/2000], Eval Loss: 25.6040\n",
      "Epoch [9/2000], Train Loss: 25.4784\n",
      "Epoch [9/2000], Eval Loss: 25.5027\n",
      "Epoch [10/2000], Train Loss: 25.3745\n",
      "Epoch [10/2000], Eval Loss: 25.3960\n",
      "Epoch [11/2000], Train Loss: 25.2512\n",
      "Epoch [11/2000], Eval Loss: 25.2863\n",
      "Epoch [12/2000], Train Loss: 25.1588\n",
      "Epoch [12/2000], Eval Loss: 25.1615\n",
      "Epoch [13/2000], Train Loss: 25.0402\n",
      "Epoch [13/2000], Eval Loss: 25.0386\n",
      "Epoch [14/2000], Train Loss: 24.9619\n",
      "Epoch [14/2000], Eval Loss: 25.0652\n",
      "Epoch [15/2000], Train Loss: 24.9524\n",
      "Epoch [15/2000], Eval Loss: 24.9423\n",
      "Epoch [16/2000], Train Loss: 24.8315\n",
      "Epoch [16/2000], Eval Loss: 24.8042\n",
      "Epoch [17/2000], Train Loss: 24.6583\n",
      "Epoch [17/2000], Eval Loss: 24.6782\n",
      "Epoch [18/2000], Train Loss: 24.5094\n",
      "Epoch [18/2000], Eval Loss: 24.5951\n",
      "Epoch [19/2000], Train Loss: 24.4235\n",
      "Epoch [19/2000], Eval Loss: 24.4887\n",
      "Epoch [20/2000], Train Loss: 24.3901\n",
      "Epoch [20/2000], Eval Loss: 24.3859\n",
      "Epoch [21/2000], Train Loss: 24.2377\n",
      "Epoch [21/2000], Eval Loss: 24.2812\n",
      "Epoch [22/2000], Train Loss: 24.1419\n",
      "Epoch [22/2000], Eval Loss: 24.1835\n",
      "Epoch [23/2000], Train Loss: 24.0399\n",
      "Epoch [23/2000], Eval Loss: 24.1009\n",
      "Epoch [24/2000], Train Loss: 23.9381\n",
      "Epoch [24/2000], Eval Loss: 24.0156\n",
      "Epoch [25/2000], Train Loss: 23.8520\n",
      "Epoch [25/2000], Eval Loss: 23.9443\n",
      "Epoch [26/2000], Train Loss: 23.7549\n",
      "Epoch [26/2000], Eval Loss: 23.8710\n",
      "Epoch [27/2000], Train Loss: 23.6681\n",
      "Epoch [27/2000], Eval Loss: 23.7778\n",
      "Epoch [28/2000], Train Loss: 23.5663\n",
      "Epoch [28/2000], Eval Loss: 23.7074\n",
      "Epoch [29/2000], Train Loss: 23.4822\n",
      "Epoch [29/2000], Eval Loss: 23.6363\n",
      "Epoch [30/2000], Train Loss: 23.5268\n",
      "Epoch [30/2000], Eval Loss: 23.5878\n",
      "Epoch [31/2000], Train Loss: 23.4129\n",
      "Epoch [31/2000], Eval Loss: 23.5254\n",
      "Epoch [32/2000], Train Loss: 23.3131\n",
      "Epoch [32/2000], Eval Loss: 23.4753\n",
      "Epoch [33/2000], Train Loss: 23.2148\n",
      "Epoch [33/2000], Eval Loss: 23.4322\n",
      "Epoch [34/2000], Train Loss: 23.2194\n",
      "Epoch [34/2000], Eval Loss: 23.3949\n",
      "Epoch [35/2000], Train Loss: 23.1037\n",
      "Epoch [35/2000], Eval Loss: 23.3210\n",
      "Epoch [36/2000], Train Loss: 23.0166\n",
      "Epoch [36/2000], Eval Loss: 23.2799\n",
      "Epoch [37/2000], Train Loss: 22.9675\n",
      "Epoch [37/2000], Eval Loss: 23.2689\n",
      "Epoch [38/2000], Train Loss: 22.9704\n",
      "Epoch [38/2000], Eval Loss: 23.3010\n",
      "Epoch [39/2000], Train Loss: 22.9675\n",
      "Epoch [39/2000], Eval Loss: 23.2244\n",
      "Epoch [40/2000], Train Loss: 22.9186\n",
      "Epoch [40/2000], Eval Loss: 23.1848\n",
      "Epoch [41/2000], Train Loss: 22.8519\n",
      "Epoch [41/2000], Eval Loss: 23.1523\n",
      "Epoch [42/2000], Train Loss: 22.8712\n",
      "Epoch [42/2000], Eval Loss: 23.0951\n",
      "Epoch [43/2000], Train Loss: 22.7504\n",
      "Epoch [43/2000], Eval Loss: 23.0535\n",
      "Epoch [44/2000], Train Loss: 22.7444\n",
      "Epoch [44/2000], Eval Loss: 23.0105\n",
      "Epoch [45/2000], Train Loss: 22.7487\n",
      "Epoch [45/2000], Eval Loss: 23.0036\n",
      "Epoch [46/2000], Train Loss: 22.6228\n",
      "Epoch [46/2000], Eval Loss: 22.9427\n",
      "Epoch [47/2000], Train Loss: 22.5846\n",
      "Epoch [47/2000], Eval Loss: 22.9324\n",
      "Epoch [48/2000], Train Loss: 22.5358\n",
      "Epoch [48/2000], Eval Loss: 22.8868\n",
      "Epoch [49/2000], Train Loss: 22.5501\n",
      "Epoch [49/2000], Eval Loss: 22.8505\n",
      "Epoch [50/2000], Train Loss: 22.4768\n",
      "Epoch [50/2000], Eval Loss: 22.8350\n",
      "Epoch [51/2000], Train Loss: 22.4195\n",
      "Epoch [51/2000], Eval Loss: 22.8073\n",
      "Epoch [52/2000], Train Loss: 22.3969\n",
      "Epoch [52/2000], Eval Loss: 22.7937\n",
      "Epoch [53/2000], Train Loss: 22.4101\n",
      "Epoch [53/2000], Eval Loss: 22.7294\n",
      "Epoch [54/2000], Train Loss: 22.3055\n",
      "Epoch [54/2000], Eval Loss: 22.7575\n",
      "Epoch [55/2000], Train Loss: 22.3515\n",
      "Epoch [55/2000], Eval Loss: 22.7104\n",
      "Epoch [56/2000], Train Loss: 22.2474\n",
      "Epoch [56/2000], Eval Loss: 22.7082\n",
      "Epoch [57/2000], Train Loss: 22.2629\n",
      "Epoch [57/2000], Eval Loss: 22.7311\n",
      "Epoch [58/2000], Train Loss: 22.2562\n",
      "Epoch [58/2000], Eval Loss: 22.6362\n",
      "Epoch [59/2000], Train Loss: 22.1425\n",
      "Epoch [59/2000], Eval Loss: 22.6070\n",
      "Epoch [60/2000], Train Loss: 22.0943\n",
      "Epoch [60/2000], Eval Loss: 22.5752\n",
      "Epoch [61/2000], Train Loss: 22.0500\n",
      "Epoch [61/2000], Eval Loss: 22.5457\n",
      "Epoch [62/2000], Train Loss: 22.1050\n",
      "Epoch [62/2000], Eval Loss: 22.5106\n",
      "Epoch [63/2000], Train Loss: 21.9731\n",
      "Epoch [63/2000], Eval Loss: 22.4915\n",
      "Epoch [64/2000], Train Loss: 22.0226\n",
      "Epoch [64/2000], Eval Loss: 22.4715\n",
      "Epoch [65/2000], Train Loss: 21.9221\n",
      "Epoch [65/2000], Eval Loss: 22.4303\n",
      "Epoch [66/2000], Train Loss: 21.9315\n",
      "Epoch [66/2000], Eval Loss: 22.4334\n",
      "Epoch [67/2000], Train Loss: 21.8833\n",
      "Epoch [67/2000], Eval Loss: 22.4188\n",
      "Epoch [68/2000], Train Loss: 21.8642\n",
      "Epoch [68/2000], Eval Loss: 22.3524\n",
      "Epoch [69/2000], Train Loss: 21.7761\n",
      "Epoch [69/2000], Eval Loss: 22.3335\n",
      "Epoch [70/2000], Train Loss: 21.7933\n",
      "Epoch [70/2000], Eval Loss: 22.3178\n",
      "Epoch [71/2000], Train Loss: 21.7143\n",
      "Epoch [71/2000], Eval Loss: 22.3368\n",
      "Epoch [72/2000], Train Loss: 21.7087\n",
      "Epoch [72/2000], Eval Loss: 22.3169\n",
      "Epoch [73/2000], Train Loss: 21.6869\n",
      "Epoch [73/2000], Eval Loss: 22.3077\n",
      "Epoch [74/2000], Train Loss: 21.6666\n",
      "Epoch [74/2000], Eval Loss: 22.2205\n",
      "Epoch [75/2000], Train Loss: 21.6658\n",
      "Epoch [75/2000], Eval Loss: 22.2105\n",
      "Epoch [76/2000], Train Loss: 21.6073\n",
      "Epoch [76/2000], Eval Loss: 22.1979\n",
      "Epoch [77/2000], Train Loss: 21.5392\n",
      "Epoch [77/2000], Eval Loss: 22.1644\n",
      "Epoch [78/2000], Train Loss: 21.4885\n",
      "Epoch [78/2000], Eval Loss: 22.1170\n",
      "Epoch [79/2000], Train Loss: 21.4723\n",
      "Epoch [79/2000], Eval Loss: 22.1148\n",
      "Epoch [80/2000], Train Loss: 21.4697\n",
      "Epoch [80/2000], Eval Loss: 22.0937\n",
      "Epoch [81/2000], Train Loss: 21.3924\n",
      "Epoch [81/2000], Eval Loss: 22.0509\n",
      "Epoch [82/2000], Train Loss: 21.3681\n",
      "Epoch [82/2000], Eval Loss: 22.0511\n",
      "Epoch [83/2000], Train Loss: 21.3129\n",
      "Epoch [83/2000], Eval Loss: 22.0285\n",
      "Epoch [84/2000], Train Loss: 21.3051\n",
      "Epoch [84/2000], Eval Loss: 22.0020\n",
      "Epoch [85/2000], Train Loss: 21.2607\n",
      "Epoch [85/2000], Eval Loss: 21.9556\n",
      "Epoch [86/2000], Train Loss: 21.2224\n",
      "Epoch [86/2000], Eval Loss: 21.9213\n",
      "Epoch [87/2000], Train Loss: 21.1583\n",
      "Epoch [87/2000], Eval Loss: 21.9420\n",
      "Epoch [88/2000], Train Loss: 21.1773\n",
      "Epoch [88/2000], Eval Loss: 21.9207\n",
      "Epoch [89/2000], Train Loss: 21.1998\n",
      "Epoch [89/2000], Eval Loss: 22.1331\n",
      "Epoch [90/2000], Train Loss: 21.3333\n",
      "Epoch [90/2000], Eval Loss: 22.2083\n",
      "Epoch [91/2000], Train Loss: 21.4024\n",
      "Epoch [91/2000], Eval Loss: 22.0029\n",
      "Epoch [92/2000], Train Loss: 21.2103\n",
      "Epoch [92/2000], Eval Loss: 21.9739\n",
      "Epoch [93/2000], Train Loss: 21.1635\n",
      "Epoch [93/2000], Eval Loss: 21.9583\n",
      "Epoch [94/2000], Train Loss: 21.1201\n",
      "Epoch [94/2000], Eval Loss: 21.9323\n",
      "Epoch [95/2000], Train Loss: 21.0550\n",
      "Epoch [95/2000], Eval Loss: 21.8603\n",
      "Epoch [96/2000], Train Loss: 21.0167\n",
      "Epoch [96/2000], Eval Loss: 21.7972\n",
      "Epoch [97/2000], Train Loss: 21.0140\n",
      "Epoch [97/2000], Eval Loss: 21.7923\n",
      "Epoch [98/2000], Train Loss: 20.9367\n",
      "Epoch [98/2000], Eval Loss: 21.7731\n",
      "Epoch [99/2000], Train Loss: 20.8833\n",
      "Epoch [99/2000], Eval Loss: 21.7167\n",
      "Epoch [100/2000], Train Loss: 20.8399\n",
      "Epoch [100/2000], Eval Loss: 21.6675\n",
      "Epoch [101/2000], Train Loss: 20.7829\n",
      "Epoch [101/2000], Eval Loss: 21.6145\n",
      "Epoch [102/2000], Train Loss: 20.7498\n",
      "Epoch [102/2000], Eval Loss: 21.6105\n",
      "Epoch [103/2000], Train Loss: 20.7341\n",
      "Epoch [103/2000], Eval Loss: 21.5871\n",
      "Epoch [104/2000], Train Loss: 20.7122\n",
      "Epoch [104/2000], Eval Loss: 21.6519\n",
      "Epoch [105/2000], Train Loss: 20.7355\n",
      "Epoch [105/2000], Eval Loss: 21.6894\n",
      "Epoch [106/2000], Train Loss: 20.8443\n",
      "Epoch [106/2000], Eval Loss: 21.6985\n",
      "Epoch [107/2000], Train Loss: 20.7573\n",
      "Epoch [107/2000], Eval Loss: 21.6766\n",
      "Epoch [108/2000], Train Loss: 20.7787\n",
      "Epoch [108/2000], Eval Loss: 21.5669\n",
      "Epoch [109/2000], Train Loss: 20.6392\n",
      "Epoch [109/2000], Eval Loss: 21.5335\n",
      "Epoch [110/2000], Train Loss: 20.6257\n",
      "Epoch [110/2000], Eval Loss: 21.5155\n",
      "Epoch [111/2000], Train Loss: 20.6002\n",
      "Epoch [111/2000], Eval Loss: 21.4519\n",
      "Epoch [112/2000], Train Loss: 20.5076\n",
      "Epoch [112/2000], Eval Loss: 21.4829\n",
      "Epoch [113/2000], Train Loss: 20.5121\n",
      "Epoch [113/2000], Eval Loss: 21.3754\n",
      "Epoch [114/2000], Train Loss: 20.4440\n",
      "Epoch [114/2000], Eval Loss: 21.3764\n",
      "Epoch [115/2000], Train Loss: 20.4154\n",
      "Epoch [115/2000], Eval Loss: 21.3233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [116/2000], Train Loss: 20.3348\n",
      "Epoch [116/2000], Eval Loss: 21.3016\n",
      "Epoch [117/2000], Train Loss: 20.3026\n",
      "Epoch [117/2000], Eval Loss: 21.2446\n",
      "Epoch [118/2000], Train Loss: 20.2919\n",
      "Epoch [118/2000], Eval Loss: 21.1880\n",
      "Epoch [119/2000], Train Loss: 20.2389\n",
      "Epoch [119/2000], Eval Loss: 21.1784\n",
      "Epoch [120/2000], Train Loss: 20.2253\n",
      "Epoch [120/2000], Eval Loss: 21.1421\n",
      "Epoch [121/2000], Train Loss: 20.1714\n",
      "Epoch [121/2000], Eval Loss: 21.1440\n",
      "Epoch [122/2000], Train Loss: 20.1712\n",
      "Epoch [122/2000], Eval Loss: 21.1053\n",
      "Epoch [123/2000], Train Loss: 20.1094\n",
      "Epoch [123/2000], Eval Loss: 21.0925\n",
      "Epoch [124/2000], Train Loss: 20.1155\n",
      "Epoch [124/2000], Eval Loss: 21.0895\n",
      "Epoch [125/2000], Train Loss: 20.0921\n",
      "Epoch [125/2000], Eval Loss: 21.0740\n",
      "Epoch [126/2000], Train Loss: 20.0639\n",
      "Epoch [126/2000], Eval Loss: 20.9672\n",
      "Epoch [127/2000], Train Loss: 19.9566\n",
      "Epoch [127/2000], Eval Loss: 20.9556\n",
      "Epoch [128/2000], Train Loss: 19.9307\n",
      "Epoch [128/2000], Eval Loss: 20.9164\n",
      "Epoch [129/2000], Train Loss: 19.8871\n",
      "Epoch [129/2000], Eval Loss: 20.9684\n",
      "Epoch [130/2000], Train Loss: 19.9497\n",
      "Epoch [130/2000], Eval Loss: 21.0337\n",
      "Epoch [131/2000], Train Loss: 20.0192\n",
      "Epoch [131/2000], Eval Loss: 20.8999\n",
      "Epoch [132/2000], Train Loss: 19.9040\n",
      "Epoch [132/2000], Eval Loss: 20.8854\n",
      "Epoch [133/2000], Train Loss: 19.8686\n",
      "Epoch [133/2000], Eval Loss: 20.7969\n",
      "Epoch [134/2000], Train Loss: 19.7861\n",
      "Epoch [134/2000], Eval Loss: 20.7666\n",
      "Epoch [135/2000], Train Loss: 19.7385\n",
      "Epoch [135/2000], Eval Loss: 20.7493\n",
      "Epoch [136/2000], Train Loss: 19.7263\n",
      "Epoch [136/2000], Eval Loss: 20.6545\n",
      "Epoch [137/2000], Train Loss: 19.6394\n",
      "Epoch [137/2000], Eval Loss: 20.6206\n",
      "Epoch [138/2000], Train Loss: 19.5945\n",
      "Epoch [138/2000], Eval Loss: 20.6355\n",
      "Epoch [139/2000], Train Loss: 19.5894\n",
      "Epoch [139/2000], Eval Loss: 20.6164\n",
      "Epoch [140/2000], Train Loss: 19.5893\n",
      "Epoch [140/2000], Eval Loss: 20.6188\n",
      "Epoch [141/2000], Train Loss: 19.5946\n",
      "Epoch [141/2000], Eval Loss: 20.5805\n",
      "Epoch [142/2000], Train Loss: 19.5377\n",
      "Epoch [142/2000], Eval Loss: 20.4801\n",
      "Epoch [143/2000], Train Loss: 19.4696\n",
      "Epoch [143/2000], Eval Loss: 20.4818\n",
      "Epoch [144/2000], Train Loss: 19.4365\n",
      "Epoch [144/2000], Eval Loss: 20.5006\n",
      "Epoch [145/2000], Train Loss: 19.4363\n",
      "Epoch [145/2000], Eval Loss: 20.4102\n",
      "Epoch [146/2000], Train Loss: 19.4022\n",
      "Epoch [146/2000], Eval Loss: 20.4170\n",
      "Epoch [147/2000], Train Loss: 19.4063\n",
      "Epoch [147/2000], Eval Loss: 20.4183\n",
      "Epoch [148/2000], Train Loss: 19.3818\n",
      "Epoch [148/2000], Eval Loss: 20.3218\n",
      "Epoch [149/2000], Train Loss: 19.2879\n",
      "Epoch [149/2000], Eval Loss: 20.1796\n",
      "Epoch [150/2000], Train Loss: 19.1814\n",
      "Epoch [150/2000], Eval Loss: 20.1633\n",
      "Epoch [151/2000], Train Loss: 19.1489\n",
      "Epoch [151/2000], Eval Loss: 20.0922\n",
      "Epoch [152/2000], Train Loss: 19.0902\n",
      "Epoch [152/2000], Eval Loss: 20.0426\n",
      "Epoch [153/2000], Train Loss: 19.0256\n",
      "Epoch [153/2000], Eval Loss: 19.9437\n",
      "Epoch [154/2000], Train Loss: 18.8986\n",
      "Epoch [154/2000], Eval Loss: 19.9433\n",
      "Epoch [155/2000], Train Loss: 18.9349\n",
      "Epoch [155/2000], Eval Loss: 19.8671\n",
      "Epoch [156/2000], Train Loss: 18.8362\n",
      "Epoch [156/2000], Eval Loss: 19.7832\n",
      "Epoch [157/2000], Train Loss: 18.7606\n",
      "Epoch [157/2000], Eval Loss: 19.7763\n",
      "Epoch [158/2000], Train Loss: 18.7918\n",
      "Epoch [158/2000], Eval Loss: 19.7478\n",
      "Epoch [159/2000], Train Loss: 18.7409\n",
      "Epoch [159/2000], Eval Loss: 19.7712\n",
      "Epoch [160/2000], Train Loss: 18.7721\n",
      "Epoch [160/2000], Eval Loss: 19.8470\n",
      "Epoch [161/2000], Train Loss: 18.8228\n",
      "Epoch [161/2000], Eval Loss: 19.7698\n",
      "Epoch [162/2000], Train Loss: 18.8035\n",
      "Epoch [162/2000], Eval Loss: 19.5249\n",
      "Epoch [163/2000], Train Loss: 18.5341\n",
      "Epoch [163/2000], Eval Loss: 19.7987\n",
      "Epoch [164/2000], Train Loss: 18.8033\n",
      "Epoch [164/2000], Eval Loss: 19.7454\n",
      "Epoch [165/2000], Train Loss: 18.7852\n",
      "Epoch [165/2000], Eval Loss: 19.5147\n",
      "Epoch [166/2000], Train Loss: 18.5727\n",
      "Epoch [166/2000], Eval Loss: 19.5163\n",
      "Epoch [167/2000], Train Loss: 18.5306\n",
      "Epoch [167/2000], Eval Loss: 19.4414\n",
      "Epoch [168/2000], Train Loss: 18.4315\n",
      "Epoch [168/2000], Eval Loss: 19.3826\n",
      "Epoch [169/2000], Train Loss: 18.4020\n",
      "Epoch [169/2000], Eval Loss: 19.2178\n",
      "Epoch [170/2000], Train Loss: 18.2448\n",
      "Epoch [170/2000], Eval Loss: 19.1696\n",
      "Epoch [171/2000], Train Loss: 18.1598\n",
      "Epoch [171/2000], Eval Loss: 19.0563\n",
      "Epoch [172/2000], Train Loss: 18.0515\n",
      "Epoch [172/2000], Eval Loss: 18.9781\n",
      "Epoch [173/2000], Train Loss: 18.0133\n",
      "Epoch [173/2000], Eval Loss: 18.8956\n",
      "Epoch [174/2000], Train Loss: 17.9015\n",
      "Epoch [174/2000], Eval Loss: 18.9007\n",
      "Epoch [175/2000], Train Loss: 17.9259\n",
      "Epoch [175/2000], Eval Loss: 18.8359\n",
      "Epoch [176/2000], Train Loss: 17.9129\n",
      "Epoch [176/2000], Eval Loss: 18.9203\n",
      "Epoch [177/2000], Train Loss: 17.9466\n",
      "Epoch [177/2000], Eval Loss: 18.9014\n",
      "Epoch [178/2000], Train Loss: 17.9383\n",
      "Epoch [178/2000], Eval Loss: 18.9375\n",
      "Epoch [179/2000], Train Loss: 17.9607\n",
      "Epoch [179/2000], Eval Loss: 18.6106\n",
      "Epoch [180/2000], Train Loss: 17.6800\n",
      "Epoch [180/2000], Eval Loss: 18.6681\n",
      "Epoch [181/2000], Train Loss: 17.7499\n",
      "Epoch [181/2000], Eval Loss: 18.4402\n",
      "Epoch [182/2000], Train Loss: 17.4931\n",
      "Epoch [182/2000], Eval Loss: 18.4144\n",
      "Epoch [183/2000], Train Loss: 17.4723\n",
      "Epoch [183/2000], Eval Loss: 18.2682\n",
      "Epoch [184/2000], Train Loss: 17.3454\n",
      "Epoch [184/2000], Eval Loss: 18.1495\n",
      "Epoch [185/2000], Train Loss: 17.2147\n",
      "Epoch [185/2000], Eval Loss: 18.0685\n",
      "Epoch [186/2000], Train Loss: 17.1678\n",
      "Epoch [186/2000], Eval Loss: 17.9641\n",
      "Epoch [187/2000], Train Loss: 17.0518\n",
      "Epoch [187/2000], Eval Loss: 17.9076\n",
      "Epoch [188/2000], Train Loss: 17.0277\n",
      "Epoch [188/2000], Eval Loss: 17.8116\n",
      "Epoch [189/2000], Train Loss: 16.9435\n",
      "Epoch [189/2000], Eval Loss: 17.7486\n",
      "Epoch [190/2000], Train Loss: 16.9054\n",
      "Epoch [190/2000], Eval Loss: 17.6250\n",
      "Epoch [191/2000], Train Loss: 16.7736\n",
      "Epoch [191/2000], Eval Loss: 17.5069\n",
      "Epoch [192/2000], Train Loss: 16.6743\n",
      "Epoch [192/2000], Eval Loss: 17.4126\n",
      "Epoch [193/2000], Train Loss: 16.5670\n",
      "Epoch [193/2000], Eval Loss: 17.4865\n",
      "Epoch [194/2000], Train Loss: 16.5845\n",
      "Epoch [194/2000], Eval Loss: 17.7184\n",
      "Epoch [195/2000], Train Loss: 16.8938\n",
      "Epoch [195/2000], Eval Loss: 18.2327\n",
      "Epoch [196/2000], Train Loss: 17.4121\n",
      "Epoch [196/2000], Eval Loss: 17.5888\n",
      "Epoch [197/2000], Train Loss: 16.7684\n",
      "Epoch [197/2000], Eval Loss: 17.6299\n",
      "Epoch [198/2000], Train Loss: 16.7997\n",
      "Epoch [198/2000], Eval Loss: 17.3843\n",
      "Epoch [199/2000], Train Loss: 16.5489\n",
      "Epoch [199/2000], Eval Loss: 17.2714\n",
      "Epoch [200/2000], Train Loss: 16.3594\n",
      "Epoch [200/2000], Eval Loss: 17.1113\n",
      "Epoch [201/2000], Train Loss: 16.2467\n",
      "Epoch [201/2000], Eval Loss: 16.8298\n",
      "Epoch [202/2000], Train Loss: 15.9733\n",
      "Epoch [202/2000], Eval Loss: 16.6624\n",
      "Epoch [203/2000], Train Loss: 15.8395\n",
      "Epoch [203/2000], Eval Loss: 16.5200\n",
      "Epoch [204/2000], Train Loss: 15.7496\n",
      "Epoch [204/2000], Eval Loss: 16.3662\n",
      "Epoch [205/2000], Train Loss: 15.5079\n",
      "Epoch [205/2000], Eval Loss: 16.2910\n",
      "Epoch [206/2000], Train Loss: 15.4571\n",
      "Epoch [206/2000], Eval Loss: 16.0294\n",
      "Epoch [207/2000], Train Loss: 15.2325\n",
      "Epoch [207/2000], Eval Loss: 15.9892\n",
      "Epoch [208/2000], Train Loss: 15.1689\n",
      "Epoch [208/2000], Eval Loss: 15.7974\n",
      "Epoch [209/2000], Train Loss: 14.9619\n",
      "Epoch [209/2000], Eval Loss: 15.7878\n",
      "Epoch [210/2000], Train Loss: 14.9945\n",
      "Epoch [210/2000], Eval Loss: 15.7557\n",
      "Epoch [211/2000], Train Loss: 15.0103\n",
      "Epoch [211/2000], Eval Loss: 16.0298\n",
      "Epoch [212/2000], Train Loss: 15.2682\n",
      "Epoch [212/2000], Eval Loss: 15.9373\n",
      "Epoch [213/2000], Train Loss: 15.1838\n",
      "Epoch [213/2000], Eval Loss: 15.8541\n",
      "Epoch [214/2000], Train Loss: 15.1509\n",
      "Epoch [214/2000], Eval Loss: 15.5952\n",
      "Epoch [215/2000], Train Loss: 14.8927\n",
      "Epoch [215/2000], Eval Loss: 15.5286\n",
      "Epoch [216/2000], Train Loss: 14.7646\n",
      "Epoch [216/2000], Eval Loss: 15.1373\n",
      "Epoch [217/2000], Train Loss: 14.4181\n",
      "Epoch [217/2000], Eval Loss: 15.1503\n",
      "Epoch [218/2000], Train Loss: 14.4733\n",
      "Epoch [218/2000], Eval Loss: 14.8712\n",
      "Epoch [219/2000], Train Loss: 14.1305\n",
      "Epoch [219/2000], Eval Loss: 14.8566\n",
      "Epoch [220/2000], Train Loss: 14.1149\n",
      "Epoch [220/2000], Eval Loss: 14.6459\n",
      "Epoch [221/2000], Train Loss: 13.8441\n",
      "Epoch [221/2000], Eval Loss: 14.5830\n",
      "Epoch [222/2000], Train Loss: 13.8247\n",
      "Epoch [222/2000], Eval Loss: 14.3704\n",
      "Epoch [223/2000], Train Loss: 13.5701\n",
      "Epoch [223/2000], Eval Loss: 14.3064\n",
      "Epoch [224/2000], Train Loss: 13.5879\n",
      "Epoch [224/2000], Eval Loss: 14.1143\n",
      "Epoch [225/2000], Train Loss: 13.3906\n",
      "Epoch [225/2000], Eval Loss: 13.9728\n",
      "Epoch [226/2000], Train Loss: 13.2472\n",
      "Epoch [226/2000], Eval Loss: 13.7234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [227/2000], Train Loss: 13.0775\n",
      "Epoch [227/2000], Eval Loss: 13.6897\n",
      "Epoch [228/2000], Train Loss: 12.9835\n",
      "Epoch [228/2000], Eval Loss: 13.6288\n",
      "Epoch [229/2000], Train Loss: 12.8732\n",
      "Epoch [229/2000], Eval Loss: 14.1053\n",
      "Epoch [230/2000], Train Loss: 13.4138\n",
      "Epoch [230/2000], Eval Loss: 15.4586\n",
      "Epoch [231/2000], Train Loss: 14.8838\n",
      "Epoch [231/2000], Eval Loss: 15.1764\n",
      "Epoch [232/2000], Train Loss: 14.5553\n",
      "Epoch [232/2000], Eval Loss: 15.2880\n",
      "Epoch [233/2000], Train Loss: 14.6108\n",
      "Epoch [233/2000], Eval Loss: 14.7387\n",
      "Epoch [234/2000], Train Loss: 14.0967\n",
      "Epoch [234/2000], Eval Loss: 14.5186\n",
      "Epoch [235/2000], Train Loss: 13.8092\n",
      "Epoch [235/2000], Eval Loss: 14.4948\n",
      "Epoch [236/2000], Train Loss: 13.8468\n",
      "Epoch [236/2000], Eval Loss: 14.3111\n",
      "Epoch [237/2000], Train Loss: 13.6481\n",
      "Epoch [237/2000], Eval Loss: 14.0845\n",
      "Epoch [238/2000], Train Loss: 13.4052\n",
      "Epoch [238/2000], Eval Loss: 13.9152\n",
      "Epoch [239/2000], Train Loss: 13.2326\n",
      "Epoch [239/2000], Eval Loss: 13.5892\n",
      "Epoch [240/2000], Train Loss: 12.9350\n",
      "Epoch [240/2000], Eval Loss: 13.4360\n",
      "Epoch [241/2000], Train Loss: 12.8106\n",
      "Epoch [241/2000], Eval Loss: 13.1631\n",
      "Epoch [242/2000], Train Loss: 12.5053\n",
      "Epoch [242/2000], Eval Loss: 13.0582\n",
      "Epoch [243/2000], Train Loss: 12.3779\n",
      "Epoch [243/2000], Eval Loss: 12.8136\n",
      "Epoch [244/2000], Train Loss: 12.0795\n",
      "Epoch [244/2000], Eval Loss: 12.6452\n",
      "Epoch [245/2000], Train Loss: 12.0868\n",
      "Epoch [245/2000], Eval Loss: 12.4509\n",
      "Epoch [246/2000], Train Loss: 11.9014\n",
      "Epoch [246/2000], Eval Loss: 12.2928\n",
      "Epoch [247/2000], Train Loss: 11.6826\n",
      "Epoch [247/2000], Eval Loss: 12.1624\n",
      "Epoch [248/2000], Train Loss: 11.5598\n",
      "Epoch [248/2000], Eval Loss: 11.9936\n",
      "Epoch [249/2000], Train Loss: 11.3263\n",
      "Epoch [249/2000], Eval Loss: 11.8254\n",
      "Epoch [250/2000], Train Loss: 11.1476\n",
      "Epoch [250/2000], Eval Loss: 11.6386\n",
      "Epoch [251/2000], Train Loss: 11.0394\n",
      "Epoch [251/2000], Eval Loss: 11.5617\n",
      "Epoch [252/2000], Train Loss: 10.9834\n",
      "Epoch [252/2000], Eval Loss: 11.4449\n",
      "Epoch [253/2000], Train Loss: 10.9433\n",
      "Epoch [253/2000], Eval Loss: 11.5429\n",
      "Epoch [254/2000], Train Loss: 10.9423\n",
      "Epoch [254/2000], Eval Loss: 11.9578\n",
      "Epoch [255/2000], Train Loss: 11.4735\n",
      "Epoch [255/2000], Eval Loss: 12.0968\n",
      "Epoch [256/2000], Train Loss: 11.5360\n",
      "Epoch [256/2000], Eval Loss: 11.7836\n",
      "Epoch [257/2000], Train Loss: 11.2084\n",
      "Epoch [257/2000], Eval Loss: 11.8207\n",
      "Epoch [258/2000], Train Loss: 11.2690\n",
      "Epoch [258/2000], Eval Loss: 11.7188\n",
      "Epoch [259/2000], Train Loss: 11.1447\n",
      "Epoch [259/2000], Eval Loss: 11.5945\n",
      "Epoch [260/2000], Train Loss: 10.9275\n",
      "Epoch [260/2000], Eval Loss: 11.4349\n",
      "Epoch [261/2000], Train Loss: 10.7821\n",
      "Epoch [261/2000], Eval Loss: 11.1333\n",
      "Epoch [262/2000], Train Loss: 10.5643\n",
      "Epoch [262/2000], Eval Loss: 11.0436\n",
      "Epoch [263/2000], Train Loss: 10.5407\n",
      "Epoch [263/2000], Eval Loss: 10.8657\n",
      "Epoch [264/2000], Train Loss: 10.3948\n",
      "Epoch [264/2000], Eval Loss: 10.7581\n",
      "Epoch [265/2000], Train Loss: 10.1107\n",
      "Epoch [265/2000], Eval Loss: 10.6307\n",
      "Epoch [266/2000], Train Loss: 10.1730\n",
      "Epoch [266/2000], Eval Loss: 10.4127\n",
      "Epoch [267/2000], Train Loss: 9.8803\n",
      "Epoch [267/2000], Eval Loss: 10.3561\n",
      "Epoch [268/2000], Train Loss: 9.9134\n",
      "Epoch [268/2000], Eval Loss: 10.2381\n",
      "Epoch [269/2000], Train Loss: 9.7598\n",
      "Epoch [269/2000], Eval Loss: 10.1522\n",
      "Epoch [270/2000], Train Loss: 9.6300\n",
      "Epoch [270/2000], Eval Loss: 9.9829\n",
      "Epoch [271/2000], Train Loss: 9.3361\n",
      "Epoch [271/2000], Eval Loss: 9.9089\n",
      "Epoch [272/2000], Train Loss: 9.3431\n",
      "Epoch [272/2000], Eval Loss: 9.7956\n",
      "Epoch [273/2000], Train Loss: 9.3349\n",
      "Epoch [273/2000], Eval Loss: 9.6761\n",
      "Epoch [274/2000], Train Loss: 9.1711\n",
      "Epoch [274/2000], Eval Loss: 9.5842\n",
      "Epoch [275/2000], Train Loss: 9.1564\n",
      "Epoch [275/2000], Eval Loss: 9.4546\n",
      "Epoch [276/2000], Train Loss: 8.9109\n",
      "Epoch [276/2000], Eval Loss: 9.3908\n",
      "Epoch [277/2000], Train Loss: 8.9351\n",
      "Epoch [277/2000], Eval Loss: 9.2839\n",
      "Epoch [278/2000], Train Loss: 8.7865\n",
      "Epoch [278/2000], Eval Loss: 9.2042\n",
      "Epoch [279/2000], Train Loss: 8.8164\n",
      "Epoch [279/2000], Eval Loss: 9.1661\n",
      "Epoch [280/2000], Train Loss: 8.5594\n",
      "Epoch [280/2000], Eval Loss: 9.2211\n",
      "Epoch [281/2000], Train Loss: 8.7194\n",
      "Epoch [281/2000], Eval Loss: 9.1953\n",
      "Epoch [282/2000], Train Loss: 8.6134\n",
      "Epoch [282/2000], Eval Loss: 9.0790\n",
      "Epoch [283/2000], Train Loss: 8.6497\n",
      "Epoch [283/2000], Eval Loss: 8.8563\n",
      "Epoch [284/2000], Train Loss: 8.4333\n",
      "Epoch [284/2000], Eval Loss: 8.8419\n",
      "Epoch [285/2000], Train Loss: 8.2834\n",
      "Epoch [285/2000], Eval Loss: 8.9937\n",
      "Epoch [286/2000], Train Loss: 8.4763\n",
      "Epoch [286/2000], Eval Loss: 9.1991\n",
      "Epoch [287/2000], Train Loss: 8.7955\n",
      "Epoch [287/2000], Eval Loss: 9.6659\n",
      "Epoch [288/2000], Train Loss: 9.1362\n",
      "Epoch [288/2000], Eval Loss: 9.9197\n",
      "Epoch [289/2000], Train Loss: 9.3639\n",
      "Epoch [289/2000], Eval Loss: 9.3651\n",
      "Epoch [290/2000], Train Loss: 8.8763\n",
      "Epoch [290/2000], Eval Loss: 9.5669\n",
      "Epoch [291/2000], Train Loss: 9.1314\n",
      "Epoch [291/2000], Eval Loss: 9.5840\n",
      "Epoch [292/2000], Train Loss: 9.0426\n",
      "Epoch [292/2000], Eval Loss: 9.1017\n",
      "Epoch [293/2000], Train Loss: 8.5681\n",
      "Epoch [293/2000], Eval Loss: 9.0360\n",
      "Epoch [294/2000], Train Loss: 8.4888\n",
      "Epoch [294/2000], Eval Loss: 8.7761\n",
      "Epoch [295/2000], Train Loss: 8.3716\n",
      "Epoch [295/2000], Eval Loss: 8.5132\n",
      "Epoch [296/2000], Train Loss: 8.1226\n",
      "Epoch [296/2000], Eval Loss: 8.6071\n",
      "Epoch [297/2000], Train Loss: 8.0800\n",
      "Epoch [297/2000], Eval Loss: 8.2888\n",
      "Epoch [298/2000], Train Loss: 7.8152\n",
      "Epoch [298/2000], Eval Loss: 8.3466\n",
      "Epoch [299/2000], Train Loss: 7.9157\n",
      "Epoch [299/2000], Eval Loss: 8.1216\n",
      "Epoch [300/2000], Train Loss: 7.6281\n",
      "Epoch [300/2000], Eval Loss: 8.0575\n",
      "Epoch [301/2000], Train Loss: 7.5906\n",
      "Epoch [301/2000], Eval Loss: 7.8908\n",
      "Epoch [302/2000], Train Loss: 7.4753\n",
      "Epoch [302/2000], Eval Loss: 7.8066\n",
      "Epoch [303/2000], Train Loss: 7.4276\n",
      "Epoch [303/2000], Eval Loss: 7.6878\n",
      "Epoch [304/2000], Train Loss: 7.2945\n",
      "Epoch [304/2000], Eval Loss: 7.6188\n",
      "Epoch [305/2000], Train Loss: 7.2306\n",
      "Epoch [305/2000], Eval Loss: 7.5408\n",
      "Epoch [306/2000], Train Loss: 7.1589\n",
      "Epoch [306/2000], Eval Loss: 7.4347\n",
      "Epoch [307/2000], Train Loss: 7.0360\n",
      "Epoch [307/2000], Eval Loss: 7.4050\n",
      "Epoch [308/2000], Train Loss: 7.0264\n",
      "Epoch [308/2000], Eval Loss: 7.2967\n",
      "Epoch [309/2000], Train Loss: 6.9084\n",
      "Epoch [309/2000], Eval Loss: 7.2438\n",
      "Epoch [310/2000], Train Loss: 6.8637\n",
      "Epoch [310/2000], Eval Loss: 7.1777\n",
      "Epoch [311/2000], Train Loss: 6.8150\n",
      "Epoch [311/2000], Eval Loss: 7.1568\n",
      "Epoch [312/2000], Train Loss: 6.7799\n",
      "Epoch [312/2000], Eval Loss: 7.0875\n",
      "Epoch [313/2000], Train Loss: 6.7196\n",
      "Epoch [313/2000], Eval Loss: 7.0416\n",
      "Epoch [314/2000], Train Loss: 6.6901\n",
      "Epoch [314/2000], Eval Loss: 6.9917\n",
      "Epoch [315/2000], Train Loss: 6.6412\n",
      "Epoch [315/2000], Eval Loss: 6.9388\n",
      "Epoch [316/2000], Train Loss: 6.5885\n",
      "Epoch [316/2000], Eval Loss: 6.9129\n",
      "Epoch [317/2000], Train Loss: 6.5674\n",
      "Epoch [317/2000], Eval Loss: 6.8554\n",
      "Epoch [318/2000], Train Loss: 6.5288\n",
      "Epoch [318/2000], Eval Loss: 6.8261\n",
      "Epoch [319/2000], Train Loss: 6.5042\n",
      "Epoch [319/2000], Eval Loss: 6.8516\n",
      "Epoch [320/2000], Train Loss: 6.5228\n",
      "Epoch [320/2000], Eval Loss: 6.9305\n",
      "Epoch [321/2000], Train Loss: 6.5849\n",
      "Epoch [321/2000], Eval Loss: 7.1369\n",
      "Epoch [322/2000], Train Loss: 6.7856\n",
      "Epoch [322/2000], Eval Loss: 7.7217\n",
      "Epoch [323/2000], Train Loss: 7.3805\n",
      "Epoch [323/2000], Eval Loss: 9.7197\n",
      "Epoch [324/2000], Train Loss: 9.3336\n",
      "Epoch [324/2000], Eval Loss: 9.8554\n",
      "Epoch [325/2000], Train Loss: 9.5750\n",
      "Epoch [325/2000], Eval Loss: 11.4602\n",
      "Epoch [326/2000], Train Loss: 11.1467\n",
      "Epoch [326/2000], Eval Loss: 10.2218\n",
      "Epoch [327/2000], Train Loss: 9.7944\n",
      "Epoch [327/2000], Eval Loss: 9.5846\n",
      "Epoch [328/2000], Train Loss: 9.1308\n",
      "Epoch [328/2000], Eval Loss: 9.3886\n",
      "Epoch [329/2000], Train Loss: 8.9277\n",
      "Epoch [329/2000], Eval Loss: 9.4356\n",
      "Epoch [330/2000], Train Loss: 8.9955\n",
      "Epoch [330/2000], Eval Loss: 9.0432\n",
      "Epoch [331/2000], Train Loss: 8.6128\n",
      "Epoch [331/2000], Eval Loss: 8.7513\n",
      "Epoch [332/2000], Train Loss: 8.3786\n",
      "Epoch [332/2000], Eval Loss: 8.4437\n",
      "Epoch [333/2000], Train Loss: 8.0828\n",
      "Epoch [333/2000], Eval Loss: 8.3768\n",
      "Epoch [334/2000], Train Loss: 7.9833\n",
      "Epoch [334/2000], Eval Loss: 8.1728\n",
      "Epoch [335/2000], Train Loss: 7.7897\n",
      "Epoch [335/2000], Eval Loss: 8.0014\n",
      "Epoch [336/2000], Train Loss: 7.5988\n",
      "Epoch [336/2000], Eval Loss: 7.9498\n",
      "Epoch [337/2000], Train Loss: 7.5329\n",
      "Epoch [337/2000], Eval Loss: 7.7345\n",
      "Epoch [338/2000], Train Loss: 7.3299\n",
      "Epoch [338/2000], Eval Loss: 7.6737\n",
      "Epoch [339/2000], Train Loss: 7.2763\n",
      "Epoch [339/2000], Eval Loss: 7.4879\n",
      "Epoch [340/2000], Train Loss: 7.1403\n",
      "Epoch [340/2000], Eval Loss: 7.3497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [341/2000], Train Loss: 7.0427\n",
      "Epoch [341/2000], Eval Loss: 7.2373\n",
      "Epoch [342/2000], Train Loss: 6.9291\n",
      "Epoch [342/2000], Eval Loss: 7.2047\n",
      "Epoch [343/2000], Train Loss: 6.8590\n",
      "Epoch [343/2000], Eval Loss: 7.1857\n",
      "Epoch [344/2000], Train Loss: 6.8066\n",
      "Epoch [344/2000], Eval Loss: 7.0857\n",
      "Epoch [345/2000], Train Loss: 6.7154\n",
      "Epoch [345/2000], Eval Loss: 6.9683\n",
      "Epoch [346/2000], Train Loss: 6.6326\n",
      "Epoch [346/2000], Eval Loss: 6.8830\n",
      "Epoch [347/2000], Train Loss: 6.5764\n",
      "Epoch [347/2000], Eval Loss: 6.8205\n",
      "Epoch [348/2000], Train Loss: 6.5210\n",
      "Epoch [348/2000], Eval Loss: 6.7707\n",
      "Epoch [349/2000], Train Loss: 6.4658\n",
      "Epoch [349/2000], Eval Loss: 6.7307\n",
      "Epoch [350/2000], Train Loss: 6.4247\n",
      "Epoch [350/2000], Eval Loss: 6.6852\n",
      "Epoch [351/2000], Train Loss: 6.3757\n",
      "Epoch [351/2000], Eval Loss: 6.6310\n",
      "Epoch [352/2000], Train Loss: 6.3318\n",
      "Epoch [352/2000], Eval Loss: 6.5778\n",
      "Epoch [353/2000], Train Loss: 6.2973\n",
      "Epoch [353/2000], Eval Loss: 6.5416\n",
      "Epoch [354/2000], Train Loss: 6.2671\n",
      "Epoch [354/2000], Eval Loss: 6.5129\n",
      "Epoch [355/2000], Train Loss: 6.2365\n",
      "Epoch [355/2000], Eval Loss: 6.4750\n",
      "Epoch [356/2000], Train Loss: 6.1980\n",
      "Epoch [356/2000], Eval Loss: 6.4502\n",
      "Epoch [357/2000], Train Loss: 6.1739\n",
      "Epoch [357/2000], Eval Loss: 6.4298\n",
      "Epoch [358/2000], Train Loss: 6.1508\n",
      "Epoch [358/2000], Eval Loss: 6.4022\n",
      "Epoch [359/2000], Train Loss: 6.1262\n",
      "Epoch [359/2000], Eval Loss: 6.3620\n",
      "Epoch [360/2000], Train Loss: 6.0953\n",
      "Epoch [360/2000], Eval Loss: 6.3351\n",
      "Epoch [361/2000], Train Loss: 6.0754\n",
      "Epoch [361/2000], Eval Loss: 6.3095\n",
      "Epoch [362/2000], Train Loss: 6.0574\n",
      "Epoch [362/2000], Eval Loss: 6.2944\n",
      "Epoch [363/2000], Train Loss: 6.0419\n",
      "Epoch [363/2000], Eval Loss: 6.2812\n",
      "Epoch [364/2000], Train Loss: 6.0205\n",
      "Epoch [364/2000], Eval Loss: 6.2713\n",
      "Epoch [365/2000], Train Loss: 6.0042\n",
      "Epoch [365/2000], Eval Loss: 6.2582\n",
      "Epoch [366/2000], Train Loss: 5.9892\n",
      "Epoch [366/2000], Eval Loss: 6.2381\n",
      "Epoch [367/2000], Train Loss: 5.9741\n",
      "Epoch [367/2000], Eval Loss: 6.2184\n",
      "Epoch [368/2000], Train Loss: 5.9598\n",
      "Epoch [368/2000], Eval Loss: 6.2035\n",
      "Epoch [369/2000], Train Loss: 5.9475\n",
      "Epoch [369/2000], Eval Loss: 6.1893\n",
      "Epoch [370/2000], Train Loss: 5.9341\n",
      "Epoch [370/2000], Eval Loss: 6.1798\n",
      "Epoch [371/2000], Train Loss: 5.9226\n",
      "Epoch [371/2000], Eval Loss: 6.1690\n",
      "Epoch [372/2000], Train Loss: 5.9111\n",
      "Epoch [372/2000], Eval Loss: 6.1579\n",
      "Epoch [373/2000], Train Loss: 5.9011\n",
      "Epoch [373/2000], Eval Loss: 6.1486\n",
      "Epoch [374/2000], Train Loss: 5.8912\n",
      "Epoch [374/2000], Eval Loss: 6.1408\n",
      "Epoch [375/2000], Train Loss: 5.8817\n",
      "Epoch [375/2000], Eval Loss: 6.1343\n",
      "Epoch [376/2000], Train Loss: 5.8720\n",
      "Epoch [376/2000], Eval Loss: 6.1265\n",
      "Epoch [377/2000], Train Loss: 5.8624\n",
      "Epoch [377/2000], Eval Loss: 6.1172\n",
      "Epoch [378/2000], Train Loss: 5.8541\n",
      "Epoch [378/2000], Eval Loss: 6.1094\n",
      "Epoch [379/2000], Train Loss: 5.8469\n",
      "Epoch [379/2000], Eval Loss: 6.1012\n",
      "Epoch [380/2000], Train Loss: 5.8401\n",
      "Epoch [380/2000], Eval Loss: 6.0942\n",
      "Epoch [381/2000], Train Loss: 5.8316\n",
      "Epoch [381/2000], Eval Loss: 6.0868\n",
      "Epoch [382/2000], Train Loss: 5.8243\n",
      "Epoch [382/2000], Eval Loss: 6.0790\n",
      "Epoch [383/2000], Train Loss: 5.8176\n",
      "Epoch [383/2000], Eval Loss: 6.0726\n",
      "Epoch [384/2000], Train Loss: 5.8108\n",
      "Epoch [384/2000], Eval Loss: 6.0669\n",
      "Epoch [385/2000], Train Loss: 5.8041\n",
      "Epoch [385/2000], Eval Loss: 6.0639\n",
      "Epoch [386/2000], Train Loss: 5.7995\n",
      "Epoch [386/2000], Eval Loss: 6.0607\n",
      "Epoch [387/2000], Train Loss: 5.7931\n",
      "Epoch [387/2000], Eval Loss: 6.0555\n",
      "Epoch [388/2000], Train Loss: 5.7865\n",
      "Epoch [388/2000], Eval Loss: 6.0503\n",
      "Epoch [389/2000], Train Loss: 5.7818\n",
      "Epoch [389/2000], Eval Loss: 6.0454\n",
      "Epoch [390/2000], Train Loss: 5.7760\n",
      "Epoch [390/2000], Eval Loss: 6.0409\n",
      "Epoch [391/2000], Train Loss: 5.7723\n",
      "Epoch [391/2000], Eval Loss: 6.0362\n",
      "Epoch [392/2000], Train Loss: 5.7655\n",
      "Epoch [392/2000], Eval Loss: 6.0309\n",
      "Epoch [393/2000], Train Loss: 5.7604\n",
      "Epoch [393/2000], Eval Loss: 6.0269\n",
      "Epoch [394/2000], Train Loss: 5.7558\n",
      "Epoch [394/2000], Eval Loss: 6.0233\n",
      "Epoch [395/2000], Train Loss: 5.7513\n",
      "Epoch [395/2000], Eval Loss: 6.0204\n",
      "Epoch [396/2000], Train Loss: 5.7466\n",
      "Epoch [396/2000], Eval Loss: 6.0166\n",
      "Epoch [397/2000], Train Loss: 5.7421\n",
      "Epoch [397/2000], Eval Loss: 6.0127\n",
      "Epoch [398/2000], Train Loss: 5.7387\n",
      "Epoch [398/2000], Eval Loss: 6.0090\n",
      "Epoch [399/2000], Train Loss: 5.7337\n",
      "Epoch [399/2000], Eval Loss: 6.0056\n",
      "Epoch [400/2000], Train Loss: 5.7307\n",
      "Epoch [400/2000], Eval Loss: 6.0031\n",
      "Epoch [401/2000], Train Loss: 5.7261\n",
      "Epoch [401/2000], Eval Loss: 6.0008\n",
      "Epoch [402/2000], Train Loss: 5.7218\n",
      "Epoch [402/2000], Eval Loss: 5.9978\n",
      "Epoch [403/2000], Train Loss: 5.7184\n",
      "Epoch [403/2000], Eval Loss: 5.9952\n",
      "Epoch [404/2000], Train Loss: 5.7156\n",
      "Epoch [404/2000], Eval Loss: 5.9929\n",
      "Epoch [405/2000], Train Loss: 5.7109\n",
      "Epoch [405/2000], Eval Loss: 5.9908\n",
      "Epoch [406/2000], Train Loss: 5.7074\n",
      "Epoch [406/2000], Eval Loss: 5.9882\n",
      "Epoch [407/2000], Train Loss: 5.7038\n",
      "Epoch [407/2000], Eval Loss: 5.9855\n",
      "Epoch [408/2000], Train Loss: 5.7005\n",
      "Epoch [408/2000], Eval Loss: 5.9826\n",
      "Epoch [409/2000], Train Loss: 5.6978\n",
      "Epoch [409/2000], Eval Loss: 5.9806\n",
      "Epoch [410/2000], Train Loss: 5.6937\n",
      "Epoch [410/2000], Eval Loss: 5.9790\n",
      "Epoch [411/2000], Train Loss: 5.6925\n",
      "Epoch [411/2000], Eval Loss: 5.9765\n",
      "Epoch [412/2000], Train Loss: 5.6878\n",
      "Epoch [412/2000], Eval Loss: 5.9737\n",
      "Epoch [413/2000], Train Loss: 5.6842\n",
      "Epoch [413/2000], Eval Loss: 5.9720\n",
      "Epoch [414/2000], Train Loss: 5.6810\n",
      "Epoch [414/2000], Eval Loss: 5.9708\n",
      "Epoch [415/2000], Train Loss: 5.6779\n",
      "Epoch [415/2000], Eval Loss: 5.9693\n",
      "Epoch [416/2000], Train Loss: 5.6750\n",
      "Epoch [416/2000], Eval Loss: 5.9676\n",
      "Epoch [417/2000], Train Loss: 5.6732\n",
      "Epoch [417/2000], Eval Loss: 5.9659\n",
      "Epoch [418/2000], Train Loss: 5.6699\n",
      "Epoch [418/2000], Eval Loss: 5.9637\n",
      "Epoch [419/2000], Train Loss: 5.6667\n",
      "Epoch [419/2000], Eval Loss: 5.9617\n",
      "Epoch [420/2000], Train Loss: 5.6646\n",
      "Epoch [420/2000], Eval Loss: 5.9607\n",
      "Epoch [421/2000], Train Loss: 5.6612\n",
      "Epoch [421/2000], Eval Loss: 5.9595\n",
      "Epoch [422/2000], Train Loss: 5.6582\n",
      "Epoch [422/2000], Eval Loss: 5.9579\n",
      "Epoch [423/2000], Train Loss: 5.6557\n",
      "Epoch [423/2000], Eval Loss: 5.9565\n",
      "Epoch [424/2000], Train Loss: 5.6538\n",
      "Epoch [424/2000], Eval Loss: 5.9553\n",
      "Epoch [425/2000], Train Loss: 5.6506\n",
      "Epoch [425/2000], Eval Loss: 5.9540\n",
      "Epoch [426/2000], Train Loss: 5.6478\n",
      "Epoch [426/2000], Eval Loss: 5.9528\n",
      "Epoch [427/2000], Train Loss: 5.6477\n",
      "Epoch [427/2000], Eval Loss: 5.9514\n",
      "Epoch [428/2000], Train Loss: 5.6451\n",
      "Epoch [428/2000], Eval Loss: 5.9496\n",
      "Epoch [429/2000], Train Loss: 5.6419\n",
      "Epoch [429/2000], Eval Loss: 5.9482\n",
      "Epoch [430/2000], Train Loss: 5.6396\n",
      "Epoch [430/2000], Eval Loss: 5.9467\n",
      "Epoch [431/2000], Train Loss: 5.6366\n",
      "Epoch [431/2000], Eval Loss: 5.9458\n",
      "Epoch [432/2000], Train Loss: 5.6344\n",
      "Epoch [432/2000], Eval Loss: 5.9453\n",
      "Epoch [433/2000], Train Loss: 5.6320\n",
      "Epoch [433/2000], Eval Loss: 5.9441\n",
      "Epoch [434/2000], Train Loss: 5.6288\n",
      "Epoch [434/2000], Eval Loss: 5.9424\n",
      "Epoch [435/2000], Train Loss: 5.6264\n",
      "Epoch [435/2000], Eval Loss: 5.9412\n",
      "Epoch [436/2000], Train Loss: 5.6246\n",
      "Epoch [436/2000], Eval Loss: 5.9407\n",
      "Epoch [437/2000], Train Loss: 5.6218\n",
      "Epoch [437/2000], Eval Loss: 5.9401\n",
      "Epoch [438/2000], Train Loss: 5.6202\n",
      "Epoch [438/2000], Eval Loss: 5.9392\n",
      "Epoch [439/2000], Train Loss: 5.6189\n",
      "Epoch [439/2000], Eval Loss: 5.9390\n",
      "Epoch [440/2000], Train Loss: 5.6152\n",
      "Epoch [440/2000], Eval Loss: 5.9384\n",
      "Epoch [441/2000], Train Loss: 5.6131\n",
      "Epoch [441/2000], Eval Loss: 5.9374\n",
      "Epoch [442/2000], Train Loss: 5.6136\n",
      "Epoch [442/2000], Eval Loss: 5.9362\n",
      "Epoch [443/2000], Train Loss: 5.6091\n",
      "Epoch [443/2000], Eval Loss: 5.9353\n",
      "Epoch [444/2000], Train Loss: 5.6073\n",
      "Epoch [444/2000], Eval Loss: 5.9337\n",
      "Epoch [445/2000], Train Loss: 5.6058\n",
      "Epoch [445/2000], Eval Loss: 5.9327\n",
      "Epoch [446/2000], Train Loss: 5.6031\n",
      "Epoch [446/2000], Eval Loss: 5.9323\n",
      "Epoch [447/2000], Train Loss: 5.6009\n",
      "Epoch [447/2000], Eval Loss: 5.9321\n",
      "Epoch [448/2000], Train Loss: 5.5991\n",
      "Epoch [448/2000], Eval Loss: 5.9314\n",
      "Epoch [449/2000], Train Loss: 5.5967\n",
      "Epoch [449/2000], Eval Loss: 5.9312\n",
      "Epoch [450/2000], Train Loss: 5.5978\n",
      "Epoch [450/2000], Eval Loss: 5.9310\n",
      "Epoch [451/2000], Train Loss: 5.5933\n",
      "Epoch [451/2000], Eval Loss: 5.9302\n",
      "Epoch [452/2000], Train Loss: 5.5917\n",
      "Epoch [452/2000], Eval Loss: 5.9292\n",
      "Epoch [453/2000], Train Loss: 5.5908\n",
      "Epoch [453/2000], Eval Loss: 5.9289\n",
      "Epoch [454/2000], Train Loss: 5.5879\n",
      "Epoch [454/2000], Eval Loss: 5.9290\n",
      "Epoch [455/2000], Train Loss: 5.5853\n",
      "Epoch [455/2000], Eval Loss: 5.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [456/2000], Train Loss: 5.5832\n",
      "Epoch [456/2000], Eval Loss: 5.9274\n",
      "Epoch [457/2000], Train Loss: 5.5825\n",
      "Epoch [457/2000], Eval Loss: 5.9269\n",
      "Epoch [458/2000], Train Loss: 5.5804\n",
      "Epoch [458/2000], Eval Loss: 5.9269\n",
      "Epoch [459/2000], Train Loss: 5.5776\n",
      "Epoch [459/2000], Eval Loss: 5.9266\n",
      "Epoch [460/2000], Train Loss: 5.5757\n",
      "Epoch [460/2000], Eval Loss: 5.9254\n",
      "Epoch [461/2000], Train Loss: 5.5740\n",
      "Epoch [461/2000], Eval Loss: 5.9259\n",
      "Epoch [462/2000], Train Loss: 5.5734\n",
      "Epoch [462/2000], Eval Loss: 5.9264\n",
      "Epoch [463/2000], Train Loss: 5.5707\n",
      "Epoch [463/2000], Eval Loss: 5.9259\n",
      "Epoch [464/2000], Train Loss: 5.5713\n",
      "Epoch [464/2000], Eval Loss: 5.9252\n",
      "Epoch [465/2000], Train Loss: 5.5705\n",
      "Epoch [465/2000], Eval Loss: 5.9248\n",
      "Epoch [466/2000], Train Loss: 5.5654\n",
      "Epoch [466/2000], Eval Loss: 5.9242\n",
      "Epoch [467/2000], Train Loss: 5.5643\n",
      "Epoch [467/2000], Eval Loss: 5.9233\n",
      "Epoch [468/2000], Train Loss: 5.5624\n",
      "Epoch [468/2000], Eval Loss: 5.9225\n",
      "Epoch [469/2000], Train Loss: 5.5618\n",
      "Epoch [469/2000], Eval Loss: 5.9226\n",
      "Epoch [470/2000], Train Loss: 5.5594\n",
      "Epoch [470/2000], Eval Loss: 5.9226\n",
      "Epoch [471/2000], Train Loss: 5.5577\n",
      "Epoch [471/2000], Eval Loss: 5.9212\n",
      "Epoch [472/2000], Train Loss: 5.5571\n",
      "Epoch [472/2000], Eval Loss: 5.9203\n",
      "Epoch [473/2000], Train Loss: 5.5534\n",
      "Epoch [473/2000], Eval Loss: 5.9216\n",
      "Epoch [474/2000], Train Loss: 5.5521\n",
      "Epoch [474/2000], Eval Loss: 5.9220\n",
      "Epoch [475/2000], Train Loss: 5.5499\n",
      "Epoch [475/2000], Eval Loss: 5.9223\n",
      "Epoch [476/2000], Train Loss: 5.5500\n",
      "Epoch [476/2000], Eval Loss: 5.9225\n",
      "Epoch [477/2000], Train Loss: 5.5466\n",
      "Epoch [477/2000], Eval Loss: 5.9224\n",
      "Epoch [478/2000], Train Loss: 5.5467\n",
      "Epoch [478/2000], Eval Loss: 5.9222\n",
      "Epoch [479/2000], Train Loss: 5.5445\n",
      "Epoch [479/2000], Eval Loss: 5.9228\n",
      "Epoch [480/2000], Train Loss: 5.5438\n",
      "Epoch [480/2000], Eval Loss: 5.9230\n",
      "Epoch [481/2000], Train Loss: 5.5405\n",
      "Epoch [481/2000], Eval Loss: 5.9229\n",
      "Epoch [482/2000], Train Loss: 5.5394\n",
      "Epoch [482/2000], Eval Loss: 5.9227\n",
      "Epoch [483/2000], Train Loss: 5.5385\n",
      "Epoch [483/2000], Eval Loss: 5.9221\n",
      "Epoch [484/2000], Train Loss: 5.5370\n",
      "Epoch [484/2000], Eval Loss: 5.9226\n",
      "Epoch [485/2000], Train Loss: 5.5341\n",
      "Epoch [485/2000], Eval Loss: 5.9226\n",
      "Epoch [486/2000], Train Loss: 5.5324\n",
      "Epoch [486/2000], Eval Loss: 5.9222\n",
      "Epoch [487/2000], Train Loss: 5.5307\n",
      "Epoch [487/2000], Eval Loss: 5.9227\n",
      "Epoch [488/2000], Train Loss: 5.5303\n",
      "Epoch [488/2000], Eval Loss: 5.9244\n",
      "Epoch [489/2000], Train Loss: 5.5279\n",
      "Epoch [489/2000], Eval Loss: 5.9254\n",
      "Epoch [490/2000], Train Loss: 5.5262\n",
      "Epoch [490/2000], Eval Loss: 5.9242\n",
      "Epoch [491/2000], Train Loss: 5.5245\n",
      "Epoch [491/2000], Eval Loss: 5.9246\n",
      "Epoch [492/2000], Train Loss: 5.5234\n",
      "Epoch [492/2000], Eval Loss: 5.9258\n",
      "Epoch [493/2000], Train Loss: 5.5234\n",
      "Epoch [493/2000], Eval Loss: 5.9249\n",
      "Epoch [494/2000], Train Loss: 5.5218\n",
      "Epoch [494/2000], Eval Loss: 5.9243\n",
      "Epoch [495/2000], Train Loss: 5.5187\n",
      "Epoch [495/2000], Eval Loss: 5.9252\n",
      "Epoch [496/2000], Train Loss: 5.5165\n",
      "Epoch [496/2000], Eval Loss: 5.9260\n",
      "Epoch [497/2000], Train Loss: 5.5156\n",
      "Epoch [497/2000], Eval Loss: 5.9269\n",
      "Epoch [498/2000], Train Loss: 5.5145\n",
      "Epoch [498/2000], Eval Loss: 5.9283\n",
      "Epoch [499/2000], Train Loss: 5.5139\n",
      "Epoch [499/2000], Eval Loss: 5.9270\n",
      "Epoch [500/2000], Train Loss: 5.5103\n",
      "Epoch [500/2000], Eval Loss: 5.9279\n",
      "Epoch [501/2000], Train Loss: 5.5105\n",
      "Epoch [501/2000], Eval Loss: 5.9283\n",
      "Epoch [502/2000], Train Loss: 5.5079\n",
      "Epoch [502/2000], Eval Loss: 5.9286\n",
      "Epoch [503/2000], Train Loss: 5.5072\n",
      "Epoch [503/2000], Eval Loss: 5.9280\n",
      "Epoch [504/2000], Train Loss: 5.5042\n",
      "Epoch [504/2000], Eval Loss: 5.9293\n",
      "Epoch [505/2000], Train Loss: 5.5027\n",
      "Epoch [505/2000], Eval Loss: 5.9308\n",
      "Epoch [506/2000], Train Loss: 5.5010\n",
      "Epoch [506/2000], Eval Loss: 5.9305\n",
      "Epoch [507/2000], Train Loss: 5.5003\n",
      "Epoch [507/2000], Eval Loss: 5.9311\n",
      "Epoch [508/2000], Train Loss: 5.4998\n",
      "Epoch [508/2000], Eval Loss: 5.9312\n",
      "Epoch [509/2000], Train Loss: 5.4975\n",
      "Epoch [509/2000], Eval Loss: 5.9310\n",
      "Epoch [510/2000], Train Loss: 5.4953\n",
      "Epoch [510/2000], Eval Loss: 5.9320\n",
      "Epoch [511/2000], Train Loss: 5.4932\n",
      "Epoch [511/2000], Eval Loss: 5.9335\n",
      "Epoch [512/2000], Train Loss: 5.4916\n",
      "Epoch [512/2000], Eval Loss: 5.9342\n",
      "Epoch [513/2000], Train Loss: 5.4916\n",
      "Epoch [513/2000], Eval Loss: 5.9347\n",
      "Epoch [514/2000], Train Loss: 5.4892\n",
      "Epoch [514/2000], Eval Loss: 5.9335\n",
      "Epoch [515/2000], Train Loss: 5.4877\n",
      "Epoch [515/2000], Eval Loss: 5.9334\n",
      "Epoch [516/2000], Train Loss: 5.4862\n",
      "Epoch [516/2000], Eval Loss: 5.9355\n",
      "Epoch [517/2000], Train Loss: 5.4840\n",
      "Epoch [517/2000], Eval Loss: 5.9359\n",
      "Epoch [518/2000], Train Loss: 5.4852\n",
      "Epoch [518/2000], Eval Loss: 5.9362\n",
      "Epoch [519/2000], Train Loss: 5.4820\n",
      "Epoch [519/2000], Eval Loss: 5.9374\n",
      "Epoch [520/2000], Train Loss: 5.4794\n",
      "Epoch [520/2000], Eval Loss: 5.9377\n",
      "Epoch [521/2000], Train Loss: 5.4785\n",
      "Epoch [521/2000], Eval Loss: 5.9380\n",
      "Epoch [522/2000], Train Loss: 5.4792\n",
      "Epoch [522/2000], Eval Loss: 5.9401\n",
      "Epoch [523/2000], Train Loss: 5.4771\n",
      "Epoch [523/2000], Eval Loss: 5.9399\n",
      "Epoch [524/2000], Train Loss: 5.4746\n",
      "Epoch [524/2000], Eval Loss: 5.9419\n",
      "Epoch [525/2000], Train Loss: 5.4737\n",
      "Epoch [525/2000], Eval Loss: 5.9429\n",
      "Epoch [526/2000], Train Loss: 5.4708\n",
      "Epoch [526/2000], Eval Loss: 5.9436\n",
      "Epoch [527/2000], Train Loss: 5.4700\n",
      "Epoch [527/2000], Eval Loss: 5.9435\n",
      "Epoch [528/2000], Train Loss: 5.4678\n",
      "Epoch [528/2000], Eval Loss: 5.9441\n",
      "Epoch [529/2000], Train Loss: 5.4670\n",
      "Epoch [529/2000], Eval Loss: 5.9443\n",
      "Epoch [530/2000], Train Loss: 5.4649\n",
      "Epoch [530/2000], Eval Loss: 5.9454\n",
      "Epoch [531/2000], Train Loss: 5.4648\n",
      "Epoch [531/2000], Eval Loss: 5.9468\n",
      "Epoch [532/2000], Train Loss: 5.4614\n",
      "Epoch [532/2000], Eval Loss: 5.9462\n",
      "Epoch [533/2000], Train Loss: 5.4606\n",
      "Epoch [533/2000], Eval Loss: 5.9483\n",
      "Epoch [534/2000], Train Loss: 5.4608\n",
      "Epoch [534/2000], Eval Loss: 5.9481\n",
      "Epoch [535/2000], Train Loss: 5.4571\n",
      "Epoch [535/2000], Eval Loss: 5.9503\n",
      "Epoch [536/2000], Train Loss: 5.4570\n",
      "Epoch [536/2000], Eval Loss: 5.9498\n",
      "Epoch [537/2000], Train Loss: 5.4576\n",
      "Epoch [537/2000], Eval Loss: 5.9529\n",
      "Epoch [538/2000], Train Loss: 5.4532\n",
      "Epoch [538/2000], Eval Loss: 5.9474\n",
      "Epoch [539/2000], Train Loss: 5.4519\n",
      "Epoch [539/2000], Eval Loss: 5.9569\n",
      "Epoch [540/2000], Train Loss: 5.4524\n",
      "Epoch [540/2000], Eval Loss: 5.9548\n",
      "Epoch [541/2000], Train Loss: 5.4517\n",
      "Epoch [541/2000], Eval Loss: 5.9665\n",
      "Epoch [542/2000], Train Loss: 5.4546\n",
      "Epoch [542/2000], Eval Loss: 5.9658\n",
      "Epoch [543/2000], Train Loss: 5.4614\n",
      "Epoch [543/2000], Eval Loss: 6.0291\n",
      "Epoch [544/2000], Train Loss: 5.4905\n",
      "Epoch [544/2000], Eval Loss: 6.2634\n",
      "Epoch [545/2000], Train Loss: 5.7350\n",
      "Epoch [545/2000], Eval Loss: 20.0610\n",
      "Epoch [546/2000], Train Loss: 19.6447\n",
      "Epoch [546/2000], Eval Loss: 71.2597\n",
      "Epoch [547/2000], Train Loss: 72.7836\n",
      "Epoch [547/2000], Eval Loss: 54.1456\n",
      "Epoch [548/2000], Train Loss: 53.9329\n",
      "Epoch [548/2000], Eval Loss: 51.2433\n",
      "Epoch [549/2000], Train Loss: 50.6243\n",
      "Epoch [549/2000], Eval Loss: 53.8654\n",
      "Epoch [550/2000], Train Loss: 53.0905\n",
      "Epoch [550/2000], Eval Loss: 46.0027\n",
      "Epoch [551/2000], Train Loss: 45.5388\n",
      "Epoch [551/2000], Eval Loss: 43.5612\n",
      "Epoch [552/2000], Train Loss: 42.8875\n",
      "Epoch [552/2000], Eval Loss: 38.4242\n",
      "Epoch [553/2000], Train Loss: 37.7141\n",
      "Epoch [553/2000], Eval Loss: 34.7473\n",
      "Epoch [554/2000], Train Loss: 34.2856\n",
      "Epoch [554/2000], Eval Loss: 33.0580\n",
      "Epoch [555/2000], Train Loss: 32.3298\n",
      "Epoch [555/2000], Eval Loss: 31.7827\n",
      "Epoch [556/2000], Train Loss: 30.8788\n",
      "Epoch [556/2000], Eval Loss: 30.9197\n",
      "Epoch [557/2000], Train Loss: 30.0357\n",
      "Epoch [557/2000], Eval Loss: 29.8442\n",
      "Epoch [558/2000], Train Loss: 29.0968\n",
      "Epoch [558/2000], Eval Loss: 28.8005\n",
      "Epoch [559/2000], Train Loss: 28.1314\n",
      "Epoch [559/2000], Eval Loss: 27.9647\n",
      "Epoch [560/2000], Train Loss: 27.4014\n",
      "Epoch [560/2000], Eval Loss: 27.4516\n",
      "Epoch [561/2000], Train Loss: 26.9445\n",
      "Epoch [561/2000], Eval Loss: 26.7953\n",
      "Epoch [562/2000], Train Loss: 26.2779\n",
      "Epoch [562/2000], Eval Loss: 26.3407\n",
      "Epoch [563/2000], Train Loss: 25.8324\n",
      "Epoch [563/2000], Eval Loss: 25.8250\n",
      "Epoch [564/2000], Train Loss: 25.4117\n",
      "Epoch [564/2000], Eval Loss: 25.2657\n",
      "Epoch [565/2000], Train Loss: 24.8407\n",
      "Epoch [565/2000], Eval Loss: 24.8600\n",
      "Epoch [566/2000], Train Loss: 24.4791\n",
      "Epoch [566/2000], Eval Loss: 24.3524\n",
      "Epoch [567/2000], Train Loss: 23.9732\n",
      "Epoch [567/2000], Eval Loss: 23.9667\n",
      "Epoch [568/2000], Train Loss: 23.5879\n",
      "Epoch [568/2000], Eval Loss: 23.6640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [569/2000], Train Loss: 23.2606\n",
      "Epoch [569/2000], Eval Loss: 23.1788\n",
      "Epoch [570/2000], Train Loss: 22.7600\n",
      "Epoch [570/2000], Eval Loss: 22.8126\n",
      "Epoch [571/2000], Train Loss: 22.3809\n",
      "Epoch [571/2000], Eval Loss: 22.5660\n",
      "Epoch [572/2000], Train Loss: 22.1056\n",
      "Epoch [572/2000], Eval Loss: 22.2652\n",
      "Epoch [573/2000], Train Loss: 21.7594\n",
      "Epoch [573/2000], Eval Loss: 21.9120\n",
      "Epoch [574/2000], Train Loss: 21.4315\n",
      "Epoch [574/2000], Eval Loss: 21.6163\n",
      "Epoch [575/2000], Train Loss: 21.1343\n",
      "Epoch [575/2000], Eval Loss: 21.3711\n",
      "Epoch [576/2000], Train Loss: 20.8653\n",
      "Epoch [576/2000], Eval Loss: 21.1462\n",
      "Epoch [577/2000], Train Loss: 20.6032\n",
      "Epoch [577/2000], Eval Loss: 20.9498\n",
      "Epoch [578/2000], Train Loss: 20.3437\n",
      "Epoch [578/2000], Eval Loss: 20.6724\n",
      "Epoch [579/2000], Train Loss: 20.0350\n",
      "Epoch [579/2000], Eval Loss: 20.4484\n",
      "Epoch [580/2000], Train Loss: 19.7560\n",
      "Epoch [580/2000], Eval Loss: 20.2401\n",
      "Epoch [581/2000], Train Loss: 19.5349\n",
      "Epoch [581/2000], Eval Loss: 19.9637\n",
      "Epoch [582/2000], Train Loss: 19.2969\n",
      "Epoch [582/2000], Eval Loss: 19.6572\n",
      "Epoch [583/2000], Train Loss: 19.0399\n",
      "Epoch [583/2000], Eval Loss: 19.4219\n",
      "Epoch [584/2000], Train Loss: 18.8326\n",
      "Epoch [584/2000], Eval Loss: 19.1965\n",
      "Epoch [585/2000], Train Loss: 18.6360\n",
      "Epoch [585/2000], Eval Loss: 19.0158\n",
      "Epoch [586/2000], Train Loss: 18.4422\n",
      "Epoch [586/2000], Eval Loss: 18.8133\n",
      "Epoch [587/2000], Train Loss: 18.2242\n",
      "Epoch [587/2000], Eval Loss: 18.5854\n",
      "Epoch [588/2000], Train Loss: 18.0057\n",
      "Epoch [588/2000], Eval Loss: 18.4106\n",
      "Epoch [589/2000], Train Loss: 17.8273\n",
      "Epoch [589/2000], Eval Loss: 18.2325\n",
      "Epoch [590/2000], Train Loss: 17.6754\n",
      "Epoch [590/2000], Eval Loss: 18.0600\n",
      "Epoch [591/2000], Train Loss: 17.5072\n",
      "Epoch [591/2000], Eval Loss: 17.8859\n",
      "Epoch [592/2000], Train Loss: 17.3292\n",
      "Epoch [592/2000], Eval Loss: 17.7323\n",
      "Epoch [593/2000], Train Loss: 17.1597\n",
      "Epoch [593/2000], Eval Loss: 17.5826\n",
      "Epoch [594/2000], Train Loss: 16.9889\n",
      "Epoch [594/2000], Eval Loss: 17.4047\n",
      "Epoch [595/2000], Train Loss: 16.8097\n",
      "Epoch [595/2000], Eval Loss: 17.2458\n",
      "Epoch [596/2000], Train Loss: 16.6507\n",
      "Epoch [596/2000], Eval Loss: 17.0778\n",
      "Epoch [597/2000], Train Loss: 16.4964\n",
      "Epoch [597/2000], Eval Loss: 16.9395\n",
      "Epoch [598/2000], Train Loss: 16.3426\n",
      "Epoch [598/2000], Eval Loss: 16.7995\n",
      "Epoch [599/2000], Train Loss: 16.1923\n",
      "Epoch [599/2000], Eval Loss: 16.6703\n",
      "Epoch [600/2000], Train Loss: 16.0451\n",
      "Epoch [600/2000], Eval Loss: 16.5361\n",
      "Epoch [601/2000], Train Loss: 15.9036\n",
      "Epoch [601/2000], Eval Loss: 16.3827\n",
      "Epoch [602/2000], Train Loss: 15.7647\n",
      "Epoch [602/2000], Eval Loss: 16.2664\n",
      "Epoch [603/2000], Train Loss: 15.6240\n",
      "Epoch [603/2000], Eval Loss: 16.1432\n",
      "Epoch [604/2000], Train Loss: 15.4943\n",
      "Epoch [604/2000], Eval Loss: 16.0180\n",
      "Epoch [605/2000], Train Loss: 15.3849\n",
      "Epoch [605/2000], Eval Loss: 15.8745\n",
      "Epoch [606/2000], Train Loss: 15.2586\n",
      "Epoch [606/2000], Eval Loss: 15.7424\n",
      "Epoch [607/2000], Train Loss: 15.1289\n",
      "Epoch [607/2000], Eval Loss: 15.6244\n",
      "Epoch [608/2000], Train Loss: 15.0008\n",
      "Epoch [608/2000], Eval Loss: 15.5162\n",
      "Epoch [609/2000], Train Loss: 14.8948\n",
      "Epoch [609/2000], Eval Loss: 15.4126\n",
      "Epoch [610/2000], Train Loss: 14.7843\n",
      "Epoch [610/2000], Eval Loss: 15.3163\n",
      "Epoch [611/2000], Train Loss: 14.6642\n",
      "Epoch [611/2000], Eval Loss: 15.2165\n",
      "Epoch [612/2000], Train Loss: 14.5393\n",
      "Epoch [612/2000], Eval Loss: 15.1204\n",
      "Epoch [613/2000], Train Loss: 14.4239\n",
      "Epoch [613/2000], Eval Loss: 15.0064\n",
      "Epoch [614/2000], Train Loss: 14.3055\n",
      "Epoch [614/2000], Eval Loss: 14.8878\n",
      "Epoch [615/2000], Train Loss: 14.2019\n",
      "Epoch [615/2000], Eval Loss: 14.7955\n",
      "Epoch [616/2000], Train Loss: 14.1026\n",
      "Epoch [616/2000], Eval Loss: 14.7013\n",
      "Epoch [617/2000], Train Loss: 14.0008\n",
      "Epoch [617/2000], Eval Loss: 14.5926\n",
      "Epoch [618/2000], Train Loss: 13.9045\n",
      "Epoch [618/2000], Eval Loss: 14.5028\n",
      "Epoch [619/2000], Train Loss: 13.8047\n",
      "Epoch [619/2000], Eval Loss: 14.4014\n",
      "Epoch [620/2000], Train Loss: 13.7126\n",
      "Epoch [620/2000], Eval Loss: 14.2802\n",
      "Epoch [621/2000], Train Loss: 13.6094\n",
      "Epoch [621/2000], Eval Loss: 14.1862\n",
      "Epoch [622/2000], Train Loss: 13.5194\n",
      "Epoch [622/2000], Eval Loss: 14.0794\n",
      "Epoch [623/2000], Train Loss: 13.4174\n",
      "Epoch [623/2000], Eval Loss: 13.9908\n",
      "Epoch [624/2000], Train Loss: 13.3322\n",
      "Epoch [624/2000], Eval Loss: 13.9089\n",
      "Epoch [625/2000], Train Loss: 13.2453\n",
      "Epoch [625/2000], Eval Loss: 13.8251\n",
      "Epoch [626/2000], Train Loss: 13.1508\n",
      "Epoch [626/2000], Eval Loss: 13.7292\n",
      "Epoch [627/2000], Train Loss: 13.0607\n",
      "Epoch [627/2000], Eval Loss: 13.6271\n",
      "Epoch [628/2000], Train Loss: 12.9672\n",
      "Epoch [628/2000], Eval Loss: 13.5266\n",
      "Epoch [629/2000], Train Loss: 12.8704\n",
      "Epoch [629/2000], Eval Loss: 13.4521\n",
      "Epoch [630/2000], Train Loss: 12.7893\n",
      "Epoch [630/2000], Eval Loss: 13.3480\n",
      "Epoch [631/2000], Train Loss: 12.7036\n",
      "Epoch [631/2000], Eval Loss: 13.2633\n",
      "Epoch [632/2000], Train Loss: 12.6176\n",
      "Epoch [632/2000], Eval Loss: 13.1764\n",
      "Epoch [633/2000], Train Loss: 12.5240\n",
      "Epoch [633/2000], Eval Loss: 13.0784\n",
      "Epoch [634/2000], Train Loss: 12.4446\n",
      "Epoch [634/2000], Eval Loss: 13.0059\n",
      "Epoch [635/2000], Train Loss: 12.3597\n",
      "Epoch [635/2000], Eval Loss: 12.9277\n",
      "Epoch [636/2000], Train Loss: 12.2781\n",
      "Epoch [636/2000], Eval Loss: 12.8547\n",
      "Epoch [637/2000], Train Loss: 12.1970\n",
      "Epoch [637/2000], Eval Loss: 12.7685\n",
      "Epoch [638/2000], Train Loss: 12.1168\n",
      "Epoch [638/2000], Eval Loss: 12.6912\n",
      "Epoch [639/2000], Train Loss: 12.0357\n",
      "Epoch [639/2000], Eval Loss: 12.6185\n",
      "Epoch [640/2000], Train Loss: 11.9517\n",
      "Epoch [640/2000], Eval Loss: 12.5396\n",
      "Epoch [641/2000], Train Loss: 11.8839\n",
      "Epoch [641/2000], Eval Loss: 12.4632\n",
      "Epoch [642/2000], Train Loss: 11.8116\n",
      "Epoch [642/2000], Eval Loss: 12.3766\n",
      "Epoch [643/2000], Train Loss: 11.7407\n",
      "Epoch [643/2000], Eval Loss: 12.2982\n",
      "Epoch [644/2000], Train Loss: 11.6662\n",
      "Epoch [644/2000], Eval Loss: 12.2318\n",
      "Epoch [645/2000], Train Loss: 11.5988\n",
      "Epoch [645/2000], Eval Loss: 12.1661\n",
      "Epoch [646/2000], Train Loss: 11.5323\n",
      "Epoch [646/2000], Eval Loss: 12.0726\n",
      "Epoch [647/2000], Train Loss: 11.4615\n",
      "Epoch [647/2000], Eval Loss: 11.9935\n",
      "Epoch [648/2000], Train Loss: 11.3797\n",
      "Epoch [648/2000], Eval Loss: 11.9461\n",
      "Epoch [649/2000], Train Loss: 11.3318\n",
      "Epoch [649/2000], Eval Loss: 11.8843\n",
      "Epoch [650/2000], Train Loss: 11.2625\n",
      "Epoch [650/2000], Eval Loss: 11.8175\n",
      "Epoch [651/2000], Train Loss: 11.1865\n",
      "Epoch [651/2000], Eval Loss: 11.7371\n",
      "Epoch [652/2000], Train Loss: 11.1190\n",
      "Epoch [652/2000], Eval Loss: 11.6761\n",
      "Epoch [653/2000], Train Loss: 11.0545\n",
      "Epoch [653/2000], Eval Loss: 11.6136\n",
      "Epoch [654/2000], Train Loss: 10.9896\n",
      "Epoch [654/2000], Eval Loss: 11.5619\n",
      "Epoch [655/2000], Train Loss: 10.9480\n",
      "Epoch [655/2000], Eval Loss: 11.5284\n",
      "Epoch [656/2000], Train Loss: 10.9286\n",
      "Epoch [656/2000], Eval Loss: 11.5591\n",
      "Epoch [657/2000], Train Loss: 10.9653\n",
      "Epoch [657/2000], Eval Loss: 11.7136\n",
      "Epoch [658/2000], Train Loss: 11.1578\n",
      "Epoch [658/2000], Eval Loss: 12.1025\n",
      "Epoch [659/2000], Train Loss: 11.5209\n",
      "Epoch [659/2000], Eval Loss: 11.9539\n",
      "Epoch [660/2000], Train Loss: 11.4176\n",
      "Epoch [660/2000], Eval Loss: 11.2846\n",
      "Epoch [661/2000], Train Loss: 10.7462\n",
      "Epoch [661/2000], Eval Loss: 11.5444\n",
      "Epoch [662/2000], Train Loss: 11.0130\n",
      "Epoch [662/2000], Eval Loss: 11.3720\n",
      "Epoch [663/2000], Train Loss: 10.8261\n",
      "Epoch [663/2000], Eval Loss: 11.2436\n",
      "Epoch [664/2000], Train Loss: 10.6957\n",
      "Epoch [664/2000], Eval Loss: 11.2008\n",
      "Epoch [665/2000], Train Loss: 10.6731\n",
      "Epoch [665/2000], Eval Loss: 11.0343\n",
      "Epoch [666/2000], Train Loss: 10.4855\n",
      "Epoch [666/2000], Eval Loss: 11.1020\n",
      "Epoch [667/2000], Train Loss: 10.5702\n",
      "Epoch [667/2000], Eval Loss: 10.9460\n",
      "Epoch [668/2000], Train Loss: 10.4097\n",
      "Epoch [668/2000], Eval Loss: 10.8542\n",
      "Epoch [669/2000], Train Loss: 10.3114\n",
      "Epoch [669/2000], Eval Loss: 10.9072\n",
      "Epoch [670/2000], Train Loss: 10.3577\n",
      "Epoch [670/2000], Eval Loss: 10.7699\n",
      "Epoch [671/2000], Train Loss: 10.2243\n",
      "Epoch [671/2000], Eval Loss: 10.7004\n",
      "Epoch [672/2000], Train Loss: 10.1635\n",
      "Epoch [672/2000], Eval Loss: 10.6163\n",
      "Epoch [673/2000], Train Loss: 10.0990\n",
      "Epoch [673/2000], Eval Loss: 10.5457\n",
      "Epoch [674/2000], Train Loss: 10.0178\n",
      "Epoch [674/2000], Eval Loss: 10.5317\n",
      "Epoch [675/2000], Train Loss: 9.9999\n",
      "Epoch [675/2000], Eval Loss: 10.5249\n",
      "Epoch [676/2000], Train Loss: 10.0093\n",
      "Epoch [676/2000], Eval Loss: 10.4286\n",
      "Epoch [677/2000], Train Loss: 9.9011\n",
      "Epoch [677/2000], Eval Loss: 10.3532\n",
      "Epoch [678/2000], Train Loss: 9.8102\n",
      "Epoch [678/2000], Eval Loss: 10.3284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [679/2000], Train Loss: 9.7946\n",
      "Epoch [679/2000], Eval Loss: 10.2658\n",
      "Epoch [680/2000], Train Loss: 9.7199\n",
      "Epoch [680/2000], Eval Loss: 10.1965\n",
      "Epoch [681/2000], Train Loss: 9.6834\n",
      "Epoch [681/2000], Eval Loss: 10.2652\n",
      "Epoch [682/2000], Train Loss: 9.7558\n",
      "Epoch [682/2000], Eval Loss: 10.4901\n",
      "Epoch [683/2000], Train Loss: 10.0123\n",
      "Epoch [683/2000], Eval Loss: 11.1316\n",
      "Epoch [684/2000], Train Loss: 10.6581\n",
      "Epoch [684/2000], Eval Loss: 11.6730\n",
      "Epoch [685/2000], Train Loss: 11.2240\n",
      "Epoch [685/2000], Eval Loss: 11.2525\n",
      "Epoch [686/2000], Train Loss: 10.7120\n",
      "Epoch [686/2000], Eval Loss: 10.8855\n",
      "Epoch [687/2000], Train Loss: 10.4018\n",
      "Epoch [687/2000], Eval Loss: 10.6915\n",
      "Epoch [688/2000], Train Loss: 10.2947\n",
      "Epoch [688/2000], Eval Loss: 10.2538\n",
      "Epoch [689/2000], Train Loss: 9.7987\n",
      "Epoch [689/2000], Eval Loss: 10.5402\n",
      "Epoch [690/2000], Train Loss: 10.1061\n",
      "Epoch [690/2000], Eval Loss: 10.1941\n",
      "Epoch [691/2000], Train Loss: 9.7588\n",
      "Epoch [691/2000], Eval Loss: 10.2300\n",
      "Epoch [692/2000], Train Loss: 9.7981\n",
      "Epoch [692/2000], Eval Loss: 10.1524\n",
      "Epoch [693/2000], Train Loss: 9.7234\n",
      "Epoch [693/2000], Eval Loss: 9.9290\n",
      "Epoch [694/2000], Train Loss: 9.4918\n",
      "Epoch [694/2000], Eval Loss: 10.0322\n",
      "Epoch [695/2000], Train Loss: 9.5926\n",
      "Epoch [695/2000], Eval Loss: 9.8299\n",
      "Epoch [696/2000], Train Loss: 9.3815\n",
      "Epoch [696/2000], Eval Loss: 9.8825\n",
      "Epoch [697/2000], Train Loss: 9.4278\n",
      "Epoch [697/2000], Eval Loss: 9.7410\n",
      "Epoch [698/2000], Train Loss: 9.2871\n",
      "Epoch [698/2000], Eval Loss: 9.7372\n",
      "Epoch [699/2000], Train Loss: 9.2937\n",
      "Epoch [699/2000], Eval Loss: 9.5733\n",
      "Epoch [700/2000], Train Loss: 9.1348\n",
      "Epoch [700/2000], Eval Loss: 9.6063\n",
      "Epoch [701/2000], Train Loss: 9.1866\n",
      "Epoch [701/2000], Eval Loss: 9.5091\n",
      "Epoch [702/2000], Train Loss: 9.0558\n",
      "Epoch [702/2000], Eval Loss: 9.4948\n",
      "Epoch [703/2000], Train Loss: 9.0466\n",
      "Epoch [703/2000], Eval Loss: 9.5531\n",
      "Epoch [704/2000], Train Loss: 9.0915\n",
      "Epoch [704/2000], Eval Loss: 9.5889\n",
      "Epoch [705/2000], Train Loss: 9.1586\n",
      "Epoch [705/2000], Eval Loss: 10.3343\n",
      "Epoch [706/2000], Train Loss: 9.8365\n",
      "Epoch [706/2000], Eval Loss: 11.3615\n",
      "Epoch [707/2000], Train Loss: 10.9668\n",
      "Epoch [707/2000], Eval Loss: 10.5614\n",
      "Epoch [708/2000], Train Loss: 10.0733\n",
      "Epoch [708/2000], Eval Loss: 10.2854\n",
      "Epoch [709/2000], Train Loss: 9.8621\n",
      "Epoch [709/2000], Eval Loss: 10.0711\n",
      "Epoch [710/2000], Train Loss: 9.6540\n",
      "Epoch [710/2000], Eval Loss: 10.2496\n",
      "Epoch [711/2000], Train Loss: 9.7966\n",
      "Epoch [711/2000], Eval Loss: 9.8737\n",
      "Epoch [712/2000], Train Loss: 9.4476\n",
      "Epoch [712/2000], Eval Loss: 9.6792\n",
      "Epoch [713/2000], Train Loss: 9.2643\n",
      "Epoch [713/2000], Eval Loss: 9.7244\n",
      "Epoch [714/2000], Train Loss: 9.3028\n",
      "Epoch [714/2000], Eval Loss: 9.5312\n",
      "Epoch [715/2000], Train Loss: 9.1128\n",
      "Epoch [715/2000], Eval Loss: 9.5373\n",
      "Epoch [716/2000], Train Loss: 9.1441\n",
      "Epoch [716/2000], Eval Loss: 9.3736\n",
      "Epoch [717/2000], Train Loss: 8.9677\n",
      "Epoch [717/2000], Eval Loss: 9.4505\n",
      "Epoch [718/2000], Train Loss: 9.0405\n",
      "Epoch [718/2000], Eval Loss: 9.2568\n",
      "Epoch [719/2000], Train Loss: 8.8345\n",
      "Epoch [719/2000], Eval Loss: 9.2577\n",
      "Epoch [720/2000], Train Loss: 8.8545\n",
      "Epoch [720/2000], Eval Loss: 9.1541\n",
      "Epoch [721/2000], Train Loss: 8.7599\n",
      "Epoch [721/2000], Eval Loss: 9.1087\n",
      "Epoch [722/2000], Train Loss: 8.7107\n",
      "Epoch [722/2000], Eval Loss: 8.9983\n",
      "Epoch [723/2000], Train Loss: 8.6077\n",
      "Epoch [723/2000], Eval Loss: 8.9747\n",
      "Epoch [724/2000], Train Loss: 8.5968\n",
      "Epoch [724/2000], Eval Loss: 8.8859\n",
      "Epoch [725/2000], Train Loss: 8.5169\n",
      "Epoch [725/2000], Eval Loss: 8.8729\n",
      "Epoch [726/2000], Train Loss: 8.4818\n",
      "Epoch [726/2000], Eval Loss: 8.8245\n",
      "Epoch [727/2000], Train Loss: 8.4098\n",
      "Epoch [727/2000], Eval Loss: 8.7721\n",
      "Epoch [728/2000], Train Loss: 8.3711\n",
      "Epoch [728/2000], Eval Loss: 8.6606\n",
      "Epoch [729/2000], Train Loss: 8.2898\n",
      "Epoch [729/2000], Eval Loss: 8.6542\n",
      "Epoch [730/2000], Train Loss: 8.2801\n",
      "Epoch [730/2000], Eval Loss: 8.5921\n",
      "Epoch [731/2000], Train Loss: 8.2188\n",
      "Epoch [731/2000], Eval Loss: 8.5845\n",
      "Epoch [732/2000], Train Loss: 8.1941\n",
      "Epoch [732/2000], Eval Loss: 8.5696\n",
      "Epoch [733/2000], Train Loss: 8.1947\n",
      "Epoch [733/2000], Eval Loss: 8.6412\n",
      "Epoch [734/2000], Train Loss: 8.2603\n",
      "Epoch [734/2000], Eval Loss: 9.0469\n",
      "Epoch [735/2000], Train Loss: 8.6781\n",
      "Epoch [735/2000], Eval Loss: 10.4832\n",
      "Epoch [736/2000], Train Loss: 10.2445\n",
      "Epoch [736/2000], Eval Loss: 12.7453\n",
      "Epoch [737/2000], Train Loss: 12.3728\n",
      "Epoch [737/2000], Eval Loss: 10.6339\n",
      "Epoch [738/2000], Train Loss: 10.3163\n",
      "Epoch [738/2000], Eval Loss: 10.7002\n",
      "Epoch [739/2000], Train Loss: 10.3380\n",
      "Epoch [739/2000], Eval Loss: 9.8745\n",
      "Epoch [740/2000], Train Loss: 9.4844\n",
      "Epoch [740/2000], Eval Loss: 10.1843\n",
      "Epoch [741/2000], Train Loss: 9.7383\n",
      "Epoch [741/2000], Eval Loss: 9.3420\n",
      "Epoch [742/2000], Train Loss: 8.9461\n",
      "Epoch [742/2000], Eval Loss: 9.7477\n",
      "Epoch [743/2000], Train Loss: 9.3654\n",
      "Epoch [743/2000], Eval Loss: 9.3448\n",
      "Epoch [744/2000], Train Loss: 8.9749\n",
      "Epoch [744/2000], Eval Loss: 9.2309\n",
      "Epoch [745/2000], Train Loss: 8.8271\n",
      "Epoch [745/2000], Eval Loss: 9.2713\n",
      "Epoch [746/2000], Train Loss: 8.8780\n",
      "Epoch [746/2000], Eval Loss: 8.9955\n",
      "Epoch [747/2000], Train Loss: 8.5939\n",
      "Epoch [747/2000], Eval Loss: 9.0072\n",
      "Epoch [748/2000], Train Loss: 8.6083\n",
      "Epoch [748/2000], Eval Loss: 8.8750\n",
      "Epoch [749/2000], Train Loss: 8.5072\n",
      "Epoch [749/2000], Eval Loss: 8.7695\n",
      "Epoch [750/2000], Train Loss: 8.3901\n",
      "Epoch [750/2000], Eval Loss: 8.7848\n",
      "Epoch [751/2000], Train Loss: 8.4045\n",
      "Epoch [751/2000], Eval Loss: 8.6125\n",
      "Epoch [752/2000], Train Loss: 8.2455\n",
      "Epoch [752/2000], Eval Loss: 8.6298\n",
      "Epoch [753/2000], Train Loss: 8.2518\n",
      "Epoch [753/2000], Eval Loss: 8.5566\n",
      "Epoch [754/2000], Train Loss: 8.1605\n",
      "Epoch [754/2000], Eval Loss: 8.4588\n",
      "Epoch [755/2000], Train Loss: 8.0609\n",
      "Epoch [755/2000], Eval Loss: 8.4473\n",
      "Epoch [756/2000], Train Loss: 8.0686\n",
      "Epoch [756/2000], Eval Loss: 8.3620\n",
      "Epoch [757/2000], Train Loss: 7.9792\n",
      "Epoch [757/2000], Eval Loss: 8.3107\n",
      "Epoch [758/2000], Train Loss: 7.9195\n",
      "Epoch [758/2000], Eval Loss: 8.2669\n",
      "Epoch [759/2000], Train Loss: 7.8848\n",
      "Epoch [759/2000], Eval Loss: 8.2022\n",
      "Epoch [760/2000], Train Loss: 7.8241\n",
      "Epoch [760/2000], Eval Loss: 8.1351\n",
      "Epoch [761/2000], Train Loss: 7.7704\n",
      "Epoch [761/2000], Eval Loss: 8.1187\n",
      "Epoch [762/2000], Train Loss: 7.7484\n",
      "Epoch [762/2000], Eval Loss: 8.0638\n",
      "Epoch [763/2000], Train Loss: 7.6846\n",
      "Epoch [763/2000], Eval Loss: 8.0341\n",
      "Epoch [764/2000], Train Loss: 7.6595\n",
      "Epoch [764/2000], Eval Loss: 7.9802\n",
      "Epoch [765/2000], Train Loss: 7.6146\n",
      "Epoch [765/2000], Eval Loss: 7.9548\n",
      "Epoch [766/2000], Train Loss: 7.5904\n",
      "Epoch [766/2000], Eval Loss: 7.9043\n",
      "Epoch [767/2000], Train Loss: 7.5321\n",
      "Epoch [767/2000], Eval Loss: 7.8712\n",
      "Epoch [768/2000], Train Loss: 7.4960\n",
      "Epoch [768/2000], Eval Loss: 7.8444\n",
      "Epoch [769/2000], Train Loss: 7.4752\n",
      "Epoch [769/2000], Eval Loss: 7.7863\n",
      "Epoch [770/2000], Train Loss: 7.4356\n",
      "Epoch [770/2000], Eval Loss: 7.7627\n",
      "Epoch [771/2000], Train Loss: 7.4117\n",
      "Epoch [771/2000], Eval Loss: 7.7300\n",
      "Epoch [772/2000], Train Loss: 7.3688\n",
      "Epoch [772/2000], Eval Loss: 7.7098\n",
      "Epoch [773/2000], Train Loss: 7.3508\n",
      "Epoch [773/2000], Eval Loss: 7.6596\n",
      "Epoch [774/2000], Train Loss: 7.3124\n",
      "Epoch [774/2000], Eval Loss: 7.6892\n",
      "Epoch [775/2000], Train Loss: 7.3408\n",
      "Epoch [775/2000], Eval Loss: 7.8645\n",
      "Epoch [776/2000], Train Loss: 7.4890\n",
      "Epoch [776/2000], Eval Loss: 9.0500\n",
      "Epoch [777/2000], Train Loss: 8.6573\n",
      "Epoch [777/2000], Eval Loss: 14.9427\n",
      "Epoch [778/2000], Train Loss: 14.5113\n",
      "Epoch [778/2000], Eval Loss: 12.5768\n",
      "Epoch [779/2000], Train Loss: 12.1789\n",
      "Epoch [779/2000], Eval Loss: 15.4055\n",
      "Epoch [780/2000], Train Loss: 14.9089\n",
      "Epoch [780/2000], Eval Loss: 15.5357\n",
      "Epoch [781/2000], Train Loss: 15.0432\n",
      "Epoch [781/2000], Eval Loss: 13.4990\n",
      "Epoch [782/2000], Train Loss: 13.0807\n",
      "Epoch [782/2000], Eval Loss: 13.8250\n",
      "Epoch [783/2000], Train Loss: 13.3137\n",
      "Epoch [783/2000], Eval Loss: 13.7280\n",
      "Epoch [784/2000], Train Loss: 13.1257\n",
      "Epoch [784/2000], Eval Loss: 12.7207\n",
      "Epoch [785/2000], Train Loss: 12.2478\n",
      "Epoch [785/2000], Eval Loss: 12.3368\n",
      "Epoch [786/2000], Train Loss: 12.0144\n",
      "Epoch [786/2000], Eval Loss: 12.0263\n",
      "Epoch [787/2000], Train Loss: 11.7571\n",
      "Epoch [787/2000], Eval Loss: 11.4251\n",
      "Epoch [788/2000], Train Loss: 11.1075\n",
      "Epoch [788/2000], Eval Loss: 11.2616\n",
      "Epoch [789/2000], Train Loss: 10.9359\n",
      "Epoch [789/2000], Eval Loss: 11.0067\n",
      "Epoch [790/2000], Train Loss: 10.6542\n",
      "Epoch [790/2000], Eval Loss: 10.8034\n",
      "Epoch [791/2000], Train Loss: 10.4263\n",
      "Epoch [791/2000], Eval Loss: 10.6437\n",
      "Epoch [792/2000], Train Loss: 10.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [792/2000], Eval Loss: 10.4880\n",
      "Epoch [793/2000], Train Loss: 10.1090\n",
      "Epoch [793/2000], Eval Loss: 10.2787\n",
      "Epoch [794/2000], Train Loss: 9.8936\n",
      "Epoch [794/2000], Eval Loss: 10.1891\n",
      "Epoch [795/2000], Train Loss: 9.7786\n",
      "Epoch [795/2000], Eval Loss: 9.9592\n",
      "Epoch [796/2000], Train Loss: 9.5553\n",
      "Epoch [796/2000], Eval Loss: 9.8087\n",
      "Epoch [797/2000], Train Loss: 9.4309\n",
      "Epoch [797/2000], Eval Loss: 9.6968\n",
      "Epoch [798/2000], Train Loss: 9.2890\n",
      "Epoch [798/2000], Eval Loss: 9.5531\n",
      "Epoch [799/2000], Train Loss: 9.1609\n",
      "Epoch [799/2000], Eval Loss: 9.3924\n",
      "Epoch [800/2000], Train Loss: 9.0202\n",
      "Epoch [800/2000], Eval Loss: 9.2400\n",
      "Epoch [801/2000], Train Loss: 8.8474\n",
      "Epoch [801/2000], Eval Loss: 9.1082\n",
      "Epoch [802/2000], Train Loss: 8.7213\n",
      "Epoch [802/2000], Eval Loss: 9.0217\n",
      "Epoch [803/2000], Train Loss: 8.6321\n",
      "Epoch [803/2000], Eval Loss: 8.8678\n",
      "Epoch [804/2000], Train Loss: 8.4902\n",
      "Epoch [804/2000], Eval Loss: 8.8004\n",
      "Epoch [805/2000], Train Loss: 8.4401\n",
      "Epoch [805/2000], Eval Loss: 8.6737\n",
      "Epoch [806/2000], Train Loss: 8.3116\n",
      "Epoch [806/2000], Eval Loss: 8.5701\n",
      "Epoch [807/2000], Train Loss: 8.2137\n",
      "Epoch [807/2000], Eval Loss: 8.4620\n",
      "Epoch [808/2000], Train Loss: 8.1140\n",
      "Epoch [808/2000], Eval Loss: 8.4055\n",
      "Epoch [809/2000], Train Loss: 8.0550\n",
      "Epoch [809/2000], Eval Loss: 8.2978\n",
      "Epoch [810/2000], Train Loss: 7.9632\n",
      "Epoch [810/2000], Eval Loss: 8.2139\n",
      "Epoch [811/2000], Train Loss: 7.8925\n",
      "Epoch [811/2000], Eval Loss: 8.1435\n",
      "Epoch [812/2000], Train Loss: 7.8138\n",
      "Epoch [812/2000], Eval Loss: 8.0851\n",
      "Epoch [813/2000], Train Loss: 7.7485\n",
      "Epoch [813/2000], Eval Loss: 8.0308\n",
      "Epoch [814/2000], Train Loss: 7.6918\n",
      "Epoch [814/2000], Eval Loss: 7.9676\n",
      "Epoch [815/2000], Train Loss: 7.6325\n",
      "Epoch [815/2000], Eval Loss: 7.9137\n",
      "Epoch [816/2000], Train Loss: 7.5815\n",
      "Epoch [816/2000], Eval Loss: 7.8705\n",
      "Epoch [817/2000], Train Loss: 7.5367\n",
      "Epoch [817/2000], Eval Loss: 7.8134\n",
      "Epoch [818/2000], Train Loss: 7.4821\n",
      "Epoch [818/2000], Eval Loss: 7.7613\n",
      "Epoch [819/2000], Train Loss: 7.4331\n",
      "Epoch [819/2000], Eval Loss: 7.7165\n",
      "Epoch [820/2000], Train Loss: 7.3927\n",
      "Epoch [820/2000], Eval Loss: 7.6658\n",
      "Epoch [821/2000], Train Loss: 7.3453\n",
      "Epoch [821/2000], Eval Loss: 7.6285\n",
      "Epoch [822/2000], Train Loss: 7.3090\n",
      "Epoch [822/2000], Eval Loss: 7.5846\n",
      "Epoch [823/2000], Train Loss: 7.2705\n",
      "Epoch [823/2000], Eval Loss: 7.5392\n",
      "Epoch [824/2000], Train Loss: 7.2270\n",
      "Epoch [824/2000], Eval Loss: 7.5034\n",
      "Epoch [825/2000], Train Loss: 7.1970\n",
      "Epoch [825/2000], Eval Loss: 7.4716\n",
      "Epoch [826/2000], Train Loss: 7.1575\n",
      "Epoch [826/2000], Eval Loss: 7.4459\n",
      "Epoch [827/2000], Train Loss: 7.1319\n",
      "Epoch [827/2000], Eval Loss: 7.4016\n",
      "Epoch [828/2000], Train Loss: 7.0951\n",
      "Epoch [828/2000], Eval Loss: 7.3769\n",
      "Epoch [829/2000], Train Loss: 7.0680\n",
      "Epoch [829/2000], Eval Loss: 7.3393\n",
      "Epoch [830/2000], Train Loss: 7.0379\n",
      "Epoch [830/2000], Eval Loss: 7.3083\n",
      "Epoch [831/2000], Train Loss: 7.0103\n",
      "Epoch [831/2000], Eval Loss: 7.2769\n",
      "Epoch [832/2000], Train Loss: 6.9825\n",
      "Epoch [832/2000], Eval Loss: 7.2481\n",
      "Epoch [833/2000], Train Loss: 6.9556\n",
      "Epoch [833/2000], Eval Loss: 7.2219\n",
      "Epoch [834/2000], Train Loss: 6.9354\n",
      "Epoch [834/2000], Eval Loss: 7.1960\n",
      "Epoch [835/2000], Train Loss: 6.9127\n",
      "Epoch [835/2000], Eval Loss: 7.1694\n",
      "Epoch [836/2000], Train Loss: 6.8888\n",
      "Epoch [836/2000], Eval Loss: 7.1511\n",
      "Epoch [837/2000], Train Loss: 6.8684\n",
      "Epoch [837/2000], Eval Loss: 7.1313\n",
      "Epoch [838/2000], Train Loss: 6.8474\n",
      "Epoch [838/2000], Eval Loss: 7.1129\n",
      "Epoch [839/2000], Train Loss: 6.8299\n",
      "Epoch [839/2000], Eval Loss: 7.0930\n",
      "Epoch [840/2000], Train Loss: 6.8088\n",
      "Epoch [840/2000], Eval Loss: 7.0769\n",
      "Epoch [841/2000], Train Loss: 6.7938\n",
      "Epoch [841/2000], Eval Loss: 7.0607\n",
      "Epoch [842/2000], Train Loss: 6.7779\n",
      "Epoch [842/2000], Eval Loss: 7.0481\n",
      "Epoch [843/2000], Train Loss: 6.7681\n",
      "Epoch [843/2000], Eval Loss: 7.0455\n",
      "Epoch [844/2000], Train Loss: 6.7668\n",
      "Epoch [844/2000], Eval Loss: 7.0602\n",
      "Epoch [845/2000], Train Loss: 6.7833\n",
      "Epoch [845/2000], Eval Loss: 7.1796\n",
      "Epoch [846/2000], Train Loss: 6.8959\n",
      "Epoch [846/2000], Eval Loss: 7.7617\n",
      "Epoch [847/2000], Train Loss: 7.4969\n",
      "Epoch [847/2000], Eval Loss: 11.9004\n",
      "Epoch [848/2000], Train Loss: 11.5751\n",
      "Epoch [848/2000], Eval Loss: 17.9039\n",
      "Epoch [849/2000], Train Loss: 17.9804\n",
      "Epoch [849/2000], Eval Loss: 15.0875\n",
      "Epoch [850/2000], Train Loss: 14.7707\n",
      "Epoch [850/2000], Eval Loss: 14.0500\n",
      "Epoch [851/2000], Train Loss: 13.7408\n",
      "Epoch [851/2000], Eval Loss: 13.6121\n",
      "Epoch [852/2000], Train Loss: 13.1239\n",
      "Epoch [852/2000], Eval Loss: 11.3738\n",
      "Epoch [853/2000], Train Loss: 10.9734\n",
      "Epoch [853/2000], Eval Loss: 11.0717\n",
      "Epoch [854/2000], Train Loss: 10.6783\n",
      "Epoch [854/2000], Eval Loss: 11.3652\n",
      "Epoch [855/2000], Train Loss: 11.0043\n",
      "Epoch [855/2000], Eval Loss: 10.2168\n",
      "Epoch [856/2000], Train Loss: 9.8826\n",
      "Epoch [856/2000], Eval Loss: 10.1757\n",
      "Epoch [857/2000], Train Loss: 9.7754\n",
      "Epoch [857/2000], Eval Loss: 10.2733\n",
      "Epoch [858/2000], Train Loss: 9.8454\n",
      "Epoch [858/2000], Eval Loss: 9.9317\n",
      "Epoch [859/2000], Train Loss: 9.5519\n",
      "Epoch [859/2000], Eval Loss: 9.5269\n",
      "Epoch [860/2000], Train Loss: 9.1666\n",
      "Epoch [860/2000], Eval Loss: 9.3971\n",
      "Epoch [861/2000], Train Loss: 9.0043\n",
      "Epoch [861/2000], Eval Loss: 9.1324\n",
      "Epoch [862/2000], Train Loss: 8.7679\n",
      "Epoch [862/2000], Eval Loss: 9.1435\n",
      "Epoch [863/2000], Train Loss: 8.7929\n",
      "Epoch [863/2000], Eval Loss: 9.0372\n",
      "Epoch [864/2000], Train Loss: 8.6841\n",
      "Epoch [864/2000], Eval Loss: 8.8643\n",
      "Epoch [865/2000], Train Loss: 8.4568\n",
      "Epoch [865/2000], Eval Loss: 8.6855\n",
      "Epoch [866/2000], Train Loss: 8.2456\n",
      "Epoch [866/2000], Eval Loss: 8.7360\n",
      "Epoch [867/2000], Train Loss: 8.2823\n",
      "Epoch [867/2000], Eval Loss: 8.6185\n",
      "Epoch [868/2000], Train Loss: 8.1892\n",
      "Epoch [868/2000], Eval Loss: 8.4309\n",
      "Epoch [869/2000], Train Loss: 8.0498\n",
      "Epoch [869/2000], Eval Loss: 8.2638\n",
      "Epoch [870/2000], Train Loss: 7.9138\n",
      "Epoch [870/2000], Eval Loss: 8.1700\n",
      "Epoch [871/2000], Train Loss: 7.8475\n",
      "Epoch [871/2000], Eval Loss: 8.0886\n",
      "Epoch [872/2000], Train Loss: 7.7793\n",
      "Epoch [872/2000], Eval Loss: 7.9809\n",
      "Epoch [873/2000], Train Loss: 7.6654\n",
      "Epoch [873/2000], Eval Loss: 7.8689\n",
      "Epoch [874/2000], Train Loss: 7.5458\n",
      "Epoch [874/2000], Eval Loss: 7.8444\n",
      "Epoch [875/2000], Train Loss: 7.5129\n",
      "Epoch [875/2000], Eval Loss: 7.7697\n",
      "Epoch [876/2000], Train Loss: 7.4275\n",
      "Epoch [876/2000], Eval Loss: 7.7189\n",
      "Epoch [877/2000], Train Loss: 7.3642\n",
      "Epoch [877/2000], Eval Loss: 7.6387\n",
      "Epoch [878/2000], Train Loss: 7.2740\n",
      "Epoch [878/2000], Eval Loss: 7.5885\n",
      "Epoch [879/2000], Train Loss: 7.2266\n",
      "Epoch [879/2000], Eval Loss: 7.5405\n",
      "Epoch [880/2000], Train Loss: 7.1844\n",
      "Epoch [880/2000], Eval Loss: 7.4670\n",
      "Epoch [881/2000], Train Loss: 7.1221\n",
      "Epoch [881/2000], Eval Loss: 7.3901\n",
      "Epoch [882/2000], Train Loss: 7.0575\n",
      "Epoch [882/2000], Eval Loss: 7.3500\n",
      "Epoch [883/2000], Train Loss: 7.0232\n",
      "Epoch [883/2000], Eval Loss: 7.3014\n",
      "Epoch [884/2000], Train Loss: 6.9730\n",
      "Epoch [884/2000], Eval Loss: 7.2574\n",
      "Epoch [885/2000], Train Loss: 6.9257\n",
      "Epoch [885/2000], Eval Loss: 7.2303\n",
      "Epoch [886/2000], Train Loss: 6.8950\n",
      "Epoch [886/2000], Eval Loss: 7.1868\n",
      "Epoch [887/2000], Train Loss: 6.8532\n",
      "Epoch [887/2000], Eval Loss: 7.1529\n",
      "Epoch [888/2000], Train Loss: 6.8210\n",
      "Epoch [888/2000], Eval Loss: 7.1234\n",
      "Epoch [889/2000], Train Loss: 6.7884\n",
      "Epoch [889/2000], Eval Loss: 7.0859\n",
      "Epoch [890/2000], Train Loss: 6.7509\n",
      "Epoch [890/2000], Eval Loss: 7.0566\n",
      "Epoch [891/2000], Train Loss: 6.7237\n",
      "Epoch [891/2000], Eval Loss: 7.0188\n",
      "Epoch [892/2000], Train Loss: 6.6964\n",
      "Epoch [892/2000], Eval Loss: 6.9952\n",
      "Epoch [893/2000], Train Loss: 6.6746\n",
      "Epoch [893/2000], Eval Loss: 6.9654\n",
      "Epoch [894/2000], Train Loss: 6.6418\n",
      "Epoch [894/2000], Eval Loss: 6.9435\n",
      "Epoch [895/2000], Train Loss: 6.6243\n",
      "Epoch [895/2000], Eval Loss: 6.9083\n",
      "Epoch [896/2000], Train Loss: 6.5982\n",
      "Epoch [896/2000], Eval Loss: 6.8863\n",
      "Epoch [897/2000], Train Loss: 6.5787\n",
      "Epoch [897/2000], Eval Loss: 6.8610\n",
      "Epoch [898/2000], Train Loss: 6.5545\n",
      "Epoch [898/2000], Eval Loss: 6.8461\n",
      "Epoch [899/2000], Train Loss: 6.5371\n",
      "Epoch [899/2000], Eval Loss: 6.8199\n",
      "Epoch [900/2000], Train Loss: 6.5211\n",
      "Epoch [900/2000], Eval Loss: 6.7993\n",
      "Epoch [901/2000], Train Loss: 6.4995\n",
      "Epoch [901/2000], Eval Loss: 6.7801\n",
      "Epoch [902/2000], Train Loss: 6.4806\n",
      "Epoch [902/2000], Eval Loss: 6.7628\n",
      "Epoch [903/2000], Train Loss: 6.4653\n",
      "Epoch [903/2000], Eval Loss: 6.7451\n",
      "Epoch [904/2000], Train Loss: 6.4482\n",
      "Epoch [904/2000], Eval Loss: 6.7250\n",
      "Epoch [905/2000], Train Loss: 6.4331\n",
      "Epoch [905/2000], Eval Loss: 6.7083\n",
      "Epoch [906/2000], Train Loss: 6.4199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [906/2000], Eval Loss: 6.6942\n",
      "Epoch [907/2000], Train Loss: 6.4074\n",
      "Epoch [907/2000], Eval Loss: 6.6819\n",
      "Epoch [908/2000], Train Loss: 6.3957\n",
      "Epoch [908/2000], Eval Loss: 6.6651\n",
      "Epoch [909/2000], Train Loss: 6.3785\n",
      "Epoch [909/2000], Eval Loss: 6.6462\n",
      "Epoch [910/2000], Train Loss: 6.3662\n",
      "Epoch [910/2000], Eval Loss: 6.6337\n",
      "Epoch [911/2000], Train Loss: 6.3540\n",
      "Epoch [911/2000], Eval Loss: 6.6189\n",
      "Epoch [912/2000], Train Loss: 6.3394\n",
      "Epoch [912/2000], Eval Loss: 6.6078\n",
      "Epoch [913/2000], Train Loss: 6.3295\n",
      "Epoch [913/2000], Eval Loss: 6.6004\n",
      "Epoch [914/2000], Train Loss: 6.3203\n",
      "Epoch [914/2000], Eval Loss: 6.5888\n",
      "Epoch [915/2000], Train Loss: 6.3092\n",
      "Epoch [915/2000], Eval Loss: 6.5769\n",
      "Epoch [916/2000], Train Loss: 6.2965\n",
      "Epoch [916/2000], Eval Loss: 6.5665\n",
      "Epoch [917/2000], Train Loss: 6.2865\n",
      "Epoch [917/2000], Eval Loss: 6.5555\n",
      "Epoch [918/2000], Train Loss: 6.2766\n",
      "Epoch [918/2000], Eval Loss: 6.5426\n",
      "Epoch [919/2000], Train Loss: 6.2670\n",
      "Epoch [919/2000], Eval Loss: 6.5350\n",
      "Epoch [920/2000], Train Loss: 6.2559\n",
      "Epoch [920/2000], Eval Loss: 6.5254\n",
      "Epoch [921/2000], Train Loss: 6.2469\n",
      "Epoch [921/2000], Eval Loss: 6.5137\n",
      "Epoch [922/2000], Train Loss: 6.2383\n",
      "Epoch [922/2000], Eval Loss: 6.5051\n",
      "Epoch [923/2000], Train Loss: 6.2300\n",
      "Epoch [923/2000], Eval Loss: 6.4971\n",
      "Epoch [924/2000], Train Loss: 6.2227\n",
      "Epoch [924/2000], Eval Loss: 6.4913\n",
      "Epoch [925/2000], Train Loss: 6.2137\n",
      "Epoch [925/2000], Eval Loss: 6.4797\n",
      "Epoch [926/2000], Train Loss: 6.2057\n",
      "Epoch [926/2000], Eval Loss: 6.4685\n",
      "Epoch [927/2000], Train Loss: 6.1958\n",
      "Epoch [927/2000], Eval Loss: 6.4608\n",
      "Epoch [928/2000], Train Loss: 6.1895\n",
      "Epoch [928/2000], Eval Loss: 6.4510\n",
      "Epoch [929/2000], Train Loss: 6.1820\n",
      "Epoch [929/2000], Eval Loss: 6.4508\n",
      "Epoch [930/2000], Train Loss: 6.1757\n",
      "Epoch [930/2000], Eval Loss: 6.4408\n",
      "Epoch [931/2000], Train Loss: 6.1678\n",
      "Epoch [931/2000], Eval Loss: 6.4319\n",
      "Epoch [932/2000], Train Loss: 6.1587\n",
      "Epoch [932/2000], Eval Loss: 6.4279\n",
      "Epoch [933/2000], Train Loss: 6.1538\n",
      "Epoch [933/2000], Eval Loss: 6.4172\n",
      "Epoch [934/2000], Train Loss: 6.1474\n",
      "Epoch [934/2000], Eval Loss: 6.4122\n",
      "Epoch [935/2000], Train Loss: 6.1387\n",
      "Epoch [935/2000], Eval Loss: 6.4042\n",
      "Epoch [936/2000], Train Loss: 6.1312\n",
      "Epoch [936/2000], Eval Loss: 6.3983\n",
      "Epoch [937/2000], Train Loss: 6.1269\n",
      "Epoch [937/2000], Eval Loss: 6.3937\n",
      "Epoch [938/2000], Train Loss: 6.1181\n",
      "Epoch [938/2000], Eval Loss: 6.3847\n",
      "Epoch [939/2000], Train Loss: 6.1100\n",
      "Epoch [939/2000], Eval Loss: 6.3781\n",
      "Epoch [940/2000], Train Loss: 6.1041\n",
      "Epoch [940/2000], Eval Loss: 6.3743\n",
      "Epoch [941/2000], Train Loss: 6.0986\n",
      "Epoch [941/2000], Eval Loss: 6.3640\n",
      "Epoch [942/2000], Train Loss: 6.0948\n",
      "Epoch [942/2000], Eval Loss: 6.3659\n",
      "Epoch [943/2000], Train Loss: 6.0892\n",
      "Epoch [943/2000], Eval Loss: 6.3553\n",
      "Epoch [944/2000], Train Loss: 6.0841\n",
      "Epoch [944/2000], Eval Loss: 6.3582\n",
      "Epoch [945/2000], Train Loss: 6.0801\n",
      "Epoch [945/2000], Eval Loss: 6.3465\n",
      "Epoch [946/2000], Train Loss: 6.0786\n",
      "Epoch [946/2000], Eval Loss: 6.3427\n",
      "Epoch [947/2000], Train Loss: 6.0666\n",
      "Epoch [947/2000], Eval Loss: 6.3332\n",
      "Epoch [948/2000], Train Loss: 6.0580\n",
      "Epoch [948/2000], Eval Loss: 6.3262\n",
      "Epoch [949/2000], Train Loss: 6.0541\n",
      "Epoch [949/2000], Eval Loss: 6.3280\n",
      "Epoch [950/2000], Train Loss: 6.0522\n",
      "Epoch [950/2000], Eval Loss: 6.3189\n",
      "Epoch [951/2000], Train Loss: 6.0490\n",
      "Epoch [951/2000], Eval Loss: 6.3203\n",
      "Epoch [952/2000], Train Loss: 6.0397\n",
      "Epoch [952/2000], Eval Loss: 6.3087\n",
      "Epoch [953/2000], Train Loss: 6.0314\n",
      "Epoch [953/2000], Eval Loss: 6.3022\n",
      "Epoch [954/2000], Train Loss: 6.0278\n",
      "Epoch [954/2000], Eval Loss: 6.3072\n",
      "Epoch [955/2000], Train Loss: 6.0292\n",
      "Epoch [955/2000], Eval Loss: 6.2993\n",
      "Epoch [956/2000], Train Loss: 6.0286\n",
      "Epoch [956/2000], Eval Loss: 6.2984\n",
      "Epoch [957/2000], Train Loss: 6.0182\n",
      "Epoch [957/2000], Eval Loss: 6.2827\n",
      "Epoch [958/2000], Train Loss: 6.0068\n",
      "Epoch [958/2000], Eval Loss: 6.2787\n",
      "Epoch [959/2000], Train Loss: 6.0036\n",
      "Epoch [959/2000], Eval Loss: 6.2799\n",
      "Epoch [960/2000], Train Loss: 5.9986\n",
      "Epoch [960/2000], Eval Loss: 6.2714\n",
      "Epoch [961/2000], Train Loss: 5.9946\n",
      "Epoch [961/2000], Eval Loss: 6.2690\n",
      "Epoch [962/2000], Train Loss: 5.9881\n",
      "Epoch [962/2000], Eval Loss: 6.2629\n",
      "Epoch [963/2000], Train Loss: 5.9837\n",
      "Epoch [963/2000], Eval Loss: 6.2586\n",
      "Epoch [964/2000], Train Loss: 5.9785\n",
      "Epoch [964/2000], Eval Loss: 6.2627\n",
      "Epoch [965/2000], Train Loss: 5.9781\n",
      "Epoch [965/2000], Eval Loss: 6.2525\n",
      "Epoch [966/2000], Train Loss: 5.9813\n",
      "Epoch [966/2000], Eval Loss: 6.2573\n",
      "Epoch [967/2000], Train Loss: 5.9758\n",
      "Epoch [967/2000], Eval Loss: 6.2416\n",
      "Epoch [968/2000], Train Loss: 5.9649\n",
      "Epoch [968/2000], Eval Loss: 6.2373\n",
      "Epoch [969/2000], Train Loss: 5.9570\n",
      "Epoch [969/2000], Eval Loss: 6.2399\n",
      "Epoch [970/2000], Train Loss: 5.9598\n",
      "Epoch [970/2000], Eval Loss: 6.2363\n",
      "Epoch [971/2000], Train Loss: 5.9647\n",
      "Epoch [971/2000], Eval Loss: 6.2418\n",
      "Epoch [972/2000], Train Loss: 5.9580\n",
      "Epoch [972/2000], Eval Loss: 6.2240\n",
      "Early stopping after 972 epochs with no improvement.\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "encoder = Encoder(input_size,32,1,1)\n",
    "decoder = Decoder(input_size,32,1,1)\n",
    "criterion = nn.KLDivLoss(reduction = \"batchmean\")\n",
    "# optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr = 0.001)\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "# scheduler_encoder = optim.lr_scheduler.StepLR(optimizer_encoder, step_size=1000, gamma=0.5)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "# scheduler_decoder = optim.lr_scheduler.StepLR(optimizer_decoder, step_size=1000, gamma=0.5)\n",
    "# optimizer_encoder = optim.SGD(encoder.parameters(), lr=0.2,momentum=0.9)\n",
    "# optimizer_decoder = optim.SGD(decoder.parameters(), lr=0.2,momentum=0.9)\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 500  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "loss_val = 0.0\n",
    "\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Loop\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "#     optimizer.zero_grad()\n",
    "    context = encoder(training_source)\n",
    "    output = decoder(context,training_target,0.2)\n",
    "    loss = criterion(output[:,:-1,:], training_target_encoded[:,1:,:-1])\n",
    "    loss.backward()\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "#     scheduler_encoder.step()\n",
    "#     scheduler_decoder.step()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Validation Loop\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        context_ = encoder(test_source)\n",
    "        output_ = decoder(context_,test_target,0.0)\n",
    "        loss_val = criterion(output_[:,:-1,:], test_target_encoded[:,1:,:-1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Eval Loss: {loss_val.item():.4f}')\n",
    "    \n",
    "        if loss_val < best_val_loss:\n",
    "            best_val_loss = loss_val\n",
    "            epochs_since_improvement = 0\n",
    "            torch.save(encoder.state_dict(), 'Model/encoder.pth')\n",
    "            torch.save(decoder.state_dict(), 'Model/decoder.pth')\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "    # Check if we should stop training early\n",
    "    if epochs_since_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch+1} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "if loss_val < best_val_loss:\n",
    "    torch.save(encoder.state_dict(), 'Model/encoder.pth')\n",
    "    torch.save(decoder.state_dict(), 'Model/decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8b42ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [972/2000], Eval Loss: 5.9203\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size,32,1,1)\n",
    "decoder = Decoder(input_size,32,1,1)\n",
    "encoder.load_state_dict(torch.load('Model/encoder.pth'))  \n",
    "decoder.load_state_dict(torch.load('Model/decoder.pth')) \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    context_ = encoder(test_source)\n",
    "    output_ = decoder(context_,test_target,0.0)\n",
    "    loss_val = criterion(output_[:,:-1,:], test_target_encoded[:,1:,:-1])\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Eval Loss: {loss_val.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f3ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average(over batch) no. of Wrong predictions per sequence : 4.0140\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    context = encoder(test_source)\n",
    "    output = decoder(context,test_target,0.0)\n",
    "    actual = torch.argmax(test_target_encoded[:,1:,:-1],dim=2)\n",
    "    predictions = torch.argmax(output[:,:-1,:],dim=2)\n",
    "    wrong_pred = torch.where(predictions != actual,1.0,0.0)\n",
    "    print(f'Average(over batch) no. of Wrong predictions per sequence : {torch.sum(wrong_pred) / wrong_pred.shape[0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7cd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how many characters match in the two strings\n",
    "def check(pred: str, true: str):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "\n",
    "    # Prediction is more than 8 letters, so penalize for every extra letter.\n",
    "    correct -= max(0, len(pred) - len(true))\n",
    "    correct = max(0, correct)\n",
    "    return correct\n",
    "\n",
    "# Function to score the model's performance\n",
    "def evaluate(encoder, decoder):\n",
    "\n",
    "    # Train data\n",
    "    print(\"Obtaining results for training data:\")\n",
    "    train_data = pd.read_csv(\"Data/train_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in train_data:\n",
    "        pred = decoder.predict(encoder.predict(x))\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Train dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    print(f\"Points: {points}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_train.csv\", index=False)\n",
    "\n",
    "    #----------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"Obtaining metrics for eval data:\")\n",
    "    eval_data = pd.read_csv(\"Data/eval_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in eval_data:\n",
    "        pred = decoder.predict(encoder.predict(x))\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Eval dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    marks = round(min(2, points / 1400 * 2) * 2) / 2  # Rounds to the nearest 0.5\n",
    "    print(f\"Points: {points}\")\n",
    "    print(f\"Marks: {marks}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_eval.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e1e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining results for training data:\n",
      "Train dataset results:\n",
      "Number of predictions with 0 correct predictions: 51\n",
      "Number of predictions with 1 correct predictions: 364\n",
      "Number of predictions with 2 correct predictions: 990\n",
      "Number of predictions with 3 correct predictions: 1831\n",
      "Number of predictions with 4 correct predictions: 1854\n",
      "Number of predictions with 5 correct predictions: 1240\n",
      "Number of predictions with 6 correct predictions: 524\n",
      "Number of predictions with 7 correct predictions: 135\n",
      "Number of predictions with 8 correct predictions: 11\n",
      "Points: 2217.0\n",
      "Obtaining metrics for eval data:\n",
      "Eval dataset results:\n",
      "Number of predictions with 0 correct predictions: 28\n",
      "Number of predictions with 1 correct predictions: 146\n",
      "Number of predictions with 2 correct predictions: 367\n",
      "Number of predictions with 3 correct predictions: 549\n",
      "Number of predictions with 4 correct predictions: 524\n",
      "Number of predictions with 5 correct predictions: 265\n",
      "Number of predictions with 6 correct predictions: 106\n",
      "Number of predictions with 7 correct predictions: 15\n",
      "Number of predictions with 8 correct predictions: 0\n",
      "Points: 515.5\n",
      "Marks: 0.5\n"
     ]
    }
   ],
   "source": [
    "evaluate(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58088a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
