{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "035ad390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38d37cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "#         self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.start = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = torch.tensor([self.start]+[ord(c) - ord('a') for c in self.data.iloc[idx, 0]])\n",
    "#         input_sentence = self.embedding(input_sentence)\n",
    "        \n",
    "        target_sentence = torch.tensor([self.start]+[ord(c) - ord('a') for c in self.data.iloc[idx, 1]])\n",
    "        target_sentence_encoded = torch.zeros(target_sentence.shape[0],26)\n",
    "        for idx in range(target_sentence.shape[0]):\n",
    "            target_sentence_encoded[idx][target_sentence[idx]] = 1.0\n",
    "#         target_sentence_ = self.embedding(target_sentence)\n",
    "        return (input_sentence,target_sentence,target_sentence_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58dd6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DataLoader(CustomDataset(\"Data/train_data.csv\"))\n",
    "training_source = []\n",
    "training_target = []\n",
    "training_target_encoded = []\n",
    "for input,target,target_encoded in training_data:\n",
    "    training_source.append(input)\n",
    "    training_target.append(target)\n",
    "    training_target_encoded.append(target_encoded)\n",
    "#     print(x.shape)\n",
    "training_source = torch.cat(training_source,dim=0)[:,1:]\n",
    "training_target = torch.cat(training_target,dim=0)\n",
    "training_target_encoded = torch.cat(training_target_encoded,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4b82990",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataLoader(CustomDataset(\"Data/eval_data.csv\"))\n",
    "test_source = []\n",
    "test_target = []\n",
    "test_target_encoded = []\n",
    "for input,target,test_encoded in test_data:\n",
    "    test_source.append(input)\n",
    "    test_target.append(target)\n",
    "    test_target_encoded.append(test_encoded)\n",
    "#     print(x.shape)\n",
    "test_source = torch.cat(test_source,dim=0)[:,1:]\n",
    "test_target = torch.cat(test_target,dim=0)\n",
    "test_target_encoded = torch.cat(test_target_encoded,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a9c5e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 8]) torch.Size([2000, 9]) torch.Size([2000, 9, 26])\n",
      "torch.Size([7000, 8]) torch.Size([7000, 9]) torch.Size([7000, 9, 26])\n"
     ]
    }
   ],
   "source": [
    "print(test_source.shape,test_target.shape,test_target_encoded.shape)\n",
    "print(training_source.shape,training_target.shape,training_target_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80467bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,bidirectional = 0):\n",
    "        super(Encoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(26,input_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.RNN(input_size, hidden_size,num_layers,batch_first = True,bidirectional=bool(self.bidirectional))\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        input_seq = self.embedding(input_)\n",
    "        hidden = torch.zeros((self.bidirectional+1)*self.num_layers,input_seq.shape[0],self.hidden_size)\n",
    "        _, encoder_hidden = self.encoder(input_seq,hidden)\n",
    "        if(self.bidirectional == 1):\n",
    "            encoder_hidden = torch.cat((encoder_hidden[0::2],encoder_hidden[1::2]),dim=2)\n",
    "        return encoder_hidden\n",
    "    \n",
    "    def predict(self,input_str):\n",
    "        input_sentence = torch.tensor([ord(c) - ord('a') for c in input_str])\n",
    "        input_seq = self.embedding(input_sentence)\n",
    "        hidden = torch.zeros((self.bidirectional+1)*self.num_layers,self.hidden_size)\n",
    "        _, encoder_hidden = self.encoder(input_seq,hidden)\n",
    "        if(self.bidirectional == 1):\n",
    "            encoder_hidden = torch.cat((encoder_hidden[::2],encoder_hidden[1::2]),dim=1)\n",
    "        return encoder_hidden\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size,num_layers,bidirectional = 0):\n",
    "        super(Decoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(26,input_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.sequence_len = 9\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.dec_cells = nn.ModuleList([nn.RNNCell((bidirectional+1)*hidden_size+input_size, (bidirectional+1)*hidden_size)])\n",
    "            \n",
    "        self.decoder = nn.GRU((bidirectional+1)*hidden_size+input_size, (bidirectional+1)*hidden_size,batch_first = True)\n",
    "        self.linear = nn.Linear((bidirectional+1)*hidden_size,26)\n",
    "        self.output_layer = nn.LogSoftmax(dim = 2)\n",
    "        self.output_layer_timestep = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, context, target_,teacher_ratio):\n",
    "        target_seq = self.embedding(target_)\n",
    "        initial_hidden = torch.zeros_like(context)\n",
    "        outputs = []\n",
    "        \n",
    "        hidden_states = []\n",
    "        for timestep in range(self.sequence_len):\n",
    "            h_t = []\n",
    "            if(timestep == 0):\n",
    "                h_t = self.dec_cells[0](torch.cat((target_seq[:,timestep],context[0]),dim=1), initial_hidden[0])\n",
    "            else:\n",
    "                input = []\n",
    "                if(torch.rand(1).item() < teacher_ratio):\n",
    "                    input = target_seq[:,timestep]\n",
    "                else:\n",
    "                    input = self.embedding(torch.argmax(outputs[-1],dim=1))\n",
    "#                     print(torch.argmax(outputs[-1],dim=1)[0],target_[0,layer_idx])\n",
    "                    \n",
    "                h_t = self.dec_cells[0](torch.cat((input,context[0]),dim=1), hidden_states[-1])\n",
    "            hidden_states.append(h_t)\n",
    "            out = self.output_layer_timestep(self.linear(h_t))\n",
    "            outputs.append(out)\n",
    "    \n",
    " \n",
    "        hidden_states = torch.cat(hidden_states,dim = 1).reshape(-1,9,context.shape[2])\n",
    "        output_prob = torch.cat(outputs,dim = 1).reshape(-1,9,26)\n",
    "        \n",
    "        \n",
    "#         out,_ = self.decoder(target_seq,context)\n",
    "#         decoder_output = self.linear(hidden_states)\n",
    "#         output_prob = self.output_layer(decoder_output)\n",
    "        return output_prob\n",
    "\n",
    "    def predict(self,context):\n",
    "        target_seq = self.embedding(torch.tensor(1))\n",
    "        initial_hidden = torch.zeros_like(context)\n",
    "        outputs = []\n",
    "        \n",
    "        hidden_states = []\n",
    "        for layer_idx in range(self.sequence_len):\n",
    "            h_t = []\n",
    "            if(layer_idx == 0):\n",
    "                h_t = self.dec_cells[0](torch.cat((target_seq,context[0]),dim=0), initial_hidden[0])\n",
    "            else:\n",
    "                input = self.embedding(torch.argmax(outputs[-1],dim=0))\n",
    "                    \n",
    "                h_t = self.dec_cells[0](torch.cat((input,context[0]),dim=0), hidden_states[-1])\n",
    "            hidden_states.append(h_t)\n",
    "            out = nn.LogSoftmax(dim=0)(self.linear(h_t))\n",
    "            outputs.append(out)\n",
    "        \n",
    "        output = ''.join([chr(torch.argmax(out,dim=0).item()+ord('a')) for out in outputs[:-1]])\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c85c073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Train Loss: 26.2044\n",
      "Epoch [1/2000], Eval Loss: 26.0461\n",
      "Epoch [2/2000], Train Loss: 26.0513\n",
      "Epoch [2/2000], Eval Loss: 25.9819\n",
      "Epoch [3/2000], Train Loss: 25.9558\n",
      "Epoch [3/2000], Eval Loss: 25.9058\n",
      "Epoch [4/2000], Train Loss: 25.8651\n",
      "Epoch [4/2000], Eval Loss: 25.8221\n",
      "Epoch [5/2000], Train Loss: 25.7872\n",
      "Epoch [5/2000], Eval Loss: 25.7317\n",
      "Epoch [6/2000], Train Loss: 25.6913\n",
      "Epoch [6/2000], Eval Loss: 25.6446\n",
      "Epoch [7/2000], Train Loss: 25.5984\n",
      "Epoch [7/2000], Eval Loss: 25.5519\n",
      "Epoch [8/2000], Train Loss: 25.5018\n",
      "Epoch [8/2000], Eval Loss: 25.4613\n",
      "Epoch [9/2000], Train Loss: 25.4208\n",
      "Epoch [9/2000], Eval Loss: 25.3639\n",
      "Epoch [10/2000], Train Loss: 25.2863\n",
      "Epoch [10/2000], Eval Loss: 25.2333\n",
      "Epoch [11/2000], Train Loss: 25.1588\n",
      "Epoch [11/2000], Eval Loss: 25.1100\n",
      "Epoch [12/2000], Train Loss: 25.0130\n",
      "Epoch [12/2000], Eval Loss: 24.9579\n",
      "Epoch [13/2000], Train Loss: 24.8637\n",
      "Epoch [13/2000], Eval Loss: 24.8280\n",
      "Epoch [14/2000], Train Loss: 24.7361\n",
      "Epoch [14/2000], Eval Loss: 24.7549\n",
      "Epoch [15/2000], Train Loss: 24.7003\n",
      "Epoch [15/2000], Eval Loss: 24.6151\n",
      "Epoch [16/2000], Train Loss: 24.5385\n",
      "Epoch [16/2000], Eval Loss: 24.5028\n",
      "Epoch [17/2000], Train Loss: 24.4224\n",
      "Epoch [17/2000], Eval Loss: 24.3746\n",
      "Epoch [18/2000], Train Loss: 24.2842\n",
      "Epoch [18/2000], Eval Loss: 24.2938\n",
      "Epoch [19/2000], Train Loss: 24.2104\n",
      "Epoch [19/2000], Eval Loss: 24.2403\n",
      "Epoch [20/2000], Train Loss: 24.1294\n",
      "Epoch [20/2000], Eval Loss: 24.2017\n",
      "Epoch [21/2000], Train Loss: 24.0892\n",
      "Epoch [21/2000], Eval Loss: 24.0756\n",
      "Epoch [22/2000], Train Loss: 23.9744\n",
      "Epoch [22/2000], Eval Loss: 24.0070\n",
      "Epoch [23/2000], Train Loss: 23.8893\n",
      "Epoch [23/2000], Eval Loss: 23.8936\n",
      "Epoch [24/2000], Train Loss: 23.7718\n",
      "Epoch [24/2000], Eval Loss: 23.8597\n",
      "Epoch [25/2000], Train Loss: 23.7085\n",
      "Epoch [25/2000], Eval Loss: 23.7865\n",
      "Epoch [26/2000], Train Loss: 23.6161\n",
      "Epoch [26/2000], Eval Loss: 23.7376\n",
      "Epoch [27/2000], Train Loss: 23.5852\n",
      "Epoch [27/2000], Eval Loss: 23.6920\n",
      "Epoch [28/2000], Train Loss: 23.5155\n",
      "Epoch [28/2000], Eval Loss: 23.6614\n",
      "Epoch [29/2000], Train Loss: 23.4657\n",
      "Epoch [29/2000], Eval Loss: 23.5820\n",
      "Epoch [30/2000], Train Loss: 23.3743\n",
      "Epoch [30/2000], Eval Loss: 23.5111\n",
      "Epoch [31/2000], Train Loss: 23.2939\n",
      "Epoch [31/2000], Eval Loss: 23.4571\n",
      "Epoch [32/2000], Train Loss: 23.2759\n",
      "Epoch [32/2000], Eval Loss: 23.4362\n",
      "Epoch [33/2000], Train Loss: 23.2625\n",
      "Epoch [33/2000], Eval Loss: 23.4045\n",
      "Epoch [34/2000], Train Loss: 23.1724\n",
      "Epoch [34/2000], Eval Loss: 23.3298\n",
      "Epoch [35/2000], Train Loss: 23.1673\n",
      "Epoch [35/2000], Eval Loss: 23.2545\n",
      "Epoch [36/2000], Train Loss: 23.0428\n",
      "Epoch [36/2000], Eval Loss: 23.2501\n",
      "Epoch [37/2000], Train Loss: 22.9919\n",
      "Epoch [37/2000], Eval Loss: 23.1929\n",
      "Epoch [38/2000], Train Loss: 22.9235\n",
      "Epoch [38/2000], Eval Loss: 23.1598\n",
      "Epoch [39/2000], Train Loss: 22.8996\n",
      "Epoch [39/2000], Eval Loss: 23.1068\n",
      "Epoch [40/2000], Train Loss: 22.8163\n",
      "Epoch [40/2000], Eval Loss: 23.0657\n",
      "Epoch [41/2000], Train Loss: 22.7764\n",
      "Epoch [41/2000], Eval Loss: 23.0319\n",
      "Epoch [42/2000], Train Loss: 22.7873\n",
      "Epoch [42/2000], Eval Loss: 22.9848\n",
      "Epoch [43/2000], Train Loss: 22.7273\n",
      "Epoch [43/2000], Eval Loss: 22.9869\n",
      "Epoch [44/2000], Train Loss: 22.6376\n",
      "Epoch [44/2000], Eval Loss: 22.9689\n",
      "Epoch [45/2000], Train Loss: 22.6342\n",
      "Epoch [45/2000], Eval Loss: 23.1282\n",
      "Epoch [46/2000], Train Loss: 22.8036\n",
      "Epoch [46/2000], Eval Loss: 23.0504\n",
      "Epoch [47/2000], Train Loss: 22.7338\n",
      "Epoch [47/2000], Eval Loss: 22.9325\n",
      "Epoch [48/2000], Train Loss: 22.6282\n",
      "Epoch [48/2000], Eval Loss: 22.8992\n",
      "Epoch [49/2000], Train Loss: 22.5443\n",
      "Epoch [49/2000], Eval Loss: 22.9169\n",
      "Epoch [50/2000], Train Loss: 22.5931\n",
      "Epoch [50/2000], Eval Loss: 22.8933\n",
      "Epoch [51/2000], Train Loss: 22.5064\n",
      "Epoch [51/2000], Eval Loss: 22.8320\n",
      "Epoch [52/2000], Train Loss: 22.4454\n",
      "Epoch [52/2000], Eval Loss: 22.7967\n",
      "Epoch [53/2000], Train Loss: 22.4188\n",
      "Epoch [53/2000], Eval Loss: 22.7750\n",
      "Epoch [54/2000], Train Loss: 22.3871\n",
      "Epoch [54/2000], Eval Loss: 22.7960\n",
      "Epoch [55/2000], Train Loss: 22.3802\n",
      "Epoch [55/2000], Eval Loss: 22.8055\n",
      "Epoch [56/2000], Train Loss: 22.3959\n",
      "Epoch [56/2000], Eval Loss: 22.7776\n",
      "Epoch [57/2000], Train Loss: 22.3368\n",
      "Epoch [57/2000], Eval Loss: 22.6690\n",
      "Epoch [58/2000], Train Loss: 22.2358\n",
      "Epoch [58/2000], Eval Loss: 22.6967\n",
      "Epoch [59/2000], Train Loss: 22.2543\n",
      "Epoch [59/2000], Eval Loss: 22.6419\n",
      "Epoch [60/2000], Train Loss: 22.1782\n",
      "Epoch [60/2000], Eval Loss: 22.6238\n",
      "Epoch [61/2000], Train Loss: 22.1614\n",
      "Epoch [61/2000], Eval Loss: 22.5806\n",
      "Epoch [62/2000], Train Loss: 22.2096\n",
      "Epoch [62/2000], Eval Loss: 22.5512\n",
      "Epoch [63/2000], Train Loss: 22.0743\n",
      "Epoch [63/2000], Eval Loss: 22.5532\n",
      "Epoch [64/2000], Train Loss: 22.0479\n",
      "Epoch [64/2000], Eval Loss: 22.5371\n",
      "Epoch [65/2000], Train Loss: 22.0441\n",
      "Epoch [65/2000], Eval Loss: 22.6552\n",
      "Epoch [66/2000], Train Loss: 22.1696\n",
      "Epoch [66/2000], Eval Loss: 22.6619\n",
      "Epoch [67/2000], Train Loss: 22.1874\n",
      "Epoch [67/2000], Eval Loss: 22.5195\n",
      "Epoch [68/2000], Train Loss: 22.0273\n",
      "Epoch [68/2000], Eval Loss: 22.5670\n",
      "Epoch [69/2000], Train Loss: 22.0799\n",
      "Epoch [69/2000], Eval Loss: 22.5130\n",
      "Epoch [70/2000], Train Loss: 21.9905\n",
      "Epoch [70/2000], Eval Loss: 22.4536\n",
      "Epoch [71/2000], Train Loss: 22.0154\n",
      "Epoch [71/2000], Eval Loss: 22.4538\n",
      "Epoch [72/2000], Train Loss: 21.9424\n",
      "Epoch [72/2000], Eval Loss: 22.4292\n",
      "Epoch [73/2000], Train Loss: 21.8859\n",
      "Epoch [73/2000], Eval Loss: 22.4020\n",
      "Epoch [74/2000], Train Loss: 21.9075\n",
      "Epoch [74/2000], Eval Loss: 22.4058\n",
      "Epoch [75/2000], Train Loss: 21.8367\n",
      "Epoch [75/2000], Eval Loss: 22.3757\n",
      "Epoch [76/2000], Train Loss: 21.8154\n",
      "Epoch [76/2000], Eval Loss: 22.3526\n",
      "Epoch [77/2000], Train Loss: 21.7906\n",
      "Epoch [77/2000], Eval Loss: 22.3558\n",
      "Epoch [78/2000], Train Loss: 21.7798\n",
      "Epoch [78/2000], Eval Loss: 22.3246\n",
      "Epoch [79/2000], Train Loss: 21.7371\n",
      "Epoch [79/2000], Eval Loss: 22.2913\n",
      "Epoch [80/2000], Train Loss: 21.6821\n",
      "Epoch [80/2000], Eval Loss: 22.2895\n",
      "Epoch [81/2000], Train Loss: 21.6739\n",
      "Epoch [81/2000], Eval Loss: 22.2559\n",
      "Epoch [82/2000], Train Loss: 21.6306\n",
      "Epoch [82/2000], Eval Loss: 22.2190\n",
      "Epoch [83/2000], Train Loss: 21.6035\n",
      "Epoch [83/2000], Eval Loss: 22.1799\n",
      "Epoch [84/2000], Train Loss: 21.5495\n",
      "Epoch [84/2000], Eval Loss: 22.2113\n",
      "Epoch [85/2000], Train Loss: 21.6023\n",
      "Epoch [85/2000], Eval Loss: 22.2056\n",
      "Epoch [86/2000], Train Loss: 21.5370\n",
      "Epoch [86/2000], Eval Loss: 22.1645\n",
      "Epoch [87/2000], Train Loss: 21.4742\n",
      "Epoch [87/2000], Eval Loss: 22.1689\n",
      "Epoch [88/2000], Train Loss: 21.5332\n",
      "Epoch [88/2000], Eval Loss: 22.2115\n",
      "Epoch [89/2000], Train Loss: 21.4992\n",
      "Epoch [89/2000], Eval Loss: 22.4103\n",
      "Epoch [90/2000], Train Loss: 21.7366\n",
      "Epoch [90/2000], Eval Loss: 22.4090\n",
      "Epoch [91/2000], Train Loss: 21.7271\n",
      "Epoch [91/2000], Eval Loss: 22.2415\n",
      "Epoch [92/2000], Train Loss: 21.5284\n",
      "Epoch [92/2000], Eval Loss: 22.2265\n",
      "Epoch [93/2000], Train Loss: 21.5109\n",
      "Epoch [93/2000], Eval Loss: 22.1897\n",
      "Epoch [94/2000], Train Loss: 21.4874\n",
      "Epoch [94/2000], Eval Loss: 22.1835\n",
      "Epoch [95/2000], Train Loss: 21.5051\n",
      "Epoch [95/2000], Eval Loss: 22.1578\n",
      "Epoch [96/2000], Train Loss: 21.4548\n",
      "Epoch [96/2000], Eval Loss: 22.1647\n",
      "Epoch [97/2000], Train Loss: 21.3917\n",
      "Epoch [97/2000], Eval Loss: 22.1328\n",
      "Epoch [98/2000], Train Loss: 21.4036\n",
      "Epoch [98/2000], Eval Loss: 22.0843\n",
      "Epoch [99/2000], Train Loss: 21.3166\n",
      "Epoch [99/2000], Eval Loss: 22.0694\n",
      "Epoch [100/2000], Train Loss: 21.2859\n",
      "Epoch [100/2000], Eval Loss: 22.0728\n",
      "Epoch [101/2000], Train Loss: 21.2960\n",
      "Epoch [101/2000], Eval Loss: 22.0339\n",
      "Epoch [102/2000], Train Loss: 21.2628\n",
      "Epoch [102/2000], Eval Loss: 22.0260\n",
      "Epoch [103/2000], Train Loss: 21.2058\n",
      "Epoch [103/2000], Eval Loss: 21.9904\n",
      "Epoch [104/2000], Train Loss: 21.1760\n",
      "Epoch [104/2000], Eval Loss: 21.9800\n",
      "Epoch [105/2000], Train Loss: 21.1494\n",
      "Epoch [105/2000], Eval Loss: 21.9533\n",
      "Epoch [106/2000], Train Loss: 21.1414\n",
      "Epoch [106/2000], Eval Loss: 21.9263\n",
      "Epoch [107/2000], Train Loss: 21.0820\n",
      "Epoch [107/2000], Eval Loss: 21.9232\n",
      "Epoch [108/2000], Train Loss: 21.0520\n",
      "Epoch [108/2000], Eval Loss: 21.9222\n",
      "Epoch [109/2000], Train Loss: 21.0397\n",
      "Epoch [109/2000], Eval Loss: 21.8931\n",
      "Epoch [110/2000], Train Loss: 21.0154\n",
      "Epoch [110/2000], Eval Loss: 21.8698\n",
      "Epoch [111/2000], Train Loss: 20.9920\n",
      "Epoch [111/2000], Eval Loss: 21.8752\n",
      "Epoch [112/2000], Train Loss: 20.9760\n",
      "Epoch [112/2000], Eval Loss: 21.9266\n",
      "Epoch [113/2000], Train Loss: 21.0276\n",
      "Epoch [113/2000], Eval Loss: 21.9939\n",
      "Epoch [114/2000], Train Loss: 21.0729\n",
      "Epoch [114/2000], Eval Loss: 21.9169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [115/2000], Train Loss: 21.0069\n",
      "Epoch [115/2000], Eval Loss: 21.8517\n",
      "Epoch [116/2000], Train Loss: 20.9250\n",
      "Epoch [116/2000], Eval Loss: 21.8753\n",
      "Epoch [117/2000], Train Loss: 20.9573\n",
      "Epoch [117/2000], Eval Loss: 21.8111\n",
      "Epoch [118/2000], Train Loss: 20.8772\n",
      "Epoch [118/2000], Eval Loss: 21.8438\n",
      "Epoch [119/2000], Train Loss: 20.9015\n",
      "Epoch [119/2000], Eval Loss: 21.7889\n",
      "Epoch [120/2000], Train Loss: 20.8205\n",
      "Epoch [120/2000], Eval Loss: 21.7760\n",
      "Epoch [121/2000], Train Loss: 20.8262\n",
      "Epoch [121/2000], Eval Loss: 21.7884\n",
      "Epoch [122/2000], Train Loss: 20.8137\n",
      "Epoch [122/2000], Eval Loss: 21.7885\n",
      "Epoch [123/2000], Train Loss: 20.8100\n",
      "Epoch [123/2000], Eval Loss: 21.7492\n",
      "Epoch [124/2000], Train Loss: 20.7524\n",
      "Epoch [124/2000], Eval Loss: 21.7077\n",
      "Epoch [125/2000], Train Loss: 20.7533\n",
      "Epoch [125/2000], Eval Loss: 21.7143\n",
      "Epoch [126/2000], Train Loss: 20.7171\n",
      "Epoch [126/2000], Eval Loss: 21.6870\n",
      "Epoch [127/2000], Train Loss: 20.6648\n",
      "Epoch [127/2000], Eval Loss: 21.6503\n",
      "Epoch [128/2000], Train Loss: 20.6218\n",
      "Epoch [128/2000], Eval Loss: 21.6244\n",
      "Epoch [129/2000], Train Loss: 20.6151\n",
      "Epoch [129/2000], Eval Loss: 21.6395\n",
      "Epoch [130/2000], Train Loss: 20.5961\n",
      "Epoch [130/2000], Eval Loss: 21.6042\n",
      "Epoch [131/2000], Train Loss: 20.5301\n",
      "Epoch [131/2000], Eval Loss: 21.6170\n",
      "Epoch [132/2000], Train Loss: 20.5520\n",
      "Epoch [132/2000], Eval Loss: 21.7097\n",
      "Epoch [133/2000], Train Loss: 20.6960\n",
      "Epoch [133/2000], Eval Loss: 22.2721\n",
      "Epoch [134/2000], Train Loss: 21.2737\n",
      "Epoch [134/2000], Eval Loss: 21.8170\n",
      "Epoch [135/2000], Train Loss: 20.7812\n",
      "Epoch [135/2000], Eval Loss: 21.9020\n",
      "Epoch [136/2000], Train Loss: 20.9000\n",
      "Epoch [136/2000], Eval Loss: 21.7846\n",
      "Epoch [137/2000], Train Loss: 20.7219\n",
      "Epoch [137/2000], Eval Loss: 21.7448\n",
      "Epoch [138/2000], Train Loss: 20.6939\n",
      "Epoch [138/2000], Eval Loss: 21.7875\n",
      "Epoch [139/2000], Train Loss: 20.7407\n",
      "Epoch [139/2000], Eval Loss: 21.7048\n",
      "Epoch [140/2000], Train Loss: 20.6395\n",
      "Epoch [140/2000], Eval Loss: 21.7197\n",
      "Epoch [141/2000], Train Loss: 20.6402\n",
      "Epoch [141/2000], Eval Loss: 21.6720\n",
      "Epoch [142/2000], Train Loss: 20.5816\n",
      "Epoch [142/2000], Eval Loss: 21.6566\n",
      "Epoch [143/2000], Train Loss: 20.5534\n",
      "Epoch [143/2000], Eval Loss: 21.6298\n",
      "Epoch [144/2000], Train Loss: 20.5304\n",
      "Epoch [144/2000], Eval Loss: 21.6249\n",
      "Epoch [145/2000], Train Loss: 20.4883\n",
      "Epoch [145/2000], Eval Loss: 21.6040\n",
      "Epoch [146/2000], Train Loss: 20.4490\n",
      "Epoch [146/2000], Eval Loss: 21.5932\n",
      "Epoch [147/2000], Train Loss: 20.4263\n",
      "Epoch [147/2000], Eval Loss: 21.5493\n",
      "Epoch [148/2000], Train Loss: 20.3839\n",
      "Epoch [148/2000], Eval Loss: 21.5344\n",
      "Epoch [149/2000], Train Loss: 20.3668\n",
      "Epoch [149/2000], Eval Loss: 21.4967\n",
      "Epoch [150/2000], Train Loss: 20.3347\n",
      "Epoch [150/2000], Eval Loss: 21.4613\n",
      "Epoch [151/2000], Train Loss: 20.2982\n",
      "Epoch [151/2000], Eval Loss: 21.4405\n",
      "Epoch [152/2000], Train Loss: 20.2661\n",
      "Epoch [152/2000], Eval Loss: 21.4281\n",
      "Epoch [153/2000], Train Loss: 20.2374\n",
      "Epoch [153/2000], Eval Loss: 21.4083\n",
      "Epoch [154/2000], Train Loss: 20.2155\n",
      "Epoch [154/2000], Eval Loss: 21.3934\n",
      "Epoch [155/2000], Train Loss: 20.1779\n",
      "Epoch [155/2000], Eval Loss: 21.3819\n",
      "Epoch [156/2000], Train Loss: 20.1491\n",
      "Epoch [156/2000], Eval Loss: 21.3473\n",
      "Epoch [157/2000], Train Loss: 20.1179\n",
      "Epoch [157/2000], Eval Loss: 21.3283\n",
      "Epoch [158/2000], Train Loss: 20.0975\n",
      "Epoch [158/2000], Eval Loss: 21.3055\n",
      "Epoch [159/2000], Train Loss: 20.0770\n",
      "Epoch [159/2000], Eval Loss: 21.3089\n",
      "Epoch [160/2000], Train Loss: 20.0714\n",
      "Epoch [160/2000], Eval Loss: 21.2905\n",
      "Epoch [161/2000], Train Loss: 20.0366\n",
      "Epoch [161/2000], Eval Loss: 21.3225\n",
      "Epoch [162/2000], Train Loss: 20.0725\n",
      "Epoch [162/2000], Eval Loss: 21.3921\n",
      "Epoch [163/2000], Train Loss: 20.1197\n",
      "Epoch [163/2000], Eval Loss: 21.3790\n",
      "Epoch [164/2000], Train Loss: 20.1415\n",
      "Epoch [164/2000], Eval Loss: 21.2861\n",
      "Epoch [165/2000], Train Loss: 20.0182\n",
      "Epoch [165/2000], Eval Loss: 21.3424\n",
      "Epoch [166/2000], Train Loss: 20.1278\n",
      "Epoch [166/2000], Eval Loss: 21.3348\n",
      "Epoch [167/2000], Train Loss: 20.0716\n",
      "Epoch [167/2000], Eval Loss: 21.2466\n",
      "Epoch [168/2000], Train Loss: 19.9740\n",
      "Epoch [168/2000], Eval Loss: 21.2530\n",
      "Epoch [169/2000], Train Loss: 19.9815\n",
      "Epoch [169/2000], Eval Loss: 21.1899\n",
      "Epoch [170/2000], Train Loss: 19.9211\n",
      "Epoch [170/2000], Eval Loss: 21.2028\n",
      "Epoch [171/2000], Train Loss: 19.9030\n",
      "Epoch [171/2000], Eval Loss: 21.1646\n",
      "Epoch [172/2000], Train Loss: 19.8686\n",
      "Epoch [172/2000], Eval Loss: 21.1425\n",
      "Epoch [173/2000], Train Loss: 19.8573\n",
      "Epoch [173/2000], Eval Loss: 21.1288\n",
      "Epoch [174/2000], Train Loss: 19.7962\n",
      "Epoch [174/2000], Eval Loss: 21.0987\n",
      "Epoch [175/2000], Train Loss: 19.7789\n",
      "Epoch [175/2000], Eval Loss: 21.0453\n",
      "Epoch [176/2000], Train Loss: 19.7372\n",
      "Epoch [176/2000], Eval Loss: 21.0590\n",
      "Epoch [177/2000], Train Loss: 19.7127\n",
      "Epoch [177/2000], Eval Loss: 21.0116\n",
      "Epoch [178/2000], Train Loss: 19.6743\n",
      "Epoch [178/2000], Eval Loss: 20.9980\n",
      "Epoch [179/2000], Train Loss: 19.6628\n",
      "Epoch [179/2000], Eval Loss: 20.9936\n",
      "Epoch [180/2000], Train Loss: 19.6479\n",
      "Epoch [180/2000], Eval Loss: 20.9415\n",
      "Epoch [181/2000], Train Loss: 19.6244\n",
      "Epoch [181/2000], Eval Loss: 20.9672\n",
      "Epoch [182/2000], Train Loss: 19.5935\n",
      "Epoch [182/2000], Eval Loss: 20.9408\n",
      "Epoch [183/2000], Train Loss: 19.5996\n",
      "Epoch [183/2000], Eval Loss: 21.1903\n",
      "Epoch [184/2000], Train Loss: 19.8345\n",
      "Epoch [184/2000], Eval Loss: 21.4753\n",
      "Epoch [185/2000], Train Loss: 20.2422\n",
      "Epoch [185/2000], Eval Loss: 21.0526\n",
      "Epoch [186/2000], Train Loss: 19.7089\n",
      "Epoch [186/2000], Eval Loss: 21.1385\n",
      "Epoch [187/2000], Train Loss: 19.7899\n",
      "Epoch [187/2000], Eval Loss: 21.1163\n",
      "Epoch [188/2000], Train Loss: 19.7767\n",
      "Epoch [188/2000], Eval Loss: 21.0516\n",
      "Epoch [189/2000], Train Loss: 19.6944\n",
      "Epoch [189/2000], Eval Loss: 21.0311\n",
      "Epoch [190/2000], Train Loss: 19.6797\n",
      "Epoch [190/2000], Eval Loss: 21.0075\n",
      "Epoch [191/2000], Train Loss: 19.6726\n",
      "Epoch [191/2000], Eval Loss: 20.9613\n",
      "Epoch [192/2000], Train Loss: 19.6083\n",
      "Epoch [192/2000], Eval Loss: 20.9384\n",
      "Epoch [193/2000], Train Loss: 19.5805\n",
      "Epoch [193/2000], Eval Loss: 20.8995\n",
      "Epoch [194/2000], Train Loss: 19.5397\n",
      "Epoch [194/2000], Eval Loss: 20.8620\n",
      "Epoch [195/2000], Train Loss: 19.4915\n",
      "Epoch [195/2000], Eval Loss: 20.8046\n",
      "Epoch [196/2000], Train Loss: 19.4393\n",
      "Epoch [196/2000], Eval Loss: 20.7923\n",
      "Epoch [197/2000], Train Loss: 19.4070\n",
      "Epoch [197/2000], Eval Loss: 20.7611\n",
      "Epoch [198/2000], Train Loss: 19.3590\n",
      "Epoch [198/2000], Eval Loss: 20.6959\n",
      "Epoch [199/2000], Train Loss: 19.3636\n",
      "Epoch [199/2000], Eval Loss: 20.7080\n",
      "Epoch [200/2000], Train Loss: 19.3180\n",
      "Epoch [200/2000], Eval Loss: 20.6675\n",
      "Epoch [201/2000], Train Loss: 19.3025\n",
      "Epoch [201/2000], Eval Loss: 20.6548\n",
      "Epoch [202/2000], Train Loss: 19.2575\n",
      "Epoch [202/2000], Eval Loss: 20.5422\n",
      "Epoch [203/2000], Train Loss: 19.1977\n",
      "Epoch [203/2000], Eval Loss: 20.5196\n",
      "Epoch [204/2000], Train Loss: 19.1435\n",
      "Epoch [204/2000], Eval Loss: 20.4858\n",
      "Epoch [205/2000], Train Loss: 19.1012\n",
      "Epoch [205/2000], Eval Loss: 20.4330\n",
      "Epoch [206/2000], Train Loss: 19.0767\n",
      "Epoch [206/2000], Eval Loss: 20.4411\n",
      "Epoch [207/2000], Train Loss: 19.0490\n",
      "Epoch [207/2000], Eval Loss: 20.3821\n",
      "Epoch [208/2000], Train Loss: 19.0186\n",
      "Epoch [208/2000], Eval Loss: 20.3816\n",
      "Epoch [209/2000], Train Loss: 19.0153\n",
      "Epoch [209/2000], Eval Loss: 20.4518\n",
      "Epoch [210/2000], Train Loss: 19.1032\n",
      "Epoch [210/2000], Eval Loss: 20.5041\n",
      "Epoch [211/2000], Train Loss: 19.1574\n",
      "Epoch [211/2000], Eval Loss: 20.6198\n",
      "Epoch [212/2000], Train Loss: 19.3282\n",
      "Epoch [212/2000], Eval Loss: 20.4629\n",
      "Epoch [213/2000], Train Loss: 19.1691\n",
      "Epoch [213/2000], Eval Loss: 20.4353\n",
      "Epoch [214/2000], Train Loss: 19.1495\n",
      "Epoch [214/2000], Eval Loss: 20.2964\n",
      "Epoch [215/2000], Train Loss: 18.9973\n",
      "Epoch [215/2000], Eval Loss: 20.3308\n",
      "Epoch [216/2000], Train Loss: 19.0265\n",
      "Epoch [216/2000], Eval Loss: 20.1720\n",
      "Epoch [217/2000], Train Loss: 18.8210\n",
      "Epoch [217/2000], Eval Loss: 20.1798\n",
      "Epoch [218/2000], Train Loss: 18.8531\n",
      "Epoch [218/2000], Eval Loss: 20.0575\n",
      "Epoch [219/2000], Train Loss: 18.7560\n",
      "Epoch [219/2000], Eval Loss: 20.0051\n",
      "Epoch [220/2000], Train Loss: 18.7082\n",
      "Epoch [220/2000], Eval Loss: 19.9441\n",
      "Epoch [221/2000], Train Loss: 18.6396\n",
      "Epoch [221/2000], Eval Loss: 19.9148\n",
      "Epoch [222/2000], Train Loss: 18.5945\n",
      "Epoch [222/2000], Eval Loss: 19.8463\n",
      "Epoch [223/2000], Train Loss: 18.5374\n",
      "Epoch [223/2000], Eval Loss: 19.8270\n",
      "Epoch [224/2000], Train Loss: 18.5497\n",
      "Epoch [224/2000], Eval Loss: 19.8113\n",
      "Epoch [225/2000], Train Loss: 18.5295\n",
      "Epoch [225/2000], Eval Loss: 19.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226/2000], Train Loss: 18.5513\n",
      "Epoch [226/2000], Eval Loss: 19.6945\n",
      "Epoch [227/2000], Train Loss: 18.3753\n",
      "Epoch [227/2000], Eval Loss: 19.5735\n",
      "Epoch [228/2000], Train Loss: 18.3043\n",
      "Epoch [228/2000], Eval Loss: 19.4933\n",
      "Epoch [229/2000], Train Loss: 18.2383\n",
      "Epoch [229/2000], Eval Loss: 19.4644\n",
      "Epoch [230/2000], Train Loss: 18.1921\n",
      "Epoch [230/2000], Eval Loss: 19.4080\n",
      "Epoch [231/2000], Train Loss: 18.1534\n",
      "Epoch [231/2000], Eval Loss: 19.4350\n",
      "Epoch [232/2000], Train Loss: 18.2267\n",
      "Epoch [232/2000], Eval Loss: 19.3869\n",
      "Epoch [233/2000], Train Loss: 18.1359\n",
      "Epoch [233/2000], Eval Loss: 19.3267\n",
      "Epoch [234/2000], Train Loss: 18.0824\n",
      "Epoch [234/2000], Eval Loss: 19.3176\n",
      "Epoch [235/2000], Train Loss: 18.1273\n",
      "Epoch [235/2000], Eval Loss: 19.3625\n",
      "Epoch [236/2000], Train Loss: 18.1107\n",
      "Epoch [236/2000], Eval Loss: 19.3886\n",
      "Epoch [237/2000], Train Loss: 18.1863\n",
      "Epoch [237/2000], Eval Loss: 19.4941\n",
      "Epoch [238/2000], Train Loss: 18.2837\n",
      "Epoch [238/2000], Eval Loss: 19.1529\n",
      "Epoch [239/2000], Train Loss: 17.9978\n",
      "Epoch [239/2000], Eval Loss: 19.0200\n",
      "Epoch [240/2000], Train Loss: 17.8598\n",
      "Epoch [240/2000], Eval Loss: 19.1276\n",
      "Epoch [241/2000], Train Loss: 17.9498\n",
      "Epoch [241/2000], Eval Loss: 18.9060\n",
      "Epoch [242/2000], Train Loss: 17.7342\n",
      "Epoch [242/2000], Eval Loss: 18.8852\n",
      "Epoch [243/2000], Train Loss: 17.7339\n",
      "Epoch [243/2000], Eval Loss: 18.7796\n",
      "Epoch [244/2000], Train Loss: 17.6271\n",
      "Epoch [244/2000], Eval Loss: 18.6055\n",
      "Epoch [245/2000], Train Loss: 17.4846\n",
      "Epoch [245/2000], Eval Loss: 18.6044\n",
      "Epoch [246/2000], Train Loss: 17.4291\n",
      "Epoch [246/2000], Eval Loss: 18.5118\n",
      "Epoch [247/2000], Train Loss: 17.3405\n",
      "Epoch [247/2000], Eval Loss: 18.3194\n",
      "Epoch [248/2000], Train Loss: 17.2372\n",
      "Epoch [248/2000], Eval Loss: 18.1960\n",
      "Epoch [249/2000], Train Loss: 17.0644\n",
      "Epoch [249/2000], Eval Loss: 18.1761\n",
      "Epoch [250/2000], Train Loss: 17.0404\n",
      "Epoch [250/2000], Eval Loss: 18.0049\n",
      "Epoch [251/2000], Train Loss: 16.8870\n",
      "Epoch [251/2000], Eval Loss: 17.8917\n",
      "Epoch [252/2000], Train Loss: 16.7977\n",
      "Epoch [252/2000], Eval Loss: 17.7332\n",
      "Epoch [253/2000], Train Loss: 16.7109\n",
      "Epoch [253/2000], Eval Loss: 17.7420\n",
      "Epoch [254/2000], Train Loss: 16.7112\n",
      "Epoch [254/2000], Eval Loss: 17.7271\n",
      "Epoch [255/2000], Train Loss: 16.7735\n",
      "Epoch [255/2000], Eval Loss: 17.6779\n",
      "Epoch [256/2000], Train Loss: 16.7139\n",
      "Epoch [256/2000], Eval Loss: 17.4659\n",
      "Epoch [257/2000], Train Loss: 16.4716\n",
      "Epoch [257/2000], Eval Loss: 17.4830\n",
      "Epoch [258/2000], Train Loss: 16.5861\n",
      "Epoch [258/2000], Eval Loss: 17.2491\n",
      "Epoch [259/2000], Train Loss: 16.2913\n",
      "Epoch [259/2000], Eval Loss: 17.1744\n",
      "Epoch [260/2000], Train Loss: 16.1774\n",
      "Epoch [260/2000], Eval Loss: 17.0051\n",
      "Epoch [261/2000], Train Loss: 16.0836\n",
      "Epoch [261/2000], Eval Loss: 16.8607\n",
      "Epoch [262/2000], Train Loss: 15.9222\n",
      "Epoch [262/2000], Eval Loss: 16.7610\n",
      "Epoch [263/2000], Train Loss: 15.8139\n",
      "Epoch [263/2000], Eval Loss: 16.4810\n",
      "Epoch [264/2000], Train Loss: 15.5854\n",
      "Epoch [264/2000], Eval Loss: 16.3663\n",
      "Epoch [265/2000], Train Loss: 15.4952\n",
      "Epoch [265/2000], Eval Loss: 16.3109\n",
      "Epoch [266/2000], Train Loss: 15.4475\n",
      "Epoch [266/2000], Eval Loss: 16.0611\n",
      "Epoch [267/2000], Train Loss: 15.2110\n",
      "Epoch [267/2000], Eval Loss: 15.9864\n",
      "Epoch [268/2000], Train Loss: 15.1475\n",
      "Epoch [268/2000], Eval Loss: 15.9329\n",
      "Epoch [269/2000], Train Loss: 15.0976\n",
      "Epoch [269/2000], Eval Loss: 15.8567\n",
      "Epoch [270/2000], Train Loss: 15.0286\n",
      "Epoch [270/2000], Eval Loss: 15.7801\n",
      "Epoch [271/2000], Train Loss: 14.9731\n",
      "Epoch [271/2000], Eval Loss: 15.8238\n",
      "Epoch [272/2000], Train Loss: 15.0050\n",
      "Epoch [272/2000], Eval Loss: 15.9290\n",
      "Epoch [273/2000], Train Loss: 15.2092\n",
      "Epoch [273/2000], Eval Loss: 16.3730\n",
      "Epoch [274/2000], Train Loss: 15.6340\n",
      "Epoch [274/2000], Eval Loss: 16.1317\n",
      "Epoch [275/2000], Train Loss: 15.3680\n",
      "Epoch [275/2000], Eval Loss: 15.8755\n",
      "Epoch [276/2000], Train Loss: 15.1398\n",
      "Epoch [276/2000], Eval Loss: 15.6466\n",
      "Epoch [277/2000], Train Loss: 14.8875\n",
      "Epoch [277/2000], Eval Loss: 15.3599\n",
      "Epoch [278/2000], Train Loss: 14.5798\n",
      "Epoch [278/2000], Eval Loss: 15.4679\n",
      "Epoch [279/2000], Train Loss: 14.6554\n",
      "Epoch [279/2000], Eval Loss: 15.0538\n",
      "Epoch [280/2000], Train Loss: 14.3166\n",
      "Epoch [280/2000], Eval Loss: 14.9833\n",
      "Epoch [281/2000], Train Loss: 14.2780\n",
      "Epoch [281/2000], Eval Loss: 14.6784\n",
      "Epoch [282/2000], Train Loss: 13.9494\n",
      "Epoch [282/2000], Eval Loss: 14.5925\n",
      "Epoch [283/2000], Train Loss: 13.8923\n",
      "Epoch [283/2000], Eval Loss: 14.4032\n",
      "Epoch [284/2000], Train Loss: 13.6652\n",
      "Epoch [284/2000], Eval Loss: 14.2452\n",
      "Epoch [285/2000], Train Loss: 13.4949\n",
      "Epoch [285/2000], Eval Loss: 14.0609\n",
      "Epoch [286/2000], Train Loss: 13.3323\n",
      "Epoch [286/2000], Eval Loss: 13.9005\n",
      "Epoch [287/2000], Train Loss: 13.1929\n",
      "Epoch [287/2000], Eval Loss: 13.7034\n",
      "Epoch [288/2000], Train Loss: 13.0488\n",
      "Epoch [288/2000], Eval Loss: 13.5339\n",
      "Epoch [289/2000], Train Loss: 12.8746\n",
      "Epoch [289/2000], Eval Loss: 13.4315\n",
      "Epoch [290/2000], Train Loss: 12.7532\n",
      "Epoch [290/2000], Eval Loss: 13.3528\n",
      "Epoch [291/2000], Train Loss: 12.6988\n",
      "Epoch [291/2000], Eval Loss: 13.4283\n",
      "Epoch [292/2000], Train Loss: 12.8025\n",
      "Epoch [292/2000], Eval Loss: 13.7632\n",
      "Epoch [293/2000], Train Loss: 13.1668\n",
      "Epoch [293/2000], Eval Loss: 14.4572\n",
      "Epoch [294/2000], Train Loss: 13.8986\n",
      "Epoch [294/2000], Eval Loss: 13.5998\n",
      "Epoch [295/2000], Train Loss: 13.0173\n",
      "Epoch [295/2000], Eval Loss: 13.8791\n",
      "Epoch [296/2000], Train Loss: 13.3017\n",
      "Epoch [296/2000], Eval Loss: 13.1943\n",
      "Epoch [297/2000], Train Loss: 12.6188\n",
      "Epoch [297/2000], Eval Loss: 13.3401\n",
      "Epoch [298/2000], Train Loss: 12.7398\n",
      "Epoch [298/2000], Eval Loss: 12.8068\n",
      "Epoch [299/2000], Train Loss: 12.2442\n",
      "Epoch [299/2000], Eval Loss: 12.8377\n",
      "Epoch [300/2000], Train Loss: 12.3415\n",
      "Epoch [300/2000], Eval Loss: 12.5393\n",
      "Epoch [301/2000], Train Loss: 11.9610\n",
      "Epoch [301/2000], Eval Loss: 12.4376\n",
      "Epoch [302/2000], Train Loss: 11.9024\n",
      "Epoch [302/2000], Eval Loss: 12.2480\n",
      "Epoch [303/2000], Train Loss: 11.7334\n",
      "Epoch [303/2000], Eval Loss: 12.0804\n",
      "Epoch [304/2000], Train Loss: 11.5588\n",
      "Epoch [304/2000], Eval Loss: 11.9720\n",
      "Epoch [305/2000], Train Loss: 11.4511\n",
      "Epoch [305/2000], Eval Loss: 11.8139\n",
      "Epoch [306/2000], Train Loss: 11.2788\n",
      "Epoch [306/2000], Eval Loss: 11.6853\n",
      "Epoch [307/2000], Train Loss: 11.1484\n",
      "Epoch [307/2000], Eval Loss: 11.5547\n",
      "Epoch [308/2000], Train Loss: 11.0427\n",
      "Epoch [308/2000], Eval Loss: 11.4225\n",
      "Epoch [309/2000], Train Loss: 10.9089\n",
      "Epoch [309/2000], Eval Loss: 11.2770\n",
      "Epoch [310/2000], Train Loss: 10.7918\n",
      "Epoch [310/2000], Eval Loss: 11.1708\n",
      "Epoch [311/2000], Train Loss: 10.6967\n",
      "Epoch [311/2000], Eval Loss: 11.0598\n",
      "Epoch [312/2000], Train Loss: 10.5791\n",
      "Epoch [312/2000], Eval Loss: 11.0074\n",
      "Epoch [313/2000], Train Loss: 10.5257\n",
      "Epoch [313/2000], Eval Loss: 11.0046\n",
      "Epoch [314/2000], Train Loss: 10.5389\n",
      "Epoch [314/2000], Eval Loss: 11.0265\n",
      "Epoch [315/2000], Train Loss: 10.5676\n",
      "Epoch [315/2000], Eval Loss: 11.2109\n",
      "Epoch [316/2000], Train Loss: 10.7677\n",
      "Epoch [316/2000], Eval Loss: 11.3588\n",
      "Epoch [317/2000], Train Loss: 10.9459\n",
      "Epoch [317/2000], Eval Loss: 11.2756\n",
      "Epoch [318/2000], Train Loss: 10.8666\n",
      "Epoch [318/2000], Eval Loss: 10.9804\n",
      "Epoch [319/2000], Train Loss: 10.5296\n",
      "Epoch [319/2000], Eval Loss: 11.5426\n",
      "Epoch [320/2000], Train Loss: 11.1283\n",
      "Epoch [320/2000], Eval Loss: 12.1864\n",
      "Epoch [321/2000], Train Loss: 11.7839\n",
      "Epoch [321/2000], Eval Loss: 11.2141\n",
      "Epoch [322/2000], Train Loss: 10.8054\n",
      "Epoch [322/2000], Eval Loss: 11.3669\n",
      "Epoch [323/2000], Train Loss: 10.9546\n",
      "Epoch [323/2000], Eval Loss: 11.3565\n",
      "Epoch [324/2000], Train Loss: 10.8817\n",
      "Epoch [324/2000], Eval Loss: 10.8943\n",
      "Epoch [325/2000], Train Loss: 10.4444\n",
      "Epoch [325/2000], Eval Loss: 10.9748\n",
      "Epoch [326/2000], Train Loss: 10.5225\n",
      "Epoch [326/2000], Eval Loss: 10.8137\n",
      "Epoch [327/2000], Train Loss: 10.3588\n",
      "Epoch [327/2000], Eval Loss: 10.6108\n",
      "Epoch [328/2000], Train Loss: 10.1725\n",
      "Epoch [328/2000], Eval Loss: 10.6069\n",
      "Epoch [329/2000], Train Loss: 10.1822\n",
      "Epoch [329/2000], Eval Loss: 10.4524\n",
      "Epoch [330/2000], Train Loss: 9.9862\n",
      "Epoch [330/2000], Eval Loss: 10.3409\n",
      "Epoch [331/2000], Train Loss: 9.8545\n",
      "Epoch [331/2000], Eval Loss: 10.2873\n",
      "Epoch [332/2000], Train Loss: 9.8106\n",
      "Epoch [332/2000], Eval Loss: 10.2057\n",
      "Epoch [333/2000], Train Loss: 9.7166\n",
      "Epoch [333/2000], Eval Loss: 10.0511\n",
      "Epoch [334/2000], Train Loss: 9.5734\n",
      "Epoch [334/2000], Eval Loss: 9.9500\n",
      "Epoch [335/2000], Train Loss: 9.4939\n",
      "Epoch [335/2000], Eval Loss: 9.8923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [336/2000], Train Loss: 9.4469\n",
      "Epoch [336/2000], Eval Loss: 9.7636\n",
      "Epoch [337/2000], Train Loss: 9.3108\n",
      "Epoch [337/2000], Eval Loss: 9.6764\n",
      "Epoch [338/2000], Train Loss: 9.2345\n",
      "Epoch [338/2000], Eval Loss: 9.6542\n",
      "Epoch [339/2000], Train Loss: 9.1933\n",
      "Epoch [339/2000], Eval Loss: 9.5631\n",
      "Epoch [340/2000], Train Loss: 9.1205\n",
      "Epoch [340/2000], Eval Loss: 9.4611\n",
      "Epoch [341/2000], Train Loss: 9.0501\n",
      "Epoch [341/2000], Eval Loss: 9.3901\n",
      "Epoch [342/2000], Train Loss: 8.9658\n",
      "Epoch [342/2000], Eval Loss: 9.3554\n",
      "Epoch [343/2000], Train Loss: 8.9222\n",
      "Epoch [343/2000], Eval Loss: 9.3079\n",
      "Epoch [344/2000], Train Loss: 8.8786\n",
      "Epoch [344/2000], Eval Loss: 9.3093\n",
      "Epoch [345/2000], Train Loss: 8.8564\n",
      "Epoch [345/2000], Eval Loss: 9.4181\n",
      "Epoch [346/2000], Train Loss: 8.9616\n",
      "Epoch [346/2000], Eval Loss: 9.8401\n",
      "Epoch [347/2000], Train Loss: 9.4475\n",
      "Epoch [347/2000], Eval Loss: 11.4277\n",
      "Epoch [348/2000], Train Loss: 10.8936\n",
      "Epoch [348/2000], Eval Loss: 10.9492\n",
      "Epoch [349/2000], Train Loss: 10.5831\n",
      "Epoch [349/2000], Eval Loss: 10.2503\n",
      "Epoch [350/2000], Train Loss: 9.8417\n",
      "Epoch [350/2000], Eval Loss: 10.3753\n",
      "Epoch [351/2000], Train Loss: 9.9174\n",
      "Epoch [351/2000], Eval Loss: 9.9173\n",
      "Epoch [352/2000], Train Loss: 9.4982\n",
      "Epoch [352/2000], Eval Loss: 9.7729\n",
      "Epoch [353/2000], Train Loss: 9.3528\n",
      "Epoch [353/2000], Eval Loss: 9.7918\n",
      "Epoch [354/2000], Train Loss: 9.3613\n",
      "Epoch [354/2000], Eval Loss: 9.5616\n",
      "Epoch [355/2000], Train Loss: 9.1400\n",
      "Epoch [355/2000], Eval Loss: 9.5245\n",
      "Epoch [356/2000], Train Loss: 9.1182\n",
      "Epoch [356/2000], Eval Loss: 9.4336\n",
      "Epoch [357/2000], Train Loss: 8.9702\n",
      "Epoch [357/2000], Eval Loss: 9.3829\n",
      "Epoch [358/2000], Train Loss: 8.9143\n",
      "Epoch [358/2000], Eval Loss: 9.2643\n",
      "Epoch [359/2000], Train Loss: 8.8077\n",
      "Epoch [359/2000], Eval Loss: 9.1687\n",
      "Epoch [360/2000], Train Loss: 8.7264\n",
      "Epoch [360/2000], Eval Loss: 9.1255\n",
      "Epoch [361/2000], Train Loss: 8.6577\n",
      "Epoch [361/2000], Eval Loss: 9.0722\n",
      "Epoch [362/2000], Train Loss: 8.6071\n",
      "Epoch [362/2000], Eval Loss: 8.9326\n",
      "Epoch [363/2000], Train Loss: 8.4950\n",
      "Epoch [363/2000], Eval Loss: 8.9437\n",
      "Epoch [364/2000], Train Loss: 8.4821\n",
      "Epoch [364/2000], Eval Loss: 8.8535\n",
      "Epoch [365/2000], Train Loss: 8.3817\n",
      "Epoch [365/2000], Eval Loss: 8.7972\n",
      "Epoch [366/2000], Train Loss: 8.3427\n",
      "Epoch [366/2000], Eval Loss: 8.7107\n",
      "Epoch [367/2000], Train Loss: 8.2803\n",
      "Epoch [367/2000], Eval Loss: 8.6601\n",
      "Epoch [368/2000], Train Loss: 8.2179\n",
      "Epoch [368/2000], Eval Loss: 8.6304\n",
      "Epoch [369/2000], Train Loss: 8.1684\n",
      "Epoch [369/2000], Eval Loss: 8.5889\n",
      "Epoch [370/2000], Train Loss: 8.1271\n",
      "Epoch [370/2000], Eval Loss: 8.5211\n",
      "Epoch [371/2000], Train Loss: 8.0638\n",
      "Epoch [371/2000], Eval Loss: 8.4811\n",
      "Epoch [372/2000], Train Loss: 8.0342\n",
      "Epoch [372/2000], Eval Loss: 8.4235\n",
      "Epoch [373/2000], Train Loss: 7.9698\n",
      "Epoch [373/2000], Eval Loss: 8.3966\n",
      "Epoch [374/2000], Train Loss: 7.9376\n",
      "Epoch [374/2000], Eval Loss: 8.3347\n",
      "Epoch [375/2000], Train Loss: 7.8990\n",
      "Epoch [375/2000], Eval Loss: 8.2845\n",
      "Epoch [376/2000], Train Loss: 7.8480\n",
      "Epoch [376/2000], Eval Loss: 8.2898\n",
      "Epoch [377/2000], Train Loss: 7.8331\n",
      "Epoch [377/2000], Eval Loss: 8.2347\n",
      "Epoch [378/2000], Train Loss: 7.8011\n",
      "Epoch [378/2000], Eval Loss: 8.2317\n",
      "Epoch [379/2000], Train Loss: 7.8033\n",
      "Epoch [379/2000], Eval Loss: 8.3872\n",
      "Epoch [380/2000], Train Loss: 7.9602\n",
      "Epoch [380/2000], Eval Loss: 9.0829\n",
      "Epoch [381/2000], Train Loss: 8.6959\n",
      "Epoch [381/2000], Eval Loss: 10.0965\n",
      "Epoch [382/2000], Train Loss: 9.7729\n",
      "Epoch [382/2000], Eval Loss: 9.9903\n",
      "Epoch [383/2000], Train Loss: 9.4478\n",
      "Epoch [383/2000], Eval Loss: 10.8642\n",
      "Epoch [384/2000], Train Loss: 10.4851\n",
      "Epoch [384/2000], Eval Loss: 10.0542\n",
      "Epoch [385/2000], Train Loss: 9.6443\n",
      "Epoch [385/2000], Eval Loss: 9.9464\n",
      "Epoch [386/2000], Train Loss: 9.4786\n",
      "Epoch [386/2000], Eval Loss: 9.8975\n",
      "Epoch [387/2000], Train Loss: 9.5277\n",
      "Epoch [387/2000], Eval Loss: 9.5844\n",
      "Epoch [388/2000], Train Loss: 9.0995\n",
      "Epoch [388/2000], Eval Loss: 9.4647\n",
      "Epoch [389/2000], Train Loss: 8.9587\n",
      "Epoch [389/2000], Eval Loss: 9.2207\n",
      "Epoch [390/2000], Train Loss: 8.7913\n",
      "Epoch [390/2000], Eval Loss: 9.1489\n",
      "Epoch [391/2000], Train Loss: 8.7074\n",
      "Epoch [391/2000], Eval Loss: 9.0605\n",
      "Epoch [392/2000], Train Loss: 8.5539\n",
      "Epoch [392/2000], Eval Loss: 8.9886\n",
      "Epoch [393/2000], Train Loss: 8.5005\n",
      "Epoch [393/2000], Eval Loss: 8.8151\n",
      "Epoch [394/2000], Train Loss: 8.3682\n",
      "Epoch [394/2000], Eval Loss: 8.7073\n",
      "Epoch [395/2000], Train Loss: 8.2594\n",
      "Epoch [395/2000], Eval Loss: 8.6225\n",
      "Epoch [396/2000], Train Loss: 8.1452\n",
      "Epoch [396/2000], Eval Loss: 8.6406\n",
      "Epoch [397/2000], Train Loss: 8.1589\n",
      "Epoch [397/2000], Eval Loss: 8.4133\n",
      "Epoch [398/2000], Train Loss: 7.9634\n",
      "Epoch [398/2000], Eval Loss: 8.3960\n",
      "Epoch [399/2000], Train Loss: 7.9468\n",
      "Epoch [399/2000], Eval Loss: 8.3141\n",
      "Epoch [400/2000], Train Loss: 7.8496\n",
      "Epoch [400/2000], Eval Loss: 8.2552\n",
      "Epoch [401/2000], Train Loss: 7.8011\n",
      "Epoch [401/2000], Eval Loss: 8.1731\n",
      "Epoch [402/2000], Train Loss: 7.7299\n",
      "Epoch [402/2000], Eval Loss: 8.1157\n",
      "Epoch [403/2000], Train Loss: 7.6662\n",
      "Epoch [403/2000], Eval Loss: 8.0681\n",
      "Epoch [404/2000], Train Loss: 7.6111\n",
      "Epoch [404/2000], Eval Loss: 7.9931\n",
      "Epoch [405/2000], Train Loss: 7.5544\n",
      "Epoch [405/2000], Eval Loss: 7.9358\n",
      "Epoch [406/2000], Train Loss: 7.5205\n",
      "Epoch [406/2000], Eval Loss: 7.8671\n",
      "Epoch [407/2000], Train Loss: 7.4445\n",
      "Epoch [407/2000], Eval Loss: 7.8711\n",
      "Epoch [408/2000], Train Loss: 7.4275\n",
      "Epoch [408/2000], Eval Loss: 7.8098\n",
      "Epoch [409/2000], Train Loss: 7.3730\n",
      "Epoch [409/2000], Eval Loss: 7.7510\n",
      "Epoch [410/2000], Train Loss: 7.3444\n",
      "Epoch [410/2000], Eval Loss: 7.6930\n",
      "Epoch [411/2000], Train Loss: 7.2858\n",
      "Epoch [411/2000], Eval Loss: 7.6873\n",
      "Epoch [412/2000], Train Loss: 7.2700\n",
      "Epoch [412/2000], Eval Loss: 7.6353\n",
      "Epoch [413/2000], Train Loss: 7.2170\n",
      "Epoch [413/2000], Eval Loss: 7.6035\n",
      "Epoch [414/2000], Train Loss: 7.1836\n",
      "Epoch [414/2000], Eval Loss: 7.5846\n",
      "Epoch [415/2000], Train Loss: 7.1668\n",
      "Epoch [415/2000], Eval Loss: 7.5227\n",
      "Epoch [416/2000], Train Loss: 7.1174\n",
      "Epoch [416/2000], Eval Loss: 7.4973\n",
      "Epoch [417/2000], Train Loss: 7.0960\n",
      "Epoch [417/2000], Eval Loss: 7.4737\n",
      "Epoch [418/2000], Train Loss: 7.0687\n",
      "Epoch [418/2000], Eval Loss: 7.4378\n",
      "Epoch [419/2000], Train Loss: 7.0317\n",
      "Epoch [419/2000], Eval Loss: 7.4111\n",
      "Epoch [420/2000], Train Loss: 7.0108\n",
      "Epoch [420/2000], Eval Loss: 7.3879\n",
      "Epoch [421/2000], Train Loss: 6.9830\n",
      "Epoch [421/2000], Eval Loss: 7.3604\n",
      "Epoch [422/2000], Train Loss: 6.9538\n",
      "Epoch [422/2000], Eval Loss: 7.3345\n",
      "Epoch [423/2000], Train Loss: 6.9272\n",
      "Epoch [423/2000], Eval Loss: 7.3091\n",
      "Epoch [424/2000], Train Loss: 6.9011\n",
      "Epoch [424/2000], Eval Loss: 7.2985\n",
      "Epoch [425/2000], Train Loss: 6.8839\n",
      "Epoch [425/2000], Eval Loss: 7.2693\n",
      "Epoch [426/2000], Train Loss: 6.8585\n",
      "Epoch [426/2000], Eval Loss: 7.2414\n",
      "Epoch [427/2000], Train Loss: 6.8391\n",
      "Epoch [427/2000], Eval Loss: 7.2313\n",
      "Epoch [428/2000], Train Loss: 6.8205\n",
      "Epoch [428/2000], Eval Loss: 7.2146\n",
      "Epoch [429/2000], Train Loss: 6.7980\n",
      "Epoch [429/2000], Eval Loss: 7.1961\n",
      "Epoch [430/2000], Train Loss: 6.7837\n",
      "Epoch [430/2000], Eval Loss: 7.1843\n",
      "Epoch [431/2000], Train Loss: 6.7705\n",
      "Epoch [431/2000], Eval Loss: 7.1869\n",
      "Epoch [432/2000], Train Loss: 6.7696\n",
      "Epoch [432/2000], Eval Loss: 7.2191\n",
      "Epoch [433/2000], Train Loss: 6.8075\n",
      "Epoch [433/2000], Eval Loss: 7.3756\n",
      "Epoch [434/2000], Train Loss: 6.9416\n",
      "Epoch [434/2000], Eval Loss: 7.9153\n",
      "Epoch [435/2000], Train Loss: 7.5171\n",
      "Epoch [435/2000], Eval Loss: 10.1825\n",
      "Epoch [436/2000], Train Loss: 9.8481\n",
      "Epoch [436/2000], Eval Loss: 17.7543\n",
      "Epoch [437/2000], Train Loss: 17.6762\n",
      "Epoch [437/2000], Eval Loss: 14.6755\n",
      "Epoch [438/2000], Train Loss: 14.5515\n",
      "Epoch [438/2000], Eval Loss: 17.3156\n",
      "Epoch [439/2000], Train Loss: 17.0596\n",
      "Epoch [439/2000], Eval Loss: 15.3757\n",
      "Epoch [440/2000], Train Loss: 14.9315\n",
      "Epoch [440/2000], Eval Loss: 14.6497\n",
      "Epoch [441/2000], Train Loss: 14.3127\n",
      "Epoch [441/2000], Eval Loss: 13.6821\n",
      "Epoch [442/2000], Train Loss: 13.2831\n",
      "Epoch [442/2000], Eval Loss: 12.7809\n",
      "Epoch [443/2000], Train Loss: 12.3541\n",
      "Epoch [443/2000], Eval Loss: 12.4681\n",
      "Epoch [444/2000], Train Loss: 11.9758\n",
      "Epoch [444/2000], Eval Loss: 12.0238\n",
      "Epoch [445/2000], Train Loss: 11.5726\n",
      "Epoch [445/2000], Eval Loss: 12.0841\n",
      "Epoch [446/2000], Train Loss: 11.6000\n",
      "Epoch [446/2000], Eval Loss: 11.5445\n",
      "Epoch [447/2000], Train Loss: 10.9932\n",
      "Epoch [447/2000], Eval Loss: 11.4371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [448/2000], Train Loss: 10.8983\n",
      "Epoch [448/2000], Eval Loss: 11.0109\n",
      "Epoch [449/2000], Train Loss: 10.4977\n",
      "Epoch [449/2000], Eval Loss: 10.8202\n",
      "Epoch [450/2000], Train Loss: 10.3017\n",
      "Epoch [450/2000], Eval Loss: 10.4123\n",
      "Epoch [451/2000], Train Loss: 9.8860\n",
      "Epoch [451/2000], Eval Loss: 10.3839\n",
      "Epoch [452/2000], Train Loss: 9.8585\n",
      "Epoch [452/2000], Eval Loss: 10.1024\n",
      "Epoch [453/2000], Train Loss: 9.5941\n",
      "Epoch [453/2000], Eval Loss: 9.9064\n",
      "Epoch [454/2000], Train Loss: 9.4273\n",
      "Epoch [454/2000], Eval Loss: 9.6541\n",
      "Epoch [455/2000], Train Loss: 9.1842\n",
      "Epoch [455/2000], Eval Loss: 9.4485\n",
      "Epoch [456/2000], Train Loss: 8.9517\n",
      "Epoch [456/2000], Eval Loss: 9.3098\n",
      "Epoch [457/2000], Train Loss: 8.8232\n",
      "Epoch [457/2000], Eval Loss: 9.1804\n",
      "Epoch [458/2000], Train Loss: 8.7098\n",
      "Epoch [458/2000], Eval Loss: 8.9690\n",
      "Epoch [459/2000], Train Loss: 8.5024\n",
      "Epoch [459/2000], Eval Loss: 8.8380\n",
      "Epoch [460/2000], Train Loss: 8.3641\n",
      "Epoch [460/2000], Eval Loss: 8.6888\n",
      "Epoch [461/2000], Train Loss: 8.2246\n",
      "Epoch [461/2000], Eval Loss: 8.5832\n",
      "Epoch [462/2000], Train Loss: 8.1437\n",
      "Epoch [462/2000], Eval Loss: 8.4031\n",
      "Epoch [463/2000], Train Loss: 7.9817\n",
      "Epoch [463/2000], Eval Loss: 8.3462\n",
      "Epoch [464/2000], Train Loss: 7.9260\n",
      "Epoch [464/2000], Eval Loss: 8.2138\n",
      "Epoch [465/2000], Train Loss: 7.8012\n",
      "Epoch [465/2000], Eval Loss: 8.1091\n",
      "Epoch [466/2000], Train Loss: 7.6924\n",
      "Epoch [466/2000], Eval Loss: 8.0226\n",
      "Epoch [467/2000], Train Loss: 7.5986\n",
      "Epoch [467/2000], Eval Loss: 7.9520\n",
      "Epoch [468/2000], Train Loss: 7.5264\n",
      "Epoch [468/2000], Eval Loss: 7.8579\n",
      "Epoch [469/2000], Train Loss: 7.4288\n",
      "Epoch [469/2000], Eval Loss: 7.7945\n",
      "Epoch [470/2000], Train Loss: 7.3743\n",
      "Epoch [470/2000], Eval Loss: 7.6962\n",
      "Epoch [471/2000], Train Loss: 7.2916\n",
      "Epoch [471/2000], Eval Loss: 7.6273\n",
      "Epoch [472/2000], Train Loss: 7.2343\n",
      "Epoch [472/2000], Eval Loss: 7.5680\n",
      "Epoch [473/2000], Train Loss: 7.1785\n",
      "Epoch [473/2000], Eval Loss: 7.5111\n",
      "Epoch [474/2000], Train Loss: 7.1199\n",
      "Epoch [474/2000], Eval Loss: 7.4586\n",
      "Epoch [475/2000], Train Loss: 7.0703\n",
      "Epoch [475/2000], Eval Loss: 7.4185\n",
      "Epoch [476/2000], Train Loss: 7.0262\n",
      "Epoch [476/2000], Eval Loss: 7.3699\n",
      "Epoch [477/2000], Train Loss: 6.9726\n",
      "Epoch [477/2000], Eval Loss: 7.3337\n",
      "Epoch [478/2000], Train Loss: 6.9344\n",
      "Epoch [478/2000], Eval Loss: 7.2921\n",
      "Epoch [479/2000], Train Loss: 6.8977\n",
      "Epoch [479/2000], Eval Loss: 7.2391\n",
      "Epoch [480/2000], Train Loss: 6.8529\n",
      "Epoch [480/2000], Eval Loss: 7.1964\n",
      "Epoch [481/2000], Train Loss: 6.8140\n",
      "Epoch [481/2000], Eval Loss: 7.1591\n",
      "Epoch [482/2000], Train Loss: 6.7803\n",
      "Epoch [482/2000], Eval Loss: 7.1272\n",
      "Epoch [483/2000], Train Loss: 6.7521\n",
      "Epoch [483/2000], Eval Loss: 7.0970\n",
      "Epoch [484/2000], Train Loss: 6.7218\n",
      "Epoch [484/2000], Eval Loss: 7.0677\n",
      "Epoch [485/2000], Train Loss: 6.6941\n",
      "Epoch [485/2000], Eval Loss: 7.0321\n",
      "Epoch [486/2000], Train Loss: 6.6643\n",
      "Epoch [486/2000], Eval Loss: 6.9990\n",
      "Epoch [487/2000], Train Loss: 6.6367\n",
      "Epoch [487/2000], Eval Loss: 6.9791\n",
      "Epoch [488/2000], Train Loss: 6.6135\n",
      "Epoch [488/2000], Eval Loss: 6.9611\n",
      "Epoch [489/2000], Train Loss: 6.5946\n",
      "Epoch [489/2000], Eval Loss: 6.9379\n",
      "Epoch [490/2000], Train Loss: 6.5653\n",
      "Epoch [490/2000], Eval Loss: 6.9193\n",
      "Epoch [491/2000], Train Loss: 6.5493\n",
      "Epoch [491/2000], Eval Loss: 6.8961\n",
      "Epoch [492/2000], Train Loss: 6.5219\n",
      "Epoch [492/2000], Eval Loss: 6.8752\n",
      "Epoch [493/2000], Train Loss: 6.5041\n",
      "Epoch [493/2000], Eval Loss: 6.8519\n",
      "Epoch [494/2000], Train Loss: 6.4848\n",
      "Epoch [494/2000], Eval Loss: 6.8325\n",
      "Epoch [495/2000], Train Loss: 6.4660\n",
      "Epoch [495/2000], Eval Loss: 6.8194\n",
      "Epoch [496/2000], Train Loss: 6.4516\n",
      "Epoch [496/2000], Eval Loss: 6.8038\n",
      "Epoch [497/2000], Train Loss: 6.4303\n",
      "Epoch [497/2000], Eval Loss: 6.7853\n",
      "Epoch [498/2000], Train Loss: 6.4164\n",
      "Epoch [498/2000], Eval Loss: 6.7647\n",
      "Epoch [499/2000], Train Loss: 6.3974\n",
      "Epoch [499/2000], Eval Loss: 6.7493\n",
      "Epoch [500/2000], Train Loss: 6.3819\n",
      "Epoch [500/2000], Eval Loss: 6.7390\n",
      "Epoch [501/2000], Train Loss: 6.3665\n",
      "Epoch [501/2000], Eval Loss: 6.7244\n",
      "Epoch [502/2000], Train Loss: 6.3543\n",
      "Epoch [502/2000], Eval Loss: 6.7096\n",
      "Epoch [503/2000], Train Loss: 6.3407\n",
      "Epoch [503/2000], Eval Loss: 6.6952\n",
      "Epoch [504/2000], Train Loss: 6.3244\n",
      "Epoch [504/2000], Eval Loss: 6.6827\n",
      "Epoch [505/2000], Train Loss: 6.3108\n",
      "Epoch [505/2000], Eval Loss: 6.6716\n",
      "Epoch [506/2000], Train Loss: 6.3019\n",
      "Epoch [506/2000], Eval Loss: 6.6592\n",
      "Epoch [507/2000], Train Loss: 6.2851\n",
      "Epoch [507/2000], Eval Loss: 6.6471\n",
      "Epoch [508/2000], Train Loss: 6.2730\n",
      "Epoch [508/2000], Eval Loss: 6.6376\n",
      "Epoch [509/2000], Train Loss: 6.2620\n",
      "Epoch [509/2000], Eval Loss: 6.6282\n",
      "Epoch [510/2000], Train Loss: 6.2494\n",
      "Epoch [510/2000], Eval Loss: 6.6203\n",
      "Epoch [511/2000], Train Loss: 6.2384\n",
      "Epoch [511/2000], Eval Loss: 6.6100\n",
      "Epoch [512/2000], Train Loss: 6.2273\n",
      "Epoch [512/2000], Eval Loss: 6.5969\n",
      "Epoch [513/2000], Train Loss: 6.2203\n",
      "Epoch [513/2000], Eval Loss: 6.5852\n",
      "Epoch [514/2000], Train Loss: 6.2060\n",
      "Epoch [514/2000], Eval Loss: 6.5758\n",
      "Epoch [515/2000], Train Loss: 6.1955\n",
      "Epoch [515/2000], Eval Loss: 6.5672\n",
      "Epoch [516/2000], Train Loss: 6.1857\n",
      "Epoch [516/2000], Eval Loss: 6.5564\n",
      "Epoch [517/2000], Train Loss: 6.1763\n",
      "Epoch [517/2000], Eval Loss: 6.5471\n",
      "Epoch [518/2000], Train Loss: 6.1670\n",
      "Epoch [518/2000], Eval Loss: 6.5392\n",
      "Epoch [519/2000], Train Loss: 6.1573\n",
      "Epoch [519/2000], Eval Loss: 6.5325\n",
      "Epoch [520/2000], Train Loss: 6.1480\n",
      "Epoch [520/2000], Eval Loss: 6.5266\n",
      "Epoch [521/2000], Train Loss: 6.1401\n",
      "Epoch [521/2000], Eval Loss: 6.5185\n",
      "Epoch [522/2000], Train Loss: 6.1313\n",
      "Epoch [522/2000], Eval Loss: 6.5087\n",
      "Epoch [523/2000], Train Loss: 6.1237\n",
      "Epoch [523/2000], Eval Loss: 6.4994\n",
      "Epoch [524/2000], Train Loss: 6.1136\n",
      "Epoch [524/2000], Eval Loss: 6.4924\n",
      "Epoch [525/2000], Train Loss: 6.1099\n",
      "Epoch [525/2000], Eval Loss: 6.4861\n",
      "Epoch [526/2000], Train Loss: 6.0981\n",
      "Epoch [526/2000], Eval Loss: 6.4789\n",
      "Epoch [527/2000], Train Loss: 6.0900\n",
      "Epoch [527/2000], Eval Loss: 6.4710\n",
      "Epoch [528/2000], Train Loss: 6.0850\n",
      "Epoch [528/2000], Eval Loss: 6.4637\n",
      "Epoch [529/2000], Train Loss: 6.0770\n",
      "Epoch [529/2000], Eval Loss: 6.4575\n",
      "Epoch [530/2000], Train Loss: 6.0698\n",
      "Epoch [530/2000], Eval Loss: 6.4515\n",
      "Epoch [531/2000], Train Loss: 6.0618\n",
      "Epoch [531/2000], Eval Loss: 6.4441\n",
      "Epoch [532/2000], Train Loss: 6.0562\n",
      "Epoch [532/2000], Eval Loss: 6.4369\n",
      "Epoch [533/2000], Train Loss: 6.0461\n",
      "Epoch [533/2000], Eval Loss: 6.4313\n",
      "Epoch [534/2000], Train Loss: 6.0387\n",
      "Epoch [534/2000], Eval Loss: 6.4266\n",
      "Epoch [535/2000], Train Loss: 6.0327\n",
      "Epoch [535/2000], Eval Loss: 6.4212\n",
      "Epoch [536/2000], Train Loss: 6.0251\n",
      "Epoch [536/2000], Eval Loss: 6.4158\n",
      "Epoch [537/2000], Train Loss: 6.0213\n",
      "Epoch [537/2000], Eval Loss: 6.4101\n",
      "Epoch [538/2000], Train Loss: 6.0141\n",
      "Epoch [538/2000], Eval Loss: 6.4046\n",
      "Epoch [539/2000], Train Loss: 6.0080\n",
      "Epoch [539/2000], Eval Loss: 6.3982\n",
      "Epoch [540/2000], Train Loss: 5.9995\n",
      "Epoch [540/2000], Eval Loss: 6.3908\n",
      "Epoch [541/2000], Train Loss: 5.9934\n",
      "Epoch [541/2000], Eval Loss: 6.3835\n",
      "Epoch [542/2000], Train Loss: 5.9870\n",
      "Epoch [542/2000], Eval Loss: 6.3792\n",
      "Epoch [543/2000], Train Loss: 5.9827\n",
      "Epoch [543/2000], Eval Loss: 6.3754\n",
      "Epoch [544/2000], Train Loss: 5.9757\n",
      "Epoch [544/2000], Eval Loss: 6.3716\n",
      "Epoch [545/2000], Train Loss: 5.9695\n",
      "Epoch [545/2000], Eval Loss: 6.3660\n",
      "Epoch [546/2000], Train Loss: 5.9649\n",
      "Epoch [546/2000], Eval Loss: 6.3612\n",
      "Epoch [547/2000], Train Loss: 5.9574\n",
      "Epoch [547/2000], Eval Loss: 6.3557\n",
      "Epoch [548/2000], Train Loss: 5.9533\n",
      "Epoch [548/2000], Eval Loss: 6.3515\n",
      "Epoch [549/2000], Train Loss: 5.9463\n",
      "Epoch [549/2000], Eval Loss: 6.3463\n",
      "Epoch [550/2000], Train Loss: 5.9433\n",
      "Epoch [550/2000], Eval Loss: 6.3417\n",
      "Epoch [551/2000], Train Loss: 5.9360\n",
      "Epoch [551/2000], Eval Loss: 6.3363\n",
      "Epoch [552/2000], Train Loss: 5.9313\n",
      "Epoch [552/2000], Eval Loss: 6.3310\n",
      "Epoch [553/2000], Train Loss: 5.9251\n",
      "Epoch [553/2000], Eval Loss: 6.3276\n",
      "Epoch [554/2000], Train Loss: 5.9203\n",
      "Epoch [554/2000], Eval Loss: 6.3234\n",
      "Epoch [555/2000], Train Loss: 5.9155\n",
      "Epoch [555/2000], Eval Loss: 6.3191\n",
      "Epoch [556/2000], Train Loss: 5.9104\n",
      "Epoch [556/2000], Eval Loss: 6.3148\n",
      "Epoch [557/2000], Train Loss: 5.9047\n",
      "Epoch [557/2000], Eval Loss: 6.3110\n",
      "Epoch [558/2000], Train Loss: 5.9024\n",
      "Epoch [558/2000], Eval Loss: 6.3076\n",
      "Epoch [559/2000], Train Loss: 5.8955\n",
      "Epoch [559/2000], Eval Loss: 6.3029\n",
      "Epoch [560/2000], Train Loss: 5.8919\n",
      "Epoch [560/2000], Eval Loss: 6.2973\n",
      "Epoch [561/2000], Train Loss: 5.8857\n",
      "Epoch [561/2000], Eval Loss: 6.2922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [562/2000], Train Loss: 5.8812\n",
      "Epoch [562/2000], Eval Loss: 6.2901\n",
      "Epoch [563/2000], Train Loss: 5.8791\n",
      "Epoch [563/2000], Eval Loss: 6.2876\n",
      "Epoch [564/2000], Train Loss: 5.8743\n",
      "Epoch [564/2000], Eval Loss: 6.2840\n",
      "Epoch [565/2000], Train Loss: 5.8683\n",
      "Epoch [565/2000], Eval Loss: 6.2807\n",
      "Epoch [566/2000], Train Loss: 5.8650\n",
      "Epoch [566/2000], Eval Loss: 6.2768\n",
      "Epoch [567/2000], Train Loss: 5.8608\n",
      "Epoch [567/2000], Eval Loss: 6.2741\n",
      "Epoch [568/2000], Train Loss: 5.8573\n",
      "Epoch [568/2000], Eval Loss: 6.2705\n",
      "Epoch [569/2000], Train Loss: 5.8514\n",
      "Epoch [569/2000], Eval Loss: 6.2654\n",
      "Epoch [570/2000], Train Loss: 5.8470\n",
      "Epoch [570/2000], Eval Loss: 6.2621\n",
      "Epoch [571/2000], Train Loss: 5.8428\n",
      "Epoch [571/2000], Eval Loss: 6.2589\n",
      "Epoch [572/2000], Train Loss: 5.8385\n",
      "Epoch [572/2000], Eval Loss: 6.2566\n",
      "Epoch [573/2000], Train Loss: 5.8343\n",
      "Epoch [573/2000], Eval Loss: 6.2543\n",
      "Epoch [574/2000], Train Loss: 5.8302\n",
      "Epoch [574/2000], Eval Loss: 6.2509\n",
      "Epoch [575/2000], Train Loss: 5.8291\n",
      "Epoch [575/2000], Eval Loss: 6.2464\n",
      "Epoch [576/2000], Train Loss: 5.8226\n",
      "Epoch [576/2000], Eval Loss: 6.2435\n",
      "Epoch [577/2000], Train Loss: 5.8186\n",
      "Epoch [577/2000], Eval Loss: 6.2416\n",
      "Epoch [578/2000], Train Loss: 5.8149\n",
      "Epoch [578/2000], Eval Loss: 6.2395\n",
      "Epoch [579/2000], Train Loss: 5.8108\n",
      "Epoch [579/2000], Eval Loss: 6.2360\n",
      "Epoch [580/2000], Train Loss: 5.8088\n",
      "Epoch [580/2000], Eval Loss: 6.2311\n",
      "Epoch [581/2000], Train Loss: 5.8040\n",
      "Epoch [581/2000], Eval Loss: 6.2296\n",
      "Epoch [582/2000], Train Loss: 5.8003\n",
      "Epoch [582/2000], Eval Loss: 6.2282\n",
      "Epoch [583/2000], Train Loss: 5.8008\n",
      "Epoch [583/2000], Eval Loss: 6.2241\n",
      "Epoch [584/2000], Train Loss: 5.7934\n",
      "Epoch [584/2000], Eval Loss: 6.2219\n",
      "Epoch [585/2000], Train Loss: 5.7931\n",
      "Epoch [585/2000], Eval Loss: 6.2186\n",
      "Epoch [586/2000], Train Loss: 5.7886\n",
      "Epoch [586/2000], Eval Loss: 6.2148\n",
      "Epoch [587/2000], Train Loss: 5.7831\n",
      "Epoch [587/2000], Eval Loss: 6.2125\n",
      "Epoch [588/2000], Train Loss: 5.7809\n",
      "Epoch [588/2000], Eval Loss: 6.2114\n",
      "Epoch [589/2000], Train Loss: 5.7760\n",
      "Epoch [589/2000], Eval Loss: 6.2096\n",
      "Epoch [590/2000], Train Loss: 5.7725\n",
      "Epoch [590/2000], Eval Loss: 6.2050\n",
      "Epoch [591/2000], Train Loss: 5.7692\n",
      "Epoch [591/2000], Eval Loss: 6.2022\n",
      "Epoch [592/2000], Train Loss: 5.7678\n",
      "Epoch [592/2000], Eval Loss: 6.2022\n",
      "Epoch [593/2000], Train Loss: 5.7653\n",
      "Epoch [593/2000], Eval Loss: 6.1988\n",
      "Epoch [594/2000], Train Loss: 5.7609\n",
      "Epoch [594/2000], Eval Loss: 6.1955\n",
      "Epoch [595/2000], Train Loss: 5.7564\n",
      "Epoch [595/2000], Eval Loss: 6.1931\n",
      "Epoch [596/2000], Train Loss: 5.7546\n",
      "Epoch [596/2000], Eval Loss: 6.1909\n",
      "Epoch [597/2000], Train Loss: 5.7512\n",
      "Epoch [597/2000], Eval Loss: 6.1874\n",
      "Epoch [598/2000], Train Loss: 5.7497\n",
      "Epoch [598/2000], Eval Loss: 6.1862\n",
      "Epoch [599/2000], Train Loss: 5.7445\n",
      "Epoch [599/2000], Eval Loss: 6.1850\n",
      "Epoch [600/2000], Train Loss: 5.7416\n",
      "Epoch [600/2000], Eval Loss: 6.1843\n",
      "Epoch [601/2000], Train Loss: 5.7381\n",
      "Epoch [601/2000], Eval Loss: 6.1819\n",
      "Epoch [602/2000], Train Loss: 5.7380\n",
      "Epoch [602/2000], Eval Loss: 6.1791\n",
      "Epoch [603/2000], Train Loss: 5.7337\n",
      "Epoch [603/2000], Eval Loss: 6.1797\n",
      "Epoch [604/2000], Train Loss: 5.7299\n",
      "Epoch [604/2000], Eval Loss: 6.1765\n",
      "Epoch [605/2000], Train Loss: 5.7267\n",
      "Epoch [605/2000], Eval Loss: 6.1744\n",
      "Epoch [606/2000], Train Loss: 5.7238\n",
      "Epoch [606/2000], Eval Loss: 6.1721\n",
      "Epoch [607/2000], Train Loss: 5.7210\n",
      "Epoch [607/2000], Eval Loss: 6.1708\n",
      "Epoch [608/2000], Train Loss: 5.7194\n",
      "Epoch [608/2000], Eval Loss: 6.1679\n",
      "Epoch [609/2000], Train Loss: 5.7155\n",
      "Epoch [609/2000], Eval Loss: 6.1669\n",
      "Epoch [610/2000], Train Loss: 5.7131\n",
      "Epoch [610/2000], Eval Loss: 6.1643\n",
      "Epoch [611/2000], Train Loss: 5.7097\n",
      "Epoch [611/2000], Eval Loss: 6.1630\n",
      "Epoch [612/2000], Train Loss: 5.7101\n",
      "Epoch [612/2000], Eval Loss: 6.1606\n",
      "Epoch [613/2000], Train Loss: 5.7091\n",
      "Epoch [613/2000], Eval Loss: 6.1597\n",
      "Epoch [614/2000], Train Loss: 5.7030\n",
      "Epoch [614/2000], Eval Loss: 6.1583\n",
      "Epoch [615/2000], Train Loss: 5.6998\n",
      "Epoch [615/2000], Eval Loss: 6.1550\n",
      "Epoch [616/2000], Train Loss: 5.6970\n",
      "Epoch [616/2000], Eval Loss: 6.1556\n",
      "Epoch [617/2000], Train Loss: 5.6980\n",
      "Epoch [617/2000], Eval Loss: 6.1566\n",
      "Epoch [618/2000], Train Loss: 5.6941\n",
      "Epoch [618/2000], Eval Loss: 6.1536\n",
      "Epoch [619/2000], Train Loss: 5.6906\n",
      "Epoch [619/2000], Eval Loss: 6.1498\n",
      "Epoch [620/2000], Train Loss: 5.6870\n",
      "Epoch [620/2000], Eval Loss: 6.1480\n",
      "Epoch [621/2000], Train Loss: 5.6849\n",
      "Epoch [621/2000], Eval Loss: 6.1485\n",
      "Epoch [622/2000], Train Loss: 5.6838\n",
      "Epoch [622/2000], Eval Loss: 6.1475\n",
      "Epoch [623/2000], Train Loss: 5.6794\n",
      "Epoch [623/2000], Eval Loss: 6.1463\n",
      "Epoch [624/2000], Train Loss: 5.6785\n",
      "Epoch [624/2000], Eval Loss: 6.1454\n",
      "Epoch [625/2000], Train Loss: 5.6743\n",
      "Epoch [625/2000], Eval Loss: 6.1428\n",
      "Epoch [626/2000], Train Loss: 5.6734\n",
      "Epoch [626/2000], Eval Loss: 6.1415\n",
      "Epoch [627/2000], Train Loss: 5.6701\n",
      "Epoch [627/2000], Eval Loss: 6.1385\n",
      "Epoch [628/2000], Train Loss: 5.6672\n",
      "Epoch [628/2000], Eval Loss: 6.1378\n",
      "Epoch [629/2000], Train Loss: 5.6674\n",
      "Epoch [629/2000], Eval Loss: 6.1387\n",
      "Epoch [630/2000], Train Loss: 5.6640\n",
      "Epoch [630/2000], Eval Loss: 6.1382\n",
      "Epoch [631/2000], Train Loss: 5.6612\n",
      "Epoch [631/2000], Eval Loss: 6.1357\n",
      "Epoch [632/2000], Train Loss: 5.6581\n",
      "Epoch [632/2000], Eval Loss: 6.1338\n",
      "Epoch [633/2000], Train Loss: 5.6567\n",
      "Epoch [633/2000], Eval Loss: 6.1340\n",
      "Epoch [634/2000], Train Loss: 5.6531\n",
      "Epoch [634/2000], Eval Loss: 6.1315\n",
      "Epoch [635/2000], Train Loss: 5.6510\n",
      "Epoch [635/2000], Eval Loss: 6.1303\n",
      "Epoch [636/2000], Train Loss: 5.6487\n",
      "Epoch [636/2000], Eval Loss: 6.1307\n",
      "Epoch [637/2000], Train Loss: 5.6461\n",
      "Epoch [637/2000], Eval Loss: 6.1289\n",
      "Epoch [638/2000], Train Loss: 5.6441\n",
      "Epoch [638/2000], Eval Loss: 6.1288\n",
      "Epoch [639/2000], Train Loss: 5.6424\n",
      "Epoch [639/2000], Eval Loss: 6.1278\n",
      "Epoch [640/2000], Train Loss: 5.6411\n",
      "Epoch [640/2000], Eval Loss: 6.1260\n",
      "Epoch [641/2000], Train Loss: 5.6376\n",
      "Epoch [641/2000], Eval Loss: 6.1241\n",
      "Epoch [642/2000], Train Loss: 5.6369\n",
      "Epoch [642/2000], Eval Loss: 6.1225\n",
      "Epoch [643/2000], Train Loss: 5.6331\n",
      "Epoch [643/2000], Eval Loss: 6.1232\n",
      "Epoch [644/2000], Train Loss: 5.6308\n",
      "Epoch [644/2000], Eval Loss: 6.1240\n",
      "Epoch [645/2000], Train Loss: 5.6287\n",
      "Epoch [645/2000], Eval Loss: 6.1241\n",
      "Epoch [646/2000], Train Loss: 5.6277\n",
      "Epoch [646/2000], Eval Loss: 6.1239\n",
      "Epoch [647/2000], Train Loss: 5.6290\n",
      "Epoch [647/2000], Eval Loss: 6.1205\n",
      "Epoch [648/2000], Train Loss: 5.6239\n",
      "Epoch [648/2000], Eval Loss: 6.1203\n",
      "Epoch [649/2000], Train Loss: 5.6215\n",
      "Epoch [649/2000], Eval Loss: 6.1193\n",
      "Epoch [650/2000], Train Loss: 5.6187\n",
      "Epoch [650/2000], Eval Loss: 6.1188\n",
      "Epoch [651/2000], Train Loss: 5.6164\n",
      "Epoch [651/2000], Eval Loss: 6.1196\n",
      "Epoch [652/2000], Train Loss: 5.6142\n",
      "Epoch [652/2000], Eval Loss: 6.1201\n",
      "Epoch [653/2000], Train Loss: 5.6124\n",
      "Epoch [653/2000], Eval Loss: 6.1182\n",
      "Epoch [654/2000], Train Loss: 5.6137\n",
      "Epoch [654/2000], Eval Loss: 6.1164\n",
      "Epoch [655/2000], Train Loss: 5.6113\n",
      "Epoch [655/2000], Eval Loss: 6.1167\n",
      "Epoch [656/2000], Train Loss: 5.6082\n",
      "Epoch [656/2000], Eval Loss: 6.1164\n",
      "Epoch [657/2000], Train Loss: 5.6049\n",
      "Epoch [657/2000], Eval Loss: 6.1144\n",
      "Epoch [658/2000], Train Loss: 5.6028\n",
      "Epoch [658/2000], Eval Loss: 6.1144\n",
      "Epoch [659/2000], Train Loss: 5.6026\n",
      "Epoch [659/2000], Eval Loss: 6.1142\n",
      "Epoch [660/2000], Train Loss: 5.5990\n",
      "Epoch [660/2000], Eval Loss: 6.1155\n",
      "Epoch [661/2000], Train Loss: 5.5979\n",
      "Epoch [661/2000], Eval Loss: 6.1131\n",
      "Epoch [662/2000], Train Loss: 5.5963\n",
      "Epoch [662/2000], Eval Loss: 6.1117\n",
      "Epoch [663/2000], Train Loss: 5.5930\n",
      "Epoch [663/2000], Eval Loss: 6.1129\n",
      "Epoch [664/2000], Train Loss: 5.5914\n",
      "Epoch [664/2000], Eval Loss: 6.1131\n",
      "Epoch [665/2000], Train Loss: 5.5901\n",
      "Epoch [665/2000], Eval Loss: 6.1142\n",
      "Epoch [666/2000], Train Loss: 5.5872\n",
      "Epoch [666/2000], Eval Loss: 6.1147\n",
      "Epoch [667/2000], Train Loss: 5.5867\n",
      "Epoch [667/2000], Eval Loss: 6.1094\n",
      "Epoch [668/2000], Train Loss: 5.5831\n",
      "Epoch [668/2000], Eval Loss: 6.1089\n",
      "Epoch [669/2000], Train Loss: 5.5818\n",
      "Epoch [669/2000], Eval Loss: 6.1107\n",
      "Epoch [670/2000], Train Loss: 5.5807\n",
      "Epoch [670/2000], Eval Loss: 6.1105\n",
      "Epoch [671/2000], Train Loss: 5.5775\n",
      "Epoch [671/2000], Eval Loss: 6.1111\n",
      "Epoch [672/2000], Train Loss: 5.5797\n",
      "Epoch [672/2000], Eval Loss: 6.1093\n",
      "Epoch [673/2000], Train Loss: 5.5776\n",
      "Epoch [673/2000], Eval Loss: 6.1104\n",
      "Epoch [674/2000], Train Loss: 5.5725\n",
      "Epoch [674/2000], Eval Loss: 6.1105\n",
      "Epoch [675/2000], Train Loss: 5.5736\n",
      "Epoch [675/2000], Eval Loss: 6.1092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [676/2000], Train Loss: 5.5697\n",
      "Epoch [676/2000], Eval Loss: 6.1076\n",
      "Epoch [677/2000], Train Loss: 5.5696\n",
      "Epoch [677/2000], Eval Loss: 6.1067\n",
      "Epoch [678/2000], Train Loss: 5.5672\n",
      "Epoch [678/2000], Eval Loss: 6.1061\n",
      "Epoch [679/2000], Train Loss: 5.5646\n",
      "Epoch [679/2000], Eval Loss: 6.1031\n",
      "Epoch [680/2000], Train Loss: 5.5630\n",
      "Epoch [680/2000], Eval Loss: 6.1039\n",
      "Epoch [681/2000], Train Loss: 5.5604\n",
      "Epoch [681/2000], Eval Loss: 6.1056\n",
      "Epoch [682/2000], Train Loss: 5.5625\n",
      "Epoch [682/2000], Eval Loss: 6.1024\n",
      "Epoch [683/2000], Train Loss: 5.5586\n",
      "Epoch [683/2000], Eval Loss: 6.1066\n",
      "Epoch [684/2000], Train Loss: 5.5550\n",
      "Epoch [684/2000], Eval Loss: 6.1083\n",
      "Epoch [685/2000], Train Loss: 5.5531\n",
      "Epoch [685/2000], Eval Loss: 6.1046\n",
      "Epoch [686/2000], Train Loss: 5.5521\n",
      "Epoch [686/2000], Eval Loss: 6.1013\n",
      "Epoch [687/2000], Train Loss: 5.5510\n",
      "Epoch [687/2000], Eval Loss: 6.1052\n",
      "Epoch [688/2000], Train Loss: 5.5524\n",
      "Epoch [688/2000], Eval Loss: 6.1026\n",
      "Epoch [689/2000], Train Loss: 5.5508\n",
      "Epoch [689/2000], Eval Loss: 6.1041\n",
      "Epoch [690/2000], Train Loss: 5.5454\n",
      "Epoch [690/2000], Eval Loss: 6.1105\n",
      "Epoch [691/2000], Train Loss: 5.5437\n",
      "Epoch [691/2000], Eval Loss: 6.1076\n",
      "Epoch [692/2000], Train Loss: 5.5413\n",
      "Epoch [692/2000], Eval Loss: 6.1041\n",
      "Epoch [693/2000], Train Loss: 5.5417\n",
      "Epoch [693/2000], Eval Loss: 6.1053\n",
      "Epoch [694/2000], Train Loss: 5.5373\n",
      "Epoch [694/2000], Eval Loss: 6.1070\n",
      "Epoch [695/2000], Train Loss: 5.5354\n",
      "Epoch [695/2000], Eval Loss: 6.1078\n",
      "Epoch [696/2000], Train Loss: 5.5340\n",
      "Epoch [696/2000], Eval Loss: 6.1061\n",
      "Epoch [697/2000], Train Loss: 5.5332\n",
      "Epoch [697/2000], Eval Loss: 6.1055\n",
      "Epoch [698/2000], Train Loss: 5.5336\n",
      "Epoch [698/2000], Eval Loss: 6.1095\n",
      "Epoch [699/2000], Train Loss: 5.5295\n",
      "Epoch [699/2000], Eval Loss: 6.1090\n",
      "Epoch [700/2000], Train Loss: 5.5269\n",
      "Epoch [700/2000], Eval Loss: 6.1083\n",
      "Epoch [701/2000], Train Loss: 5.5262\n",
      "Epoch [701/2000], Eval Loss: 6.1100\n",
      "Epoch [702/2000], Train Loss: 5.5244\n",
      "Epoch [702/2000], Eval Loss: 6.1091\n",
      "Epoch [703/2000], Train Loss: 5.5214\n",
      "Epoch [703/2000], Eval Loss: 6.1080\n",
      "Epoch [704/2000], Train Loss: 5.5204\n",
      "Epoch [704/2000], Eval Loss: 6.1096\n",
      "Epoch [705/2000], Train Loss: 5.5183\n",
      "Epoch [705/2000], Eval Loss: 6.1117\n",
      "Epoch [706/2000], Train Loss: 5.5220\n",
      "Epoch [706/2000], Eval Loss: 6.1090\n",
      "Epoch [707/2000], Train Loss: 5.5155\n",
      "Epoch [707/2000], Eval Loss: 6.1138\n",
      "Epoch [708/2000], Train Loss: 5.5134\n",
      "Epoch [708/2000], Eval Loss: 6.1185\n",
      "Epoch [709/2000], Train Loss: 5.5127\n",
      "Epoch [709/2000], Eval Loss: 6.1140\n",
      "Epoch [710/2000], Train Loss: 5.5116\n",
      "Epoch [710/2000], Eval Loss: 6.1112\n",
      "Epoch [711/2000], Train Loss: 5.5090\n",
      "Epoch [711/2000], Eval Loss: 6.1162\n",
      "Epoch [712/2000], Train Loss: 5.5069\n",
      "Epoch [712/2000], Eval Loss: 6.1166\n",
      "Epoch [713/2000], Train Loss: 5.5059\n",
      "Epoch [713/2000], Eval Loss: 6.1175\n",
      "Epoch [714/2000], Train Loss: 5.5031\n",
      "Epoch [714/2000], Eval Loss: 6.1181\n",
      "Epoch [715/2000], Train Loss: 5.5012\n",
      "Epoch [715/2000], Eval Loss: 6.1257\n",
      "Epoch [716/2000], Train Loss: 5.4997\n",
      "Epoch [716/2000], Eval Loss: 6.1221\n",
      "Epoch [717/2000], Train Loss: 5.4979\n",
      "Epoch [717/2000], Eval Loss: 6.1239\n",
      "Epoch [718/2000], Train Loss: 5.4960\n",
      "Epoch [718/2000], Eval Loss: 6.1242\n",
      "Epoch [719/2000], Train Loss: 5.4964\n",
      "Epoch [719/2000], Eval Loss: 6.1272\n",
      "Epoch [720/2000], Train Loss: 5.4950\n",
      "Epoch [720/2000], Eval Loss: 6.1249\n",
      "Epoch [721/2000], Train Loss: 5.4928\n",
      "Epoch [721/2000], Eval Loss: 6.1326\n",
      "Epoch [722/2000], Train Loss: 5.4931\n",
      "Epoch [722/2000], Eval Loss: 6.1319\n",
      "Epoch [723/2000], Train Loss: 5.4963\n",
      "Epoch [723/2000], Eval Loss: 6.1467\n",
      "Epoch [724/2000], Train Loss: 5.5067\n",
      "Epoch [724/2000], Eval Loss: 6.1703\n",
      "Epoch [725/2000], Train Loss: 5.5370\n",
      "Epoch [725/2000], Eval Loss: 6.5306\n",
      "Epoch [726/2000], Train Loss: 5.8035\n",
      "Epoch [726/2000], Eval Loss: 18.6814\n",
      "Epoch [727/2000], Train Loss: 18.0744\n",
      "Epoch [727/2000], Eval Loss: 57.7925\n",
      "Epoch [728/2000], Train Loss: 56.4661\n",
      "Epoch [728/2000], Eval Loss: 57.9265\n",
      "Epoch [729/2000], Train Loss: 56.2028\n",
      "Epoch [729/2000], Eval Loss: 56.0448\n",
      "Epoch [730/2000], Train Loss: 55.8808\n",
      "Epoch [730/2000], Eval Loss: 45.4613\n",
      "Epoch [731/2000], Train Loss: 44.9388\n",
      "Epoch [731/2000], Eval Loss: 42.2954\n",
      "Epoch [732/2000], Train Loss: 41.2706\n",
      "Epoch [732/2000], Eval Loss: 37.1770\n",
      "Epoch [733/2000], Train Loss: 36.1328\n",
      "Epoch [733/2000], Eval Loss: 34.0181\n",
      "Epoch [734/2000], Train Loss: 32.9470\n",
      "Epoch [734/2000], Eval Loss: 32.0003\n",
      "Epoch [735/2000], Train Loss: 30.8648\n",
      "Epoch [735/2000], Eval Loss: 30.8227\n",
      "Epoch [736/2000], Train Loss: 29.6388\n",
      "Epoch [736/2000], Eval Loss: 29.4763\n",
      "Epoch [737/2000], Train Loss: 28.4729\n",
      "Epoch [737/2000], Eval Loss: 28.7051\n",
      "Epoch [738/2000], Train Loss: 27.8996\n",
      "Epoch [738/2000], Eval Loss: 27.6775\n",
      "Epoch [739/2000], Train Loss: 26.9220\n",
      "Epoch [739/2000], Eval Loss: 26.7423\n",
      "Epoch [740/2000], Train Loss: 26.0118\n",
      "Epoch [740/2000], Eval Loss: 25.5006\n",
      "Epoch [741/2000], Train Loss: 24.8765\n",
      "Epoch [741/2000], Eval Loss: 24.5860\n",
      "Epoch [742/2000], Train Loss: 23.9863\n",
      "Epoch [742/2000], Eval Loss: 24.1095\n",
      "Epoch [743/2000], Train Loss: 23.5520\n",
      "Epoch [743/2000], Eval Loss: 23.4996\n",
      "Epoch [744/2000], Train Loss: 22.9627\n",
      "Epoch [744/2000], Eval Loss: 22.7647\n",
      "Epoch [745/2000], Train Loss: 22.2315\n",
      "Epoch [745/2000], Eval Loss: 22.1961\n",
      "Epoch [746/2000], Train Loss: 21.5856\n",
      "Epoch [746/2000], Eval Loss: 21.6813\n",
      "Epoch [747/2000], Train Loss: 21.0141\n",
      "Epoch [747/2000], Eval Loss: 21.1684\n",
      "Epoch [748/2000], Train Loss: 20.4560\n",
      "Epoch [748/2000], Eval Loss: 20.6346\n",
      "Epoch [749/2000], Train Loss: 19.9170\n",
      "Epoch [749/2000], Eval Loss: 20.0482\n",
      "Epoch [750/2000], Train Loss: 19.2904\n",
      "Epoch [750/2000], Eval Loss: 19.5366\n",
      "Epoch [751/2000], Train Loss: 18.8119\n",
      "Epoch [751/2000], Eval Loss: 19.0847\n",
      "Epoch [752/2000], Train Loss: 18.3715\n",
      "Epoch [752/2000], Eval Loss: 18.6946\n",
      "Epoch [753/2000], Train Loss: 17.9756\n",
      "Epoch [753/2000], Eval Loss: 18.3481\n",
      "Epoch [754/2000], Train Loss: 17.5882\n",
      "Epoch [754/2000], Eval Loss: 17.9252\n",
      "Epoch [755/2000], Train Loss: 17.1634\n",
      "Epoch [755/2000], Eval Loss: 17.6113\n",
      "Epoch [756/2000], Train Loss: 16.7878\n",
      "Epoch [756/2000], Eval Loss: 17.3000\n",
      "Epoch [757/2000], Train Loss: 16.4275\n",
      "Epoch [757/2000], Eval Loss: 16.9636\n",
      "Epoch [758/2000], Train Loss: 16.1003\n",
      "Epoch [758/2000], Eval Loss: 16.6053\n",
      "Epoch [759/2000], Train Loss: 15.7604\n",
      "Epoch [759/2000], Eval Loss: 16.2293\n",
      "Epoch [760/2000], Train Loss: 15.4108\n",
      "Epoch [760/2000], Eval Loss: 15.9160\n",
      "Epoch [761/2000], Train Loss: 15.1227\n",
      "Epoch [761/2000], Eval Loss: 15.5910\n",
      "Epoch [762/2000], Train Loss: 14.8390\n",
      "Epoch [762/2000], Eval Loss: 15.3229\n",
      "Epoch [763/2000], Train Loss: 14.5708\n",
      "Epoch [763/2000], Eval Loss: 15.0463\n",
      "Epoch [764/2000], Train Loss: 14.2844\n",
      "Epoch [764/2000], Eval Loss: 14.8137\n",
      "Epoch [765/2000], Train Loss: 14.0177\n",
      "Epoch [765/2000], Eval Loss: 14.6009\n",
      "Epoch [766/2000], Train Loss: 13.7869\n",
      "Epoch [766/2000], Eval Loss: 14.3917\n",
      "Epoch [767/2000], Train Loss: 13.5753\n",
      "Epoch [767/2000], Eval Loss: 14.1904\n",
      "Epoch [768/2000], Train Loss: 13.3745\n",
      "Epoch [768/2000], Eval Loss: 13.9786\n",
      "Epoch [769/2000], Train Loss: 13.1533\n",
      "Epoch [769/2000], Eval Loss: 13.7581\n",
      "Epoch [770/2000], Train Loss: 12.9500\n",
      "Epoch [770/2000], Eval Loss: 13.5227\n",
      "Epoch [771/2000], Train Loss: 12.7411\n",
      "Epoch [771/2000], Eval Loss: 13.3170\n",
      "Epoch [772/2000], Train Loss: 12.5576\n",
      "Epoch [772/2000], Eval Loss: 13.1158\n",
      "Epoch [773/2000], Train Loss: 12.3710\n",
      "Epoch [773/2000], Eval Loss: 12.9382\n",
      "Epoch [774/2000], Train Loss: 12.1949\n",
      "Epoch [774/2000], Eval Loss: 12.7659\n",
      "Epoch [775/2000], Train Loss: 12.0201\n",
      "Epoch [775/2000], Eval Loss: 12.6038\n",
      "Epoch [776/2000], Train Loss: 11.8615\n",
      "Epoch [776/2000], Eval Loss: 12.4402\n",
      "Epoch [777/2000], Train Loss: 11.7041\n",
      "Epoch [777/2000], Eval Loss: 12.2668\n",
      "Epoch [778/2000], Train Loss: 11.5488\n",
      "Epoch [778/2000], Eval Loss: 12.0902\n",
      "Epoch [779/2000], Train Loss: 11.3969\n",
      "Epoch [779/2000], Eval Loss: 11.9309\n",
      "Epoch [780/2000], Train Loss: 11.2512\n",
      "Epoch [780/2000], Eval Loss: 11.7863\n",
      "Epoch [781/2000], Train Loss: 11.1083\n",
      "Epoch [781/2000], Eval Loss: 11.6503\n",
      "Epoch [782/2000], Train Loss: 10.9711\n",
      "Epoch [782/2000], Eval Loss: 11.5209\n",
      "Epoch [783/2000], Train Loss: 10.8403\n",
      "Epoch [783/2000], Eval Loss: 11.3915\n",
      "Epoch [784/2000], Train Loss: 10.7062\n",
      "Epoch [784/2000], Eval Loss: 11.2580\n",
      "Epoch [785/2000], Train Loss: 10.5825\n",
      "Epoch [785/2000], Eval Loss: 11.1269\n",
      "Epoch [786/2000], Train Loss: 10.4585\n",
      "Epoch [786/2000], Eval Loss: 10.9963\n",
      "Epoch [787/2000], Train Loss: 10.3363\n",
      "Epoch [787/2000], Eval Loss: 10.8759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [788/2000], Train Loss: 10.2213\n",
      "Epoch [788/2000], Eval Loss: 10.7492\n",
      "Epoch [789/2000], Train Loss: 10.1017\n",
      "Epoch [789/2000], Eval Loss: 10.6270\n",
      "Epoch [790/2000], Train Loss: 9.9853\n",
      "Epoch [790/2000], Eval Loss: 10.5146\n",
      "Epoch [791/2000], Train Loss: 9.8738\n",
      "Epoch [791/2000], Eval Loss: 10.4131\n",
      "Epoch [792/2000], Train Loss: 9.7651\n",
      "Epoch [792/2000], Eval Loss: 10.3032\n",
      "Epoch [793/2000], Train Loss: 9.6535\n",
      "Epoch [793/2000], Eval Loss: 10.1922\n",
      "Epoch [794/2000], Train Loss: 9.5443\n",
      "Epoch [794/2000], Eval Loss: 10.0878\n",
      "Epoch [795/2000], Train Loss: 9.4418\n",
      "Epoch [795/2000], Eval Loss: 9.9824\n",
      "Epoch [796/2000], Train Loss: 9.3385\n",
      "Epoch [796/2000], Eval Loss: 9.8780\n",
      "Epoch [797/2000], Train Loss: 9.2403\n",
      "Epoch [797/2000], Eval Loss: 9.7713\n",
      "Epoch [798/2000], Train Loss: 9.1453\n",
      "Epoch [798/2000], Eval Loss: 9.6741\n",
      "Epoch [799/2000], Train Loss: 9.0461\n",
      "Epoch [799/2000], Eval Loss: 9.5741\n",
      "Epoch [800/2000], Train Loss: 8.9516\n",
      "Epoch [800/2000], Eval Loss: 9.4776\n",
      "Epoch [801/2000], Train Loss: 8.8626\n",
      "Epoch [801/2000], Eval Loss: 9.3863\n",
      "Epoch [802/2000], Train Loss: 8.7782\n",
      "Epoch [802/2000], Eval Loss: 9.3220\n",
      "Epoch [803/2000], Train Loss: 8.7311\n",
      "Epoch [803/2000], Eval Loss: 9.3675\n",
      "Epoch [804/2000], Train Loss: 8.7889\n",
      "Epoch [804/2000], Eval Loss: 9.8467\n",
      "Epoch [805/2000], Train Loss: 9.2834\n",
      "Epoch [805/2000], Eval Loss: 10.6048\n",
      "Epoch [806/2000], Train Loss: 10.1226\n",
      "Epoch [806/2000], Eval Loss: 9.9772\n",
      "Epoch [807/2000], Train Loss: 9.4639\n",
      "Epoch [807/2000], Eval Loss: 9.6943\n",
      "Epoch [808/2000], Train Loss: 9.1739\n",
      "Epoch [808/2000], Eval Loss: 9.5177\n",
      "Epoch [809/2000], Train Loss: 8.9911\n",
      "Epoch [809/2000], Eval Loss: 9.3678\n",
      "Epoch [810/2000], Train Loss: 8.8263\n",
      "Epoch [810/2000], Eval Loss: 9.1892\n",
      "Epoch [811/2000], Train Loss: 8.6535\n",
      "Epoch [811/2000], Eval Loss: 9.1791\n",
      "Epoch [812/2000], Train Loss: 8.6319\n",
      "Epoch [812/2000], Eval Loss: 8.9037\n",
      "Epoch [813/2000], Train Loss: 8.3948\n",
      "Epoch [813/2000], Eval Loss: 8.9100\n",
      "Epoch [814/2000], Train Loss: 8.4249\n",
      "Epoch [814/2000], Eval Loss: 8.7090\n",
      "Epoch [815/2000], Train Loss: 8.2064\n",
      "Epoch [815/2000], Eval Loss: 8.7382\n",
      "Epoch [816/2000], Train Loss: 8.2075\n",
      "Epoch [816/2000], Eval Loss: 8.5865\n",
      "Epoch [817/2000], Train Loss: 8.0530\n",
      "Epoch [817/2000], Eval Loss: 8.5095\n",
      "Epoch [818/2000], Train Loss: 8.0085\n",
      "Epoch [818/2000], Eval Loss: 8.4203\n",
      "Epoch [819/2000], Train Loss: 7.9181\n",
      "Epoch [819/2000], Eval Loss: 8.2981\n",
      "Epoch [820/2000], Train Loss: 7.7876\n",
      "Epoch [820/2000], Eval Loss: 8.2811\n",
      "Epoch [821/2000], Train Loss: 7.7673\n",
      "Epoch [821/2000], Eval Loss: 8.1489\n",
      "Epoch [822/2000], Train Loss: 7.6581\n",
      "Epoch [822/2000], Eval Loss: 8.1074\n",
      "Epoch [823/2000], Train Loss: 7.6221\n",
      "Epoch [823/2000], Eval Loss: 8.0087\n",
      "Epoch [824/2000], Train Loss: 7.5190\n",
      "Epoch [824/2000], Eval Loss: 7.9630\n",
      "Epoch [825/2000], Train Loss: 7.4715\n",
      "Epoch [825/2000], Eval Loss: 7.8901\n",
      "Epoch [826/2000], Train Loss: 7.4119\n",
      "Epoch [826/2000], Eval Loss: 7.8056\n",
      "Epoch [827/2000], Train Loss: 7.3451\n",
      "Epoch [827/2000], Eval Loss: 7.7509\n",
      "Epoch [828/2000], Train Loss: 7.2968\n",
      "Epoch [828/2000], Eval Loss: 7.6739\n",
      "Epoch [829/2000], Train Loss: 7.2295\n",
      "Epoch [829/2000], Eval Loss: 7.6503\n",
      "Epoch [830/2000], Train Loss: 7.1932\n",
      "Epoch [830/2000], Eval Loss: 7.5889\n",
      "Epoch [831/2000], Train Loss: 7.1271\n",
      "Epoch [831/2000], Eval Loss: 7.5404\n",
      "Epoch [832/2000], Train Loss: 7.0899\n",
      "Epoch [832/2000], Eval Loss: 7.4668\n",
      "Epoch [833/2000], Train Loss: 7.0396\n",
      "Epoch [833/2000], Eval Loss: 7.4187\n",
      "Epoch [834/2000], Train Loss: 6.9954\n",
      "Epoch [834/2000], Eval Loss: 7.3847\n",
      "Epoch [835/2000], Train Loss: 6.9557\n",
      "Epoch [835/2000], Eval Loss: 7.3438\n",
      "Epoch [836/2000], Train Loss: 6.9132\n",
      "Epoch [836/2000], Eval Loss: 7.3048\n",
      "Epoch [837/2000], Train Loss: 6.8789\n",
      "Epoch [837/2000], Eval Loss: 7.2613\n",
      "Epoch [838/2000], Train Loss: 6.8384\n",
      "Epoch [838/2000], Eval Loss: 7.2190\n",
      "Epoch [839/2000], Train Loss: 6.8046\n",
      "Epoch [839/2000], Eval Loss: 7.1742\n",
      "Epoch [840/2000], Train Loss: 6.7751\n",
      "Epoch [840/2000], Eval Loss: 7.1381\n",
      "Epoch [841/2000], Train Loss: 6.7377\n",
      "Epoch [841/2000], Eval Loss: 7.1101\n",
      "Epoch [842/2000], Train Loss: 6.7081\n",
      "Epoch [842/2000], Eval Loss: 7.0738\n",
      "Epoch [843/2000], Train Loss: 6.6772\n",
      "Epoch [843/2000], Eval Loss: 7.0375\n",
      "Epoch [844/2000], Train Loss: 6.6466\n",
      "Epoch [844/2000], Eval Loss: 7.0081\n",
      "Epoch [845/2000], Train Loss: 6.6185\n",
      "Epoch [845/2000], Eval Loss: 6.9775\n",
      "Epoch [846/2000], Train Loss: 6.5913\n",
      "Epoch [846/2000], Eval Loss: 6.9441\n",
      "Epoch [847/2000], Train Loss: 6.5658\n",
      "Epoch [847/2000], Eval Loss: 6.9215\n",
      "Epoch [848/2000], Train Loss: 6.5425\n",
      "Epoch [848/2000], Eval Loss: 6.8977\n",
      "Epoch [849/2000], Train Loss: 6.5167\n",
      "Epoch [849/2000], Eval Loss: 6.8707\n",
      "Epoch [850/2000], Train Loss: 6.4960\n",
      "Epoch [850/2000], Eval Loss: 6.8378\n",
      "Epoch [851/2000], Train Loss: 6.4757\n",
      "Epoch [851/2000], Eval Loss: 6.8120\n",
      "Epoch [852/2000], Train Loss: 6.4493\n",
      "Epoch [852/2000], Eval Loss: 6.7876\n",
      "Epoch [853/2000], Train Loss: 6.4345\n",
      "Epoch [853/2000], Eval Loss: 6.7607\n",
      "Epoch [854/2000], Train Loss: 6.4087\n",
      "Epoch [854/2000], Eval Loss: 6.7399\n",
      "Epoch [855/2000], Train Loss: 6.3939\n",
      "Epoch [855/2000], Eval Loss: 6.7194\n",
      "Epoch [856/2000], Train Loss: 6.3725\n",
      "Epoch [856/2000], Eval Loss: 6.6994\n",
      "Epoch [857/2000], Train Loss: 6.3561\n",
      "Epoch [857/2000], Eval Loss: 6.6772\n",
      "Epoch [858/2000], Train Loss: 6.3398\n",
      "Epoch [858/2000], Eval Loss: 6.6615\n",
      "Epoch [859/2000], Train Loss: 6.3274\n",
      "Epoch [859/2000], Eval Loss: 6.6447\n",
      "Epoch [860/2000], Train Loss: 6.3113\n",
      "Epoch [860/2000], Eval Loss: 6.6316\n",
      "Epoch [861/2000], Train Loss: 6.3018\n",
      "Epoch [861/2000], Eval Loss: 6.6222\n",
      "Epoch [862/2000], Train Loss: 6.2917\n",
      "Epoch [862/2000], Eval Loss: 6.6245\n",
      "Epoch [863/2000], Train Loss: 6.2906\n",
      "Epoch [863/2000], Eval Loss: 6.6327\n",
      "Epoch [864/2000], Train Loss: 6.3023\n",
      "Epoch [864/2000], Eval Loss: 6.6801\n",
      "Epoch [865/2000], Train Loss: 6.3458\n",
      "Epoch [865/2000], Eval Loss: 6.8597\n",
      "Epoch [866/2000], Train Loss: 6.5166\n",
      "Epoch [866/2000], Eval Loss: 7.8287\n",
      "Epoch [867/2000], Train Loss: 7.4497\n",
      "Epoch [867/2000], Eval Loss: 13.0879\n",
      "Epoch [868/2000], Train Loss: 12.5835\n",
      "Epoch [868/2000], Eval Loss: 14.2150\n",
      "Epoch [869/2000], Train Loss: 14.1034\n",
      "Epoch [869/2000], Eval Loss: 15.7933\n",
      "Epoch [870/2000], Train Loss: 15.6699\n",
      "Epoch [870/2000], Eval Loss: 13.9744\n",
      "Epoch [871/2000], Train Loss: 13.6616\n",
      "Epoch [871/2000], Eval Loss: 12.5829\n",
      "Epoch [872/2000], Train Loss: 12.0725\n",
      "Epoch [872/2000], Eval Loss: 12.5798\n",
      "Epoch [873/2000], Train Loss: 11.9823\n",
      "Epoch [873/2000], Eval Loss: 10.8199\n",
      "Epoch [874/2000], Train Loss: 10.4157\n",
      "Epoch [874/2000], Eval Loss: 10.2284\n",
      "Epoch [875/2000], Train Loss: 9.9594\n",
      "Epoch [875/2000], Eval Loss: 9.9345\n",
      "Epoch [876/2000], Train Loss: 9.6528\n",
      "Epoch [876/2000], Eval Loss: 9.6939\n",
      "Epoch [877/2000], Train Loss: 9.3857\n",
      "Epoch [877/2000], Eval Loss: 9.3042\n",
      "Epoch [878/2000], Train Loss: 9.0034\n",
      "Epoch [878/2000], Eval Loss: 9.5022\n",
      "Epoch [879/2000], Train Loss: 9.1584\n",
      "Epoch [879/2000], Eval Loss: 9.1694\n",
      "Epoch [880/2000], Train Loss: 8.7847\n",
      "Epoch [880/2000], Eval Loss: 8.9923\n",
      "Epoch [881/2000], Train Loss: 8.5763\n",
      "Epoch [881/2000], Eval Loss: 8.7618\n",
      "Epoch [882/2000], Train Loss: 8.3684\n",
      "Epoch [882/2000], Eval Loss: 8.5560\n",
      "Epoch [883/2000], Train Loss: 8.2006\n",
      "Epoch [883/2000], Eval Loss: 8.4425\n",
      "Epoch [884/2000], Train Loss: 8.0716\n",
      "Epoch [884/2000], Eval Loss: 8.3514\n",
      "Epoch [885/2000], Train Loss: 7.9496\n",
      "Epoch [885/2000], Eval Loss: 8.2846\n",
      "Epoch [886/2000], Train Loss: 7.8977\n",
      "Epoch [886/2000], Eval Loss: 8.1678\n",
      "Epoch [887/2000], Train Loss: 7.7581\n",
      "Epoch [887/2000], Eval Loss: 7.9799\n",
      "Epoch [888/2000], Train Loss: 7.5774\n",
      "Epoch [888/2000], Eval Loss: 7.8464\n",
      "Epoch [889/2000], Train Loss: 7.4715\n",
      "Epoch [889/2000], Eval Loss: 7.7507\n",
      "Epoch [890/2000], Train Loss: 7.3877\n",
      "Epoch [890/2000], Eval Loss: 7.6609\n",
      "Epoch [891/2000], Train Loss: 7.3129\n",
      "Epoch [891/2000], Eval Loss: 7.5391\n",
      "Epoch [892/2000], Train Loss: 7.1887\n",
      "Epoch [892/2000], Eval Loss: 7.4380\n",
      "Epoch [893/2000], Train Loss: 7.0838\n",
      "Epoch [893/2000], Eval Loss: 7.4078\n",
      "Epoch [894/2000], Train Loss: 7.0399\n",
      "Epoch [894/2000], Eval Loss: 7.3418\n",
      "Epoch [895/2000], Train Loss: 6.9508\n",
      "Epoch [895/2000], Eval Loss: 7.2744\n",
      "Epoch [896/2000], Train Loss: 6.8765\n",
      "Epoch [896/2000], Eval Loss: 7.1607\n",
      "Epoch [897/2000], Train Loss: 6.7823\n",
      "Epoch [897/2000], Eval Loss: 7.1051\n",
      "Epoch [898/2000], Train Loss: 6.7454\n",
      "Epoch [898/2000], Eval Loss: 7.0321\n",
      "Epoch [899/2000], Train Loss: 6.6935\n",
      "Epoch [899/2000], Eval Loss: 6.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [900/2000], Train Loss: 6.6145\n",
      "Epoch [900/2000], Eval Loss: 6.8971\n",
      "Epoch [901/2000], Train Loss: 6.5746\n",
      "Epoch [901/2000], Eval Loss: 6.8402\n",
      "Epoch [902/2000], Train Loss: 6.5274\n",
      "Epoch [902/2000], Eval Loss: 6.7971\n",
      "Epoch [903/2000], Train Loss: 6.4916\n",
      "Epoch [903/2000], Eval Loss: 6.7548\n",
      "Epoch [904/2000], Train Loss: 6.4507\n",
      "Epoch [904/2000], Eval Loss: 6.7116\n",
      "Epoch [905/2000], Train Loss: 6.4060\n",
      "Epoch [905/2000], Eval Loss: 6.6864\n",
      "Epoch [906/2000], Train Loss: 6.3820\n",
      "Epoch [906/2000], Eval Loss: 6.6522\n",
      "Epoch [907/2000], Train Loss: 6.3465\n",
      "Epoch [907/2000], Eval Loss: 6.6331\n",
      "Epoch [908/2000], Train Loss: 6.3236\n",
      "Epoch [908/2000], Eval Loss: 6.5988\n",
      "Epoch [909/2000], Train Loss: 6.2873\n",
      "Epoch [909/2000], Eval Loss: 6.5711\n",
      "Epoch [910/2000], Train Loss: 6.2613\n",
      "Epoch [910/2000], Eval Loss: 6.5428\n",
      "Epoch [911/2000], Train Loss: 6.2381\n",
      "Epoch [911/2000], Eval Loss: 6.5171\n",
      "Epoch [912/2000], Train Loss: 6.2173\n",
      "Epoch [912/2000], Eval Loss: 6.4936\n",
      "Epoch [913/2000], Train Loss: 6.1957\n",
      "Epoch [913/2000], Eval Loss: 6.4718\n",
      "Epoch [914/2000], Train Loss: 6.1730\n",
      "Epoch [914/2000], Eval Loss: 6.4491\n",
      "Epoch [915/2000], Train Loss: 6.1531\n",
      "Epoch [915/2000], Eval Loss: 6.4317\n",
      "Epoch [916/2000], Train Loss: 6.1387\n",
      "Epoch [916/2000], Eval Loss: 6.4089\n",
      "Epoch [917/2000], Train Loss: 6.1199\n",
      "Epoch [917/2000], Eval Loss: 6.3917\n",
      "Epoch [918/2000], Train Loss: 6.1028\n",
      "Epoch [918/2000], Eval Loss: 6.3776\n",
      "Epoch [919/2000], Train Loss: 6.0893\n",
      "Epoch [919/2000], Eval Loss: 6.3594\n",
      "Epoch [920/2000], Train Loss: 6.0755\n",
      "Epoch [920/2000], Eval Loss: 6.3426\n",
      "Epoch [921/2000], Train Loss: 6.0584\n",
      "Epoch [921/2000], Eval Loss: 6.3320\n",
      "Epoch [922/2000], Train Loss: 6.0474\n",
      "Epoch [922/2000], Eval Loss: 6.3190\n",
      "Epoch [923/2000], Train Loss: 6.0343\n",
      "Epoch [923/2000], Eval Loss: 6.3075\n",
      "Epoch [924/2000], Train Loss: 6.0222\n",
      "Epoch [924/2000], Eval Loss: 6.2956\n",
      "Epoch [925/2000], Train Loss: 6.0116\n",
      "Epoch [925/2000], Eval Loss: 6.2822\n",
      "Epoch [926/2000], Train Loss: 6.0001\n",
      "Epoch [926/2000], Eval Loss: 6.2692\n",
      "Epoch [927/2000], Train Loss: 5.9907\n",
      "Epoch [927/2000], Eval Loss: 6.2571\n",
      "Epoch [928/2000], Train Loss: 5.9813\n",
      "Epoch [928/2000], Eval Loss: 6.2448\n",
      "Epoch [929/2000], Train Loss: 5.9716\n",
      "Epoch [929/2000], Eval Loss: 6.2368\n",
      "Epoch [930/2000], Train Loss: 5.9651\n",
      "Epoch [930/2000], Eval Loss: 6.2299\n",
      "Epoch [931/2000], Train Loss: 5.9535\n",
      "Epoch [931/2000], Eval Loss: 6.2219\n",
      "Epoch [932/2000], Train Loss: 5.9448\n",
      "Epoch [932/2000], Eval Loss: 6.2130\n",
      "Epoch [933/2000], Train Loss: 5.9384\n",
      "Epoch [933/2000], Eval Loss: 6.2046\n",
      "Epoch [934/2000], Train Loss: 5.9305\n",
      "Epoch [934/2000], Eval Loss: 6.1965\n",
      "Epoch [935/2000], Train Loss: 5.9226\n",
      "Epoch [935/2000], Eval Loss: 6.1891\n",
      "Epoch [936/2000], Train Loss: 5.9148\n",
      "Epoch [936/2000], Eval Loss: 6.1818\n",
      "Epoch [937/2000], Train Loss: 5.9074\n",
      "Epoch [937/2000], Eval Loss: 6.1744\n",
      "Epoch [938/2000], Train Loss: 5.9005\n",
      "Epoch [938/2000], Eval Loss: 6.1667\n",
      "Epoch [939/2000], Train Loss: 5.8947\n",
      "Epoch [939/2000], Eval Loss: 6.1588\n",
      "Epoch [940/2000], Train Loss: 5.8875\n",
      "Epoch [940/2000], Eval Loss: 6.1513\n",
      "Epoch [941/2000], Train Loss: 5.8812\n",
      "Epoch [941/2000], Eval Loss: 6.1455\n",
      "Epoch [942/2000], Train Loss: 5.8758\n",
      "Epoch [942/2000], Eval Loss: 6.1404\n",
      "Epoch [943/2000], Train Loss: 5.8696\n",
      "Epoch [943/2000], Eval Loss: 6.1361\n",
      "Epoch [944/2000], Train Loss: 5.8639\n",
      "Epoch [944/2000], Eval Loss: 6.1317\n",
      "Epoch [945/2000], Train Loss: 5.8597\n",
      "Epoch [945/2000], Eval Loss: 6.1276\n",
      "Epoch [946/2000], Train Loss: 5.8536\n",
      "Epoch [946/2000], Eval Loss: 6.1234\n",
      "Epoch [947/2000], Train Loss: 5.8487\n",
      "Epoch [947/2000], Eval Loss: 6.1181\n",
      "Epoch [948/2000], Train Loss: 5.8432\n",
      "Epoch [948/2000], Eval Loss: 6.1116\n",
      "Epoch [949/2000], Train Loss: 5.8384\n",
      "Epoch [949/2000], Eval Loss: 6.1056\n",
      "Epoch [950/2000], Train Loss: 5.8334\n",
      "Epoch [950/2000], Eval Loss: 6.1014\n",
      "Epoch [951/2000], Train Loss: 5.8293\n",
      "Epoch [951/2000], Eval Loss: 6.0987\n",
      "Epoch [952/2000], Train Loss: 5.8243\n",
      "Epoch [952/2000], Eval Loss: 6.0956\n",
      "Epoch [953/2000], Train Loss: 5.8218\n",
      "Epoch [953/2000], Eval Loss: 6.0911\n",
      "Epoch [954/2000], Train Loss: 5.8153\n",
      "Epoch [954/2000], Eval Loss: 6.0859\n",
      "Epoch [955/2000], Train Loss: 5.8110\n",
      "Epoch [955/2000], Eval Loss: 6.0818\n",
      "Epoch [956/2000], Train Loss: 5.8072\n",
      "Epoch [956/2000], Eval Loss: 6.0785\n",
      "Epoch [957/2000], Train Loss: 5.8026\n",
      "Epoch [957/2000], Eval Loss: 6.0751\n",
      "Epoch [958/2000], Train Loss: 5.7986\n",
      "Epoch [958/2000], Eval Loss: 6.0714\n",
      "Epoch [959/2000], Train Loss: 5.7948\n",
      "Epoch [959/2000], Eval Loss: 6.0671\n",
      "Epoch [960/2000], Train Loss: 5.7914\n",
      "Epoch [960/2000], Eval Loss: 6.0634\n",
      "Epoch [961/2000], Train Loss: 5.7869\n",
      "Epoch [961/2000], Eval Loss: 6.0607\n",
      "Epoch [962/2000], Train Loss: 5.7831\n",
      "Epoch [962/2000], Eval Loss: 6.0580\n",
      "Epoch [963/2000], Train Loss: 5.7821\n",
      "Epoch [963/2000], Eval Loss: 6.0547\n",
      "Epoch [964/2000], Train Loss: 5.7784\n",
      "Epoch [964/2000], Eval Loss: 6.0510\n",
      "Epoch [965/2000], Train Loss: 5.7727\n",
      "Epoch [965/2000], Eval Loss: 6.0470\n",
      "Epoch [966/2000], Train Loss: 5.7696\n",
      "Epoch [966/2000], Eval Loss: 6.0435\n",
      "Epoch [967/2000], Train Loss: 5.7662\n",
      "Epoch [967/2000], Eval Loss: 6.0407\n",
      "Epoch [968/2000], Train Loss: 5.7642\n",
      "Epoch [968/2000], Eval Loss: 6.0378\n",
      "Epoch [969/2000], Train Loss: 5.7611\n",
      "Epoch [969/2000], Eval Loss: 6.0349\n",
      "Epoch [970/2000], Train Loss: 5.7562\n",
      "Epoch [970/2000], Eval Loss: 6.0318\n",
      "Epoch [971/2000], Train Loss: 5.7531\n",
      "Epoch [971/2000], Eval Loss: 6.0288\n",
      "Epoch [972/2000], Train Loss: 5.7520\n",
      "Epoch [972/2000], Eval Loss: 6.0266\n",
      "Epoch [973/2000], Train Loss: 5.7476\n",
      "Epoch [973/2000], Eval Loss: 6.0240\n",
      "Epoch [974/2000], Train Loss: 5.7446\n",
      "Epoch [974/2000], Eval Loss: 6.0211\n",
      "Epoch [975/2000], Train Loss: 5.7411\n",
      "Epoch [975/2000], Eval Loss: 6.0184\n",
      "Epoch [976/2000], Train Loss: 5.7401\n",
      "Epoch [976/2000], Eval Loss: 6.0158\n",
      "Epoch [977/2000], Train Loss: 5.7353\n",
      "Epoch [977/2000], Eval Loss: 6.0132\n",
      "Epoch [978/2000], Train Loss: 5.7329\n",
      "Epoch [978/2000], Eval Loss: 6.0105\n",
      "Epoch [979/2000], Train Loss: 5.7297\n",
      "Epoch [979/2000], Eval Loss: 6.0081\n",
      "Epoch [980/2000], Train Loss: 5.7288\n",
      "Epoch [980/2000], Eval Loss: 6.0061\n",
      "Epoch [981/2000], Train Loss: 5.7247\n",
      "Epoch [981/2000], Eval Loss: 6.0040\n",
      "Epoch [982/2000], Train Loss: 5.7221\n",
      "Epoch [982/2000], Eval Loss: 6.0014\n",
      "Epoch [983/2000], Train Loss: 5.7198\n",
      "Epoch [983/2000], Eval Loss: 5.9991\n",
      "Epoch [984/2000], Train Loss: 5.7167\n",
      "Epoch [984/2000], Eval Loss: 5.9972\n",
      "Epoch [985/2000], Train Loss: 5.7145\n",
      "Epoch [985/2000], Eval Loss: 5.9951\n",
      "Epoch [986/2000], Train Loss: 5.7136\n",
      "Epoch [986/2000], Eval Loss: 5.9932\n",
      "Epoch [987/2000], Train Loss: 5.7095\n",
      "Epoch [987/2000], Eval Loss: 5.9905\n",
      "Epoch [988/2000], Train Loss: 5.7081\n",
      "Epoch [988/2000], Eval Loss: 5.9882\n",
      "Epoch [989/2000], Train Loss: 5.7057\n",
      "Epoch [989/2000], Eval Loss: 5.9864\n",
      "Epoch [990/2000], Train Loss: 5.7029\n",
      "Epoch [990/2000], Eval Loss: 5.9849\n",
      "Epoch [991/2000], Train Loss: 5.7004\n",
      "Epoch [991/2000], Eval Loss: 5.9829\n",
      "Epoch [992/2000], Train Loss: 5.6977\n",
      "Epoch [992/2000], Eval Loss: 5.9807\n",
      "Epoch [993/2000], Train Loss: 5.6952\n",
      "Epoch [993/2000], Eval Loss: 5.9788\n",
      "Epoch [994/2000], Train Loss: 5.6941\n",
      "Epoch [994/2000], Eval Loss: 5.9769\n",
      "Epoch [995/2000], Train Loss: 5.6911\n",
      "Epoch [995/2000], Eval Loss: 5.9753\n",
      "Epoch [996/2000], Train Loss: 5.6888\n",
      "Epoch [996/2000], Eval Loss: 5.9734\n",
      "Epoch [997/2000], Train Loss: 5.6864\n",
      "Epoch [997/2000], Eval Loss: 5.9714\n",
      "Epoch [998/2000], Train Loss: 5.6851\n",
      "Epoch [998/2000], Eval Loss: 5.9692\n",
      "Epoch [999/2000], Train Loss: 5.6823\n",
      "Epoch [999/2000], Eval Loss: 5.9675\n",
      "Epoch [1000/2000], Train Loss: 5.6804\n",
      "Epoch [1000/2000], Eval Loss: 5.9661\n",
      "Epoch [1001/2000], Train Loss: 5.6790\n",
      "Epoch [1001/2000], Eval Loss: 5.9648\n",
      "Epoch [1002/2000], Train Loss: 5.6760\n",
      "Epoch [1002/2000], Eval Loss: 5.9633\n",
      "Epoch [1003/2000], Train Loss: 5.6742\n",
      "Epoch [1003/2000], Eval Loss: 5.9620\n",
      "Epoch [1004/2000], Train Loss: 5.6719\n",
      "Epoch [1004/2000], Eval Loss: 5.9607\n",
      "Epoch [1005/2000], Train Loss: 5.6702\n",
      "Epoch [1005/2000], Eval Loss: 5.9592\n",
      "Epoch [1006/2000], Train Loss: 5.6680\n",
      "Epoch [1006/2000], Eval Loss: 5.9579\n",
      "Epoch [1007/2000], Train Loss: 5.6660\n",
      "Epoch [1007/2000], Eval Loss: 5.9566\n",
      "Epoch [1008/2000], Train Loss: 5.6656\n",
      "Epoch [1008/2000], Eval Loss: 5.9544\n",
      "Epoch [1009/2000], Train Loss: 5.6622\n",
      "Epoch [1009/2000], Eval Loss: 5.9522\n",
      "Epoch [1010/2000], Train Loss: 5.6603\n",
      "Epoch [1010/2000], Eval Loss: 5.9510\n",
      "Epoch [1011/2000], Train Loss: 5.6595\n",
      "Epoch [1011/2000], Eval Loss: 5.9501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1012/2000], Train Loss: 5.6566\n",
      "Epoch [1012/2000], Eval Loss: 5.9492\n",
      "Epoch [1013/2000], Train Loss: 5.6547\n",
      "Epoch [1013/2000], Eval Loss: 5.9481\n",
      "Epoch [1014/2000], Train Loss: 5.6529\n",
      "Epoch [1014/2000], Eval Loss: 5.9468\n",
      "Epoch [1015/2000], Train Loss: 5.6516\n",
      "Epoch [1015/2000], Eval Loss: 5.9452\n",
      "Epoch [1016/2000], Train Loss: 5.6494\n",
      "Epoch [1016/2000], Eval Loss: 5.9437\n",
      "Epoch [1017/2000], Train Loss: 5.6480\n",
      "Epoch [1017/2000], Eval Loss: 5.9418\n",
      "Epoch [1018/2000], Train Loss: 5.6462\n",
      "Epoch [1018/2000], Eval Loss: 5.9406\n",
      "Epoch [1019/2000], Train Loss: 5.6445\n",
      "Epoch [1019/2000], Eval Loss: 5.9399\n",
      "Epoch [1020/2000], Train Loss: 5.6428\n",
      "Epoch [1020/2000], Eval Loss: 5.9396\n",
      "Epoch [1021/2000], Train Loss: 5.6422\n",
      "Epoch [1021/2000], Eval Loss: 5.9388\n",
      "Epoch [1022/2000], Train Loss: 5.6393\n",
      "Epoch [1022/2000], Eval Loss: 5.9375\n",
      "Epoch [1023/2000], Train Loss: 5.6378\n",
      "Epoch [1023/2000], Eval Loss: 5.9357\n",
      "Epoch [1024/2000], Train Loss: 5.6372\n",
      "Epoch [1024/2000], Eval Loss: 5.9348\n",
      "Epoch [1025/2000], Train Loss: 5.6345\n",
      "Epoch [1025/2000], Eval Loss: 5.9339\n",
      "Epoch [1026/2000], Train Loss: 5.6331\n",
      "Epoch [1026/2000], Eval Loss: 5.9332\n",
      "Epoch [1027/2000], Train Loss: 5.6310\n",
      "Epoch [1027/2000], Eval Loss: 5.9320\n",
      "Epoch [1028/2000], Train Loss: 5.6294\n",
      "Epoch [1028/2000], Eval Loss: 5.9306\n",
      "Epoch [1029/2000], Train Loss: 5.6280\n",
      "Epoch [1029/2000], Eval Loss: 5.9293\n",
      "Epoch [1030/2000], Train Loss: 5.6264\n",
      "Epoch [1030/2000], Eval Loss: 5.9285\n",
      "Epoch [1031/2000], Train Loss: 5.6248\n",
      "Epoch [1031/2000], Eval Loss: 5.9280\n",
      "Epoch [1032/2000], Train Loss: 5.6235\n",
      "Epoch [1032/2000], Eval Loss: 5.9276\n",
      "Epoch [1033/2000], Train Loss: 5.6216\n",
      "Epoch [1033/2000], Eval Loss: 5.9268\n",
      "Epoch [1034/2000], Train Loss: 5.6206\n",
      "Epoch [1034/2000], Eval Loss: 5.9262\n",
      "Epoch [1035/2000], Train Loss: 5.6188\n",
      "Epoch [1035/2000], Eval Loss: 5.9250\n",
      "Epoch [1036/2000], Train Loss: 5.6175\n",
      "Epoch [1036/2000], Eval Loss: 5.9240\n",
      "Epoch [1037/2000], Train Loss: 5.6160\n",
      "Epoch [1037/2000], Eval Loss: 5.9231\n",
      "Epoch [1038/2000], Train Loss: 5.6146\n",
      "Epoch [1038/2000], Eval Loss: 5.9222\n",
      "Epoch [1039/2000], Train Loss: 5.6141\n",
      "Epoch [1039/2000], Eval Loss: 5.9214\n",
      "Epoch [1040/2000], Train Loss: 5.6114\n",
      "Epoch [1040/2000], Eval Loss: 5.9205\n",
      "Epoch [1041/2000], Train Loss: 5.6106\n",
      "Epoch [1041/2000], Eval Loss: 5.9191\n",
      "Epoch [1042/2000], Train Loss: 5.6090\n",
      "Epoch [1042/2000], Eval Loss: 5.9185\n",
      "Epoch [1043/2000], Train Loss: 5.6077\n",
      "Epoch [1043/2000], Eval Loss: 5.9182\n",
      "Epoch [1044/2000], Train Loss: 5.6059\n",
      "Epoch [1044/2000], Eval Loss: 5.9181\n",
      "Epoch [1045/2000], Train Loss: 5.6058\n",
      "Epoch [1045/2000], Eval Loss: 5.9177\n",
      "Epoch [1046/2000], Train Loss: 5.6031\n",
      "Epoch [1046/2000], Eval Loss: 5.9167\n",
      "Epoch [1047/2000], Train Loss: 5.6017\n",
      "Epoch [1047/2000], Eval Loss: 5.9152\n",
      "Epoch [1048/2000], Train Loss: 5.6005\n",
      "Epoch [1048/2000], Eval Loss: 5.9143\n",
      "Epoch [1049/2000], Train Loss: 5.5989\n",
      "Epoch [1049/2000], Eval Loss: 5.9135\n",
      "Epoch [1050/2000], Train Loss: 5.5977\n",
      "Epoch [1050/2000], Eval Loss: 5.9129\n",
      "Epoch [1051/2000], Train Loss: 5.5976\n",
      "Epoch [1051/2000], Eval Loss: 5.9122\n",
      "Epoch [1052/2000], Train Loss: 5.5957\n",
      "Epoch [1052/2000], Eval Loss: 5.9122\n",
      "Epoch [1053/2000], Train Loss: 5.5936\n",
      "Epoch [1053/2000], Eval Loss: 5.9117\n",
      "Epoch [1054/2000], Train Loss: 5.5923\n",
      "Epoch [1054/2000], Eval Loss: 5.9110\n",
      "Epoch [1055/2000], Train Loss: 5.5909\n",
      "Epoch [1055/2000], Eval Loss: 5.9102\n",
      "Epoch [1056/2000], Train Loss: 5.5898\n",
      "Epoch [1056/2000], Eval Loss: 5.9095\n",
      "Epoch [1057/2000], Train Loss: 5.5886\n",
      "Epoch [1057/2000], Eval Loss: 5.9087\n",
      "Epoch [1058/2000], Train Loss: 5.5882\n",
      "Epoch [1058/2000], Eval Loss: 5.9080\n",
      "Epoch [1059/2000], Train Loss: 5.5861\n",
      "Epoch [1059/2000], Eval Loss: 5.9077\n",
      "Epoch [1060/2000], Train Loss: 5.5847\n",
      "Epoch [1060/2000], Eval Loss: 5.9073\n",
      "Epoch [1061/2000], Train Loss: 5.5835\n",
      "Epoch [1061/2000], Eval Loss: 5.9068\n",
      "Epoch [1062/2000], Train Loss: 5.5825\n",
      "Epoch [1062/2000], Eval Loss: 5.9063\n",
      "Epoch [1063/2000], Train Loss: 5.5811\n",
      "Epoch [1063/2000], Eval Loss: 5.9065\n",
      "Epoch [1064/2000], Train Loss: 5.5796\n",
      "Epoch [1064/2000], Eval Loss: 5.9062\n",
      "Epoch [1065/2000], Train Loss: 5.5783\n",
      "Epoch [1065/2000], Eval Loss: 5.9057\n",
      "Epoch [1066/2000], Train Loss: 5.5785\n",
      "Epoch [1066/2000], Eval Loss: 5.9047\n",
      "Epoch [1067/2000], Train Loss: 5.5759\n",
      "Epoch [1067/2000], Eval Loss: 5.9034\n",
      "Epoch [1068/2000], Train Loss: 5.5760\n",
      "Epoch [1068/2000], Eval Loss: 5.9022\n",
      "Epoch [1069/2000], Train Loss: 5.5742\n",
      "Epoch [1069/2000], Eval Loss: 5.9011\n",
      "Epoch [1070/2000], Train Loss: 5.5726\n",
      "Epoch [1070/2000], Eval Loss: 5.9013\n",
      "Epoch [1071/2000], Train Loss: 5.5722\n",
      "Epoch [1071/2000], Eval Loss: 5.9021\n",
      "Epoch [1072/2000], Train Loss: 5.5702\n",
      "Epoch [1072/2000], Eval Loss: 5.9022\n",
      "Epoch [1073/2000], Train Loss: 5.5688\n",
      "Epoch [1073/2000], Eval Loss: 5.9018\n",
      "Epoch [1074/2000], Train Loss: 5.5676\n",
      "Epoch [1074/2000], Eval Loss: 5.9010\n",
      "Epoch [1075/2000], Train Loss: 5.5667\n",
      "Epoch [1075/2000], Eval Loss: 5.9005\n",
      "Epoch [1076/2000], Train Loss: 5.5655\n",
      "Epoch [1076/2000], Eval Loss: 5.9002\n",
      "Epoch [1077/2000], Train Loss: 5.5655\n",
      "Epoch [1077/2000], Eval Loss: 5.8996\n",
      "Epoch [1078/2000], Train Loss: 5.5635\n",
      "Epoch [1078/2000], Eval Loss: 5.8990\n",
      "Epoch [1079/2000], Train Loss: 5.5619\n",
      "Epoch [1079/2000], Eval Loss: 5.8984\n",
      "Epoch [1080/2000], Train Loss: 5.5610\n",
      "Epoch [1080/2000], Eval Loss: 5.8977\n",
      "Epoch [1081/2000], Train Loss: 5.5596\n",
      "Epoch [1081/2000], Eval Loss: 5.8982\n",
      "Epoch [1082/2000], Train Loss: 5.5591\n",
      "Epoch [1082/2000], Eval Loss: 5.8988\n",
      "Epoch [1083/2000], Train Loss: 5.5579\n",
      "Epoch [1083/2000], Eval Loss: 5.8992\n",
      "Epoch [1084/2000], Train Loss: 5.5572\n",
      "Epoch [1084/2000], Eval Loss: 5.8986\n",
      "Epoch [1085/2000], Train Loss: 5.5557\n",
      "Epoch [1085/2000], Eval Loss: 5.8977\n",
      "Epoch [1086/2000], Train Loss: 5.5548\n",
      "Epoch [1086/2000], Eval Loss: 5.8975\n",
      "Epoch [1087/2000], Train Loss: 5.5535\n",
      "Epoch [1087/2000], Eval Loss: 5.8970\n",
      "Epoch [1088/2000], Train Loss: 5.5518\n",
      "Epoch [1088/2000], Eval Loss: 5.8971\n",
      "Epoch [1089/2000], Train Loss: 5.5509\n",
      "Epoch [1089/2000], Eval Loss: 5.8971\n",
      "Epoch [1090/2000], Train Loss: 5.5499\n",
      "Epoch [1090/2000], Eval Loss: 5.8971\n",
      "Epoch [1091/2000], Train Loss: 5.5485\n",
      "Epoch [1091/2000], Eval Loss: 5.8971\n",
      "Epoch [1092/2000], Train Loss: 5.5479\n",
      "Epoch [1092/2000], Eval Loss: 5.8965\n",
      "Epoch [1093/2000], Train Loss: 5.5466\n",
      "Epoch [1093/2000], Eval Loss: 5.8956\n",
      "Epoch [1094/2000], Train Loss: 5.5456\n",
      "Epoch [1094/2000], Eval Loss: 5.8956\n",
      "Epoch [1095/2000], Train Loss: 5.5443\n",
      "Epoch [1095/2000], Eval Loss: 5.8956\n",
      "Epoch [1096/2000], Train Loss: 5.5435\n",
      "Epoch [1096/2000], Eval Loss: 5.8956\n",
      "Epoch [1097/2000], Train Loss: 5.5419\n",
      "Epoch [1097/2000], Eval Loss: 5.8958\n",
      "Epoch [1098/2000], Train Loss: 5.5408\n",
      "Epoch [1098/2000], Eval Loss: 5.8963\n",
      "Epoch [1099/2000], Train Loss: 5.5413\n",
      "Epoch [1099/2000], Eval Loss: 5.8962\n",
      "Epoch [1100/2000], Train Loss: 5.5387\n",
      "Epoch [1100/2000], Eval Loss: 5.8957\n",
      "Epoch [1101/2000], Train Loss: 5.5386\n",
      "Epoch [1101/2000], Eval Loss: 5.8950\n",
      "Epoch [1102/2000], Train Loss: 5.5374\n",
      "Epoch [1102/2000], Eval Loss: 5.8947\n",
      "Epoch [1103/2000], Train Loss: 5.5364\n",
      "Epoch [1103/2000], Eval Loss: 5.8948\n",
      "Epoch [1104/2000], Train Loss: 5.5352\n",
      "Epoch [1104/2000], Eval Loss: 5.8950\n",
      "Epoch [1105/2000], Train Loss: 5.5344\n",
      "Epoch [1105/2000], Eval Loss: 5.8950\n",
      "Epoch [1106/2000], Train Loss: 5.5332\n",
      "Epoch [1106/2000], Eval Loss: 5.8956\n",
      "Epoch [1107/2000], Train Loss: 5.5322\n",
      "Epoch [1107/2000], Eval Loss: 5.8957\n",
      "Epoch [1108/2000], Train Loss: 5.5305\n",
      "Epoch [1108/2000], Eval Loss: 5.8951\n",
      "Epoch [1109/2000], Train Loss: 5.5293\n",
      "Epoch [1109/2000], Eval Loss: 5.8942\n",
      "Epoch [1110/2000], Train Loss: 5.5283\n",
      "Epoch [1110/2000], Eval Loss: 5.8940\n",
      "Epoch [1111/2000], Train Loss: 5.5277\n",
      "Epoch [1111/2000], Eval Loss: 5.8952\n",
      "Epoch [1112/2000], Train Loss: 5.5272\n",
      "Epoch [1112/2000], Eval Loss: 5.8960\n",
      "Epoch [1113/2000], Train Loss: 5.5261\n",
      "Epoch [1113/2000], Eval Loss: 5.8954\n",
      "Epoch [1114/2000], Train Loss: 5.5242\n",
      "Epoch [1114/2000], Eval Loss: 5.8942\n",
      "Epoch [1115/2000], Train Loss: 5.5234\n",
      "Epoch [1115/2000], Eval Loss: 5.8936\n",
      "Epoch [1116/2000], Train Loss: 5.5227\n",
      "Epoch [1116/2000], Eval Loss: 5.8940\n",
      "Epoch [1117/2000], Train Loss: 5.5217\n",
      "Epoch [1117/2000], Eval Loss: 5.8953\n",
      "Epoch [1118/2000], Train Loss: 5.5199\n",
      "Epoch [1118/2000], Eval Loss: 5.8960\n",
      "Epoch [1119/2000], Train Loss: 5.5189\n",
      "Epoch [1119/2000], Eval Loss: 5.8956\n",
      "Epoch [1120/2000], Train Loss: 5.5177\n",
      "Epoch [1120/2000], Eval Loss: 5.8953\n",
      "Epoch [1121/2000], Train Loss: 5.5185\n",
      "Epoch [1121/2000], Eval Loss: 5.8953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1122/2000], Train Loss: 5.5161\n",
      "Epoch [1122/2000], Eval Loss: 5.8953\n",
      "Epoch [1123/2000], Train Loss: 5.5153\n",
      "Epoch [1123/2000], Eval Loss: 5.8953\n",
      "Epoch [1124/2000], Train Loss: 5.5136\n",
      "Epoch [1124/2000], Eval Loss: 5.8955\n",
      "Epoch [1125/2000], Train Loss: 5.5141\n",
      "Epoch [1125/2000], Eval Loss: 5.8954\n",
      "Epoch [1126/2000], Train Loss: 5.5132\n",
      "Epoch [1126/2000], Eval Loss: 5.8953\n",
      "Epoch [1127/2000], Train Loss: 5.5106\n",
      "Epoch [1127/2000], Eval Loss: 5.8954\n",
      "Epoch [1128/2000], Train Loss: 5.5095\n",
      "Epoch [1128/2000], Eval Loss: 5.8960\n",
      "Epoch [1129/2000], Train Loss: 5.5092\n",
      "Epoch [1129/2000], Eval Loss: 5.8965\n",
      "Epoch [1130/2000], Train Loss: 5.5082\n",
      "Epoch [1130/2000], Eval Loss: 5.8968\n",
      "Epoch [1131/2000], Train Loss: 5.5067\n",
      "Epoch [1131/2000], Eval Loss: 5.8965\n",
      "Epoch [1132/2000], Train Loss: 5.5061\n",
      "Epoch [1132/2000], Eval Loss: 5.8964\n",
      "Epoch [1133/2000], Train Loss: 5.5057\n",
      "Epoch [1133/2000], Eval Loss: 5.8968\n",
      "Epoch [1134/2000], Train Loss: 5.5033\n",
      "Epoch [1134/2000], Eval Loss: 5.8971\n",
      "Epoch [1135/2000], Train Loss: 5.5032\n",
      "Epoch [1135/2000], Eval Loss: 5.8976\n",
      "Epoch [1136/2000], Train Loss: 5.5021\n",
      "Epoch [1136/2000], Eval Loss: 5.8988\n",
      "Epoch [1137/2000], Train Loss: 5.5014\n",
      "Epoch [1137/2000], Eval Loss: 5.8992\n",
      "Epoch [1138/2000], Train Loss: 5.4999\n",
      "Epoch [1138/2000], Eval Loss: 5.8986\n",
      "Epoch [1139/2000], Train Loss: 5.4993\n",
      "Epoch [1139/2000], Eval Loss: 5.8979\n",
      "Epoch [1140/2000], Train Loss: 5.4973\n",
      "Epoch [1140/2000], Eval Loss: 5.8986\n",
      "Epoch [1141/2000], Train Loss: 5.4971\n",
      "Epoch [1141/2000], Eval Loss: 5.8999\n",
      "Epoch [1142/2000], Train Loss: 5.4960\n",
      "Epoch [1142/2000], Eval Loss: 5.9001\n",
      "Epoch [1143/2000], Train Loss: 5.4952\n",
      "Epoch [1143/2000], Eval Loss: 5.8997\n",
      "Epoch [1144/2000], Train Loss: 5.4939\n",
      "Epoch [1144/2000], Eval Loss: 5.9006\n",
      "Epoch [1145/2000], Train Loss: 5.4923\n",
      "Epoch [1145/2000], Eval Loss: 5.9022\n",
      "Epoch [1146/2000], Train Loss: 5.4923\n",
      "Epoch [1146/2000], Eval Loss: 5.9026\n",
      "Epoch [1147/2000], Train Loss: 5.4902\n",
      "Epoch [1147/2000], Eval Loss: 5.9018\n",
      "Epoch [1148/2000], Train Loss: 5.4895\n",
      "Epoch [1148/2000], Eval Loss: 5.9015\n",
      "Epoch [1149/2000], Train Loss: 5.4882\n",
      "Epoch [1149/2000], Eval Loss: 5.9021\n",
      "Epoch [1150/2000], Train Loss: 5.4891\n",
      "Epoch [1150/2000], Eval Loss: 5.9033\n",
      "Epoch [1151/2000], Train Loss: 5.4871\n",
      "Epoch [1151/2000], Eval Loss: 5.9039\n",
      "Epoch [1152/2000], Train Loss: 5.4854\n",
      "Epoch [1152/2000], Eval Loss: 5.9039\n",
      "Epoch [1153/2000], Train Loss: 5.4846\n",
      "Epoch [1153/2000], Eval Loss: 5.9040\n",
      "Epoch [1154/2000], Train Loss: 5.4837\n",
      "Epoch [1154/2000], Eval Loss: 5.9052\n",
      "Epoch [1155/2000], Train Loss: 5.4820\n",
      "Epoch [1155/2000], Eval Loss: 5.9065\n",
      "Epoch [1156/2000], Train Loss: 5.4817\n",
      "Epoch [1156/2000], Eval Loss: 5.9070\n",
      "Epoch [1157/2000], Train Loss: 5.4800\n",
      "Epoch [1157/2000], Eval Loss: 5.9067\n",
      "Epoch [1158/2000], Train Loss: 5.4789\n",
      "Epoch [1158/2000], Eval Loss: 5.9064\n",
      "Epoch [1159/2000], Train Loss: 5.4780\n",
      "Epoch [1159/2000], Eval Loss: 5.9075\n",
      "Epoch [1160/2000], Train Loss: 5.4775\n",
      "Epoch [1160/2000], Eval Loss: 5.9097\n",
      "Epoch [1161/2000], Train Loss: 5.4758\n",
      "Epoch [1161/2000], Eval Loss: 5.9109\n",
      "Epoch [1162/2000], Train Loss: 5.4751\n",
      "Epoch [1162/2000], Eval Loss: 5.9098\n",
      "Epoch [1163/2000], Train Loss: 5.4741\n",
      "Epoch [1163/2000], Eval Loss: 5.9089\n",
      "Epoch [1164/2000], Train Loss: 5.4729\n",
      "Epoch [1164/2000], Eval Loss: 5.9097\n",
      "Epoch [1165/2000], Train Loss: 5.4719\n",
      "Epoch [1165/2000], Eval Loss: 5.9117\n",
      "Epoch [1166/2000], Train Loss: 5.4706\n",
      "Epoch [1166/2000], Eval Loss: 5.9136\n",
      "Epoch [1167/2000], Train Loss: 5.4695\n",
      "Epoch [1167/2000], Eval Loss: 5.9146\n",
      "Epoch [1168/2000], Train Loss: 5.4699\n",
      "Epoch [1168/2000], Eval Loss: 5.9145\n",
      "Epoch [1169/2000], Train Loss: 5.4676\n",
      "Epoch [1169/2000], Eval Loss: 5.9148\n",
      "Epoch [1170/2000], Train Loss: 5.4678\n",
      "Epoch [1170/2000], Eval Loss: 5.9158\n",
      "Epoch [1171/2000], Train Loss: 5.4654\n",
      "Epoch [1171/2000], Eval Loss: 5.9164\n",
      "Epoch [1172/2000], Train Loss: 5.4678\n",
      "Epoch [1172/2000], Eval Loss: 5.9167\n",
      "Epoch [1173/2000], Train Loss: 5.4635\n",
      "Epoch [1173/2000], Eval Loss: 5.9176\n",
      "Epoch [1174/2000], Train Loss: 5.4624\n",
      "Epoch [1174/2000], Eval Loss: 5.9183\n",
      "Epoch [1175/2000], Train Loss: 5.4617\n",
      "Epoch [1175/2000], Eval Loss: 5.9193\n",
      "Epoch [1176/2000], Train Loss: 5.4634\n",
      "Epoch [1176/2000], Eval Loss: 5.9208\n",
      "Epoch [1177/2000], Train Loss: 5.4608\n",
      "Epoch [1177/2000], Eval Loss: 5.9211\n",
      "Epoch [1178/2000], Train Loss: 5.4584\n",
      "Epoch [1178/2000], Eval Loss: 5.9204\n",
      "Epoch [1179/2000], Train Loss: 5.4577\n",
      "Epoch [1179/2000], Eval Loss: 5.9205\n",
      "Epoch [1180/2000], Train Loss: 5.4564\n",
      "Epoch [1180/2000], Eval Loss: 5.9230\n",
      "Epoch [1181/2000], Train Loss: 5.4551\n",
      "Epoch [1181/2000], Eval Loss: 5.9261\n",
      "Epoch [1182/2000], Train Loss: 5.4566\n",
      "Epoch [1182/2000], Eval Loss: 5.9256\n",
      "Epoch [1183/2000], Train Loss: 5.4549\n",
      "Epoch [1183/2000], Eval Loss: 5.9241\n",
      "Epoch [1184/2000], Train Loss: 5.4521\n",
      "Epoch [1184/2000], Eval Loss: 5.9246\n",
      "Epoch [1185/2000], Train Loss: 5.4512\n",
      "Epoch [1185/2000], Eval Loss: 5.9262\n",
      "Epoch [1186/2000], Train Loss: 5.4501\n",
      "Epoch [1186/2000], Eval Loss: 5.9277\n",
      "Epoch [1187/2000], Train Loss: 5.4516\n",
      "Epoch [1187/2000], Eval Loss: 5.9282\n",
      "Epoch [1188/2000], Train Loss: 5.4488\n",
      "Epoch [1188/2000], Eval Loss: 5.9275\n",
      "Epoch [1189/2000], Train Loss: 5.4483\n",
      "Epoch [1189/2000], Eval Loss: 5.9285\n",
      "Epoch [1190/2000], Train Loss: 5.4491\n",
      "Epoch [1190/2000], Eval Loss: 5.9293\n",
      "Epoch [1191/2000], Train Loss: 5.4462\n",
      "Epoch [1191/2000], Eval Loss: 5.9314\n",
      "Epoch [1192/2000], Train Loss: 5.4448\n",
      "Epoch [1192/2000], Eval Loss: 5.9330\n",
      "Epoch [1193/2000], Train Loss: 5.4428\n",
      "Epoch [1193/2000], Eval Loss: 5.9331\n",
      "Epoch [1194/2000], Train Loss: 5.4421\n",
      "Epoch [1194/2000], Eval Loss: 5.9331\n",
      "Epoch [1195/2000], Train Loss: 5.4404\n",
      "Epoch [1195/2000], Eval Loss: 5.9351\n",
      "Epoch [1196/2000], Train Loss: 5.4418\n",
      "Epoch [1196/2000], Eval Loss: 5.9364\n",
      "Epoch [1197/2000], Train Loss: 5.4391\n",
      "Epoch [1197/2000], Eval Loss: 5.9361\n",
      "Epoch [1198/2000], Train Loss: 5.4396\n",
      "Epoch [1198/2000], Eval Loss: 5.9363\n",
      "Epoch [1199/2000], Train Loss: 5.4367\n",
      "Epoch [1199/2000], Eval Loss: 5.9374\n",
      "Epoch [1200/2000], Train Loss: 5.4378\n",
      "Epoch [1200/2000], Eval Loss: 5.9384\n",
      "Epoch [1201/2000], Train Loss: 5.4366\n",
      "Epoch [1201/2000], Eval Loss: 5.9382\n",
      "Epoch [1202/2000], Train Loss: 5.4354\n",
      "Epoch [1202/2000], Eval Loss: 5.9393\n",
      "Epoch [1203/2000], Train Loss: 5.4365\n",
      "Epoch [1203/2000], Eval Loss: 5.9390\n",
      "Epoch [1204/2000], Train Loss: 5.4318\n",
      "Epoch [1204/2000], Eval Loss: 5.9406\n",
      "Epoch [1205/2000], Train Loss: 5.4306\n",
      "Epoch [1205/2000], Eval Loss: 5.9442\n",
      "Epoch [1206/2000], Train Loss: 5.4311\n",
      "Epoch [1206/2000], Eval Loss: 5.9439\n",
      "Epoch [1207/2000], Train Loss: 5.4308\n",
      "Epoch [1207/2000], Eval Loss: 5.9433\n",
      "Epoch [1208/2000], Train Loss: 5.4292\n",
      "Epoch [1208/2000], Eval Loss: 5.9449\n",
      "Epoch [1209/2000], Train Loss: 5.4284\n",
      "Epoch [1209/2000], Eval Loss: 5.9451\n",
      "Epoch [1210/2000], Train Loss: 5.4255\n",
      "Epoch [1210/2000], Eval Loss: 5.9448\n",
      "Epoch [1211/2000], Train Loss: 5.4246\n",
      "Epoch [1211/2000], Eval Loss: 5.9475\n",
      "Epoch [1212/2000], Train Loss: 5.4237\n",
      "Epoch [1212/2000], Eval Loss: 5.9504\n",
      "Epoch [1213/2000], Train Loss: 5.4243\n",
      "Epoch [1213/2000], Eval Loss: 5.9510\n",
      "Epoch [1214/2000], Train Loss: 5.4215\n",
      "Epoch [1214/2000], Eval Loss: 5.9506\n",
      "Epoch [1215/2000], Train Loss: 5.4203\n",
      "Epoch [1215/2000], Eval Loss: 5.9511\n",
      "Epoch [1216/2000], Train Loss: 5.4193\n",
      "Epoch [1216/2000], Eval Loss: 5.9518\n",
      "Epoch [1217/2000], Train Loss: 5.4199\n",
      "Epoch [1217/2000], Eval Loss: 5.9532\n",
      "Epoch [1218/2000], Train Loss: 5.4185\n",
      "Epoch [1218/2000], Eval Loss: 5.9564\n",
      "Epoch [1219/2000], Train Loss: 5.4158\n",
      "Epoch [1219/2000], Eval Loss: 5.9574\n",
      "Epoch [1220/2000], Train Loss: 5.4167\n",
      "Epoch [1220/2000], Eval Loss: 5.9557\n",
      "Epoch [1221/2000], Train Loss: 5.4164\n",
      "Epoch [1221/2000], Eval Loss: 5.9560\n",
      "Epoch [1222/2000], Train Loss: 5.4154\n",
      "Epoch [1222/2000], Eval Loss: 5.9579\n",
      "Epoch [1223/2000], Train Loss: 5.4118\n",
      "Epoch [1223/2000], Eval Loss: 5.9608\n",
      "Epoch [1224/2000], Train Loss: 5.4108\n",
      "Epoch [1224/2000], Eval Loss: 5.9621\n",
      "Epoch [1225/2000], Train Loss: 5.4111\n",
      "Epoch [1225/2000], Eval Loss: 5.9643\n",
      "Epoch [1226/2000], Train Loss: 5.4086\n",
      "Epoch [1226/2000], Eval Loss: 5.9642\n",
      "Epoch [1227/2000], Train Loss: 5.4078\n",
      "Epoch [1227/2000], Eval Loss: 5.9636\n",
      "Epoch [1228/2000], Train Loss: 5.4064\n",
      "Epoch [1228/2000], Eval Loss: 5.9644\n",
      "Epoch [1229/2000], Train Loss: 5.4074\n",
      "Epoch [1229/2000], Eval Loss: 5.9652\n",
      "Epoch [1230/2000], Train Loss: 5.4041\n",
      "Epoch [1230/2000], Eval Loss: 5.9669\n",
      "Epoch [1231/2000], Train Loss: 5.4031\n",
      "Epoch [1231/2000], Eval Loss: 5.9694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1232/2000], Train Loss: 5.4071\n",
      "Epoch [1232/2000], Eval Loss: 5.9687\n",
      "Epoch [1233/2000], Train Loss: 5.4022\n",
      "Epoch [1233/2000], Eval Loss: 5.9702\n",
      "Epoch [1234/2000], Train Loss: 5.4036\n",
      "Epoch [1234/2000], Eval Loss: 5.9697\n",
      "Epoch [1235/2000], Train Loss: 5.3991\n",
      "Epoch [1235/2000], Eval Loss: 5.9705\n",
      "Epoch [1236/2000], Train Loss: 5.3998\n",
      "Epoch [1236/2000], Eval Loss: 5.9715\n",
      "Epoch [1237/2000], Train Loss: 5.3979\n",
      "Epoch [1237/2000], Eval Loss: 5.9740\n",
      "Epoch [1238/2000], Train Loss: 5.3992\n",
      "Epoch [1238/2000], Eval Loss: 5.9739\n",
      "Epoch [1239/2000], Train Loss: 5.3955\n",
      "Epoch [1239/2000], Eval Loss: 5.9777\n",
      "Epoch [1240/2000], Train Loss: 5.3937\n",
      "Epoch [1240/2000], Eval Loss: 5.9778\n",
      "Epoch [1241/2000], Train Loss: 5.3925\n",
      "Epoch [1241/2000], Eval Loss: 5.9772\n",
      "Epoch [1242/2000], Train Loss: 5.3931\n",
      "Epoch [1242/2000], Eval Loss: 5.9811\n",
      "Epoch [1243/2000], Train Loss: 5.3937\n",
      "Epoch [1243/2000], Eval Loss: 5.9789\n",
      "Epoch [1244/2000], Train Loss: 5.3891\n",
      "Epoch [1244/2000], Eval Loss: 5.9795\n",
      "Epoch [1245/2000], Train Loss: 5.3906\n",
      "Epoch [1245/2000], Eval Loss: 5.9821\n",
      "Epoch [1246/2000], Train Loss: 5.3886\n",
      "Epoch [1246/2000], Eval Loss: 5.9838\n",
      "Epoch [1247/2000], Train Loss: 5.3863\n",
      "Epoch [1247/2000], Eval Loss: 5.9859\n",
      "Epoch [1248/2000], Train Loss: 5.3850\n",
      "Epoch [1248/2000], Eval Loss: 5.9867\n",
      "Epoch [1249/2000], Train Loss: 5.3866\n",
      "Epoch [1249/2000], Eval Loss: 5.9854\n",
      "Epoch [1250/2000], Train Loss: 5.3827\n",
      "Epoch [1250/2000], Eval Loss: 5.9888\n",
      "Epoch [1251/2000], Train Loss: 5.3818\n",
      "Epoch [1251/2000], Eval Loss: 5.9920\n",
      "Epoch [1252/2000], Train Loss: 5.3809\n",
      "Epoch [1252/2000], Eval Loss: 5.9926\n",
      "Epoch [1253/2000], Train Loss: 5.3793\n",
      "Epoch [1253/2000], Eval Loss: 5.9942\n",
      "Epoch [1254/2000], Train Loss: 5.3815\n",
      "Epoch [1254/2000], Eval Loss: 5.9924\n",
      "Epoch [1255/2000], Train Loss: 5.3774\n",
      "Epoch [1255/2000], Eval Loss: 5.9920\n",
      "Epoch [1256/2000], Train Loss: 5.3788\n",
      "Epoch [1256/2000], Eval Loss: 5.9919\n",
      "Epoch [1257/2000], Train Loss: 5.3749\n",
      "Epoch [1257/2000], Eval Loss: 5.9973\n",
      "Epoch [1258/2000], Train Loss: 5.3791\n",
      "Epoch [1258/2000], Eval Loss: 5.9979\n",
      "Epoch [1259/2000], Train Loss: 5.3730\n",
      "Epoch [1259/2000], Eval Loss: 5.9983\n",
      "Epoch [1260/2000], Train Loss: 5.3716\n",
      "Epoch [1260/2000], Eval Loss: 5.9997\n",
      "Epoch [1261/2000], Train Loss: 5.3729\n",
      "Epoch [1261/2000], Eval Loss: 6.0019\n",
      "Epoch [1262/2000], Train Loss: 5.3697\n",
      "Epoch [1262/2000], Eval Loss: 6.0044\n",
      "Epoch [1263/2000], Train Loss: 5.3682\n",
      "Epoch [1263/2000], Eval Loss: 6.0015\n",
      "Epoch [1264/2000], Train Loss: 5.3707\n",
      "Epoch [1264/2000], Eval Loss: 6.0016\n",
      "Epoch [1265/2000], Train Loss: 5.3659\n",
      "Epoch [1265/2000], Eval Loss: 6.0076\n",
      "Epoch [1266/2000], Train Loss: 5.3684\n",
      "Epoch [1266/2000], Eval Loss: 6.0077\n",
      "Epoch [1267/2000], Train Loss: 5.3669\n",
      "Epoch [1267/2000], Eval Loss: 6.0108\n",
      "Epoch [1268/2000], Train Loss: 5.3630\n",
      "Epoch [1268/2000], Eval Loss: 6.0101\n",
      "Epoch [1269/2000], Train Loss: 5.3618\n",
      "Epoch [1269/2000], Eval Loss: 6.0101\n",
      "Epoch [1270/2000], Train Loss: 5.3605\n",
      "Epoch [1270/2000], Eval Loss: 6.0137\n",
      "Epoch [1271/2000], Train Loss: 5.3593\n",
      "Epoch [1271/2000], Eval Loss: 6.0155\n",
      "Epoch [1272/2000], Train Loss: 5.3611\n",
      "Epoch [1272/2000], Eval Loss: 6.0153\n",
      "Epoch [1273/2000], Train Loss: 5.3610\n",
      "Epoch [1273/2000], Eval Loss: 6.0119\n",
      "Epoch [1274/2000], Train Loss: 5.3566\n",
      "Epoch [1274/2000], Eval Loss: 6.0148\n",
      "Epoch [1275/2000], Train Loss: 5.3557\n",
      "Epoch [1275/2000], Eval Loss: 6.0192\n",
      "Epoch [1276/2000], Train Loss: 5.3537\n",
      "Epoch [1276/2000], Eval Loss: 6.0258\n",
      "Epoch [1277/2000], Train Loss: 5.3526\n",
      "Epoch [1277/2000], Eval Loss: 6.0222\n",
      "Epoch [1278/2000], Train Loss: 5.3547\n",
      "Epoch [1278/2000], Eval Loss: 6.0222\n",
      "Epoch [1279/2000], Train Loss: 5.3511\n",
      "Epoch [1279/2000], Eval Loss: 6.0229\n",
      "Epoch [1280/2000], Train Loss: 5.3524\n",
      "Epoch [1280/2000], Eval Loss: 6.0235\n",
      "Epoch [1281/2000], Train Loss: 5.3482\n",
      "Epoch [1281/2000], Eval Loss: 6.0296\n",
      "Epoch [1282/2000], Train Loss: 5.3486\n",
      "Epoch [1282/2000], Eval Loss: 6.0268\n",
      "Epoch [1283/2000], Train Loss: 5.3479\n",
      "Epoch [1283/2000], Eval Loss: 6.0301\n",
      "Epoch [1284/2000], Train Loss: 5.3462\n",
      "Epoch [1284/2000], Eval Loss: 6.0286\n",
      "Epoch [1285/2000], Train Loss: 5.3466\n",
      "Epoch [1285/2000], Eval Loss: 6.0305\n",
      "Epoch [1286/2000], Train Loss: 5.3424\n",
      "Epoch [1286/2000], Eval Loss: 6.0367\n",
      "Epoch [1287/2000], Train Loss: 5.3413\n",
      "Epoch [1287/2000], Eval Loss: 6.0348\n",
      "Epoch [1288/2000], Train Loss: 5.3415\n",
      "Epoch [1288/2000], Eval Loss: 6.0450\n",
      "Epoch [1289/2000], Train Loss: 5.3392\n",
      "Epoch [1289/2000], Eval Loss: 6.0343\n",
      "Epoch [1290/2000], Train Loss: 5.3403\n",
      "Epoch [1290/2000], Eval Loss: 6.0453\n",
      "Epoch [1291/2000], Train Loss: 5.3435\n",
      "Epoch [1291/2000], Eval Loss: 6.0324\n",
      "Epoch [1292/2000], Train Loss: 5.3477\n",
      "Epoch [1292/2000], Eval Loss: 6.1149\n",
      "Epoch [1293/2000], Train Loss: 5.3863\n",
      "Epoch [1293/2000], Eval Loss: 6.9751\n",
      "Epoch [1294/2000], Train Loss: 6.3325\n",
      "Epoch [1294/2000], Eval Loss: 13.4703\n",
      "Epoch [1295/2000], Train Loss: 13.2215\n",
      "Epoch [1295/2000], Eval Loss: 22.9114\n",
      "Epoch [1296/2000], Train Loss: 22.4145\n",
      "Epoch [1296/2000], Eval Loss: 51.6098\n",
      "Epoch [1297/2000], Train Loss: 51.5641\n",
      "Epoch [1297/2000], Eval Loss: 42.3071\n",
      "Epoch [1298/2000], Train Loss: 42.0874\n",
      "Epoch [1298/2000], Eval Loss: 39.7696\n",
      "Epoch [1299/2000], Train Loss: 39.3892\n",
      "Epoch [1299/2000], Eval Loss: 33.0713\n",
      "Epoch [1300/2000], Train Loss: 32.2776\n",
      "Epoch [1300/2000], Eval Loss: 33.2330\n",
      "Epoch [1301/2000], Train Loss: 32.4080\n",
      "Epoch [1301/2000], Eval Loss: 30.9410\n",
      "Epoch [1302/2000], Train Loss: 30.5684\n",
      "Epoch [1302/2000], Eval Loss: 25.4882\n",
      "Epoch [1303/2000], Train Loss: 25.1489\n",
      "Epoch [1303/2000], Eval Loss: 24.1210\n",
      "Epoch [1304/2000], Train Loss: 23.6398\n",
      "Epoch [1304/2000], Eval Loss: 22.4124\n",
      "Epoch [1305/2000], Train Loss: 22.1059\n",
      "Epoch [1305/2000], Eval Loss: 21.6391\n",
      "Epoch [1306/2000], Train Loss: 21.1652\n",
      "Epoch [1306/2000], Eval Loss: 20.7575\n",
      "Epoch [1307/2000], Train Loss: 20.2650\n",
      "Epoch [1307/2000], Eval Loss: 20.2113\n",
      "Epoch [1308/2000], Train Loss: 19.4810\n",
      "Epoch [1308/2000], Eval Loss: 19.4838\n",
      "Epoch [1309/2000], Train Loss: 18.8326\n",
      "Epoch [1309/2000], Eval Loss: 18.8493\n",
      "Epoch [1310/2000], Train Loss: 18.3480\n",
      "Epoch [1310/2000], Eval Loss: 18.3380\n",
      "Epoch [1311/2000], Train Loss: 17.9185\n",
      "Epoch [1311/2000], Eval Loss: 18.0018\n",
      "Epoch [1312/2000], Train Loss: 17.5343\n",
      "Epoch [1312/2000], Eval Loss: 17.6004\n",
      "Epoch [1313/2000], Train Loss: 17.1076\n",
      "Epoch [1313/2000], Eval Loss: 16.9884\n",
      "Epoch [1314/2000], Train Loss: 16.5066\n",
      "Epoch [1314/2000], Eval Loss: 16.3687\n",
      "Epoch [1315/2000], Train Loss: 15.8243\n",
      "Epoch [1315/2000], Eval Loss: 15.8474\n",
      "Epoch [1316/2000], Train Loss: 15.2588\n",
      "Epoch [1316/2000], Eval Loss: 15.4113\n",
      "Epoch [1317/2000], Train Loss: 14.7975\n",
      "Epoch [1317/2000], Eval Loss: 14.9751\n",
      "Epoch [1318/2000], Train Loss: 14.3144\n",
      "Epoch [1318/2000], Eval Loss: 14.5013\n",
      "Epoch [1319/2000], Train Loss: 13.7828\n",
      "Epoch [1319/2000], Eval Loss: 14.0662\n",
      "Epoch [1320/2000], Train Loss: 13.3459\n",
      "Epoch [1320/2000], Eval Loss: 13.6332\n",
      "Epoch [1321/2000], Train Loss: 12.9082\n",
      "Epoch [1321/2000], Eval Loss: 13.1735\n",
      "Epoch [1322/2000], Train Loss: 12.5159\n",
      "Epoch [1322/2000], Eval Loss: 12.7539\n",
      "Epoch [1323/2000], Train Loss: 12.1443\n",
      "Epoch [1323/2000], Eval Loss: 12.4166\n",
      "Epoch [1324/2000], Train Loss: 11.8146\n",
      "Epoch [1324/2000], Eval Loss: 12.0960\n",
      "Epoch [1325/2000], Train Loss: 11.4617\n",
      "Epoch [1325/2000], Eval Loss: 11.7019\n",
      "Epoch [1326/2000], Train Loss: 11.0852\n",
      "Epoch [1326/2000], Eval Loss: 11.3347\n",
      "Epoch [1327/2000], Train Loss: 10.7503\n",
      "Epoch [1327/2000], Eval Loss: 11.0698\n",
      "Epoch [1328/2000], Train Loss: 10.4852\n",
      "Epoch [1328/2000], Eval Loss: 10.7995\n",
      "Epoch [1329/2000], Train Loss: 10.1807\n",
      "Epoch [1329/2000], Eval Loss: 10.5683\n",
      "Epoch [1330/2000], Train Loss: 9.9214\n",
      "Epoch [1330/2000], Eval Loss: 10.3578\n",
      "Epoch [1331/2000], Train Loss: 9.6938\n",
      "Epoch [1331/2000], Eval Loss: 10.0907\n",
      "Epoch [1332/2000], Train Loss: 9.4311\n",
      "Epoch [1332/2000], Eval Loss: 9.8268\n",
      "Epoch [1333/2000], Train Loss: 9.2037\n",
      "Epoch [1333/2000], Eval Loss: 9.6286\n",
      "Epoch [1334/2000], Train Loss: 8.9905\n",
      "Epoch [1334/2000], Eval Loss: 9.4444\n",
      "Epoch [1335/2000], Train Loss: 8.7715\n",
      "Epoch [1335/2000], Eval Loss: 9.2326\n",
      "Epoch [1336/2000], Train Loss: 8.5754\n",
      "Epoch [1336/2000], Eval Loss: 9.0326\n",
      "Epoch [1337/2000], Train Loss: 8.3997\n",
      "Epoch [1337/2000], Eval Loss: 8.8255\n",
      "Epoch [1338/2000], Train Loss: 8.2076\n",
      "Epoch [1338/2000], Eval Loss: 8.6250\n",
      "Epoch [1339/2000], Train Loss: 8.0475\n",
      "Epoch [1339/2000], Eval Loss: 8.4494\n",
      "Epoch [1340/2000], Train Loss: 7.8992\n",
      "Epoch [1340/2000], Eval Loss: 8.3232\n",
      "Epoch [1341/2000], Train Loss: 7.7662\n",
      "Epoch [1341/2000], Eval Loss: 8.1791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1342/2000], Train Loss: 7.6290\n",
      "Epoch [1342/2000], Eval Loss: 8.0350\n",
      "Epoch [1343/2000], Train Loss: 7.5031\n",
      "Epoch [1343/2000], Eval Loss: 7.8982\n",
      "Epoch [1344/2000], Train Loss: 7.3844\n",
      "Epoch [1344/2000], Eval Loss: 7.7575\n",
      "Epoch [1345/2000], Train Loss: 7.2660\n",
      "Epoch [1345/2000], Eval Loss: 7.6469\n",
      "Epoch [1346/2000], Train Loss: 7.1519\n",
      "Epoch [1346/2000], Eval Loss: 7.5591\n",
      "Epoch [1347/2000], Train Loss: 7.0640\n",
      "Epoch [1347/2000], Eval Loss: 7.4507\n",
      "Epoch [1348/2000], Train Loss: 6.9724\n",
      "Epoch [1348/2000], Eval Loss: 7.3624\n",
      "Epoch [1349/2000], Train Loss: 6.8845\n",
      "Epoch [1349/2000], Eval Loss: 7.2693\n",
      "Epoch [1350/2000], Train Loss: 6.8032\n",
      "Epoch [1350/2000], Eval Loss: 7.1835\n",
      "Epoch [1351/2000], Train Loss: 6.7345\n",
      "Epoch [1351/2000], Eval Loss: 7.0938\n",
      "Epoch [1352/2000], Train Loss: 6.6684\n",
      "Epoch [1352/2000], Eval Loss: 7.0114\n",
      "Epoch [1353/2000], Train Loss: 6.6083\n",
      "Epoch [1353/2000], Eval Loss: 6.9635\n",
      "Epoch [1354/2000], Train Loss: 6.5534\n",
      "Epoch [1354/2000], Eval Loss: 6.9024\n",
      "Epoch [1355/2000], Train Loss: 6.4962\n",
      "Epoch [1355/2000], Eval Loss: 6.8538\n",
      "Epoch [1356/2000], Train Loss: 6.4449\n",
      "Epoch [1356/2000], Eval Loss: 6.8045\n",
      "Epoch [1357/2000], Train Loss: 6.4009\n",
      "Epoch [1357/2000], Eval Loss: 6.7522\n",
      "Epoch [1358/2000], Train Loss: 6.3624\n",
      "Epoch [1358/2000], Eval Loss: 6.7104\n",
      "Epoch [1359/2000], Train Loss: 6.3210\n",
      "Epoch [1359/2000], Eval Loss: 6.6630\n",
      "Epoch [1360/2000], Train Loss: 6.2825\n",
      "Epoch [1360/2000], Eval Loss: 6.6202\n",
      "Epoch [1361/2000], Train Loss: 6.2478\n",
      "Epoch [1361/2000], Eval Loss: 6.5866\n",
      "Epoch [1362/2000], Train Loss: 6.2158\n",
      "Epoch [1362/2000], Eval Loss: 6.5498\n",
      "Epoch [1363/2000], Train Loss: 6.1844\n",
      "Epoch [1363/2000], Eval Loss: 6.5201\n",
      "Epoch [1364/2000], Train Loss: 6.1554\n",
      "Epoch [1364/2000], Eval Loss: 6.4881\n",
      "Epoch [1365/2000], Train Loss: 6.1271\n",
      "Epoch [1365/2000], Eval Loss: 6.4635\n",
      "Epoch [1366/2000], Train Loss: 6.1047\n",
      "Epoch [1366/2000], Eval Loss: 6.4404\n",
      "Epoch [1367/2000], Train Loss: 6.0796\n",
      "Epoch [1367/2000], Eval Loss: 6.4166\n",
      "Epoch [1368/2000], Train Loss: 6.0586\n",
      "Epoch [1368/2000], Eval Loss: 6.3940\n",
      "Epoch [1369/2000], Train Loss: 6.0373\n",
      "Epoch [1369/2000], Eval Loss: 6.3645\n",
      "Epoch [1370/2000], Train Loss: 6.0167\n",
      "Epoch [1370/2000], Eval Loss: 6.3406\n",
      "Epoch [1371/2000], Train Loss: 5.9988\n",
      "Epoch [1371/2000], Eval Loss: 6.3224\n",
      "Epoch [1372/2000], Train Loss: 5.9810\n",
      "Epoch [1372/2000], Eval Loss: 6.3031\n",
      "Epoch [1373/2000], Train Loss: 5.9652\n",
      "Epoch [1373/2000], Eval Loss: 6.2859\n",
      "Epoch [1374/2000], Train Loss: 5.9505\n",
      "Epoch [1374/2000], Eval Loss: 6.2694\n",
      "Epoch [1375/2000], Train Loss: 5.9359\n",
      "Epoch [1375/2000], Eval Loss: 6.2526\n",
      "Epoch [1376/2000], Train Loss: 5.9221\n",
      "Epoch [1376/2000], Eval Loss: 6.2411\n",
      "Epoch [1377/2000], Train Loss: 5.9118\n",
      "Epoch [1377/2000], Eval Loss: 6.2254\n",
      "Epoch [1378/2000], Train Loss: 5.8967\n",
      "Epoch [1378/2000], Eval Loss: 6.2112\n",
      "Epoch [1379/2000], Train Loss: 5.8847\n",
      "Epoch [1379/2000], Eval Loss: 6.2000\n",
      "Epoch [1380/2000], Train Loss: 5.8760\n",
      "Epoch [1380/2000], Eval Loss: 6.1872\n",
      "Epoch [1381/2000], Train Loss: 5.8637\n",
      "Epoch [1381/2000], Eval Loss: 6.1779\n",
      "Epoch [1382/2000], Train Loss: 5.8538\n",
      "Epoch [1382/2000], Eval Loss: 6.1670\n",
      "Epoch [1383/2000], Train Loss: 5.8446\n",
      "Epoch [1383/2000], Eval Loss: 6.1563\n",
      "Epoch [1384/2000], Train Loss: 5.8352\n",
      "Epoch [1384/2000], Eval Loss: 6.1496\n",
      "Epoch [1385/2000], Train Loss: 5.8286\n",
      "Epoch [1385/2000], Eval Loss: 6.1394\n",
      "Epoch [1386/2000], Train Loss: 5.8195\n",
      "Epoch [1386/2000], Eval Loss: 6.1301\n",
      "Epoch [1387/2000], Train Loss: 5.8104\n",
      "Epoch [1387/2000], Eval Loss: 6.1207\n",
      "Epoch [1388/2000], Train Loss: 5.8028\n",
      "Epoch [1388/2000], Eval Loss: 6.1110\n",
      "Epoch [1389/2000], Train Loss: 5.7968\n",
      "Epoch [1389/2000], Eval Loss: 6.1032\n",
      "Epoch [1390/2000], Train Loss: 5.7883\n",
      "Epoch [1390/2000], Eval Loss: 6.0972\n",
      "Epoch [1391/2000], Train Loss: 5.7817\n",
      "Epoch [1391/2000], Eval Loss: 6.0887\n",
      "Epoch [1392/2000], Train Loss: 5.7754\n",
      "Epoch [1392/2000], Eval Loss: 6.0817\n",
      "Epoch [1393/2000], Train Loss: 5.7706\n",
      "Epoch [1393/2000], Eval Loss: 6.0761\n",
      "Epoch [1394/2000], Train Loss: 5.7642\n",
      "Epoch [1394/2000], Eval Loss: 6.0696\n",
      "Epoch [1395/2000], Train Loss: 5.7575\n",
      "Epoch [1395/2000], Eval Loss: 6.0642\n",
      "Epoch [1396/2000], Train Loss: 5.7524\n",
      "Epoch [1396/2000], Eval Loss: 6.0585\n",
      "Epoch [1397/2000], Train Loss: 5.7481\n",
      "Epoch [1397/2000], Eval Loss: 6.0526\n",
      "Epoch [1398/2000], Train Loss: 5.7414\n",
      "Epoch [1398/2000], Eval Loss: 6.0486\n",
      "Epoch [1399/2000], Train Loss: 5.7364\n",
      "Epoch [1399/2000], Eval Loss: 6.0445\n",
      "Epoch [1400/2000], Train Loss: 5.7324\n",
      "Epoch [1400/2000], Eval Loss: 6.0395\n",
      "Epoch [1401/2000], Train Loss: 5.7270\n",
      "Epoch [1401/2000], Eval Loss: 6.0363\n",
      "Epoch [1402/2000], Train Loss: 5.7227\n",
      "Epoch [1402/2000], Eval Loss: 6.0321\n",
      "Epoch [1403/2000], Train Loss: 5.7188\n",
      "Epoch [1403/2000], Eval Loss: 6.0269\n",
      "Epoch [1404/2000], Train Loss: 5.7145\n",
      "Epoch [1404/2000], Eval Loss: 6.0232\n",
      "Epoch [1405/2000], Train Loss: 5.7096\n",
      "Epoch [1405/2000], Eval Loss: 6.0193\n",
      "Epoch [1406/2000], Train Loss: 5.7057\n",
      "Epoch [1406/2000], Eval Loss: 6.0152\n",
      "Epoch [1407/2000], Train Loss: 5.7014\n",
      "Epoch [1407/2000], Eval Loss: 6.0122\n",
      "Epoch [1408/2000], Train Loss: 5.6980\n",
      "Epoch [1408/2000], Eval Loss: 6.0088\n",
      "Epoch [1409/2000], Train Loss: 5.6946\n",
      "Epoch [1409/2000], Eval Loss: 6.0048\n",
      "Epoch [1410/2000], Train Loss: 5.6901\n",
      "Epoch [1410/2000], Eval Loss: 6.0018\n",
      "Epoch [1411/2000], Train Loss: 5.6866\n",
      "Epoch [1411/2000], Eval Loss: 5.9990\n",
      "Epoch [1412/2000], Train Loss: 5.6838\n",
      "Epoch [1412/2000], Eval Loss: 5.9955\n",
      "Epoch [1413/2000], Train Loss: 5.6797\n",
      "Epoch [1413/2000], Eval Loss: 5.9920\n",
      "Epoch [1414/2000], Train Loss: 5.6763\n",
      "Epoch [1414/2000], Eval Loss: 5.9899\n",
      "Epoch [1415/2000], Train Loss: 5.6731\n",
      "Epoch [1415/2000], Eval Loss: 5.9878\n",
      "Epoch [1416/2000], Train Loss: 5.6701\n",
      "Epoch [1416/2000], Eval Loss: 5.9859\n",
      "Epoch [1417/2000], Train Loss: 5.6674\n",
      "Epoch [1417/2000], Eval Loss: 5.9837\n",
      "Epoch [1418/2000], Train Loss: 5.6645\n",
      "Epoch [1418/2000], Eval Loss: 5.9810\n",
      "Epoch [1419/2000], Train Loss: 5.6621\n",
      "Epoch [1419/2000], Eval Loss: 5.9783\n",
      "Epoch [1420/2000], Train Loss: 5.6589\n",
      "Epoch [1420/2000], Eval Loss: 5.9760\n",
      "Epoch [1421/2000], Train Loss: 5.6562\n",
      "Epoch [1421/2000], Eval Loss: 5.9734\n",
      "Epoch [1422/2000], Train Loss: 5.6526\n",
      "Epoch [1422/2000], Eval Loss: 5.9708\n",
      "Epoch [1423/2000], Train Loss: 5.6503\n",
      "Epoch [1423/2000], Eval Loss: 5.9690\n",
      "Epoch [1424/2000], Train Loss: 5.6480\n",
      "Epoch [1424/2000], Eval Loss: 5.9679\n",
      "Epoch [1425/2000], Train Loss: 5.6447\n",
      "Epoch [1425/2000], Eval Loss: 5.9667\n",
      "Epoch [1426/2000], Train Loss: 5.6442\n",
      "Epoch [1426/2000], Eval Loss: 5.9644\n",
      "Epoch [1427/2000], Train Loss: 5.6398\n",
      "Epoch [1427/2000], Eval Loss: 5.9625\n",
      "Epoch [1428/2000], Train Loss: 5.6384\n",
      "Epoch [1428/2000], Eval Loss: 5.9606\n",
      "Epoch [1429/2000], Train Loss: 5.6351\n",
      "Epoch [1429/2000], Eval Loss: 5.9593\n",
      "Epoch [1430/2000], Train Loss: 5.6327\n",
      "Epoch [1430/2000], Eval Loss: 5.9579\n",
      "Epoch [1431/2000], Train Loss: 5.6307\n",
      "Epoch [1431/2000], Eval Loss: 5.9555\n",
      "Epoch [1432/2000], Train Loss: 5.6280\n",
      "Epoch [1432/2000], Eval Loss: 5.9540\n",
      "Epoch [1433/2000], Train Loss: 5.6270\n",
      "Epoch [1433/2000], Eval Loss: 5.9526\n",
      "Epoch [1434/2000], Train Loss: 5.6233\n",
      "Epoch [1434/2000], Eval Loss: 5.9514\n",
      "Epoch [1435/2000], Train Loss: 5.6211\n",
      "Epoch [1435/2000], Eval Loss: 5.9498\n",
      "Epoch [1436/2000], Train Loss: 5.6200\n",
      "Epoch [1436/2000], Eval Loss: 5.9473\n",
      "Epoch [1437/2000], Train Loss: 5.6173\n",
      "Epoch [1437/2000], Eval Loss: 5.9457\n",
      "Epoch [1438/2000], Train Loss: 5.6148\n",
      "Epoch [1438/2000], Eval Loss: 5.9447\n",
      "Epoch [1439/2000], Train Loss: 5.6127\n",
      "Epoch [1439/2000], Eval Loss: 5.9439\n",
      "Epoch [1440/2000], Train Loss: 5.6122\n",
      "Epoch [1440/2000], Eval Loss: 5.9425\n",
      "Epoch [1441/2000], Train Loss: 5.6097\n",
      "Epoch [1441/2000], Eval Loss: 5.9409\n",
      "Epoch [1442/2000], Train Loss: 5.6072\n",
      "Epoch [1442/2000], Eval Loss: 5.9396\n",
      "Epoch [1443/2000], Train Loss: 5.6048\n",
      "Epoch [1443/2000], Eval Loss: 5.9380\n",
      "Epoch [1444/2000], Train Loss: 5.6032\n",
      "Epoch [1444/2000], Eval Loss: 5.9364\n",
      "Epoch [1445/2000], Train Loss: 5.6013\n",
      "Epoch [1445/2000], Eval Loss: 5.9351\n",
      "Epoch [1446/2000], Train Loss: 5.5990\n",
      "Epoch [1446/2000], Eval Loss: 5.9343\n",
      "Epoch [1447/2000], Train Loss: 5.5975\n",
      "Epoch [1447/2000], Eval Loss: 5.9338\n",
      "Epoch [1448/2000], Train Loss: 5.5962\n",
      "Epoch [1448/2000], Eval Loss: 5.9335\n",
      "Epoch [1449/2000], Train Loss: 5.5937\n",
      "Epoch [1449/2000], Eval Loss: 5.9326\n",
      "Epoch [1450/2000], Train Loss: 5.5918\n",
      "Epoch [1450/2000], Eval Loss: 5.9309\n",
      "Epoch [1451/2000], Train Loss: 5.5904\n",
      "Epoch [1451/2000], Eval Loss: 5.9298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1452/2000], Train Loss: 5.5882\n",
      "Epoch [1452/2000], Eval Loss: 5.9295\n",
      "Epoch [1453/2000], Train Loss: 5.5872\n",
      "Epoch [1453/2000], Eval Loss: 5.9291\n",
      "Epoch [1454/2000], Train Loss: 5.5850\n",
      "Epoch [1454/2000], Eval Loss: 5.9282\n",
      "Epoch [1455/2000], Train Loss: 5.5837\n",
      "Epoch [1455/2000], Eval Loss: 5.9269\n",
      "Epoch [1456/2000], Train Loss: 5.5827\n",
      "Epoch [1456/2000], Eval Loss: 5.9251\n",
      "Epoch [1457/2000], Train Loss: 5.5804\n",
      "Epoch [1457/2000], Eval Loss: 5.9239\n",
      "Epoch [1458/2000], Train Loss: 5.5785\n",
      "Epoch [1458/2000], Eval Loss: 5.9234\n",
      "Epoch [1459/2000], Train Loss: 5.5767\n",
      "Epoch [1459/2000], Eval Loss: 5.9229\n",
      "Epoch [1460/2000], Train Loss: 5.5765\n",
      "Epoch [1460/2000], Eval Loss: 5.9223\n",
      "Epoch [1461/2000], Train Loss: 5.5736\n",
      "Epoch [1461/2000], Eval Loss: 5.9223\n",
      "Epoch [1462/2000], Train Loss: 5.5719\n",
      "Epoch [1462/2000], Eval Loss: 5.9215\n",
      "Epoch [1463/2000], Train Loss: 5.5715\n",
      "Epoch [1463/2000], Eval Loss: 5.9204\n",
      "Epoch [1464/2000], Train Loss: 5.5686\n",
      "Epoch [1464/2000], Eval Loss: 5.9192\n",
      "Epoch [1465/2000], Train Loss: 5.5672\n",
      "Epoch [1465/2000], Eval Loss: 5.9186\n",
      "Epoch [1466/2000], Train Loss: 5.5671\n",
      "Epoch [1466/2000], Eval Loss: 5.9176\n",
      "Epoch [1467/2000], Train Loss: 5.5650\n",
      "Epoch [1467/2000], Eval Loss: 5.9178\n",
      "Epoch [1468/2000], Train Loss: 5.5635\n",
      "Epoch [1468/2000], Eval Loss: 5.9179\n",
      "Epoch [1469/2000], Train Loss: 5.5618\n",
      "Epoch [1469/2000], Eval Loss: 5.9172\n",
      "Epoch [1470/2000], Train Loss: 5.5599\n",
      "Epoch [1470/2000], Eval Loss: 5.9164\n",
      "Epoch [1471/2000], Train Loss: 5.5585\n",
      "Epoch [1471/2000], Eval Loss: 5.9159\n",
      "Epoch [1472/2000], Train Loss: 5.5572\n",
      "Epoch [1472/2000], Eval Loss: 5.9156\n",
      "Epoch [1473/2000], Train Loss: 5.5557\n",
      "Epoch [1473/2000], Eval Loss: 5.9141\n",
      "Epoch [1474/2000], Train Loss: 5.5552\n",
      "Epoch [1474/2000], Eval Loss: 5.9144\n",
      "Epoch [1475/2000], Train Loss: 5.5529\n",
      "Epoch [1475/2000], Eval Loss: 5.9142\n",
      "Epoch [1476/2000], Train Loss: 5.5516\n",
      "Epoch [1476/2000], Eval Loss: 5.9133\n",
      "Epoch [1477/2000], Train Loss: 5.5503\n",
      "Epoch [1477/2000], Eval Loss: 5.9127\n",
      "Epoch [1478/2000], Train Loss: 5.5485\n",
      "Epoch [1478/2000], Eval Loss: 5.9126\n",
      "Epoch [1479/2000], Train Loss: 5.5481\n",
      "Epoch [1479/2000], Eval Loss: 5.9127\n",
      "Epoch [1480/2000], Train Loss: 5.5466\n",
      "Epoch [1480/2000], Eval Loss: 5.9131\n",
      "Epoch [1481/2000], Train Loss: 5.5454\n",
      "Epoch [1481/2000], Eval Loss: 5.9135\n",
      "Epoch [1482/2000], Train Loss: 5.5440\n",
      "Epoch [1482/2000], Eval Loss: 5.9130\n",
      "Epoch [1483/2000], Train Loss: 5.5421\n",
      "Epoch [1483/2000], Eval Loss: 5.9119\n",
      "Epoch [1484/2000], Train Loss: 5.5405\n",
      "Epoch [1484/2000], Eval Loss: 5.9107\n",
      "Epoch [1485/2000], Train Loss: 5.5392\n",
      "Epoch [1485/2000], Eval Loss: 5.9103\n",
      "Epoch [1486/2000], Train Loss: 5.5392\n",
      "Epoch [1486/2000], Eval Loss: 5.9103\n",
      "Epoch [1487/2000], Train Loss: 5.5365\n",
      "Epoch [1487/2000], Eval Loss: 5.9107\n",
      "Epoch [1488/2000], Train Loss: 5.5352\n",
      "Epoch [1488/2000], Eval Loss: 5.9110\n",
      "Epoch [1489/2000], Train Loss: 5.5342\n",
      "Epoch [1489/2000], Eval Loss: 5.9112\n",
      "Epoch [1490/2000], Train Loss: 5.5333\n",
      "Epoch [1490/2000], Eval Loss: 5.9115\n",
      "Epoch [1491/2000], Train Loss: 5.5327\n",
      "Epoch [1491/2000], Eval Loss: 5.9111\n",
      "Epoch [1492/2000], Train Loss: 5.5309\n",
      "Epoch [1492/2000], Eval Loss: 5.9106\n",
      "Epoch [1493/2000], Train Loss: 5.5289\n",
      "Epoch [1493/2000], Eval Loss: 5.9098\n",
      "Epoch [1494/2000], Train Loss: 5.5293\n",
      "Epoch [1494/2000], Eval Loss: 5.9096\n",
      "Epoch [1495/2000], Train Loss: 5.5264\n",
      "Epoch [1495/2000], Eval Loss: 5.9103\n",
      "Epoch [1496/2000], Train Loss: 5.5263\n",
      "Epoch [1496/2000], Eval Loss: 5.9113\n",
      "Epoch [1497/2000], Train Loss: 5.5239\n",
      "Epoch [1497/2000], Eval Loss: 5.9113\n",
      "Epoch [1498/2000], Train Loss: 5.5247\n",
      "Epoch [1498/2000], Eval Loss: 5.9111\n",
      "Epoch [1499/2000], Train Loss: 5.5216\n",
      "Epoch [1499/2000], Eval Loss: 5.9109\n",
      "Epoch [1500/2000], Train Loss: 5.5210\n",
      "Epoch [1500/2000], Eval Loss: 5.9107\n",
      "Epoch [1501/2000], Train Loss: 5.5203\n",
      "Epoch [1501/2000], Eval Loss: 5.9103\n",
      "Epoch [1502/2000], Train Loss: 5.5185\n",
      "Epoch [1502/2000], Eval Loss: 5.9102\n",
      "Epoch [1503/2000], Train Loss: 5.5170\n",
      "Epoch [1503/2000], Eval Loss: 5.9103\n",
      "Epoch [1504/2000], Train Loss: 5.5161\n",
      "Epoch [1504/2000], Eval Loss: 5.9101\n",
      "Epoch [1505/2000], Train Loss: 5.5147\n",
      "Epoch [1505/2000], Eval Loss: 5.9107\n",
      "Epoch [1506/2000], Train Loss: 5.5132\n",
      "Epoch [1506/2000], Eval Loss: 5.9115\n",
      "Epoch [1507/2000], Train Loss: 5.5125\n",
      "Epoch [1507/2000], Eval Loss: 5.9118\n",
      "Epoch [1508/2000], Train Loss: 5.5114\n",
      "Epoch [1508/2000], Eval Loss: 5.9118\n",
      "Epoch [1509/2000], Train Loss: 5.5095\n",
      "Epoch [1509/2000], Eval Loss: 5.9119\n",
      "Epoch [1510/2000], Train Loss: 5.5086\n",
      "Epoch [1510/2000], Eval Loss: 5.9119\n",
      "Epoch [1511/2000], Train Loss: 5.5081\n",
      "Epoch [1511/2000], Eval Loss: 5.9119\n",
      "Epoch [1512/2000], Train Loss: 5.5074\n",
      "Epoch [1512/2000], Eval Loss: 5.9127\n",
      "Epoch [1513/2000], Train Loss: 5.5051\n",
      "Epoch [1513/2000], Eval Loss: 5.9130\n",
      "Epoch [1514/2000], Train Loss: 5.5037\n",
      "Epoch [1514/2000], Eval Loss: 5.9124\n",
      "Epoch [1515/2000], Train Loss: 5.5036\n",
      "Epoch [1515/2000], Eval Loss: 5.9123\n",
      "Epoch [1516/2000], Train Loss: 5.5021\n",
      "Epoch [1516/2000], Eval Loss: 5.9132\n",
      "Epoch [1517/2000], Train Loss: 5.5002\n",
      "Epoch [1517/2000], Eval Loss: 5.9138\n",
      "Epoch [1518/2000], Train Loss: 5.4996\n",
      "Epoch [1518/2000], Eval Loss: 5.9131\n",
      "Epoch [1519/2000], Train Loss: 5.4984\n",
      "Epoch [1519/2000], Eval Loss: 5.9132\n",
      "Epoch [1520/2000], Train Loss: 5.4971\n",
      "Epoch [1520/2000], Eval Loss: 5.9142\n",
      "Epoch [1521/2000], Train Loss: 5.4965\n",
      "Epoch [1521/2000], Eval Loss: 5.9155\n",
      "Epoch [1522/2000], Train Loss: 5.4952\n",
      "Epoch [1522/2000], Eval Loss: 5.9158\n",
      "Epoch [1523/2000], Train Loss: 5.4941\n",
      "Epoch [1523/2000], Eval Loss: 5.9158\n",
      "Epoch [1524/2000], Train Loss: 5.4926\n",
      "Epoch [1524/2000], Eval Loss: 5.9157\n",
      "Epoch [1525/2000], Train Loss: 5.4925\n",
      "Epoch [1525/2000], Eval Loss: 5.9159\n",
      "Epoch [1526/2000], Train Loss: 5.4903\n",
      "Epoch [1526/2000], Eval Loss: 5.9163\n",
      "Epoch [1527/2000], Train Loss: 5.4893\n",
      "Epoch [1527/2000], Eval Loss: 5.9172\n",
      "Epoch [1528/2000], Train Loss: 5.4907\n",
      "Epoch [1528/2000], Eval Loss: 5.9188\n",
      "Epoch [1529/2000], Train Loss: 5.4867\n",
      "Epoch [1529/2000], Eval Loss: 5.9195\n",
      "Epoch [1530/2000], Train Loss: 5.4858\n",
      "Epoch [1530/2000], Eval Loss: 5.9195\n",
      "Epoch [1531/2000], Train Loss: 5.4852\n",
      "Epoch [1531/2000], Eval Loss: 5.9197\n",
      "Epoch [1532/2000], Train Loss: 5.4855\n",
      "Epoch [1532/2000], Eval Loss: 5.9204\n",
      "Epoch [1533/2000], Train Loss: 5.4844\n",
      "Epoch [1533/2000], Eval Loss: 5.9220\n",
      "Epoch [1534/2000], Train Loss: 5.4833\n",
      "Epoch [1534/2000], Eval Loss: 5.9222\n",
      "Epoch [1535/2000], Train Loss: 5.4819\n",
      "Epoch [1535/2000], Eval Loss: 5.9212\n",
      "Epoch [1536/2000], Train Loss: 5.4804\n",
      "Epoch [1536/2000], Eval Loss: 5.9210\n",
      "Epoch [1537/2000], Train Loss: 5.4780\n",
      "Epoch [1537/2000], Eval Loss: 5.9219\n",
      "Epoch [1538/2000], Train Loss: 5.4769\n",
      "Epoch [1538/2000], Eval Loss: 5.9230\n",
      "Epoch [1539/2000], Train Loss: 5.4769\n",
      "Epoch [1539/2000], Eval Loss: 5.9246\n",
      "Epoch [1540/2000], Train Loss: 5.4747\n",
      "Epoch [1540/2000], Eval Loss: 5.9259\n",
      "Epoch [1541/2000], Train Loss: 5.4736\n",
      "Epoch [1541/2000], Eval Loss: 5.9258\n",
      "Epoch [1542/2000], Train Loss: 5.4724\n",
      "Epoch [1542/2000], Eval Loss: 5.9252\n",
      "Epoch [1543/2000], Train Loss: 5.4731\n",
      "Epoch [1543/2000], Eval Loss: 5.9265\n",
      "Epoch [1544/2000], Train Loss: 5.4729\n",
      "Epoch [1544/2000], Eval Loss: 5.9287\n",
      "Epoch [1545/2000], Train Loss: 5.4700\n",
      "Epoch [1545/2000], Eval Loss: 5.9288\n",
      "Epoch [1546/2000], Train Loss: 5.4685\n",
      "Epoch [1546/2000], Eval Loss: 5.9286\n",
      "Epoch [1547/2000], Train Loss: 5.4679\n",
      "Epoch [1547/2000], Eval Loss: 5.9300\n",
      "Epoch [1548/2000], Train Loss: 5.4666\n",
      "Epoch [1548/2000], Eval Loss: 5.9306\n",
      "Epoch [1549/2000], Train Loss: 5.4657\n",
      "Epoch [1549/2000], Eval Loss: 5.9305\n",
      "Epoch [1550/2000], Train Loss: 5.4647\n",
      "Epoch [1550/2000], Eval Loss: 5.9303\n",
      "Epoch [1551/2000], Train Loss: 5.4638\n",
      "Epoch [1551/2000], Eval Loss: 5.9314\n",
      "Epoch [1552/2000], Train Loss: 5.4637\n",
      "Epoch [1552/2000], Eval Loss: 5.9328\n",
      "Epoch [1553/2000], Train Loss: 5.4607\n",
      "Epoch [1553/2000], Eval Loss: 5.9330\n",
      "Epoch [1554/2000], Train Loss: 5.4596\n",
      "Epoch [1554/2000], Eval Loss: 5.9330\n",
      "Epoch [1555/2000], Train Loss: 5.4588\n",
      "Epoch [1555/2000], Eval Loss: 5.9327\n",
      "Epoch [1556/2000], Train Loss: 5.4595\n",
      "Epoch [1556/2000], Eval Loss: 5.9352\n",
      "Epoch [1557/2000], Train Loss: 5.4563\n",
      "Epoch [1557/2000], Eval Loss: 5.9379\n",
      "Epoch [1558/2000], Train Loss: 5.4579\n",
      "Epoch [1558/2000], Eval Loss: 5.9381\n",
      "Epoch [1559/2000], Train Loss: 5.4552\n",
      "Epoch [1559/2000], Eval Loss: 5.9381\n",
      "Epoch [1560/2000], Train Loss: 5.4561\n",
      "Epoch [1560/2000], Eval Loss: 5.9404\n",
      "Epoch [1561/2000], Train Loss: 5.4521\n",
      "Epoch [1561/2000], Eval Loss: 5.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1562/2000], Train Loss: 5.4512\n",
      "Epoch [1562/2000], Eval Loss: 5.9406\n",
      "Epoch [1563/2000], Train Loss: 5.4503\n",
      "Epoch [1563/2000], Eval Loss: 5.9415\n",
      "Epoch [1564/2000], Train Loss: 5.4501\n",
      "Epoch [1564/2000], Eval Loss: 5.9430\n",
      "Epoch [1565/2000], Train Loss: 5.4494\n",
      "Epoch [1565/2000], Eval Loss: 5.9447\n",
      "Epoch [1566/2000], Train Loss: 5.4469\n",
      "Epoch [1566/2000], Eval Loss: 5.9448\n",
      "Epoch [1567/2000], Train Loss: 5.4456\n",
      "Epoch [1567/2000], Eval Loss: 5.9440\n",
      "Epoch [1568/2000], Train Loss: 5.4447\n",
      "Epoch [1568/2000], Eval Loss: 5.9443\n",
      "Epoch [1569/2000], Train Loss: 5.4433\n",
      "Epoch [1569/2000], Eval Loss: 5.9466\n",
      "Epoch [1570/2000], Train Loss: 5.4447\n",
      "Epoch [1570/2000], Eval Loss: 5.9487\n",
      "Epoch [1571/2000], Train Loss: 5.4421\n",
      "Epoch [1571/2000], Eval Loss: 5.9493\n",
      "Epoch [1572/2000], Train Loss: 5.4404\n",
      "Epoch [1572/2000], Eval Loss: 5.9500\n",
      "Epoch [1573/2000], Train Loss: 5.4389\n",
      "Epoch [1573/2000], Eval Loss: 5.9513\n",
      "Epoch [1574/2000], Train Loss: 5.4390\n",
      "Epoch [1574/2000], Eval Loss: 5.9528\n",
      "Epoch [1575/2000], Train Loss: 5.4376\n",
      "Epoch [1575/2000], Eval Loss: 5.9532\n",
      "Epoch [1576/2000], Train Loss: 5.4366\n",
      "Epoch [1576/2000], Eval Loss: 5.9538\n",
      "Epoch [1577/2000], Train Loss: 5.4345\n",
      "Epoch [1577/2000], Eval Loss: 5.9541\n",
      "Epoch [1578/2000], Train Loss: 5.4338\n",
      "Epoch [1578/2000], Eval Loss: 5.9550\n",
      "Epoch [1579/2000], Train Loss: 5.4352\n",
      "Epoch [1579/2000], Eval Loss: 5.9564\n",
      "Epoch [1580/2000], Train Loss: 5.4339\n",
      "Epoch [1580/2000], Eval Loss: 5.9567\n",
      "Epoch [1581/2000], Train Loss: 5.4338\n",
      "Epoch [1581/2000], Eval Loss: 5.9567\n",
      "Epoch [1582/2000], Train Loss: 5.4301\n",
      "Epoch [1582/2000], Eval Loss: 5.9584\n",
      "Epoch [1583/2000], Train Loss: 5.4282\n",
      "Epoch [1583/2000], Eval Loss: 5.9596\n",
      "Epoch [1584/2000], Train Loss: 5.4275\n",
      "Epoch [1584/2000], Eval Loss: 5.9601\n",
      "Epoch [1585/2000], Train Loss: 5.4274\n",
      "Epoch [1585/2000], Eval Loss: 5.9617\n",
      "Epoch [1586/2000], Train Loss: 5.4255\n",
      "Epoch [1586/2000], Eval Loss: 5.9628\n",
      "Epoch [1587/2000], Train Loss: 5.4238\n",
      "Epoch [1587/2000], Eval Loss: 5.9644\n",
      "Epoch [1588/2000], Train Loss: 5.4250\n",
      "Epoch [1588/2000], Eval Loss: 5.9660\n",
      "Epoch [1589/2000], Train Loss: 5.4228\n",
      "Epoch [1589/2000], Eval Loss: 5.9655\n",
      "Epoch [1590/2000], Train Loss: 5.4223\n",
      "Epoch [1590/2000], Eval Loss: 5.9647\n",
      "Epoch [1591/2000], Train Loss: 5.4230\n",
      "Epoch [1591/2000], Eval Loss: 5.9657\n",
      "Epoch [1592/2000], Train Loss: 5.4183\n",
      "Epoch [1592/2000], Eval Loss: 5.9670\n",
      "Epoch [1593/2000], Train Loss: 5.4173\n",
      "Epoch [1593/2000], Eval Loss: 5.9670\n",
      "Epoch [1594/2000], Train Loss: 5.4165\n",
      "Epoch [1594/2000], Eval Loss: 5.9692\n",
      "Epoch [1595/2000], Train Loss: 5.4153\n",
      "Epoch [1595/2000], Eval Loss: 5.9715\n",
      "Epoch [1596/2000], Train Loss: 5.4137\n",
      "Epoch [1596/2000], Eval Loss: 5.9731\n",
      "Epoch [1597/2000], Train Loss: 5.4129\n",
      "Epoch [1597/2000], Eval Loss: 5.9744\n",
      "Epoch [1598/2000], Train Loss: 5.4118\n",
      "Epoch [1598/2000], Eval Loss: 5.9750\n",
      "Epoch [1599/2000], Train Loss: 5.4122\n",
      "Epoch [1599/2000], Eval Loss: 5.9762\n",
      "Epoch [1600/2000], Train Loss: 5.4099\n",
      "Epoch [1600/2000], Eval Loss: 5.9770\n",
      "Epoch [1601/2000], Train Loss: 5.4086\n",
      "Epoch [1601/2000], Eval Loss: 5.9772\n",
      "Epoch [1602/2000], Train Loss: 5.4082\n",
      "Epoch [1602/2000], Eval Loss: 5.9787\n",
      "Epoch [1603/2000], Train Loss: 5.4060\n",
      "Epoch [1603/2000], Eval Loss: 5.9817\n",
      "Epoch [1604/2000], Train Loss: 5.4049\n",
      "Epoch [1604/2000], Eval Loss: 5.9833\n",
      "Epoch [1605/2000], Train Loss: 5.4085\n",
      "Epoch [1605/2000], Eval Loss: 5.9833\n",
      "Epoch [1606/2000], Train Loss: 5.4072\n",
      "Epoch [1606/2000], Eval Loss: 5.9841\n",
      "Epoch [1607/2000], Train Loss: 5.4024\n",
      "Epoch [1607/2000], Eval Loss: 5.9832\n",
      "Epoch [1608/2000], Train Loss: 5.4015\n",
      "Epoch [1608/2000], Eval Loss: 5.9840\n",
      "Epoch [1609/2000], Train Loss: 5.4000\n",
      "Epoch [1609/2000], Eval Loss: 5.9874\n",
      "Epoch [1610/2000], Train Loss: 5.4028\n",
      "Epoch [1610/2000], Eval Loss: 5.9903\n",
      "Epoch [1611/2000], Train Loss: 5.4000\n",
      "Epoch [1611/2000], Eval Loss: 5.9905\n",
      "Epoch [1612/2000], Train Loss: 5.3965\n",
      "Epoch [1612/2000], Eval Loss: 5.9907\n",
      "Epoch [1613/2000], Train Loss: 5.3971\n",
      "Epoch [1613/2000], Eval Loss: 5.9918\n",
      "Epoch [1614/2000], Train Loss: 5.3983\n",
      "Epoch [1614/2000], Eval Loss: 5.9945\n",
      "Epoch [1615/2000], Train Loss: 5.3933\n",
      "Epoch [1615/2000], Eval Loss: 5.9927\n",
      "Early stopping after 1615 epochs with no improvement.\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "encoder = Encoder(input_size,32,1,1)\n",
    "decoder = Decoder(input_size,32,1,1)\n",
    "criterion = nn.KLDivLoss(reduction = \"batchmean\")\n",
    "# optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr = 0.001)\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "# scheduler_encoder = optim.lr_scheduler.StepLR(optimizer_encoder, step_size=1000, gamma=0.5)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "# scheduler_decoder = optim.lr_scheduler.StepLR(optimizer_decoder, step_size=1000, gamma=0.5)\n",
    "# optimizer_encoder = optim.SGD(encoder.parameters(), lr=0.2,momentum=0.9)\n",
    "# optimizer_decoder = optim.SGD(decoder.parameters(), lr=0.2,momentum=0.9)\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 500  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "loss_val = 0.0\n",
    "\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Loop\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "#     optimizer.zero_grad()\n",
    "    context = encoder(training_source)\n",
    "    output = decoder(context,training_target,0.2)\n",
    "    loss = criterion(output[:,:-1,:], training_target_encoded[:,1:,:])\n",
    "    loss.backward()\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "#     scheduler_encoder.step()\n",
    "#     scheduler_decoder.step()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Validation Loop\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        context_ = encoder(test_source)\n",
    "        output_ = decoder(context_,test_target,0.0)\n",
    "        loss_val = criterion(output_[:,:-1,:], test_target_encoded[:,1:,:])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Eval Loss: {loss_val.item():.4f}')\n",
    "    \n",
    "        if loss_val < best_val_loss:\n",
    "            best_val_loss = loss_val\n",
    "            epochs_since_improvement = 0\n",
    "            torch.save(encoder.state_dict(), 'Model/encoder.pth')\n",
    "            torch.save(decoder.state_dict(), 'Model/decoder.pth')\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "    # Check if we should stop training early\n",
    "    if epochs_since_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch+1} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "if loss_val < best_val_loss:\n",
    "    torch.save(encoder.state_dict(), 'Model/encoder.pth')\n",
    "    torch.save(decoder.state_dict(), 'Model/decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc8b42ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1615/2000], Eval Loss: 5.8936\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size,32,1,1)\n",
    "decoder = Decoder(input_size,32,1,1)\n",
    "encoder.load_state_dict(torch.load('Model/encoder.pth'))  \n",
    "decoder.load_state_dict(torch.load('Model/decoder.pth')) \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    context_ = encoder(test_source)\n",
    "    output_ = decoder(context_,test_target,0.0)\n",
    "    loss_val = criterion(output_[:,:-1,:], test_target_encoded[:,1:,:])\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Eval Loss: {loss_val.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3f3ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average(over batch) no. of Wrong predictions per sequence : 3.9985\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    context = encoder(test_source)\n",
    "    output = decoder(context,test_target,0.0)\n",
    "    actual = torch.argmax(test_target_encoded[:,1:,:],dim=2)\n",
    "    predictions = torch.argmax(output[:,:-1,:],dim=2)\n",
    "    wrong_pred = torch.where(predictions != actual,1.0,0.0)\n",
    "    print(f'Average(over batch) no. of Wrong predictions per sequence : {torch.sum(wrong_pred) / wrong_pred.shape[0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb7cd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how many characters match in the two strings\n",
    "def check(pred: str, true: str):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "\n",
    "    # Prediction is more than 8 letters, so penalize for every extra letter.\n",
    "    correct -= max(0, len(pred) - len(true))\n",
    "    correct = max(0, correct)\n",
    "    return correct\n",
    "\n",
    "# Function to score the model's performance\n",
    "def evaluate(encoder, decoder):\n",
    "\n",
    "    # Train data\n",
    "    print(\"Obtaining results for training data:\")\n",
    "    train_data = pd.read_csv(\"Data/train_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in train_data:\n",
    "        pred = decoder.predict(encoder.predict(x))\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Train dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    print(f\"Points: {points}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_train.csv\", index=False)\n",
    "\n",
    "    #----------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"Obtaining metrics for eval data:\")\n",
    "    eval_data = pd.read_csv(\"Data/eval_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in eval_data:\n",
    "        pred = decoder.predict(encoder.predict(x))\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Eval dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    marks = round(min(2, points / 1400 * 2) * 2) / 2  # Rounds to the nearest 0.5\n",
    "    print(f\"Points: {points}\")\n",
    "    print(f\"Marks: {marks}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_eval.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5e1e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining results for training data:\n",
      "Train dataset results:\n",
      "Number of predictions with 0 correct predictions: 12\n",
      "Number of predictions with 1 correct predictions: 96\n",
      "Number of predictions with 2 correct predictions: 397\n",
      "Number of predictions with 3 correct predictions: 1049\n",
      "Number of predictions with 4 correct predictions: 1688\n",
      "Number of predictions with 5 correct predictions: 1865\n",
      "Number of predictions with 6 correct predictions: 1336\n",
      "Number of predictions with 7 correct predictions: 479\n",
      "Number of predictions with 8 correct predictions: 78\n",
      "Points: 3669.5\n",
      "Obtaining metrics for eval data:\n",
      "Eval dataset results:\n",
      "Number of predictions with 0 correct predictions: 11\n",
      "Number of predictions with 1 correct predictions: 72\n",
      "Number of predictions with 2 correct predictions: 220\n",
      "Number of predictions with 3 correct predictions: 417\n",
      "Number of predictions with 4 correct predictions: 557\n",
      "Number of predictions with 5 correct predictions: 422\n",
      "Number of predictions with 6 correct predictions: 215\n",
      "Number of predictions with 7 correct predictions: 76\n",
      "Number of predictions with 8 correct predictions: 10\n",
      "Points: 790.5\n",
      "Marks: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58088a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
